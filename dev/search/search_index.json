{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, with an added <code>List of PRs</code> section and links to the relevant PRs on the individual updates. This project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Added configuration options to specify number of workers and maximum partition size for parallel operations. #777</li> <li>Added vast_pipeline.utils.delete_run.py to enable deletion of pipeline runs using raw SQL #775</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Small memory optimisations: Use <code>itertuples</code> in favour of <code>iterrows</code>, Loop over mappings rather than converting them to lists up-front. #776</li> <li>Updated clearpiperun to delete runs using raw SQL rather than via django #775</li> <li>Shortened forced fits measurement names to ensure they fit within the character limits - remove image prefix and limited to 1000 forced fits per source #734</li> <li>Cleaned up Code of Conduct including adding Zenodo DOI #773</li> <li>Updated changelog release instructions to remove each release having an empty \"Unreleased\" section at the start #772</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Fixed errant <code>&lt;strong&gt;</code> tag inside changelog and added verbatim formatting to other variables throughout #772</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":""},{"location":"changelog/#list-of-prs","title":"List of PRs","text":"<ul> <li>#777: feat: Allow user to specify number of cores and memory size of partitions via configuration.</li> <li>#776: fix: Minor memory optimisations</li> <li>#775: fix, feat: Enabled deletion of pipeline runs directly using SQL rather than via django</li> <li>#734: Shortened forced fits measurement names</li> <li>#773: docs: Cleaned up Code of Conduct including adding Zenodo DOI</li> <li>#772: fix, docs: Fixed changelog formatting and updated changelog release instructions</li> </ul>"},{"location":"changelog/#111-2024-10-15","title":"1.1.1 (2024-10-15)","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Added Zenodo DOI to README #761</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Refreshed docs text #766</li> <li>Updated preferred citation to Zenodo DOI #761</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Removed errant <code>&lt;strong&gt;</code> tag from docs header #766</li> </ul>"},{"location":"changelog/#list-of-prs_1","title":"List of PRs","text":"<ul> <li>#766: docs: Removed errant <code>&lt;strong&gt;</code> tag from docs header and refreshed docs text</li> <li>#761: docs: Add Zenodo DOI</li> </ul>"},{"location":"changelog/#110-2024-10-14","title":"1.1.0 (2024-10-14)","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Added further memory usage and timing debug logging #725</li> <li>Add support for python 3.10 #740</li> <li>Added support <code>calculate_n_partitions</code> for sensible dask dataframe partitioning #724</li> <li>Added support for compressed FITS files #694</li> <li>Added links to Data Central DAS and the Fink Broker to the source page #697</li> <li>Added <code>n_new_sources</code> column to run model to store the number of new sources in a pipeline run #676.</li> <li>Added <code>MAX_CUTOUT_IMAGES</code> to the pipeline settings to limit the number of postage stamps displayed on the source detail page #658.</li> <li>Added run config option to skip calculating measurement pair metrics #655.</li> <li>Added support for Python 3.10 #641.</li> <li>Added documentation versioning #627.</li> <li>Added a ZTF cone search button on the source detail page #626.</li> <li>Added the 0-based index of each measurement to the image cutout card headers #625.</li> <li>Added Bokeh hover tooltip to measurement pair graph to display pair metrics #625.</li> <li>Added new VAST surveys (13-21) to the Aladin Lite panel #622.</li> <li>Added eta-V plot analysis page along with documentation #586.</li> <li>Added thumbnails to light curve tooltips #586.</li> <li>Added logfile dropdown selection to run detail page #595.</li> <li>Added datetime stamps to all log files #595.</li> <li>Added new log files for arrow file creation and restore run and added to run detail page #580.</li> <li>Added restore run test #580.</li> <li>Added new run status of <code>DEL</code>, <code>Deleting</code> #580.</li> <li>Added documentation pages on new action buttons #580.</li> <li>Added UI action buttons to run-detail page to allow arrow file generation, deletion and restoration #580.</li> <li>Added try-except error capture on pre-run checks to correctly assign pipeline run as failed if an error occurs #576.</li> <li>Added support for ingesting Selavy catalogues in VOTable (XML) and CSV format #565</li> <li>Added new commands: <code>initingest</code> and <code>ingestimages</code> #544</li> <li>Added documentation on the data required to run the pipeline #572.</li> <li>Added support for ingesting Selavy catalogues in VOTable (XML) and CSV format #565.</li> <li>Added new commands: <code>initingest</code> and <code>ingestimages</code> #544.</li> <li>Added TNS cone search to the external search results on the source detail page #557.</li> <li>Added <code>HOME_DATA_ROOT</code> to the pipeline settings to override the OS default home directory location #559.</li> <li>Added processing spinner to source query table #551.</li> <li>Added <code>site_url</code> to the mkdocs config so static asset URLs have the correct base URL #543.</li> <li>Added basic linter to CI/CD #546</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Updated <code>README.md</code> #758</li> <li>Force dask&lt;2022.4.2, numpy&lt;1.23, param&lt;2.0 #728</li> <li>Bumped versions for github actions packages #728</li> <li>Changed pipeline.new_sources.parallel_get_rms_measurements to drop all but one RMS measurmeents #730</li> <li>Use chunks for associations upload #726</li> <li>Updated all FITS loading to use a wrapper that can handle compressed FITS files #694</li> <li>Downgrade ci-docs to python 3.8 #702</li> <li>Update Gr1N poetry to v8, force python 3.8.10 #701</li> <li>Updated path to test data in github actions and docs #699</li> <li>Changed GitHub actions test suite to install pipeline via poetry #699.</li> <li>Updated GitHub actions ubuntu version to 20.04 #699.</li> <li>Removed python 3.10 testing from GitHub actions #699.</li> <li>Updated GitHub actions Gr1N/setup-poetry to v7 #665.</li> <li>Changed lightcurve plot cursor hit-test mode from \"vline\" to \"mouse\" to avoid a regression in Bokeh #652.</li> <li>Updated Bokeh from v2.3.3 to v2.4.2 #652.</li> <li>Source query results table is no longer populated by default, a query must be submitted first #638.</li> <li>Bumped major versions of astropy (5.0) and pyarrow (7.0) #641.</li> <li>Addressed future pandas append deprecation, migrated all uses to pd.concat #643.</li> <li>Bumped all documentation dependancies to latest versions (incl. mkdocs-material minimum 8.2.4) #627.</li> <li>Changed GitHub workflows for new documentation versioning #627.</li> <li>Bumped Jinja2 to 3.0.3 to fix a Markupsafe error caused by a removed function #634.</li> <li>Dependancies updated using npm audit fix (non-breaking) #620.</li> <li>Refactored adding source to favourites button to use ajax to avoid page reload #614.</li> <li>Bumped test python versions to 3.7.12, 3.8.12 and 3.9.10 #586.</li> <li>Bumped various dependencies using a fresh poetry.lock file #586.</li> <li>Bumped bokeh packages to 2.3.3 #586.</li> <li>Django-Q config variable <code>max_attempts</code> is configurable in the .env file #595.</li> <li>Replaced <code>models.MeasurementPair</code> model with a dataclass #590.</li> <li>Django-Q config variables <code>timeout</code> and <code>retry</code> are configurable in the .env file #589.</li> <li>Changed restore run command to only allow one run as input #580.</li> <li>Changed existing documentation pages to reflect new buttons #580.</li> <li>Moved creation of output backup files to occur before the config check #576.</li> <li>Association test data updated with d2d fix #574.</li> <li>Removed the timezone from the Timestamps being written to the the arrow file as this causes problems with vaex #571.</li> <li>Reduced the memory footprint for computing the ideal source coverages by sky regions #555.</li> <li>Gulp will only read <code>webinterface/.env</code> if the required vars are undefined in the current environment #548.</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Fixed handling of NaNs and negatives in noise image statistics #755</li> <li>Optimise YAML config parsing #754</li> <li>Fixed mkdocs serving issues #728</li> <li>Fixed python 3.9 testing failing on github actions #728</li> <li>Updated github action syntax to correctly call docker compose #736</li> <li>Fix memory leak in new_sources merge #730</li> <li>Fix memory leak in model upload #726</li> <li>Implemented sensible dask dataframe partitioning #724</li> <li>Fixed outdated <code>runpipeline</code> section on CLI docs page #685.</li> <li>Fixed link to JupyterHub #676.</li> <li>Ensure Image models are not created if the catalogue ingest fails #648.</li> <li>Fixed run failures caused by attempting to force fit images with empty catalogues #653.</li> <li>Fixed a Bokeh regression that requires LabelSet values to be strings #652.</li> <li>Fixed deprecation warning on astroquery Ned import #644.</li> <li>Fixed a regression from pandas==1.4.0 that caused empty groups to be passed to an apply function in parallel association #642.</li> <li>Fixed docs issue that stopped serializers and views being shown in the code reference #627.</li> <li>Fixed broken links to external search results from NED by URI encoding source names #633.</li> <li>Fixed a regression from pandas==1.4.0 that caused empty groups to be passed to an apply function #632.</li> <li>Fixed source names to be IAU compliant #618.</li> <li>Fixed broken NED links for coordinates with many decimal places #623.</li> <li>Added an error handler for the external source queries (e.g. SIMBAD) #616.</li> <li>Stopped JS9 from changing the page titles #597.</li> <li>Fixed regression issues with pandas 1.4 #586.</li> <li>Fixed config being copied before run was confirmed to actually go ahead for existing runs #595.</li> <li>Fixed forced measurements being removed from associations during the restore run process #600.</li> <li>Fixed measurement FITS cutout bug #588.</li> <li>Fixed removal of image and sky region objects when a run is deleted #585.</li> <li>Fixed testing pandas equal deprecation warning #580.</li> <li>Fixed restore run relations issue #580.</li> <li>Fixed logic for full re-run requirement when UI run is being re-run from an error status #576.</li> <li>Fixed d2d not being carried through the advanced association process #574.</li> <li>Fixed old dictionary references in the documentation run config page #572.</li> <li>Fixed a regression from pandas=1.3.0 that caused non-numeric columns to be dropped after a groupby sum operation #567.</li> <li>Fixed permission error for regular users when trying to launch an initialised run #563.</li> <li>Fixed outdated installation link in README #543.</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>Removed sky coverage plot from homepage #676.</li> <li>Removed various counts from homepage #676.</li> <li>Removed support for Python 3.7 #641.</li> <li>Removed Measurements table page and measurements table from run detail page #636.</li> <li>Removed the unique constraint on <code>models.Measurement.name</code> #583.</li> </ul>"},{"location":"changelog/#list-of-prs_2","title":"List of PRs","text":"<ul> <li>#758: docs: Updated <code>README.md</code> </li> <li>#755: fix: Fixed handling of NaNs and negatives in noise image statistics</li> <li>#754: fix: Optimise YAML config parsing</li> <li>#725: feat: Added further memory usage and timing debug logging </li> <li>#740: feat: Add support for python 3.10</li> <li>#728: fix: Adjust package versions and fix mkdocs serve issues</li> <li>#728: fix: Adjust package versions and fix python 3.9 tests breaking on github actions</li> <li>#736: fix: Updated github action syntax to correctly call docker compose </li> <li>#730: fix: Fix memory leak in new_sources merge</li> <li>#726: fix: Fix memory leak in model upload and use chunks for associations upload</li> <li>#724: fix, feat: Implemented sensible dask dataframe partitioning</li> <li>#694: feat: Handle compressed fits files.</li> <li>#702: fix: Downgrade ci-docs to python 3.8.</li> <li>#701: fix: Update Gr1N poetry to v8, force python 3.8.10.</li> <li>#699: docs, feat: Add new regression data download URL and updates to Github Actions.</li> <li>#697: feat: Added links to Data Central DAS and the Fink Broker to the source page.</li> <li>#685: docs: Updated <code>runpipeline</code> section on CLI docs.</li> <li>#676: Removed home counts and new source count.</li> <li>#665: Update Gr1N/setup-poetry to v7.</li> <li>#658: feat: Add <code>MAX_CUTOUT_IMAGES</code> setting.</li> <li>#655: feat: Add run config option to disable measurement pairs.</li> <li>#648: fix: make Image and Measurement creation atomic together.</li> <li>#653: fix: Allow forced fitting on images with empty catalogues.</li> <li>#652: dep, fix: Bump bokeh 2.4.2.</li> <li>#644: fix: Fix astroquery Ned import deprecation.</li> <li>#638: feat: Support defer loading of dataTables data.</li> <li>#641: dep: Drop Python 3.7 and dependency refresh.</li> <li>#643: fix: Addressed pandas DataFrame.append deprecation.</li> <li>#642: fix: Fix empty groups in parallel association.</li> <li>#636: fix, doc: Remove excessive measurement tables.</li> <li>#627: dep, docs: Documentation update and versioning.</li> <li>#633: fix: URI encode NED object names.</li> <li>#632: fix: skip empty groups in new sources groupby-apply.</li> <li>#634: dep: bump Jinja2 to v3.</li> <li>#626: feat: Add a ZTF cone search link.</li> <li>#618: fix: Produce IAU compliant source names.</li> <li>#625: feat: Pair metrics hover tooltip.</li> <li>#620: dep: Non-breaking npm audit fix update.</li> <li>#622: feat: Updated Aladin VAST surveys.</li> <li>#623: fix: fixed NED links.</li> <li>#616: fix: added error handling to external queries.</li> <li>#614: feat: Refactored add to favourites button to avoid refresh.</li> <li>#597: fix: Update detail page titles.</li> <li>#586: feat, dep, doc: Add an eta-v analysis page for the source query.</li> <li>#595: fix: Add date and time stamp to log files.</li> <li>#600: fix: Fixed restore run forced measurements associations.</li> <li>#590: fix: Remove MeasurementPair model.</li> <li>#589: fix: expose django-q timeout and retry to env vars.</li> <li>#588: fix: change cutout endpoint to use measurement ID.</li> <li>#585: fix: Clean up m2m related objects when deleting a run.</li> <li>#583: fix: remove unique constraint from Measurement.name.</li> <li>#580: feat, fix, doc: Added UI run action buttons.</li> <li>#576: fix: Fixed UI re-run from errored status.</li> <li>#574: fix: Fixed d2d assignment in advanced association.</li> <li>#572: doc: Added required data page to documentation.</li> <li>#571: fix: Removed timezone from measurements arrow file time column</li> <li>#565: feat: added support for reading selavy VOTables and CSVs.</li> <li>#567: fix: fixed pandas=1.3.0 groupby sum regression.</li> <li>#563: fix: fixed launch run user permission bug.</li> <li>#544: feat: new command to ingest images without running the full pipeline.</li> <li>#557: feat: Add TNS external search for sources.</li> <li>#559: feat: added HOME_DATA_ROOT setting.</li> <li>#555: fix: compute ideal source coverage with astropy xmatch.</li> <li>#551: feat: added processing spinner to source query table.</li> <li>#550: fix: missing changelog entry</li> <li>#548: fix: only read .env if required vars are undefined.</li> <li>#546: feat, fix: remove unused imports, and added basic linter during CI/CD.</li> <li>#543: fix, doc: Fix README link and documentation 404 assets.</li> </ul>"},{"location":"changelog/#100-2021-05-21","title":"1.0.0 (2021-05-21)","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>When searching by source names, any \"VAST\" prefix on the name will be silently removed to make searching for published VAST sources easier #536.</li> <li>Added acknowledgements and help section to docs #535.</li> <li>Added <code>vast_pipeline/_version.py</code> to store the current software version and updated release documentation #532.</li> <li>Added created and last updated dates to doc pages using mkdocs-git-revision-date-localized-plugin #514.</li> <li>Added support for glob expressions when specifying input files in the run config file #504</li> <li>Added <code>DEFAULT_AUTO_FIELD</code> to <code>settings.py</code> to silence Django 3.2 warnings #507</li> <li>Added lightgallery support for all images in the documentation #494.</li> <li>Added new entries in the documentation contributing section #494.</li> <li>Added new entries in the documentation FAQ section #491.</li> <li>Added new home page for documentation #491.</li> <li>Added dark mode switch on documentation #487.</li> <li>Added .env file information to documentation #487.</li> <li>Added further epoch based association information to documentation page #487.</li> <li>Added script to auto-generate code reference documentation pages #480.</li> <li>Added code reference section to documentation #480.</li> <li>Added new pages and sections to documentation #471</li> <li>Added <code>requirements/environment.yml</code> so make it easier for Miniconda users to get the non-Python dependencies #472.</li> <li>Added <code>pyproject.toml</code> and <code>poetry.lock</code> #472.</li> <li>Added <code>init-tools/init-db.py</code> #472.</li> <li>Added image add mode run restore command 'restorepiperun' #463</li> <li>Added documentation folder and files for <code>mkdocs</code> and CI #433</li> <li>Added add image to existing run feature #443</li> <li>Added networkx to base reqiurements #460.</li> <li>Added CI/CD workflow to run tests on pull requests #446</li> <li>Added basic regression tests #425</li> <li>Added image length validation for config #425</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Changed source naming convention to <code>Jhhmmss.s(+/-)ddmmss</code> to match VAST-P1 paper (Murphy, et al. 2021) convention #536</li> <li>Updated npm packages to resolve dependabot security alert #533.</li> <li>Updated homepage text to reflect new features and documentation #534.</li> <li>Changed layout of source detail page #526.</li> <li>Updated mkdocs-material to 7.1.4 for native creation date support #518.</li> <li>Updated developing docs to specify the main development branch as dev instead of master #521.</li> <li>Updated tests to account for relation fix #510.</li> <li>All file examples in docs are now enclosed in an example admonition #494.</li> <li>Further changes to layout of documentation #494.</li> <li>Changed arrow file generation from <code>vaex</code> to <code>pyarrow</code> #503.</li> <li>Changed layout of documentation to use tabs #491.</li> <li>Dependabot: Bump y18n from 3.2.1 to 3.2.2 #482.</li> <li>Replaced run config .py format with .yaml #483.</li> <li>Changed docs VAST logo to icon format to avoid stretched appearence #487.</li> <li>Bumped Browsersync from 2.26.13 to 2.26.14 #481.</li> <li>Dependabot: Bump prismjs from 1.22.0 to 1.23.0 #469.</li> <li>Changed non-google format docstrings to google format #480.</li> <li>Changed some documentation layout and updated content #471.</li> <li>Changed the <code>vaex</code> dependency to <code>vaex-arrow</code> #472.</li> <li>Set <code>CREATE_MEASUREMENTS_ARROW_FILES = True</code> in the basic association test config #472.</li> <li>Bumped minimum Python version to 3.7.1 #472.</li> <li>Replaced npm package <code>gulp-sass</code> with <code>@mr-hope/gulp-sass</code>, a fork which drops the dependency on the deprecated <code>node-sass</code> which is difficult to install #472.</li> <li>Changed the installation documentation to instruct users to use a PostgreSQL Docker image with Q3C already installed #472.</li> <li>Changed 'cmd' flag in run pipeline to 'cli' #466.</li> <li>Changed <code>CONTRIBUTING.md</code> and <code>README.md</code> #433</li> <li>Changed forced extraction name suffix to run id rather than datetime #443</li> <li>Changed tests to run on smaller cutouts #443</li> <li>Changed particles style on login page #459.</li> <li>Dependabot: Bump ini from 1.3.5 to 1.3.8 #436</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Fixed the broken link to the image detail page on measurement detail pages #528.</li> <li>Fixed simbad and ned external search results table nan values #523.</li> <li>Fixed inaccurate total results reported by some paginators #517.</li> <li>Removed excess whitespace from coordinates that get copied to the clipboard #515</li> <li>Fixed rogue relations being created during one-to-many functions #510.</li> <li>Fixed JS9 regions so that the selected source components are always on top #508</li> <li>Fixed docstring in config.py #494.</li> <li>Fixed arrow files being generated via the website #503.</li> <li>Fixed a bug that returned all sources when performing a cone search where one of the coords = 0 #501</li> <li>Fixed the missing hover tool for lightcurve plots of non-variable sources #493</li> <li>Fixed the default Dask multiprocessing context to \"fork\" #472.</li> <li>Fixed Selavy catalogue ingest to discard the unit row before reading the data #473.</li> <li>Fixed initial job processing from the UI #466.</li> <li>Fixed links in <code>README.md</code> #464.</li> <li>Fixed basic association new sources created through relations #443</li> <li>Fixed tests running pipeline multiple times #443</li> <li>Fixed particles canvas sizing on login page #459.</li> <li>Fixed breadcrumb new line on small resolutitons #459.</li> <li>Fixed config files in tests #430</li> <li>Fixed sources table on measurement detail page #429.</li> <li>Fixed missing meta columns in parallel association #427.</li> </ul>"},{"location":"changelog/#removed_2","title":"Removed","text":"<ul> <li>Removed <code>SURVEYS_WORKING_DIR</code> from settings and env file #538.</li> <li>Removed <code>default_survey</code> from run configuration file #538.</li> <li>Removed importsurvey command and catalogue.py #538.</li> <li>Removed SurveySource, Survey and SurveySourceQuerySet models #538.</li> <li>Removed email and Slack links from docs footer #535.</li> <li>Removed bootstrap as the required version is bundled with startbootstrap-sb-admin-2 #533.</li> <li>Removed <code>docs/readme.md</code> softlink as it is no longer used #494.</li> <li>Removed <code>vaex-arrow</code> from the dependancies #503.</li> <li>Removed <code>requirements/*.txt</code> files. Development dependency management moved to Poetry #472.</li> <li>Removed <code>init-tools/init-db.sh</code> #472.</li> <li>Removed <code>INSTALL.md</code>, <code>PROFILE.md</code> and <code>static/README.md</code> #433</li> <li>Removed aplpy from base requirements #460.</li> </ul>"},{"location":"changelog/#list-of-prs_3","title":"List of PRs","text":"<ul> <li>#538 feat: Removed survey source models, commands and references.</li> <li>#536 feat: changed source naming convention.</li> <li>#535 doc: added help and acknowledgement doc page.</li> <li>#534 feat: Update homepage text.</li> <li>#532 feat, doc: Versioning.</li> <li>#533 dep: updated npm deps; removed bootstrap.</li> <li>#528 fix: fixed broken image detail link.</li> <li>#526 feat: Updated source detail page layout.</li> <li>#518 dep: Updated mkdocs-material for native creation date support.</li> <li>#523 fix: Fixed external search results table nan values.</li> <li>#521 doc: update doc related to default dev branch.</li> <li>#517 fix: pin djangorestframework-datatables to 0.5.1.</li> <li>#515 fix: remove linebreaks from coordinates.</li> <li>#514 dep: Added created and updated dates to doc pages.</li> <li>#510 fix: Fix rogue relations.</li> <li>#508 fix: Draw selected source components on top in JS9.</li> <li>#504 feat: Add glob expression support to yaml run config.</li> <li>#507 fix: set default auto field model.</li> <li>#494 doc, dep: Docs: Added lightgallery support, layout update, minor fixes and additions.</li> <li>#503 fix, dep: Change arrow file generation from vaex to pyarrow.</li> <li>#501 fix: fix broken cone search when coord = 0</li> <li>#491 doc: Updated the docs layout, home page and FAQs.</li> <li>#493 fix: Fix bokeh hover tool for lightcurve plots.</li> <li>#482 dep: Bump y18n from 3.2.1 to 3.2.2.</li> <li>#483 feat: replace run config .py files with .yaml.</li> <li>#487 doc: Minor documentation improvements.</li> <li>#481 dep: Bump Browsersync from 2.26.13 to 2.26.14.</li> <li>#469 dep: Bump prismjs from 1.22.0 to 1.23.0.</li> <li>#480 feat: Code reference documentation update.</li> <li>#471 feat: Documentation update.</li> <li>#472 feat: Simplify install.</li> <li>#473 fix: discard the selavy unit row before reading.</li> <li>#466 fix: Fixed initial job processing from the UI.</li> <li>#463 feat: Added image add mode run restore command.</li> <li>#433 doc: add documentation GitHub pages website with CI.</li> <li>#443 feat, fix: Adds the ability to add images to an existing run.</li> <li>#460 dep: Removed aplpy from base requirements.</li> <li>#446 feat: CI/CD workflow.</li> <li>#459 fix: Fix particles and breadcrumb issues on mobile.</li> <li>#436 dep: Bump ini from 1.3.5 to 1.3.8.</li> <li>#430 fix: Test config files.</li> <li>#425 feat: Basic regression tests.</li> <li>#429 fix: Fixed sources table on measurement detail page.</li> <li>#427 fix: Fixed missing meta columns in parallel association.</li> </ul>"},{"location":"changelog/#020-2020-11-30","title":"0.2.0 (2020-11-30)","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Added a check in the UI running that the job is not already running or queued #421.</li> <li>Added the deletion of all parquet and arrow files upon a re-run #421.</li> <li>Added source selection by name or ID on source query page #401.</li> <li>Added test cases #412</li> <li>Added askap-vast/forced_phot to pip requirements #408.</li> <li>Added pipeline configuration parameter, <code>SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS</code>, to filter measurement pairs before calculating aggregate metrics #407.</li> <li>Added custom 404.html and 500.html templates for error pages #415</li> <li>Added ability to export measurement_pairs.parqyet as an arrow file #393.</li> <li>Added new fields to detail pages and source and measurement tables #406.</li> <li>Added new fields to source query page (island flux ratio, min and max fluxes) #406.</li> <li>Added min, max flux values to sources and agg min island flux ratio field #406.</li> <li>Added island flux ratio column to measurements, component flux divided by total island flux (peak and int) #406.</li> <li>Added a maximum number of images for runs through the UI #404.</li> <li>Added the ability to run a pipeline run through the UI #404.</li> <li>Added <code>Queued</code> status to the list of pipeline run statuses #404.</li> <li>Added the dependancy <code>django-q</code> that enables scheduled tasks to be processed #404.</li> <li>Added source tagging #396.</li> <li>Added link to measurement table from the lightcurve to source detail page #387.</li> <li>Added 'epoch based' parameter to pipeline run detail page #387.</li> <li>Adds basic commenting functionality for sources, measurements, images, and runs #368.</li> <li>Custom CSS now processed with Sass: Bootstrap and sb-admin-2 theme are compiled into a single stylesheet #370.</li> <li>Added <code>vast_pipeline/pipeline/generators.py</code> which contains generator functions #382.</li> <li>Range and NaN check on new source analysis to match forced extraction #374.</li> <li>Added the ability for the pipeline to read in groups of images which are defined as a single <code>epoch</code> #277.</li> <li>Added the ability of the pipeline to remove duplicated measurements from an epoch #277.</li> <li>Added option to control separation measurements which are defined as a duplicate #277.</li> <li>Added the ability of the pipeline to separate images to associate into unique sky region groups #277.</li> <li>Added option to perform assocication of separate sky region groups in parallel #277.</li> <li>Added new options to webinterface pipeline run creation #277.</li> <li>Added <code>epoch_based</code> column run model #277.</li> <li>Added links to tables and postage stamps on source detail page #379.</li> <li>Updates image <code>background_path</code> from current run when not originally provided #377.</li> <li>Added csv export button to datatables on webinterface #363.</li> <li>Added support for Excel export button to datatables on webinterface (waiting on datatables buttons fix) #363.</li> <li>Added column visibility button to datatables on webinterface #363.</li> <li>Added dependancy datatables-buttons 1.6.4 #363.</li> <li>Added dependancy jszip (required for Excel export) #363.</li> <li>Adds <code>n_selavy_measurements</code> and <code>n_forced_measurements</code> to run model #362.</li> <li>Adds steps to populate new measurement count fields in pipeline run #362.</li> <li>Source order from the query is preserved on source detail view #364.</li> <li>Setting <code>HOME_DATA_DIR</code> to specify a directory relative to the user's home directory to scan for FITS and text files to use in a Run initialised with the UI #361.</li> <li>Adds a node graph to accompany the lightcurve that shows which measurement pairs exceed the default variability metric thresholds (<code>Vs &gt;= 4.3</code>, <code>|m| &gt;= 0.26</code>) #305.</li> <li>Adds the <code>MeasurementPair</code> model to store two variability metrics for each flux type: Vs, the t-statistic; and m, the modulation index. The maximum of these metrics are also added to the <code>Source</code> model for joinless queries. These metrics are calculated during the pipeline run #305.</li> <li>Adds radio buttons to change the lightcurve data points between peak and integrated fluxes #305.</li> <li>Fills out information on all webinterface detail pages #345.</li> <li>Adds frequency information the measurements and images webinterface tables. #345.</li> <li>Adds celestial plot and tables to webinterface pipeline detail page #345.</li> <li>Adds useful links to webinterface navbar #345.</li> <li>Adds tool tips to webinterface source query #345.</li> <li>Adds hash reading to webinterface source query to allow filling from URL hash parameters #345.</li> <li>Add links to number cards on webinterface #345.</li> <li>Added home icon on hover on webinterface #345.</li> <li>Added copy-to-clipboard functionality on coordinates on webinterface #345.</li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Renamed 'alert-wrapper' container to 'toast-wrapper' #419.</li> <li>Changed alerts to use the Bootstrap toasts system #419.</li> <li>Bumped some npm package versions to address dependabot security alerts #411.</li> <li>Images table on pipeline run detail page changed to order by datetime by default #417.</li> <li>Changed config argument <code>CREATE_MEASUREMENTS_ARROW_FILE</code> -&gt; <code>CREATE_MEASUREMENTS_ARROW_FILES</code> #393.</li> <li>Naming of average flux query fields to account for other min max flux fields #406.</li> <li>Expanded <code>README.md</code> to include <code>DjangoQ</code> and UI job scheduling information #404.</li> <li>Shifted alerts location to the top right #404.</li> <li>Log file card now expanded by default on pipeline run detail page #404.</li> <li>Changed user comments on source detail pages to incorporate tagging feature #396.</li> <li>Updated RACS HiPS URL in Aladin #399.</li> <li>Changed home page changelog space to welcome/help messages #387.</li> <li>The <code>comment</code> field in the Run model has been renamed to <code>description</code>. A <code>comment</code> many-to-many relationship was added to permit user comments on Run instances #368.</li> <li>Moved sb-admin-2 Bootstrap theme static assets to NPM package dependency #370.</li> <li>Refactored bulk uploading to use iterable generator objects #382.</li> <li>Updated validation of config file to check that all options are present and valid #373.</li> <li>Rewritten relation functions to improve speed #307.</li> <li>Minor changes to association to increase speed #307.</li> <li>Changes to decrease memory usage during the calculation of the ideal coverage dataframe #307.</li> <li>Updated the <code>get_src_skyregion_merged_df</code> logic to account for epochs #277.</li> <li>Updated the job creation modal layout #277.</li> <li>Bumped datatables-buttons to 1.6.5 and enabled excel export buttton #380.</li> <li>Bumped datatables to 1.10.22 #363.</li> <li>Changed <code>dom</code> layout on datatables #363.</li> <li>Changed external results table pagination buttons on source detail webinterface page pagination to include less numbers to avoid overlap #363.</li> <li>Changes measurement counts view on website to use new model parameters #362.</li> <li>Lightcurve plot now generated using Bokeh #305.</li> <li>Multiple changes to webinterface page layouts #345.</li> <li>Changes source names to the format <code>ASKAP_hhmmss.ss(+/-)ddmmss.ss</code> #345.</li> <li>Simplified webinterface navbar #345.</li> <li>Excludes sources and pipeline runs from being listed in the source query page that are not complete on the webinterface #345.</li> <li>Clarifies number of measurements on webinterface detail pages #345.</li> <li>Changed <code>N.A.</code> labels to <code>N/A</code> on the webinterface #345.</li> </ul>"},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Fixed pipeline run DB loading in command line runpipeline command #401.</li> <li>Fixed nodejs version #412</li> <li>Fixed npm start failure #412</li> <li>All queries using the 2-epoch metric <code>Vs</code> now operate on <code>abs(Vs)</code>. The original <code>Vs</code> stored in MeasurementPair objects is still signed #407.</li> <li>Changed aggregate 2-epoch metric calculation for Source objects to ensure they come from the same pair #407.</li> <li>Fixed new sources rms measurement returns when no measurements are valid #417.</li> <li>Fixed measuring rms values from selavy created NAXIS=3 FITS images #417.</li> <li>Fixed rms value calculation in non-cluster forced extractions #402.</li> <li>Increase request limit for gunicorn #398.</li> <li>Fixed max source Vs metric to being an absolute value #391.</li> <li>Fixed misalignment of lightcurve card header text and the flux type radio buttons #386.</li> <li>Fixes incorrently named GitHub <code>social-auth</code> settings variable that prevented users from logging in with GitHub #372.</li> <li>Fixes webinterface navbar overspill at small sizes #345.</li> <li>Fixes webinterface favourite source table #345.</li> </ul>"},{"location":"changelog/#removed_3","title":"Removed","text":"<ul> <li>Removed/Disabled obsolete test cases#412</li> <li>Removed <code>vast_pipeline/pipeline/forced_phot.py</code> #408.</li> <li>Removed 'selavy' from homepage measurements count label #391.</li> <li>Removed leftover <code>pipeline/plots.py</code> file #391.</li> <li>Removed <code>static/css/pipeline.css</code>, this file is now produced by compiling the Sass (<code>scss/**/*.scss</code>) files with Gulp #370.</li> <li>Removed any storage of <code>meas_dj_obj</code> or <code>src_dj_obj</code> in the pipeline #382.</li> <li>Removed <code>static/vendor/chart-js</code> package #305.</li> <li>Removed <code>static/css/collapse-box.css</code>, content moved to <code>pipeline.css</code> #345.</li> </ul>"},{"location":"changelog/#list-of-prs_4","title":"List of PRs","text":"<ul> <li>#421 feat: Delete output files on re-run &amp; UI run check.</li> <li>#401 feat: Added source selection by name or id to query page.</li> <li>#412 feat: added some unit tests.</li> <li>#419 feat: Update alerts to use toasts.</li> <li>#408 feat: use forced_phot dependency instead of copied code.</li> <li>#407 fix, model: modified 2-epoch metric calculation.</li> <li>#411 fix: updated npm deps to fix security vulnerabilities.</li> <li>#415 feat: Added custom 404 and 500 templates.</li> <li>#393 feat: Added measurement_pairs arrow export.</li> <li>#406 feat, model: Added island flux ratio columns.</li> <li>#402 fix: Fixed rms value calculation in non-cluster forced extractions.</li> <li>#404 feat, dep, model: Completed schedule pipe run.</li> <li>#396 feat: added source tagging.</li> <li>#398 fix: gunicorn request limit</li> <li>#399 fix: Updated RACS HiPS path.</li> <li>#391 fix: Vs metric fix and removed pipeline/plots.py.</li> <li>#387 feat: Minor website updates.</li> <li>#386 fix: fix lightcurve header floats.</li> <li>#368 feat: vast-candidates merger: Add user commenting</li> <li>#370 feat: moved sb-admin-2 assets to dependencies.</li> <li>#382 feat: Refactored bulk uploading of objects.</li> <li>#374 feat, fix: Bring new source checks inline with forced extraction.</li> <li>#373 fix: Check all options are valid and present in validate_cfg.</li> <li>#307 feat: Improve relation functions and general association speed ups.</li> <li>#277 feat,model: Parallel and epoch based association.</li> <li>#380 feat, dep: Enable Excel export button.</li> <li>#379 feat: Add links to source detail template.</li> <li>#377 fix: Update image bkg path when not originally provided.</li> <li>#363 feat, dep: Add export and column visibility buttons to tables.</li> <li>#362 feat, model: Added number of measurements to Run DB model.</li> <li>#364 feat: preserve source query order on detail view.</li> <li>#361 feat, fix: restrict home dir scan to specified directory.</li> <li>#372 fix: fix social auth scope setting name.</li> <li>#305 feat: 2 epoch metrics</li> <li>#345 feat, fix: Website improvements.</li> </ul>"},{"location":"changelog/#010-2020-09-27","title":"0.1.0 (2020-09-27)","text":"<p>First release of the Vast Pipeline. This was able to process 707 images (EPOCH01 to EPOCH11x) on a machine with 64 GB of RAM.</p>"},{"location":"changelog/#list-of-prs_5","title":"List of PRs","text":"<ul> <li>#347 feat: Towards first release</li> <li>#354 fix, model: Updated Band model fields to floats</li> <li>#346 fix: fix JS9 overflow in measurement detail view</li> <li>#349 dep: Bump lodash from 4.17.15 to 4.17.20</li> <li>#348 dep: Bump django from 3.0.5 to 3.0.7 in /requirements</li> <li>#344 fix: fixed aladin init for all pages</li> <li>#340 break: rename pipeline folder to vast_pipeline</li> <li>#342 fix: Hotfix - fixed parquet path on job detail view</li> <li>#336 feat: Simbad/NED async cone search</li> <li>#284 fix: Update Aladin surveys with RACS and VAST</li> <li>#333 feat: auth to GitHub org, add logging and docstring</li> <li>#325 fix, feat: fix forced extraction using Dask bags backend</li> <li>#334 doc: better migration management explanation</li> <li>#332 fix: added clean to build task, removed commented lines</li> <li>#322 fix, model: add unique to image name, remove timestamp from image folder</li> <li>#321 feat: added css and js sourcemaps</li> <li>#314 feat: query form redesign, sesame resolver, coord validator</li> <li>#318 feat: Suppress astropy warnings</li> <li>#317 fix: Forced photometry fixes for #298 and #312</li> <li>#316 fix: fix migration file 0001_initial.py</li> <li>#310 fix: Fix run detail number of measurements display</li> <li>#309 fix: Added JS9 overlay filters and changed JS9 overlay behaviour on sources and measurements</li> <li>#303 fix: Fix write config feedback and validation</li> <li>#306 feat: Add config validation checks</li> <li>#302 fix: Fix RA correction for d3 celestial</li> <li>#300 fix: increase line limit for gunicorn server</li> <li>#299 fix: fix admin \"view site\" redirect</li> <li>#294 fix: Make lightcurves start at zero</li> <li>#268 feat: Production set up with static files and command</li> <li>#291 fix: Bug fix for forced_photom cluster allow_nan</li> <li>#289 fix: Fix broken UI run creation</li> <li>#287 fix: Fix forced measurement parquet files write</li> <li>#286 fix: compile JS9 without helper option</li> <li>#285 fix: Fix removing forced parquet and clear images from piperun</li> </ul>"},{"location":"code_of_conduct/","title":"Code of Conduct","text":""},{"location":"code_of_conduct/#code-of-conduct","title":"Code Of Conduct","text":"<p>By joining the VAST collaboration you agree to adhere to the VAST Code of Conduct which is reproduced below.</p> <p>We are committed to making this collaboration productive and enjoyable for everyone, regardless of gender, sexual orientation, disability, physical appearance, body size, race, nationality or religion. We will not tolerate harassment of colleagues and students in any form. To achieve this, VAST members must endeavour to work together in a cooperative way on scientific projects that fall within the scope of VAST. In particular all members must:</p> <ul> <li>Exercise their best professional and ethical judgement and carry out their duties and functions with integrity and objectivity;</li> <li>Act fairly and reasonably, and treat colleagues and students with respect, impartiality, courtesy and sensitivity;</li> <li>Avoid conflicts of interest;</li> <li>Adhere to the VAST membership and publication policies.</li> </ul> <p>Please follow these guidelines in all your interactions (including online) within VAST:</p> <ul> <li>Behave professionally. Harassment and sexist, racist, or exclusionary comments or jokes are not appropriate. Harassment includes sustained disruption of talks or other events, inappropriate physical contact, sexual attention or innuendo, deliberate intimidation, stalking, and photography or recording of an individual without consent. It also includes offensive comments related to gender, sexual orientation, disability, physical appearance, body size, race or religion.</li> <li>All communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery is not appropriate.</li> <li>Be kind to others. Do not insult or put down other collaboration members.</li> <li>Participants asked to stop any inappropriate behaviour are expected to comply immediately. Members violating these rules may be asked to leave the collaboration at the discretion of the PIs.</li> </ul> <p>Any participant who wishes to report a violation of this policy is asked to speak, in confidence, to Tara Murphy (tara.murphy@sydney.edu.au) or David Kaplan (kaplan@uwm.edu).</p>"},{"location":"code_of_conduct/#extra-guidelines","title":"Extra Guidelines","text":"<p>We are a community based on openness, as well as friendly and didactic discussions.</p> <p>We aspire to treat everybody equally, and value their contributions.</p> <p>Decisions are made based on technical merit and consensus.</p> <p>Code is not the only way to help the project. Reviewing pull requests, answering questions to help others on mailing lists or issues, organizing and teaching tutorials, working on the website, improving the documentation, are all priceless contributions.</p> <p>We abide by the principles of openness, respect, and consideration of others of the Python Software Foundation: https://www.python.org/psf/codeofconduct/</p>"},{"location":"code_of_conduct/#acknowledgements","title":"Acknowledgements","text":"<p>If you use this software in your work please acknowledge it by citing the DOI: 10.5281/zenodo.13927015.</p> <p>All refereed publications using ASKAP data should carry the standard CSIRO acknowledgement:</p> <p>The Australian SKA Pathfinder is part of the Australia Telescope National Facility which is managed by CSIRO. Operation of ASKAP is funded by the Australian Government with support from the National Collaborative Research Infrastructure Strategy. ASKAP uses the resources of the Pawsey Supercomputing Centre. Establishment of ASKAP, the Murchison Radio-astronomy Observatory and the Pawsey Supercomputing Centre are initiatives of the Australian Government, with support from the Government of Western Australia and the Science and Industry Endowment Fund. We acknowledge the Wajarri Yamatji people as the traditional owners of the Observatory site.</p> <p>This project is supported by the University of Sydney, the Australian Research Council, and the CSIRO.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#how-should-i-acknowledge-the-vast-pipeline-in-my-work","title":"How should I acknowledge the VAST pipeline in my work?","text":"<p>Please cite the Zenodo release: 10.5281/zenodo.13927015.</p>"},{"location":"faq/#can-the-vast-pipeline-be-used-with-images-from-other-telescopes","title":"Can the VAST Pipeline be used with images from other telescopes?","text":"<p>The base answer to this question is that the pipeline has been designed specifically for ASKAPsoft and ASKAPpipeline products, so compatibility with data from other telescopes is not supported.</p> <p>However, it's important to remember that the pipeline performs no source extraction itself, instead it reads in source catalogues that is expected to be in the format of the output of the <code>Selavy</code> source extractor. As seen from the Image Ingest page, the pipeline does not use any special or out of the ordinary FITS headers when reading the images, and the only inputs required are the images, catalogues, noise images and background images - which are standard products. Hence, the real answer to this question is yes, if one of the following is performed:</p> <ul> <li>Run the <code>Selavy</code> source extractor on the images to process.</li> <li>Convert the component output from a different source extractor to match that of the <code>Selavy</code> component file.</li> </ul> <p>The pipeline was also designed in a way such that other source extractor 'translators' could be plugged into the pipeline.  So a further option is to develop new translators such that the pipeline can read in output from other source extractors.  The translators can be found in <code>vast_pipeline/surveys/translators.py</code>. Please open a discussion or issue on GitHub if you intend to give this a go!</p> <p>Bug</p> <p>In reading the code recently I have a suspicion the FITS reading code is reliant on the <code>TELESCOP</code> FITS header being equal to <code>ASKAP</code>.  This is unintentional as there is nothing special about the FITS headers being read.  Worth to check if anyone goes down this path. - Adam, March 2021.</p>"},{"location":"faq/#does-the-pipeline-support-any-other-stokes-products-such-as-stokes-v","title":"Does the pipeline support any other Stokes products such as Stokes V?","text":"<p>Currently the pipeline only supports Stokes I data.</p> <p>Users can view Stokes V HIPS maps of the RACS and VAST surveys in the Aladin Lite tool on the source detail page.</p> <p>The support of Stokes V is planned in a future update.</p>"},{"location":"faq/#can-the-pipeline-handle-multi-frequency-datasets","title":"Can the pipeline handle multi-frequency datasets?","text":"<p>Currently the pipeline does not support multi-frequency datasets. Any images that are put through in a run are assumed by the pipeline to be directly comparable to one another. For example, all variability metrics are calculated directly on the fluxes provided from the source catalogues.</p> <p>Multi-frequency support is planned in a future update.</p>"},{"location":"help_and_acknowledgements/","title":"Help and Acknowledgements","text":""},{"location":"help_and_acknowledgements/#getting-help","title":"Getting Help","text":"<p>The best way to get help with the VAST Pipeline software is to contact the development team on GitHub. If you have encountered a specific problem while using the pipeline or attempting to install any of the components, please open a new issue. If you have a more general question or an idea for a new feature, please create a new discussion thread.</p> <p>General enquiries about VAST may also be sent via email to the VAST project Principal Investigators:</p> <ul> <li>Tara Murphy</li> <li>David Kaplan</li> </ul>"},{"location":"help_and_acknowledgements/#contributors","title":"Contributors","text":""},{"location":"help_and_acknowledgements/#active","title":"Active","text":"<ul> <li>Dougal Dobie \u2013 Sydney Institute for Astronomy and OzGrav</li> <li>Tom Mauch \u2013 Sydney Informatics Hub</li> <li>Adam Stewart \u2013 Sydney Institute for Astronomy</li> </ul>"},{"location":"help_and_acknowledgements/#former","title":"Former","text":"<ul> <li>Sergio Pintaldi \u2013 Sydney Informatics Hub</li> <li>Andrew O'Brien \u2013 Department of Physics, University of Wisconsin-Milwaukee</li> <li>Tara Murphy \u2013 Sydney Institute for Astronomy</li> <li>David Kaplan \u2013 Department of Physics, University of Wisconsin-Milwaukee</li> <li>Shibli Saleheen \u2013 ADACS</li> <li>David Liptai \u2013 ADACS</li> <li>Ella Xi Wang \u2013 ADACS</li> </ul>"},{"location":"help_and_acknowledgements/#acknowledgements","title":"Acknowledgements","text":"<p>The VAST Pipeline development was supported by:</p> <ul> <li>The Australian Research Council through grants FT150100099 and DP190100561.</li> <li>The Australian Research Council Centre of Excellence for Gravitational Wave Discovery (OzGrav), project numbers CE170100004 and CE230100016.</li> <li>The Sydney Informatics Hub (SIH), a core research facility at the University of Sydney.</li> <li>Software support resources awarded under the Astronomy Data and Computing Services (ADACS) Merit Allocation Program. ADACS is funded from the Astronomy National Collaborative Research Infrastructure Strategy (NCRIS) allocation provided by the Australian Government and managed by Astronomy Australia Limited (AAL).</li> <li>NSF grant AST-1816492.</li> </ul> <p>We also acknowledge the LOFAR Transients Pipeline (TraP) (Swinbank, et al. 2015) from which various concepts and design choices have been implemented in the VAST Pipeline.</p> <p>The developers thank the creators of SB Admin 2 to make the dashboard template freely available.</p>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2020-2025 ASKAP VAST Organisation, The University of Sydney (Sydney Informatics Hub),</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"adminusage/app/","title":"Web App","text":"<p>This section describes how to run the pipeline Django web app/server.</p>"},{"location":"adminusage/app/#starting-the-pipeline-web-app","title":"Starting the Pipeline Web App","text":"<p>Make sure you installed and compiled correctly the frontend assets see guide</p> <ol> <li> <p>Start the Django development web server:</p> <pre><code>(pipeline_env)$ ./manage.py runserver\n</code></pre> </li> <li> <p>Test the webserver by pointing your browser at http://127.0.0.1:8000 or http://localhost:8000.</p> </li> </ol> <p>The webserver is independent of <code>runpipeline</code> and you can use the website while the pipeline commands are running.</p>"},{"location":"adminusage/app/#running-a-pipeline-run-via-the-web-server","title":"Running a pipeline run via the web server","text":"<p>It is possible to launch the processing of a pipeline run by using the relevant option on the pipeline run detail page. This uses <code>DjangoQ</code> to schedule and process the runs and a cluster needs to be set up in order for the runs to process:</p> <ol> <li> <p>Check the <code>Q_CLUSTER</code> options in <code>./webinterface/settings.py</code>. Refer to the DjangoQ docs if you are unsure on the meaning of any parameters.</p> </li> <li> <p>Launch the cluster using the following command, making sure you are in the pipeline environment:</p> <pre><code>(pipeline_env)$ ./manage.py qcluster\n</code></pre> </li> </ol> <p>If the pipeline is updated then the <code>qcluster</code> also needs to be be restarted. A warning that if you submit jobs before the cluster is set up, or is taken down, then these jobs will begin immediately once the cluster is back online.</p>"},{"location":"adminusage/cli/","title":"Command Line Interface (CLI)","text":"<p>This section describes the commands available to the administrators of the pipelines.</p>"},{"location":"adminusage/cli/#pipeline-usage","title":"Pipeline Usage","text":"<p>All the pipeline commands are run using the Django global <code>./manage.py &lt;command&gt;</code> interface. Therefore you need to activate the <code>Python</code> environment. You can have a look at the available commands for the pipeline app:</p> <pre><code>(pipeline_env)$ ./manage.py help\n</code></pre> <p>Output:</p> <pre><code> ...\n\n[vast_pipeline]\n  clearpiperun\n  createmeasarrow\n  debugrun\n  ingestimages\n  initingest\n  initpiperun\n  restorepiperun\n  runpipeline\n\n ...\n</code></pre> <p>There are 8 commands, described in detail below.</p>"},{"location":"adminusage/cli/#clearpiperun","title":"clearpiperun","text":"<p>Resetting a pipeline run can be done using the <code>clearpiperun</code> command. This will delete all images and related objects such as sources associated with that pipeline run. Images that have been used in other pipeline runs will not be deleted.</p> <pre><code>./manage.py clearpiperun --help\nusage: manage.py clearpiperun [-h] [--keep-parquet] [--remove-all] [--version]\n                              [-v {0,1,2,3}] [--settings SETTINGS]\n                              [--pythonpath PYTHONPATH] [--traceback]\n                              [--no-color] [--force-color] [--skip-checks]\n                              piperuns [piperuns ...]\n\nDelete a pipeline run and all related images, sources, etc. Will not delete\nobjects if they are also related to another pipeline run.\n\npositional arguments:\n  piperuns              Name or path of pipeline run(s) to delete. Pass\n                        \"clearall\" to delete all the runs.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --keep-parquet        Flag to keep the pipeline run(s) parquet files. Will\n                        also apply to arrow files if present.\n  --remove-all          Flag to remove all the content of the pipeline run(s)\n                        folder.\n  --version             show program's version number and exit\n  -v {0,1,2,3}, --verbosity {0,1,2,3}\n                        Verbosity level; 0=minimal output, 1=normal output,\n                        2=verbose output, 3=very verbose output\n  --settings SETTINGS   The Python path to a settings module, e.g.\n                        \"myproject.settings.main\". If this isn't provided, the\n                        DJANGO_SETTINGS_MODULE environment variable will be\n                        used.\n  --pythonpath PYTHONPATH\n                        A directory to add to the Python path, e.g.\n                        \"/home/djangoprojects/myproject\".\n  --traceback           Raise on CommandError exceptions\n  --no-color            Don't colorize the command output.\n  --force-color         Force colorization of the command output.\n  --skip-checks         Skip system checks.\n</code></pre> <p>Example usage:</p> <pre><code>(pipeline_env)$ ./manage.py clearpiperun path/to/my_pipe_run\n# or\n(pipeline_env)$ ./manage.py clearpiperun my_pipe_run\n</code></pre> <p>Tip</p> <p>Further information on clearing a specific run, or resetting the database, can be found in the Contributing and Developing section.</p>"},{"location":"adminusage/cli/#createmeasarrow","title":"createmeasarrow","text":"<p>This command allows for the creation of the <code>measurements.arrow</code> and <code>measurement_pairs.arrow</code> files after a run has been successfully completed. See Arrow Files for more information.</p> <p>Info</p> <p>The <code>measurement_pairs.arrow</code> file will only be created if the run was configured to calculate pair metrics.</p> <pre><code>./manage.py createmeasarrow --help\nusage: manage.py createmeasarrow [-h] [--overwrite] [--version] [-v {0,1,2,3}]\n                                 [--settings SETTINGS]\n                                 [--pythonpath PYTHONPATH] [--traceback]\n                                 [--no-color] [--force-color] [--skip-checks]\n                                 piperun\n\nCreate `measurements.arrow` and `measurement_pairs.arrow` files for a\ncompleted pipeline run.\n\npositional arguments:\n  piperun               Path or name of the pipeline run.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --overwrite           Overwrite previous 'measurements.arrow' file.\n  --version             show program's version number and exit\n  -v {0,1,2,3}, --verbosity {0,1,2,3}\n                        Verbosity level; 0=minimal output, 1=normal output,\n                        2=verbose output, 3=very verbose output\n  --settings SETTINGS   The Python path to a settings module, e.g.\n                        \"myproject.settings.main\". If this isn't provided, the\n                        DJANGO_SETTINGS_MODULE environment variable will be\n                        used.\n  --pythonpath PYTHONPATH\n                        A directory to add to the Python path, e.g.\n                        \"/home/djangoprojects/myproject\".\n  --traceback           Raise on CommandError exceptions\n  --no-color            Don't colorize the command output.\n  --force-color         Force colorization of the co\n</code></pre> <p>Example usage:</p> <pre><code>./manage.py createmeasarrow docs_example_run\n2021-03-30 10:48:40,952 createmeasarrow INFO Creating measurements arrow file for 'docs_example_run'.\n2021-03-30 10:48:40,952 utils INFO Creating measurements.arrow for run docs_example_run.\n2021-03-30 10:48:41,829 createmeasarrow INFO Creating measurement pairs arrow file for 'docs_example_run'.\n2021-03-30 10:48:41,829 utils INFO Creating measurement_pairs.arrow for run docs_example_run.\n</code></pre>"},{"location":"adminusage/cli/#debugrun","title":"debugrun","text":"<p>The <code>debugrun</code> command is used to print out a summary of the pipeline run to the terminal. A single pipeline run can be entered as an argument or <code>all</code> can be entered to print the statistics of all the pipeline runs in the database.</p> <pre><code>./manage.py debugrun --help\nusage: manage.py debugrun [-h] [--version] [-v {0,1,2,3}]\n                          [--settings SETTINGS] [--pythonpath PYTHONPATH]\n                          [--traceback] [--no-color] [--force-color]\n                          [--skip-checks]\n                          piperuns [piperuns ...]\n\nPrint out total metrics such as nr of measurements for runs\n\npositional arguments:\n  piperuns              Name or path of pipeline run(s) to debug.Pass \"all\" to\n                        print summary data of all the runs.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --version             show program's version number and exit\n  -v {0,1,2,3}, --verbosity {0,1,2,3}\n                        Verbosity level; 0=minimal output, 1=normal output,\n                        2=verbose output, 3=very verbose output\n  --settings SETTINGS   The Python path to a settings module, e.g.\n                        \"myproject.settings.main\". If this isn't provided, the\n                        DJANGO_SETTINGS_MODULE environment variable will be\n                        used.\n  --pythonpath PYTHONPATH\n                        A directory to add to the Python path, e.g.\n                        \"/home/djangoprojects/myproject\".\n  --traceback           Raise on CommandError exceptions\n  --no-color            Don't colorize the command output.\n  --force-color         Force colorization of the command output.\n  --skip-checks         Skip system checks.\n</code></pre> <p>Example usage:</p> <pre><code>./manage.py debugrun docs_example_run\n* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\nPrinting summary data of pipeline run \"docs_example_run\"\nNr of images: 14\nNr of measurements: 4312\nNr of forced measurements: 2156\nNr of sources: 557\nNr of association: 3276\n* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n</code></pre>"},{"location":"adminusage/cli/#ingestimages","title":"ingestimages","text":"<p>This command runs the first part of the pipeline only. It ingests/adds a set of images, and their measurements, to the database. It requires an image ingestion configuration file as input. A template ingest configuration file can be generated with the <code>initingest</code> command (below).</p> <pre><code>(pipeline_env)$ ./manage.py ingestimages --help\n</code></pre> <p>Output:</p> <pre><code>usage: manage.py ingestimages [-h] [--version] [-v {0,1,2,3}]\n                              [--settings SETTINGS]\n                              [--pythonpath PYTHONPATH] [--traceback]\n                              [--no-color] [--force-color]\n                              [--skip-checks]\n                              image_ingest_config\n\nIngest/add a set of images to the database\n\npositional arguments:\n  image_ingest_config   Image ingestion configuration filename/path.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --version             show program's version number and exit\n  -v {0,1,2,3}, --verbosity {0,1,2,3}\n                        Verbosity level; 0=minimal output, 1=normal\n                        output, 2=verbose output, 3=very verbose output\n  --settings SETTINGS   The Python path to a settings module, e.g.\n                        \"myproject.settings.main\". If this isn't\n                        provided, the DJANGO_SETTINGS_MODULE environment\n                        variable will be used.\n  --pythonpath PYTHONPATH\n                        A directory to add to the Python path, e.g.\n                        \"/home/djangoprojects/myproject\".\n  --traceback           Raise on CommandError exceptions\n  --no-color            Don't colorize the command output.\n  --force-color         Force colorization of the command output.\n  --skip-checks         Skip system checks.\n</code></pre> <p>General usage:</p> <pre><code>(pipeline_env)$ ./manage.py ingestimages ./ingest_config.yml\n</code></pre> <p>Output: <pre><code>2021-06-25 03:08:44,313 loading INFO Reading image epoch01.fits ...\n2021-06-25 03:08:44,348 utils INFO Adding new frequency band: 888\n2021-06-25 03:08:44,390 utils INFO Created sky region 150.001, -30.001\n2021-06-25 03:08:44,441 loading INFO Processed measurements dataframe of shape: (4, 40)\n2021-06-25 03:08:44,452 loading INFO Bulk created #4 Measurement\n2021-06-25 03:08:44,504 loading INFO Reading image epoch02.fits ...\n...\n2021-06-25 03:08:44,731 loading INFO Reading image epoch04.fits ...\n2021-06-25 03:08:44,771 utils INFO Created sky region 150.021, -30.017\n2021-06-25 03:08:44,805 loading INFO Processed measurements dataframe of shape: (5, 40)\n2021-06-25 03:08:44,810 loading INFO Bulk created #5 Measurement\n2021-06-25 03:08:44,819 loading INFO Total images upload/loading time: 0.97 seconds\n</code></pre></p>"},{"location":"adminusage/cli/#initingest","title":"initingest","text":"<p>This command generates a template configuration file for use with the <code>ingestimages</code> command.</p> <pre><code>(pipeline_env)$ ./manage.py initingest --help\n</code></pre> <p>Output:</p> <pre><code>usage: manage.py initingest [-h] [--version] [-v {0,1,2,3}]\n                            [--settings SETTINGS]\n                            [--pythonpath PYTHONPATH] [--traceback]\n                            [--no-color] [--force-color] [--skip-checks]\n                            config_file_name\n\nCreate a template image ingestion configuration file\n\npositional arguments:\n  config_file_name      Filename to write template ingest configuration to.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --version             show program's version number and exit\n  -v {0,1,2,3}, --verbosity {0,1,2,3}\n                        Verbosity level; 0=minimal output, 1=normal\n                        output, 2=verbose output, 3=very verbose output\n  --settings SETTINGS   The Python path to a settings module, e.g.\n                        \"myproject.settings.main\". If this isn't\n                        provided, the DJANGO_SETTINGS_MODULE environment\n                        variable will be used.\n  --pythonpath PYTHONPATH\n                        A directory to add to the Python path, e.g.\n                        \"/home/djangoprojects/myproject\".\n  --traceback           Raise on CommandError exceptions\n  --no-color            Don't colorize the command output.\n  --force-color         Force colorization of the command output.\n  --skip-checks         Skip system checks.\n</code></pre> <p>General usage:</p> <pre><code>(pipeline_env)$ ./manage.py initingest ingest_config.yml\n</code></pre> <p>Output: <pre><code>Writing template to:  ingest_config.yml\n</code></pre></p> <p>Then modify <code>ingest_config.yml</code> to suit your needs.</p>"},{"location":"adminusage/cli/#initpiperun","title":"initpiperun","text":"<p>In order to process the images in the pipeline, you must create/initialise a pipeline run first.</p> <p>The pipeline run creation is done using the <code>initpiperun</code> django command, which requires a pipeline run folder. The command creates a folder with the pipeline run name under the settings <code>PROJECT_WORKING_DIR</code> defined in settings.</p> <pre><code>(pipeline_env)$ ./manage.py initpiperun --help\n</code></pre> <p>Output:</p> <pre><code>usage: manage.py initpiperun [-h] [--version] [-v {0,1,2,3}]\n                             [--settings SETTINGS] [--pythonpath PYTHONPATH]\n                             [--traceback] [--no-color] [--force-color]\n                             runname\n\nCreate the pipeline run folder structure to run a pipeline instance\n\npositional arguments:\n  runname       Name of the pipeline run.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --version             show program's version number and exit\n  -v {0,1,2,3}, --verbosity {0,1,2,3}\n                        Verbosity level; 0=minimal output, 1=normal output,\n                        2=verbose output, 3=very verbose output\n  --settings SETTINGS   The Python path to a settings module, e.g.\n                        \"myproject.settings.main\". If this isn't provided, the\n                        DJANGO_SETTINGS_MODULE environment variable will be\n                        used.\n  --pythonpath PYTHONPATH\n                        A directory to add to the Python path, e.g.\n                        \"/home/djangoprojects/myproject\".\n  --traceback           Raise on CommandError exceptions\n  --no-color            Don't colorize the command output.\n  --force-color         Force colorization of the command output.\n</code></pre> <p>The command yields the following folder structure:</p> <pre><code>(pipeline_env)$ ./manage.py initpiperun my_pipe_run\n</code></pre> <p>Output:</p> <pre><code>2020-02-27 23:04:33,344 initpiperun INFO creating pipeline run folder\n2020-02-27 23:04:33,344 initpiperun INFO copying default config in pipeline run folder\n2020-02-27 23:04:33,344 initpiperun INFO pipeline run initialisation successful! Please modify the \"config.yaml\"\n</code></pre>"},{"location":"adminusage/cli/#restorepiperun","title":"restorepiperun","text":"<p>Details on the add images feature can be found here.</p> <p>It allows for a pipeline run that has had an image added to the run to be restored to the state it was in before the image addition was made. By default the command will ask for confirmation that the run is to be restored (the option <code>--no-confirm</code> skips this).</p> <pre><code>./manage.py restorepiperun --help\nusage: manage.py restorepiperun [-h] [--no-confirm] [--version] [-v {0,1,2,3}]\n                                [--settings SETTINGS]\n                                [--pythonpath PYTHONPATH] [--traceback]\n                                [--no-color] [--force-color] [--skip-checks]\n                                piperuns [piperuns ...]\n\nRestore a pipeline run to the previous person after image add mode has been\nused.\n\npositional arguments:\n  piperuns              Name or path of pipeline run(s) to restore.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --no-confirm          Flag to skip the confirmation stage and proceed to\n                        restore the pipeline run.\n  --version             show program's version number and exit\n  -v {0,1,2,3}, --verbosity {0,1,2,3}\n                        Verbosity level; 0=minimal output, 1=normal output,\n                        2=verbose output, 3=very verbose output\n  --settings SETTINGS   The Python path to a settings module, e.g.\n                        \"myproject.settings.main\". If this isn't provided, the\n                        DJANGO_SETTINGS_MODULE environment variable will be\n                        used.\n  --pythonpath PYTHONPATH\n                        A directory to add to the Python path, e.g.\n                        \"/home/djangoprojects/myproject\".\n  --traceback           Raise on CommandError exceptions\n  --no-color            Don't colorize the command output.\n  --force-color         Force colorization of the command output.\n  --skip-checks         Skip system checks.\n</code></pre> <pre><code>(pipeline_env)$ ./manage.py restorepiperun path/to/my_pipe_run\n# or\n(pipeline_env)$ ./manage.py restorepiperun my_pipe_run\n</code></pre> <p>Example usage:</p> <pre><code>(pipeline_env)$ ./manage.py restorepiperun docs_example_run\n2021-04-02 21:24:20,497 restorepiperun INFO Will restore the run to the following config:\nrun:\n  path: /Users/obrienan/sandbox/vast-pipeline-dirs/pipeline-runs/docs_example_run\n  suppress_astropy_warnings: yes\ninputs:\n  image:\n    1:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout.fits\n    2:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout.fits\n    3:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout.fits\n    4:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout.fits\n    5:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout.fits\n    6:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout.fits\n    7:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout.fits\n  selavy:\n    1:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout.components.txt\n    2:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout.components.txt\n    3:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout.components.txt\n    4:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout.components.txt\n    5:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout.components.txt\n    6:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout.components.txt\n    7:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout.components.txt\n  noise:\n    1:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout_rms.fits\n    2:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout_rms.fits\n    3:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout_rms.fits\n    4:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout_rms.fits\n    5:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout_rms.fits\n    6:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout_rms.fits\n    7:\n    - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout_rms.fits\nsource_monitoring:\n  monitor: no\n  min_sigma: 3.0\n  edge_buffer_scale: 1.2\n  cluster_threshold: 3.0\n  allow_nan: no\nsource_association:\n  method: basic\n  radius: 10.0\n  deruiter_radius: 5.68\n  deruiter_beamwidth_limit: 1.5\n  parallel: no\n  epoch_duplicate_radius: 2.5\nnew_sources:\n  min_sigma: 5.0\nmeasurements:\n  source_finder: selavy\n  flux_fractional_error: 0.0\n  condon_errors: yes\n  selavy_local_rms_fill_value: 0.2\n  write_arrow_files: no\n  ra_uncertainty: 1.0\n  dec_uncertainty: 1.0\nvariability:\n  source_aggregate_pair_metrics_min_abs_vs: 4.3\n\nWould you like to restore the run ? (y/n): y\n2021-04-02 21:24:28,685 restorepiperun INFO Restoring 'docs_example_run' from backup parquet files.\n2021-04-02 21:24:29,602 restorepiperun INFO Deleting new sources and associated objects to restore run Total objects deleted: 433\n2021-04-02 21:24:29,624 restorepiperun INFO Restoring metrics for 461 sources.\n2021-04-02 21:24:29,663 restorepiperun INFO Removing 7 images from the run.\n2021-04-02 21:24:29,754 restorepiperun INFO Deleting associations to restore run. Total objects deleted: 846\n2021-04-02 21:24:29,979 restorepiperun INFO Deleting measurement pairs to restore run. Total objects deleted: 4212\n2021-04-02 21:24:29,981 restorepiperun INFO Restoring run metrics.\n2021-04-02 21:24:29,990 restorepiperun INFO Restoring parquet files and removing .bak files.\n2021-04-02 21:24:29,995 restorepiperun INFO Restore complete.\n</code></pre>"},{"location":"adminusage/cli/#runpipeline","title":"runpipeline","text":"<p>The pipeline is run using <code>runpipeline</code> django command.</p> <p>The <code>--full-rerun</code> option allows for the requested pipeline run to be cleared prior to processing so a fresh run is performed.</p> <p>Warning</p> <p>Using <code>--full-rerun</code> cannot be undone and all prior results will be deleted, including any source comments associated with the pipeline run. Use with caution.</p> <pre><code>(pipeline_env)$ ./manage.py runpipeline --help\n</code></pre> <p>Output:</p> <pre><code>usage: manage.py runpipeline [-h] [--full-rerun] [--version] [-v {0,1,2,3}]\n                             [--settings SETTINGS] [--pythonpath PYTHONPATH]\n                             [--traceback] [--no-color] [--force-color] [--skip-checks]\n                             piperun\n\nProcess the pipeline for a list of images and Selavy catalogs\n\npositional arguments:\n  piperun               Path or name of the pipeline run.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --full-rerun          Flag to signify that a full re-run is requested. Old data is\n                        completely removed and replaced.\n  --version             show program's version number and exit\n  -v {0,1,2,3}, --verbosity {0,1,2,3}\n                        Verbosity level; 0=minimal output, 1=normal output, 2=verbose\n                        output, 3=very verbose output\n  --settings SETTINGS   The Python path to a settings module, e.g.\n                        \"myproject.settings.main\". If this isn't provided, the\n                        DJANGO_SETTINGS_MODULE environment variable will be used.\n  --pythonpath PYTHONPATH\n                        A directory to add to the Python path, e.g.\n                        \"/home/djangoprojects/myproject\".\n  --traceback           Raise on CommandError exceptions\n  --no-color            Don't colorize the command output.\n  --force-color         Force colorization of the command output.\n  --skip-checks         Skip system checks.\n</code></pre> <p>General usage:</p> <pre><code>(pipeline_env)$ ./manage.py runpipeline path/to/my_pipe_run\n</code></pre>"},{"location":"adminusage/ingestimages/","title":"Standalone Image Ingestion","text":"<p>Administrators have the added option of ingesting new images via the command line, without running the full pipeline.</p> <p>Note</p> <p>Currently, when non-admin users run the pipeline, any images listed in the configuration that do not exist in the database will be processed and uploaded to the database. Hence, it is not a requirement to ingest images using this process before a run is processed that uses new images.</p> <p>The relevant command is <code>ingestimages</code>, which requires and ingest configuration file as input. The configuration file must contain the images to be listed, as well as a subset of options from the main run configuration file.</p> <p>In particular, along with the images to be ingested, the configuration requires the following options:</p> <p><code>measurements.condon_errors</code> Boolean. Calculate the Condon errors of the extractions when read in from the source extraction file. If <code>False</code> then the errors directly from the source finder output are used. Recommended to set to <code>True</code> for selavy extractions. Defaults to <code>True</code>.</p> <p><code>measurements.selavy_local_rms_fill_value</code> Float. Value to substitute for the <code>local_rms</code> parameter in selavy extractions if a <code>0.0</code> value is found. Unit is mJy. Defaults to <code>0.2</code>.</p> <p><code>measurements.ra_uncertainty</code> Float. Defines an uncertainty error to the RA that will be added in quadrature to the existing source extraction error. Used to represent a systematic positional error. Unit is arcseconds. Defaults to 1.0.</p> <p><code>measurements.dec_uncertainty</code> Float. Defines an uncertainty error to the Dec that will be added in quadrature to the existing source extraction error. Used to represent systematic positional error. Unit is arcseconds. Defaults to 1.0.</p> <p>ingest_config.yaml</p> <pre><code># This file is used for ingest only mode\n# It specifies the images to be processed and the relevant\n# settings to be used.\n\ninputs:\n    # NOTE: all the inputs must match with each other, i.e. the catalogue for the first\n    # input image (inputs.image[0]) must be the first input catalogue (inputs.selavy[0])\n    # and so on.\n    image:\n    # list input images here, e.g. (note the leading hyphens)\n        glob: ./data/epoch0?.fits\n\n    selavy:\n    # list input selavy catalogues here, as above with the images\n        glob: ./data/epoch0?.selavy.components.txt\n\n    noise:\n    # list input noise (rms) images here, as above with the images\n        glob: ./data/epoch0?.noiseMap.fits\n\n    # Required only if source_monitoring.monitor is true, otherwise optional. If not providing\n    # background images, remove the entire background section below.\n    # background:\n    # list input background images here, as above with the images\n\nmeasurements:\n    # Replace the selavy errors with Condon (1997) errors.\n    condon_errors: True\n\n    # Sometimes the local rms for a source is reported as 0 by selavy.\n    # Choose a value to use for the local rms in these cases in mJy/beam.\n    selavy_local_rms_fill_value: 0.2\n\n    # The positional uncertainty of a measurement is in reality the fitting errors and the\n    # astrometric uncertainty of the image/survey/instrument combined in quadrature.\n    # These two parameters are the astrometric uncertainty in RA/Dec and they may be different.\n    ra_uncertainty: 1 # arcsec\n    dec_uncertainty: 1  # arcsec\n</code></pre>"},{"location":"architecture/database/","title":"Database Schema","text":"<p>This section describes the relationships between the objects/tables stored in the database.</p>"},{"location":"architecture/database/#django-web-app-schema","title":"Django Web App Schema","text":"<p>The following figure shows a detailed schematics of the schema and relationships as well as tables parameters of the Django App.</p> <p></p>"},{"location":"architecture/database/#pipeline-detailed-schema","title":"Pipeline Detailed Schema","text":"<p>A focussed view of the pipeline schema is shown below:</p> <p></p>"},{"location":"architecture/database/#important-points","title":"Important points","text":"<p>Some of the key points of the above relationship diagram are:</p> <ul> <li>each image object is indipendent from the others and can belong to multiple pipeline runs to avoid duplication. An image can belong to multiple pipeline run objects and a run object can have multiple images. If a user want to upload an image object with different characteristic (i.e. using a custom source extraction tool), is free to do so but the image name need to be unique. So we suggest to assign a custom name to your image files.</li> <li>Each image is linked to a set of source measurement objects by means of a foreign key. Therefore those objects can belong to multiple source objects. A source object can have multiple measurements and a measurements can belong to multiple source objects.</li> <li>The pipeline schema has been mainly designed to allow for completely disjoint run objects so that each users can run their own processing with their specific settings, defined in the configuration file.</li> </ul>"},{"location":"architecture/intro/","title":"VAST Pipeline Architecture","text":"<p>The pipeline is essentially a Django app in which the pipeline is run as a Django admin command. The main structure of the pipeline is described in this schematics:</p> <p></p>"},{"location":"architecture/intro/#design-philosophy","title":"Design Philosophy","text":"<p>We design the pipeline in order to make it easy to use but at the same time powerful and fast. We decided to use the familiar Pandas Dataframe structure to wrap all the data manipulations, including the association operations, in the back-end. The Python community, as well as the research and scientific communities (including the astro-physicists) are very familiar with Pandas, and they should be able to understand, mantain and develop the code base.</p> <p>Usually in the \"Big Data\" world the commond tools adopted by the industry and research are Apache Hadoop and Spark. We decided to use Dask which is similar to Spark in same ways, but it integrates well with Pandas Dataframe and its syntax is quite similar to Pandas. Further it provides scalability by means of clustering and integrating with HPC (High Performance Comptuing) stacks.</p> <p>The pipeline code itself and the web app are integrated into one code base, for the sake of simplicity, easy to develop using one central repository. The user can still run the pipeline via CLI (Command Line Interface), using Django Admin Commands, as well as thorugh the web app itself. The integration avoid duplication in code, especially on regards the declaration of the schema in the ORM (Object Relational Mapping), and add user and permission management on the underlyng data, through the in-built functionality of Django framework.</p> <p>The front-end is built in simple HTML, CSS and Javascript using a freely available Bootstrap 4 template. The developers know best practices in the web development are focusing mostly on single page applications using framework such as ReactJS and AngularJS. The choice of using just the basic web stack (HTML + CSS + JS) was driven by the fact that future developers do not need to learn modern web frameworks such as React and Angular, but the fundamental web programming which is still the core of those tools.</p>"},{"location":"architecture/intro/#technology-stack","title":"Technology Stack","text":""},{"location":"architecture/intro/#back-end","title":"Back-End","text":"<ul> <li>Astropy 4+</li> <li>Astroquery 0.4+</li> <li>Bokeh 2+</li> <li>Dask 2+</li> <li>Django 3+</li> <li>Django Rest Framework</li> <li>Rest Framework Datatables</li> <li>Django Q</li> <li>Python Social Auth - Django</li> <li>Django Crispy Forms</li> <li>Django Tagulous</li> <li>Pandas 1+</li> <li>Python 3.8+</li> <li>Pyarrow 0.17+</li> <li>Postgres 10+</li> <li>Q3C</li> </ul>"},{"location":"architecture/intro/#front-end","title":"Front-End","text":"<ul> <li>Aladin Lite</li> <li>Bokeh</li> <li>Bootstrap 4</li> <li>DataTables</li> <li>D3 Celestial</li> <li>Jquery</li> <li>JS9</li> <li>ParticleJS</li> <li>PrismJS</li> <li>SB Admin 2 template</li> </ul>"},{"location":"architecture/intro/#additional","title":"Additional","text":"<ul> <li>Docker</li> <li>node 12+</li> <li>npm 6+</li> <li>gulp 4+</li> <li>GitHub Actions</li> </ul>"},{"location":"design/association/","title":"Source Association","text":"<p>This page details the association stage of a pipeline run.</p> <p>There are three association methods available which are summarised in the table below, and detailed in the following sections.</p> <p>Tip</p> <p>For complex fields and large surveys the <code>De Ruiter</code> method is recommended.</p> Method Fixed Assoc. Radius Astropy function Possible Relation Types Basic Yes <code>match_coordinates_sky</code> one-to-many Advanced Yes <code>search_around_sky</code> many-to-many, many-to-one, one-to-many de Ruiter (TraP) No <code>search_around_sky</code> many-to-many, many-to-one, one-to-many"},{"location":"design/association/#general-association-notes","title":"General Association Notes","text":""},{"location":"design/association/#terminology","title":"Terminology","text":"<p>During association, <code>measurements</code> are associated into unique <code>sources</code>.</p>"},{"location":"design/association/#association-process","title":"Association Process","text":"<p>By default, association is performed on an image-by-image basis, ordered by the observational date. The only time this isn't the case is when Epoch Based Association is used.</p> <p>Note</p> <p>Epoch Based Association is not an association method, rather it changes how the measurements are handled when passed to one of the three methods for association.</p>"},{"location":"design/association/#weighted-average-coordinates","title":"Weighted Average Coordinates","text":"<p>After every iteration of each association method, the average RA and Dec, weighted by the positional uncertainty, are calculated for each source. These weighted averages are then used as the base catalogue for the next association iteration. In other words, as the measurements are associated, new measurements are associated against the weighted average of the sources identified to that point in the process.</p> <p>Sources positions are reported using the weighted averages.</p>"},{"location":"design/association/#association-methods","title":"Association Methods","text":"<p>Tip</p> <p>For a better understanding on the underlying process, see this page in the astropy documentation for examples on matching catalogues.</p>"},{"location":"design/association/#basic","title":"Basic","text":"<p>The most basic association method uses the astropy <code>match_coordinates_sky</code> function which:</p> <ul> <li>Associates measurements using only the nearest neighbour for each source when comparing catalogues.</li> <li>Uses a fixed association radius as a threshold for a 'match'.</li> <li>Only one-to-many relations are possible.</li> </ul>"},{"location":"design/association/#advanced","title":"Advanced","text":"<p>This method uses the same process as <code>Basic</code>, however the astropy function <code>search_around_sky</code> is used instead. This means:</p> <ul> <li>All possible matches between the two catalogues are found, rather than only the nearest neighbour.</li> <li>A fixed association radius is still applied as the threshold.</li> <li>All types of relations are possible.</li> </ul>"},{"location":"design/association/#de-ruiter","title":"de Ruiter","text":"<p>The de Ruiter method is a translation of the association method used by the LOFAR Transients Pipeline (TraP), which uses the <code>de Ruiter radius</code> in order to define associations.</p> <p>The <code>search_around_sky</code> astropy method is still used, but the threshold for a potential match is first limited by a <code>beamwidth limit</code> value which is defined in the pipeline run configuration file (<code>source_association.deruiter_beamwidth_limit</code>), such that the initial threshold separation distance is set to</p> \\[ \\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,img}}}{2}, \\] <p>where \\(\\theta_{\\text{bmaj,img}}\\) is the major axis of the restoring beam of the image being associated. Then, the de Ruiter radius is calculated for all potential matches which is defined as</p> \\[ r_{i,j} = \\sqrt{   \\frac{ (\\alpha_{i} - \\alpha_{j})^{2}((\\delta_{i} + \\delta_{j})/2)}{\\sigma^{2}_{\\alpha_{i}} + \\sigma^{2}_{\\alpha_{j}}}   \\\\+ \\frac{(\\delta_{i} + \\delta_{j})^{2}}{\\sigma^{2}_{\\delta_{i}} + \\sigma^{2}_{\\delta_{j}}} } \\] <p>where \\(\\alpha_{n}\\) is the right ascension of source n, \\(\\delta_{n}\\) is its declination, and \\(\\sigma_{y}\\) represents the error on the quantity y. Matches are then identified by applying a threshold maximum value to the de Ruiter radius which is defined by the user in the pipeline run configuration file (<code>source_association.deruiter_radius</code>).</p> <p>All relation types are possible using this method.</p> <p>Epoch Based Association Note</p> <p>When the de Ruiter association method is used with epoch based assocation, the <code>beamwidth limit</code> is applied to the maximum <code>bmaj</code> value out of all the images included in the epoch. See the de Ruiter and Epoch Based Association section below for further details.</p>"},{"location":"design/association/#relations","title":"Relations","text":"<p>Situations can arise where a source is associated with more than one source in the catalogue being cross-matched (or vice versa). Internally these types of associations are called:</p> <ul> <li><code>many-to-many</code></li> <li><code>one-to-many</code></li> <li><code>many-to-one</code></li> </ul> <p>a good explanation of these situations is presented in the TraP documentation here. The VAST Pipeline follows the TraP methods in handling these types of associations, which is also detailed in the linked documentation. In short:</p> <ul> <li><code>many-to-many</code> associations are reduced to <code>one-to-one</code> or <code>one-to-many</code> associations.</li> <li><code>one-to-many</code> and <code>many-to-one</code> associations create \"forked\" unique sources. I.e. an individual datapoint can belong to two different sources.</li> </ul> <p>The VAST Pipeline reports the <code>one-to-many</code> and <code>many-to-one</code> associations by <code>relating</code> sources. A source may have one or more <code>relations</code> which signifies the the source could be associated with more than one other source. This often happens for complex sources with many closely packed components.</p> <p>A read-through of the TraP documentation is highly encouraged on this point as it contains an excellent description.</p>"},{"location":"design/association/#relations-false-variability","title":"Relations False Variability","text":"<p>The VAST Pipeline builds associations only using the component information. What this means is that, while the island information from the selavy source finder is stored, it is not considered during the association stage. Because of this, the relation process detailed above has the potential to cause sources to appear variable, when in reality it is not the case.</p> <p>For an example consider the source below:</p> <p> </p> <p>In the 3rd, 7th, and 8th measurement (EPOCH02, EPOCH09, and EPOCH12), the source is detected as an island with two Gaussian components, as opposed to the one component in all other epochs. The source lightcurve shows how the flux has reduced by approximately 50% in these three epochs, which makes the source appear variable. The pipeline provides information for each source that allows for these kind of situations to be swiftly identified:</p> <ul> <li>the number number of measurements that contain siblings, and</li> <li>the number of relations. </li> </ul> <p>These are the columns <code>n_sibl</code> and <code>n_rel</code>, respectively, in the pipeline sources output file (refer to the Column Descriptions section).  If these values are not 0 for a source then care must be taken when analysing variability.</p> <p>For the example source above, the values are <code>n_sibl = 3</code> and <code>n_rel = 1</code>. The missing flux can be seen in the lightcurve of the related source:</p> <p></p>"},{"location":"design/association/#epoch-based-association","title":"Epoch Based Association","text":"<p>The pipeline is able to associate inputs on an epoch basis. What this means is that, for example, all VAST Pilot Epoch 1 measurements are grouped together and are associated with grouped together Epoch 2 measurements, and so on. In doing this, duplicate measurements from within the same epoch are cut with the measurement kept being that which is closest to the centre of its respective image. The separation distance that defines a duplicate is defined in the pipeline run configuration file (<code>source_association.epoch_duplicate_radius</code>).</p> <p>The mode is activated by entering the images to be processed under an extra heading in the <code>.yaml</code> configuration file as demonstrated below. The heading acts as the epoch 'key', hence be sure to use a string that can be ordered as the heading to maintain the correct epoch order.</p> <p>config.yaml</p> <pre><code>inputs:\n  image:\n    epoch01:\n    - /full/path/to/image1.fits\n    - /full/path/to/image2.fits\n    epoch02:\n    - /full/path/to/image3.fits\n</code></pre> <p>The lightcurves below show the difference between 'regular' association (top) and 'epoch based' association (lower) for a source.</p> <p> </p> <p>For large surveys where transient and variablity searches on the epoch timescale is required, using this mode can greatly speed up the association stage.</p> <p>Warning</p> <p>Epoch based association does eliminate the full time resolution of your data! The base time resolution will be between the defined epochs.</p>"},{"location":"design/association/#de-ruiter-and-epoch-based-association","title":"de Ruiter and Epoch Based Association","text":"<p>During the standard de Ruiter assoication, an initial on sky separation cut is made of \\(\\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,img}}}{2}\\), where <code>beamwidth limit</code> is a value entered by the user and \\(\\theta_{\\text{bmaj,img}}\\) is the major component size of the restoring beam of the image being associated.</p> <p>When using epoch based association, an epoch that contains more than one image will have multiple values of \\(\\theta_{\\text{bmaj,img}}\\) to apply to the combined measurements. In this case, the maximum major axis value of all the images, \\(\\theta_{\\text{bmaj,max}}\\), is used.  Hence, the initial de Ruiter association step threshold becomes</p> \\[ \\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,max}}}{2}. \\]"},{"location":"design/association/#parallel-association","title":"Parallel Association","text":"<p>When parallel association is used, the images to process are analysed and grouped into distinct patches of the sky that do not overlap. These distinct regions are then processed through the source association in parallel. It is recommended to use parallel association when your dataset covers three or more distinct patches of sky.</p>"},{"location":"design/imageingest/","title":"Image &amp; Selavy Catalogue Ingest","text":"<p>This page details the stage of the pipeline that ingests the images to be processed.</p> <p>When the pipeline encounters an image for the first time (in any pipeline run), the image and accompanying selavy catalogue are uploaded to the pipeline database. The portion of a pipeline log file below shows the messages for the ingestion of three images.</p> <p>Note</p> <p>Once an image is uploaded then that image is available for all other runs to use without having to re-upload.</p> <p>2021-03-11-12-48-21_log.txt</p> <pre><code>2021-03-11 12:59:49,751 loading INFO Reading image VAST_0127-73A.EPOCH01.I.cutout.fits ...\n2021-03-11 12:59:49,756 utils INFO Adding new frequency band: 887\n2021-03-11 12:59:49,771 utils INFO Created sky region 21.838, -73.121\n2021-03-11 12:59:49,775 utils INFO Adding new-test-data to sky region 21.838, -73.121\n2021-03-11 12:59:50,100 loading INFO Processed measurements dataframe of shape: (203, 40)\n2021-03-11 12:59:50,273 loading INFO Bulk created #203 Measurement\n2021-03-11 12:59:50,334 loading INFO Reading image VAST_2118+00A.EPOCH01.I.cutout.fits ...\n2021-03-11 12:59:50,345 utils INFO Created sky region 322.439, -3.987\n2021-03-11 12:59:50,347 utils INFO Adding new-test-data to sky region 322.439, -3.987\n2021-03-11 12:59:50,577 loading INFO Processed measurements dataframe of shape: (148, 40)\n2021-03-11 12:59:50,708 loading INFO Bulk created #148 Measurement\n2021-03-11 12:59:50,736 loading INFO Reading image VAST_2118-06A.EPOCH01.I.cutout.fits ...\n2021-03-11 12:59:50,749 utils INFO Created sky region 322.439, -4.487\n2021-03-11 12:59:50,752 utils INFO Adding new-test-data to sky region 322.439, -4.487\n2021-03-11 12:59:50,977 loading INFO Processed measurements dataframe of shape: (159, 40)\n2021-03-11 12:59:51,111 loading INFO Bulk created #159 Measurement\n</code></pre>"},{"location":"design/imageingest/#ingest-steps-summary","title":"Ingest Steps Summary","text":"<ol> <li>The FITS file is opened and read (the header is used to obtain metadata) along with the selavy component catalogue text file.</li> <li>The selavy component file is cleaned for erroneous components along with the calculation of extra measurements metrics such as <code>signal-to-noise ratio</code>, <code>compactness</code> and positional uncertainties. Also, optionally, flux errors are recalculated using the Condon (1997) method. See the Selavy Measurements Processing section below for further details.</li> <li>Median, minimum and maximum root-mean-square (RMS) values are read from the accompanying RMS image provided by the user and these values are attached to the image.</li> <li>The image is also attached to a sky region and a frequency band based on its properties (see Sky Region and Frequency Band).</li> <li>The cleaned measurements (selavy components) are saved to a parquet file for repeated easy access.</li> <li>The overall image, band and sky region information for the pipeline run are written to a parquet file.</li> </ol> <p>See Ingest Steps Details for further details on the steps.</p>"},{"location":"design/imageingest/#uniqueness","title":"Uniqueness","text":"<p>The image uniqueness is defined by the filename. If you wish to upload a different version of the same image, e.g. a version where different Selavy settings were used in the source extraction, then you would have to make sure the image filename was different to the previously ingested image.</p>"},{"location":"design/imageingest/#ingest-steps-details","title":"Ingest Steps Details","text":""},{"location":"design/imageingest/#selavy-measurements-processing","title":"Selavy Measurements Processing","text":""},{"location":"design/imageingest/#cleaning","title":"Cleaning","text":"<p>The selavy measurements are checked for erroneous values that could cause issues with the source association. Any sources that are found to have the following properties are removed:</p> <ul> <li>Sources that have a peak or integrated flux value of 0.</li> <li>Sources that have a <code>bmaj</code> or <code>bmin</code> value of 0.</li> <li>Sources that have a <code>bmaj</code> or <code>bmin</code> value less than half of the respective values of the image restoring beam.</li> </ul> <p>In addition, components are also checked for zero values that can be corrected, where the correction values to apply are defined in either the user or overall pipeline configuration files. The field names of these zero checks are defined in the table below.</p> Field Name Correct with Location <code>flux_int_err</code> <code>FLUX_DEFAULT_MIN_ERROR</code> <code>settings.py</code> <code>flux_peak_err</code> <code>FLUX_DEFAULT_MIN_ERROR</code> <code>settings.py</code> <code>ra_err</code> <code>POS_DEFAULT_MIN_ERROR</code> <code>settings.py</code> <code>dec_err</code> <code>POS_DEFAULT_MIN_ERROR</code> <code>settings.py</code> <code>local_rms</code> <code>measurements.selavy_local_rms_fill_value</code> <code>config.yaml</code> <p>Note</p> <p><code>settings.py</code> refers to the pipeline configuration file <code>webinterface/settings.py</code> which is configured by the system administrator and cannot be modified by regular users. <code>config.yaml</code> refers to a pipeline run configuration file which is set by the user.</p>"},{"location":"design/imageingest/#condon-1997-flux-positional-errors","title":"Condon (1997) Flux &amp; Positional Errors","text":"<p>If selected in the pipeline run configuration file, the flux and positional errors are recalculated using the Condon (1997) method. The following errors are replaced with those that are recalculated:</p> <ul> <li><code>flux_peak_err</code></li> <li><code>flux_int_err</code></li> <li><code>err_bmaj</code></li> <li><code>err_bmin</code></li> <li><code>err_pa</code></li> <li><code>ra_err</code></li> <li><code>dec_err</code></li> </ul>"},{"location":"design/imageingest/#positional-errors-de-ruiter-method","title":"Positional Errors (de Ruiter method)","text":"<p>Firstly, the systematic astrometry error from the user pipeline run configuration file (<code>measurements.ra_uncertainty</code> and <code>measurements.dec_uncertainty</code>) are applied to the measurement. These values are saved as <code>ew_sys_err</code> and <code>ns_sys_err</code>.</p> <p>Warning</p> <p>Currently the systematic errors applied at the pipeline run stage are then permanently fixed to the measurements, meaning that all subsequent runs using these measurements will use the fixed astrometic error.</p> <p>It is recommended to leave the values to the default value of 1.0.</p> <p>In order to apply the <code>TraP</code> de Ruiter association method, some extra positional error values are calculated. Firstly the <code>ra_err</code> and <code>dec_err</code> are used to estimate the largest angular uncertainty of the measurement which is recorded as the <code>error_radius</code>. It is estimated by finding the largest angular separation between the measurement coordinate and every coordinate combination of \\(ra \\pm \\delta ra\\) and \\(dec \\pm \\delta dec\\).</p> <p>The final uncertainties are then defined as the hypotenuse values of <code>ew_sys_err</code>/<code>ns_sys_err</code> and the <code>error_radius</code>. These are defined as the <code>uncertainty_ew</code> and <code>uncertainty_ns</code>, respectively. The weights of the errors are defined as \\(\\frac{1}{\\text{uncertainty_x}^{2}}\\) where <code>x</code> is either <code>ew</code> or <code>ns</code>.</p>"},{"location":"design/imageingest/#other-metrics","title":"Other Metrics","text":"<p>The table below defines extra metrics that are added to the measurements.</p> Field Name Description <code>time</code> The image datetime applied to the measurement. <code>snr</code> \\(\\frac{\\text{flux_peak}}{\\text{local_rms}}\\). <code>compactness</code> \\(\\frac{\\text{flux_int}}{\\text{flux_peak}}\\). <code>flux_int_isl_ratio</code> \\(\\frac{\\text{flux_int}}{\\text{total_island_int_flux}}\\). <code>flux_peak_isl_ratio</code> \\(\\frac{\\text{flux_peak}}{\\text{total_island_peak_flux}}\\)."},{"location":"design/imageingest/#sky-region","title":"Sky Region","text":"<p>The pipeline defines <code>sky regions</code> that are used to easily find images that cover the same region of the sky. A sky region is defined by:</p> <ul> <li>The central coodinate.</li> <li>The width in both ra and dec (the <code>physical_bmaj</code> and <code>physical_bmin</code> values are used here, see Uploaded Image Information).</li> <li>An extraction radius (<code>xtr_radius</code>; <code>fov_bmin</code> is used here, again see Uploaded Image Information).</li> </ul> <p>Hence, images that cover the exact same patch of sky will be assigned to the same sky region.</p>"},{"location":"design/imageingest/#frequency-band","title":"Frequency Band","text":"<p>The image is associated to a frequency object in the pipeline that represents the observational frequency information of the image. The <code>frequency</code> and the <code>bandwidth</code> are recorded.</p>"},{"location":"design/imageingest/#image-rms-values","title":"Image RMS Values","text":"<p>The median, minimum and maximum values are calculated directly from the RMS map supplied by the user as a required input. This is achieved by loading the data from the FITS file and using the respective <code>numpy</code> operations on the data array to obtain the values.</p>"},{"location":"design/imageingest/#fits-headers-used","title":"FITS Headers Used","text":"<p>The table below defines which header fields are used to read the image information.</p> Header Field Used For <code>DATE-OBS</code> The date and time of the observation. <code>TIMESYS</code> The timezone of the date and time. <code>DURATION</code> Duration of the observation in seconds. <code>STOKES</code> Stokes parameter of the image. <code>TELESCOP</code> Telescope name. <code>BMAJ</code> Major axis size of the restoring beam. <code>BMIN</code> Minor axis size of the restoring beam. <code>BPA</code> Position angle of the restoring beam. <code>NAXIS1</code> Size of the image RA axis in pixels. <code>NAXIS2</code> Size of the image Dec axis in pixels. <code>CTYPE3(or 4)</code> Check if equal to <code>FREQ</code> to use for frequency information. <code>CRVAL3(or 4)</code> Central frequency. <code>CDELT3(or 4)</code> Bandwidth. <p><code>RESTFREQ</code> and <code>RESTBW</code> can also be used as fallback options for frequency detection.</p> <p>The pixel scales are obtained with <code>astropy.wcs.utils.proj_plane_pixel_scales</code>.</p>"},{"location":"design/imageingest/#uploaded-image-information","title":"Uploaded Image Information","text":"<p>The table below defines what is defined and uploaded using the meta data (FITS header) and other inputs.</p> Field Name Default Description <code>measurements_path</code> n/a The system path to the corresponding selavy components file (saved as a parquet file by the pipeline) <code>polarisation</code> <code>I</code> The polarisation of the image (currently only Stokes <code>I</code> is supported). <code>name</code> n/a The name of the image which is taken from the filename. <code>path</code> n/a The system path to the image FITS file. <code>noise_path</code> <code>''</code> The system path to the related noise image FITS file. <code>background_path</code> <code>''</code> The system path to the related background image FITS file. <code>datetime</code> n/a Observational datetime of the image. <code>jd</code> n/a Observational datetime of the image in Julian days format. <code>duration</code> <code>0</code> Duration of the observation (if found in header). Seconds. <code>ra</code> n/a The Right Ascension of the image pointing centre. Degrees. <code>dec</code> n/a The Declination of the image pointing centre. Degrees. <code>fov_bmaj</code> n/a The estimated major axis field-of-view value - the <code>radius_pixels</code> multiplied by the major axis pixel size. Degrees. <code>fov_bmin</code> n/a The estimated minor axis field-of-view value - the <code>radius_pixels</code> multiplied by the minor axis pixel size. Degrees. <code>physical_bmaj</code> n/a The actual major axis on-sky size - the number of pixels on the major axis multiplied by the major axis pixel size. Degrees. <code>physical_bmin</code> n/a The actual minor axis on-sky size - the number of pixels on the minor axis multiplied by the minor axis pixel size. Degrees. <code>radius_pixels</code> n/a Estimated 'diameter' of the useable image area. Pixels. <code>beam_bmaj</code> n/a The size of the major axis of the image restoring beam. Degrees. <code>beam_bmin</code> n/a The size of the minor axis of the image restoring beam. Degrees. <code>beam_bpa</code> n/a The position angle of the image restoring beam. Degrees East of North. <code>rms_median</code> n/a The median RMS value from the RMS map. mJy/beam. <code>rms_min</code> n/a The minimum RMS value from the RMS map (pixel value). mJy/beam. <code>rms_max</code> n/a The maximum RMS value from the RMS map (pixel value). mJy/beam."},{"location":"design/monitor/","title":"Forced Measurements","text":"<p>This page details the forced measurements obtained by the pipeline.</p>"},{"location":"design/monitor/#definition","title":"Definition","text":"<p>When <code>source_monitoring.monitor: true</code> is set in the pipeline run configuration file, any sources that have non-detections in their lightcurve will have these measurements 'filled in' by performing <code>forced measurements</code>. This means that the flux at the source's current average position in the non-detection image is forcefully measured by fitting a Gaussian with the same shape as the respective image restoring beam.</p> <p>Forced measurements are labelled in the measurements table and parquet files by the column <code>forced</code>.</p> <p>Note</p> <p>Forced measurements are local to a pipeline run - they will not appear in any other pipeline run.</p> <p>Warning</p> <p>Forced measurements are not performed within 3 beamwidths of the image edge.</p>"},{"location":"design/monitor/#minimum-sigma-filter","title":"Minimum Sigma Filter","text":"<p>Before forced measurements are processed, a minimum sigma check is made to make sure that the forced measurements would provide useful information. For example, a dataset may contain an image that has significantly less sensitivity than the other images. In this case a faint source in the more sensitive images will not be expected to be seen in the less sensitive image. To avoid unnecessary computation, this source is not forcefully measured.</p> <p>The check is performed like that which is made in the New Sources process where the signal-to-noise ratio is calculated using the rms\\(_{min}\\) of the image it is to be extracted from. Hence, for a forced measurement to take place the following condition must be met:</p> \\[ \\frac{f_{peak,det}}{\\text{rms}_{min,i}} &gt; \\text{source_monitoring.min_sigma}, \\] <p>where \\(i\\) is the image for which the measurement is to be forcefully measured.</p> <p><code>source_monitoring.min_sigma</code> is able to be controlled by the user in the pipeline run configuration file. By default the value is set to 3.0.</p> <p>Tip</p> <p>Setting <code>source_monitoring.min_sigma: 0.0</code> will ensure that all forced measurements are performed regardless of signal-to-noise.</p>"},{"location":"design/monitor/#configuration-file-options","title":"Configuration File Options","text":"<p>The following options are present in the pipeline run configuration file to users along with their defaults:</p> <p>config.yaml</p> <pre><code>source_monitoring:\n  monitor: false\n  min_sigma: 3.0\n  edge_buffer_scale: 1.2\n  cluster_threshold: 3.0\n  allow_nan: false\n</code></pre> <ul> <li><code>source_monitoring.monitor</code> turns forced measurements on (<code>true</code>) or off (<code>false</code>).</li> <li><code>source_monitoring.min_sigma</code> controls the the minimum sigma check threshold as explained in Minimum Sigma Filter.</li> <li><code>source_monitoring.edge_buffer_scale</code> controls the size of the buffer from the image edge where forced measurements are not performed (<code>source_monitoring.edge_buffer_scale</code> \\(\\times 3\\theta_{beam}\\)). An error can sometimes occur that increasing this value can solve.</li> <li><code>source_monitoring.cluster_threshold</code> is directly passed to the forced photometry package used by the pipeline. It defines the multiple of <code>major_axes</code> to use for identifying clusters.</li> <li><code>source_monitoring.allow_nan</code> is directly passed to the forced photometry package used by the pipeline. It defines whether <code>NaN</code> values are allowed to be present in the extraction area in the rms or background maps, i.e. <code>true</code> would mean that <code>NaN</code> values are allowed.</li> </ul>"},{"location":"design/monitor/#software-forced_phot","title":"Software - forced_phot","text":"<p>The software used to perform the forced measurements, <code>forced_phot</code>, was written by David Kaplan and can be found on the VAST GitHub here.</p>"},{"location":"design/newsources/","title":"New Sources","text":"<p>This page details the new source analysis performed by the pipeline.</p>"},{"location":"design/newsources/#definition","title":"Definition","text":"<p>A <code>new source</code> is defined as a source that is detected during the pipeline run that was not detected in any previous epoch of the location of the source, and has a peak flux such that it should have been detected.</p> <p>Note</p> <p>Remember that pipeline runs are self-contained - i.e. a run does not have any knowledge of another run, hence new sources are local to their pipeline run.</p>"},{"location":"design/newsources/#new-source-process","title":"New Source Process","text":"<p>The pipeline identifies new sources by using the following steps:</p> <ol> <li>Sources are found that have 'incomplete' light curves, i.e. there are epochs in the pipeline run of the source location where the source is not detected.     The would-be 'ideal' coverage is then calculated to determine which images contain the source location but have a non-detection.</li> <li>Remove sources where the epoch of the first detection is also the earliest possible detection epoch.</li> <li> <p>For the remaining sources a general rms check is made to answer the question of should this source be expected to be detected at all in the previous epochs. This is done by taking the minimum \\(\\text{rms}_{min}\\) of all the non-detection images and making sure that </p> \\[ \\frac{f_{peak,det}}{\\text{minimum rms}_{min}} &gt; \\text{new_sources.min_sigma}, \\] <p>where \\(f_{peak,det}\\) is the peak flux density of the first detection of the source. The default value of <code>new_sources.min_sigma</code> is 5.0, but the parameter can be controlled by the user in the pipeline run configuration file.</p> </li> <li> <p>The sources that meet the above criteria are marked as a <code>new source</code>.</p> </li> <li>The <code>new source high sigma</code> value is calculated for all new sources.</li> </ol>"},{"location":"design/newsources/#new-source-high-sigma","title":"New Source High Sigma","text":"<p>In the process detailed above, the rms check is made against the minimum rms of the previous non-detection images. This might not be an accurate representation of the rms at the source's actual location in the image, for example, the rms might be high at the source location such that a detection of the source wouldn't be expected at the \\(5\\sigma\\) level.</p> <p>To account for this the <code>new source high sigma</code> value is calculated for all new sources. For each non-detection image for a source, the true signal-to-noise ratio the source would have in the non-detection image is calculated, i.e.</p> \\[ \\text{new source true sigma}_i = \\frac{f_{peak,det}}{\\text{rms}_{location,i}} \\] <p>where \\(\\text{rms}_{location,i}\\) is the rms at the source location for each non-detection image, \\(i\\).</p> <p>The <code>new source high sigma</code> is then equal to the maximum \\(\\text{rms}_{location,i}\\). The value can be used to filter those new sources which would be borderline detections, or not expected to be detected at all, at the actual location in the previous images. This allows users to concentrate on the significant new sources.</p>"},{"location":"design/newsources/#viewing-new-sources","title":"Viewing New Sources","text":"<p>New sources are marked as new on the website interface (see Source Pages) and in the source parquet file output there is a boolean column named <code>new</code>.</p>"},{"location":"design/overview/","title":"Pipeline Steps Overview","text":"<p>This page gives an overview of the processing steps of a pipeline run. Each section contains a link to a feature page that contains more details.</p>"},{"location":"design/overview/#terminology","title":"Terminology","text":"<p><code>Run</code> A single pipeline dataset defined by a configuration file.</p> <p><code>Image</code> A FITS image that is being processed as part of a pipeline run. It also has related inputs of the selavy source catalogue, and the noise and background images also produced by selavy.</p> <p><code>Measurement</code> An extracted measurement read from the selavy source catalogue from an associated image. The only measurements produced by the pipeline are <code>forced measurements</code> which are performed when monitoring is used.</p> <p><code>Source</code> A group of measurements that have been identified as the same astrophysical source by a pipeline association method.</p>"},{"location":"design/overview/#pipeline-processing-steps","title":"Pipeline Processing Steps","text":"<p>Note</p> <p>Each pipeline run is self-aware only, which means that each run does not draw on the results of other runs. However, since images and their measurements don't change, subsequent runs that use any image that was ingested as part of a previous run will not be ingested again.</p>"},{"location":"design/overview/#1-image-selavy-catalogue-ingest","title":"1. Image &amp; Selavy Catalogue Ingest","text":"<p>Full details: Image &amp; Selavy Catalogue Ingest.</p> <p>The first stage of the pipeline is to read and ingest to the database the input data that has been provided in the configuration file. This includes determing statistics about the image footprint and properties, and also importing and cleaning the associated measurements from the selavy file. The errors on the measurements can also be recalculated at this stage based upon the Condon (1997) method.</p> <p>Image uniqueness is determined by the filename, and once the image is ingested, it is available for other pipeline runs to use without having to re-ingest.</p>"},{"location":"design/overview/#2-source-association","title":"2. Source Association","text":"<p>Full details: Source Association.</p> <p>Once all images and measurments have been ingested the source association step is performed, where measurements over time are associated to a unique astrophysical source. Images are arranged chronologically and association is performed on an image by image basis, or as a grouped \"epoch\" if epoch based association is used. The association is performed as per the settings entered in the run configuration file.</p>"},{"location":"design/overview/#3-ideal-coverage-new-source-analysis","title":"3. Ideal Coverage &amp; New Source Analysis","text":"<p>Full details: New Sources.</p> <p>With the measurements associated the sources are analysed to check for non-detections over time and whether the source should have been seen in any non-detection images. The ideal coverage calculation is also used to determine any sources that should be marked as <code>new</code>, i.e. a source that has appeared over time that was not detected in the first image of its location on the sky. The non-detections are then passed to the forced monitoring step.</p>"},{"location":"design/overview/#4-monitoring-forced-measurements","title":"4. Monitoring Forced Measurements","text":"<p>Full details: Forced Measurements.</p> <p>This step is optional. The non-detections which form gaps in the lightcurves of each source are filled in by forcefully extracting a flux measurement at the location of the source.</p>"},{"location":"design/overview/#5-source-statistics-calculation","title":"5. Source Statistics Calculation","text":"<p>Full details: Source Statistics.</p> <p>Statistics are calculated for each source such as the weighted average sky position, average flux values, variability metrics (including two-epoch pair metrics) and various counts.</p>"},{"location":"design/overview/#6-database-upload","title":"6. Database Upload","text":"<p>All the results from the pipeline run are uploaded to the database. Specifically at the end of the run the following is written to the database:</p> <ul> <li>Sources and their statistics.</li> <li>Relations between sources.</li> <li>Associations.</li> </ul> <p>For large runs this can be a substantial component of the pipeline run time. <code>Bulk upload</code> statements will be seen in the pipeline run log file such as these shown below:</p> <p>2021-03-11-12-48-21_log.txt</p> <pre><code>2021-03-11 13:00:04,893 loading INFO Bulk created #557 Source\n2021-03-11 13:00:04,910 loading INFO Populate \"related\" field of sources...\n2021-03-11 13:00:04,919 loading INFO Bulk created #29 RelatedSource\n2021-03-11 13:00:04,943 loading INFO Upload associations...\n2021-03-11 13:00:05,650 loading INFO Bulk created #3276 Association\n</code></pre>"},{"location":"design/sourcestats/","title":"Source Statistics","text":""},{"location":"design/sourcestats/#source-statistics","title":"Source Statistics","text":"<p>This page details the source statistics that are calculated by the pipeline.</p>"},{"location":"design/sourcestats/#overview","title":"Overview","text":"<p>The table below provides a summary of all the statistic and counts provided by the pipeline. See the Variability Statistics section for the table containing the variability metrics.</p> <p>Note</p> <p>Remember that all source statistics and counts are calculated from the individual measurements that are associated with the source.</p> Parameter  Includes Forced Meas. Description <code>wavg_ra</code> No The weighted average of the Right Ascension, degrees. <code>wavg_dec</code> No The weighted average of the Declination, degrees. <code>wavg_uncertainty_ew</code> No The weighted average uncertainty in the east-west (RA) direction, degrees. <code>wavg_uncertainty_ns</code> No The weighted average uncertainty in the north-south (Dec) direction, degrees. <code>avg_flux_int</code> Yes The average integrated flux, mJy. <code>max_flux_int</code> Yes The maximum integrated flux value, mJy. <code>min_flux_int</code> Yes The minimum integrated flux value, mJy. <code>avg_flux_peak</code> Yes The average peak flux, mJy/beam. <code>max_flux_peak</code> Yes The maximum peak flux value, mJy/beam. <code>min_flux_peak</code> Yes The minimum peak flux value, mJy/beam. <code>min_flux_int_isl_ratio</code> Yes The minimum integrated flux value island ratio (int_flux / total_isl_int_flux). <code>min_flux_peak_isl_ratio</code> Yes The minimum peak flux value island ratio (peak_flux / total_isl_peak_flux). <code>avg_compactness</code> No The average compactness of the source (compactness is defined by int_flux / peak_flux). <code>min_snr</code> No The minimum signal-to-noise ratio of the source. <code>max_snr</code> No The maximum signal-to-noise ratio of the source. <code>n_neighbour_dist</code> n/a On sky separation distance to the nearest neighbour within the same run, degrees (arcmin on webserver). <code>new_high_sigma</code> n/a The largest sigma value a new source would have if it was placed at its location in the previous images it was not detected in. See New Sources for more information. New sources only. <code>n_meas</code> Yes The total number of measurements associated to the source. Named <code>Total Datapoints</code> on the webserver. <code>n_meas_sel</code> No The total number of selavy measurements associated to the source. Named <code>Selavy Datapoints</code> on the webserver. <code>n_meas_forced</code> Yes The total number of forced measurements associated to the source. Named <code>Forced Datapoints</code> on the webserver. <code>n_rel</code> n/a The total number of relations the source has. See Source Association. Named <code>Relations</code> on the webserver. <code>n_sibl</code> n/a The total number measurements that has a sibling. On the webserver tables this is firstly presented as a boolean column of if the source contains measurements that have a sibling."},{"location":"design/sourcestats/#variability-statistics","title":"Variability Statistics","text":"<p>Below is a table describing the variability metrics of the source. See the following sections for further explanation of these metrics.</p> Parameter   Includes Forced Meas. Description <code>v_int</code> Yes The \\(V\\) metric for the integrated flux. <code>v_peak</code> Yes The \\(V\\) metric for the peak flux. <code>eta_int</code> Yes The \\(\\eta\\) metric for the integrated flux. <code>eta_peak</code> Yes The \\(\\eta\\) metric for the peak flux. <code>vs_abs_significant_max_int</code> Yes The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be <code>0</code> if no significant pair. <code>m_abs_significant_max_int</code> Yes The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be <code>0</code> if no significant pair. <code>vs_abs_significant_max_peak</code> Yes The \\(\\mid V_s \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be <code>0</code> if no significant pair. <code>m_abs_significant_max_peak</code> Yes The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be <code>0</code> if no significant pair."},{"location":"design/sourcestats/#v-and-metrics","title":"V and \u03b7 Metrics","text":"<p>The \\(V\\) and \\(\\eta\\) metrics are the same as those used by the LOFAR Transients Pipeline (TraP), for a complete description please refer to Swinbank et al. (2015). In the VAST Pipeline, the metrics are calculated twice, for both the integrated and peak fluxes.</p> <p>\\(V\\) is the proportional flux variability of the source and is given by the ratio of the sample standard deviation (\\(s\\)) and mean of the flux, \\(I\\):</p> \\[ V = \\frac{s}{\\overline{I}} = \\frac{1}{\\overline{I}} \\sqrt{\\frac{N}{N - 1}\\left(\\overline{I^{2}}-\\overline{I}^{2}\\right)}. \\] <p>The \\(\\eta\\) value is the significance of the variability, based on \\(\\chi^{2}\\) statistics, and is given by:</p> \\[ \\eta = \\frac{N}{N - 1}\\left(\\overline{wI^{2}} - \\frac{\\overline{wI}^{2}}{\\overline{w}}\\right) \\] <p>where \\(w\\) is the uncertainty (\\(e\\)) in \\(I\\) of a measurement, and is given by \\(w=\\frac{1}{e}\\).</p>"},{"location":"design/sourcestats/#two-epoch-metrics","title":"Two-Epoch Metrics","text":"<p>By default, alternative variability metrics, \\(V_s\\) and \\(m\\), are also calculated which we refer to as the 'two-epoch metrics'. They are calculated for each unique pair of measurements associated with the source, with the most significant pair of values attached to the source (see section below). Please refer to Mooley et al. (2016) for further details.</p> <p>Warning</p> <p>The number of measurement pairs scales exponentially with the number of measurements per source (i.e. the number of images). Runs that contain a large number of input images per source may run out memory while calculating the two-epoch metrics. If this occurs, it is recommended that the pair metric calculation is turned off in the run configuration by setting <code>variability.pair_metrics: False</code> (see variability run configuration options).</p> <p>Note</p> <p>All the two-epoch pair \\(V_s\\) and \\(m\\) values for a run are saved in the output file <code>measurement_pairs.parquet</code> for offline analysis.</p> <p>\\(V_s\\) is a statistic to compare the flux densities of a source between two-epochs and is given by:</p> \\[ V_s = \\frac{\\Delta S}{\\sigma} = \\frac{S_1 - S_2}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{2}^{2}}} \\] <p>where \\(S\\) is the flux and \\(\\sigma\\) is the associated error. This metric is known to follow a Student-t distribution. Typically, in the literature, a source is defined as variable if this parameter is beyond the 95% confidence interval, i.e.:</p> \\[ \\mid V_s \\mid \\geq 4.3. \\] <p>\\(m\\) is a moduluation index variable given by:</p> \\[ m = \\frac{\\Delta S}{\\overline{S}} \\] <p>where \\(\\overline{S}\\) is the mean of the flux densities \\(S_1\\) and \\(S_2\\). Typically, in the literature, the threshold for this value for a source to be considered variable is:</p> \\[ \\mid m \\mid \\gt 0.26, \\] <p>which equates to a variability of 30%. However the user is free to set their own level to define variablity.</p>"},{"location":"design/sourcestats/#significant-source-values","title":"Significant Source Values","text":"<p>The \\(V_s\\) and \\(m\\) metrics of the 'maximum significant pair' is attached to the source. The maximum significant pair is determined by selecting the most significant \\(\\mid m \\mid\\) value given a minimum \\(V_s\\) threshold which is defined in the pipeline configuration file <code>variability.source_aggregate_pair_metrics_min_abs_vs</code>:</p> <p>config.yaml</p> <pre><code>variability:\n  # Only measurement pairs where the Vs metric exceeds this value are selected for the\n  # aggregate pair metrics that are stored in Source objects.\n  source_aggregate_pair_metrics_min_abs_vs: 4.3\n</code></pre> <p>By default this value is set to 4.3. For example, if a source with three associated measurements gave the following pair metrics:</p> Pair \\(\\mid V_s \\mid\\) \\(\\mid m \\mid\\) A-B 4.5 0.1 B-C 2.5 0.05 A-C 4.3 0.4 <p>then the <code>A-C</code> pair metrics are attached to the source as the most significant. This can be used to quickly determine significant two-epoch variability for a source. If there are no pair values above the minimum \\(V_s\\) threshold then these values attached to the source will be 0. The <code>measurement_pairs.parquet</code> file can be used to manually explore the measurement pairs if one wishes to lower the threshold.</p>"},{"location":"developing/docsdev/","title":"Development Guidelines for Documentation","text":"<p>The pipeline documentation has been developed using the python package <code>mkdocs</code>and the material theme. It is published as a static website using GitHub pages.</p>"},{"location":"developing/docsdev/#documentation-development-server","title":"Documentation development server","text":"<p>Note</p> <p>Remember to install the development dependencies which include the modules required for the documentation.</p> <p>This section describes how to set up a development server to live reload your changes to the pipeline documentation.</p> <p>The main code of the documentation is located in the <code>docs</code> directory. In order to keep the repository, <code>CHANGELOG.md</code>, <code>LICENSE.txt</code> and <code>CODE_OF_CONDUCT.md</code> on the root path, relative soft links have been created under the docs folder:</p> <pre><code>adminusage\narchitecture\nchangelog.md -&gt; ../CHANGELOG.md\ncode_of_conduct.md -&gt; ../CODE_OF_CONDUCT.md\ndesign\ndeveloping\nexploringwebsite\nfaqs.md\ngen_doc_stubs.py\ngettingstarted\nimg\nindex.md\nlicense.md -&gt; ../LICENSE.txt\noutputs\nreference\ntheme\nusing\n</code></pre> <p>Start the development server:</p> <pre><code>(pipeline_env)$ mkdocs serve\n</code></pre> <p>Files in the docs directory are then 'watched' such that any changes will cause the mkdocs server to reload the new content.</p> <p>The structure of the site (see <code>nav</code> section) and the settings are in the <code>mkdocs.yml</code> in the root of the repository.</p>"},{"location":"developing/docsdev/#plugins-and-customisations","title":"Plugins and Customisations","text":"<p><code>mkdocs</code> and the material theme have a lot of customisations and plugins available. The more involved plugins or customisations that are in use for this documentation are detailed in the following sections.</p>"},{"location":"developing/docsdev/#custom-theme-directory","title":"Custom <code>theme</code> directory","text":"<p>The <code>theme</code> directory contains various overrides to the standard material template. This is enabled by the following line in the <code>mkdocs.yml</code> file:</p> <p>mkdocs.yml</p> <pre><code>theme:\n  custom_dir: docs/theme\n</code></pre> <p>Custom javascript and css files are also stored in this directory and are enabled in the following <code>mkdocs.yml</code> file:</p> <p>mkdocs.yml</p> <pre><code>extra_css:\n  - theme/css/extra.css\nextra_javascript:\n  - theme/js/extra.js\n  - https://polyfill.io/v3/polyfill.min.js?features=es6\n  - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\n</code></pre> <p>The contents of the <code>theme</code> directory are shown below:</p> <pre><code>docs/theme\n\u251c\u2500\u2500 assets\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 stylesheets\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 overrides.css\n\u251c\u2500\u2500 css\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 extra.css\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgallery.min.css\n\u251c\u2500\u2500 fonts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 lg.svg\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 lg.ttf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lg.woff\n\u251c\u2500\u2500 home.html\n\u251c\u2500\u2500 img\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 loading.gif\n\u251c\u2500\u2500 js\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 extra.js\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 lg-zoom.js\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgallery.min.js\n\u2514\u2500\u2500 main.html\n</code></pre> <p>Full documentation on customising the <code>mkdocs-material</code> theme can be found in the documentation.</p> <p><code>main.html</code> contains edits to the <code>main.html</code> as described in the documentation linked to above. The other files shown are used for the custom homepage and the Lightgallery.</p>"},{"location":"developing/docsdev/#custom-homepage","title":"Custom Homepage","text":"<p>The homepage is overwritten by a custom stylised HTML page. This is achieved using the following files:</p> <pre><code>docs\n\u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 theme\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 assets\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 stylesheets\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 overrides.css\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 home.html\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 main.html\n</code></pre> <p>The <code>index.md</code> file in the main <code>docs</code> directory should only contain the following.</p> <p>index.md</p> <pre><code>---\ntitle: VAST Pipeline\ntemplate: home.html\n---\n</code></pre>"},{"location":"developing/docsdev/#creation-and-last-updated-dates","title":"Creation and Last Updated Dates","text":"<p>Each page displays a date of creation and also a \"last updated\" date.  This is done by using the plugin mkdocs-git-revision-date-localized-plugin. The following options are used:</p> <p>mkdocs.yml</p> <pre><code>plugins:\n  - git-revision-date-localized:\n      fallback_to_build_date: true\n      enable_creation_date: true\n</code></pre> <p>Dates and Code Reference Pages</p> <p>The option <code>fallback_to_build_date: true</code> is required for the code reference pages that are auto generated by the <code>mkdocs-gen-files</code> plugin (see the Python Docstrings and Source Code section below). These pages will show the build date rather than the \"last updated\" date. During the build process the following warning will be seen, which is expected and ok to ignore: <pre><code>WARNING -  [git-revision-date-localized-plugin] Unable to find a git directory and/or git is not installed. Option 'fallback_to_build_date' set to 'true': Falling back to build date\n</code></pre></p>"},{"location":"developing/docsdev/#lightgallery","title":"Lightgallery","text":"<p>Images are displayed in the documentation using the <code>lightgallery-markdown</code> plugin. As described in the repository linked to above, certain assets from <code>lightgallery.js</code> are copied over to make the extension work, the files that apply here are:</p> <pre><code>docs/theme\n\u251c\u2500\u2500 css\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgallery.min.css\n\u251c\u2500\u2500 fonts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 lg.svg\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 lg.ttf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lg.woff\n\u251c\u2500\u2500 img\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 loading.gif\n\u251c\u2500\u2500 js\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 extra.js\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 lg-zoom.js\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgallery.min.js\n\u2514\u2500\u2500 main.html\n</code></pre> <p><code>lg-zoom.js</code> is also copied over to activate the zoom feature on the gallery images.</p> <p>Note</p> <p>The javascript to select the lightgallery class objects and run them through the lightgallery function is placed in  <code>theme/js/extra.js</code> such that the plugin works with instant navigation.</p>"},{"location":"developing/docsdev/#mathjax","title":"MathJax","text":"<p><code>MathJax</code> is enabled as described in the <code>mkdocs-material</code> documentation.</p>"},{"location":"developing/docsdev/#python-docstrings-and-source-code","title":"Python Docstrings and Source Code","text":"<p>Python docstrings and source code are rendered using the <code>mkdocstrings</code> plugin.</p> <p>Docstrings Format</p> <p>All docstrings must be done using the Google format.</p> <p>The markdown files for the docstrings are automatically generated using <code>mkdocs-gen-files</code> in the file <code>docs/gen_doc_stubs.py</code>. It is activated by the following lines in the <code>mkdocs.yml</code> file:</p> <p>mkdocs.yml</p> <pre><code>plugins:\n  - search\n  - gen-files:\n      scripts:\n      - docs/gen_doc_stubs.py\n</code></pre> <p>The markdown files are programmatically generated, this means that the markdown files don't actually appear, but are virtually part of the site build. The files must still be referenced in the <code>nav:</code> section of <code>mkdocs.yml</code>, for example:</p> <p>mkdocs.yml</p> <pre><code>- nav:\n  - Code Reference:\n    - vast_pipeline:\n      - image:\n        - main.py: reference/image/main.md\n        - utils.py: reference/image/utils.md\n</code></pre> <p>The files are virtually created relative to the doc path that is stated in <code>gen_doc_stubs.py</code>.  For example, using the <code>vast_pipeline</code> as the base for the python files and <code>reference</code> as the doc path, the markdown file for the docstrings in <code>vast_pipeline/image/main.py</code> is created at <code>reference/image/main.md</code>.</p> <p>Please refer to the <code>mkdocstrings</code> documentation  for full details of the options available to declare in <code>mkdocs.yml</code>.</p>"},{"location":"developing/docsdev/#versioning","title":"Versioning","text":"<p>Versioning is enabled on the documentation by using the <code>mike</code> package. The documentation is published for each release version, along with a development version that is updated with every commit to the default <code>dev</code> branch. Setup of this has been completed on the initial deployment, and the GitHub workflows will automatically take care of the continual deployment. Vist the <code>mike</code> package page linked above for details on general management commands such as how to delete a version.</p>"},{"location":"developing/docsdev/#deployment-to-github-pages","title":"Deployment to GitHub pages","text":"<p>Automatic deployment to GitHub pages is set up using GitHub actions and workflows.  See the workflows <code>ci-docs-dev.yml</code> and <code>ci-docs-release.yml</code>.</p>"},{"location":"developing/github/","title":"GitHub Platform Guidelines","text":"<p>This section explains how to interact with GitHub platform for opening issues, starting discussions, creating pull requests (PR), and some notes how to make a release of the pipeline if you are a maintainer of the code base.</p> <p>The VAST team uses the \"git flow\" branching model which we briefly summarise here. More detail can be found here.</p> <p>There are two main branches, both with infinite lifetimes (they are never deleted):</p> <ul> <li><code>master</code> for stable, production-ready code that has been released, and</li> <li><code>dev</code> for the latest reviewed updates for the next release.</li> </ul> <p>Other branches for bug fixes and new features are created as needed, branching off and merging back into <code>dev</code>. An exception to this is for critical patches for a released version called a \"hotfix\". These are branched off <code>master</code> and merged back into both <code>master</code> and <code>dev</code>.</p> <p>Branches are also created for each new release candidate, which are branched off <code>dev</code> and merged into <code>master</code> and <code>dev</code> when completed. See the Releases section below for more information.</p>"},{"location":"developing/github/#issues","title":"Issues","text":"<p>An issue can be created by anyone with access to the repository. Users are encouraged to create issues for problems they encounter while using the pipeline or to request a new feature be added to the software. Issues are created by clicking the \"New issue\" button near the top-right of the issues page.</p> <p>When creating a new issue, please consider the following:</p> <ul> <li>Search for a similar issue before opening a new one by using the search box near the top of the issues page.</li> <li>When opening a new issue, please specify the issue type (e.g. bug, feature, etc.) and provide a detailed description with use cases when appropriate.</li> </ul>"},{"location":"developing/github/#discussions","title":"Discussions","text":"<p>GitHub repositories also have a discussions page which serves as a collaborative forum to discuss ideas and ask questions. Users are encouraged to ask general questions, or start a conversation about potential new features to the software by creating a new discussion thread on the discussions page. Note that new software features may also be requested by creating an issue, but a discussion thread is more appropriate if the details of the new feature are still yet to be determined or require wider discussion \u2013 issues can be created from discussions once a consensus is reached.</p>"},{"location":"developing/github/#pull-requests","title":"Pull Requests","text":"<p>Pull requests are created when a developer wishes to merge changes they have made in a branch into another branch. They enable others to review the changes and make comments. While issues typically describe in detail a specific problem or proposed feature, pull requests contain a detailed description and the required code changes for a solution to a problem or implementation of a new feature.</p>"},{"location":"developing/github/#opening-a-pr","title":"Opening a PR","text":"<p>First consider ...</p> <ol> <li>Search existing issues for similar problems or feature proposals.</li> <li>Opening an issue to describe the problem or feature before creating a PR. This will help separate problems from solutions.</li> </ol> <p>Steps to issue a pull request:</p> <ol> <li>Create a new issue on GitHub, giving it a succinct title and describe the problem. GitHub will assign an ID e.g. <code>#123</code>.</li> <li>Create a new branch off the <code>dev</code> branch and name the branch based on the issue title, e.g. <code>fix-123-some-problem</code> (keep it short please).</li> <li>Make your changes.</li> <li>Run the test suite locally with <code>python manage.py test vast_pipeline</code>. See the complete guide on the test for more details.</li> <li>Run the webserver and check the functionality. This is important as the test suite does not currently check the web UI.</li> <li>Commit the changes to your branch, push it to GitHub, and open a PR for the branch.</li> <li>Update the <code>CHANGELOG.md</code> file by adding the link to your PR and briefly describing the changes. An example of the change log format can be found here</li> <li>Assign the review to one or more reviewers if none are assigned by default.</li> </ol> <p>Warning</p> <p>PRs not branched off dev will be rejected!.</p>"},{"location":"developing/github/#reviewing-a-pr","title":"Reviewing a PR","text":"<p>The guidelines to dealing with reviews and conversations on GitHub are essentially:</p> <ul> <li>Be nice  with the review and do not offend the author of the PR: Nobody is a perfect developer or born so!</li> <li>The reviewers will in general mark the conversation as \"resolved\" (e.g. he/she is satisfied with the answer from the PR author).</li> <li>The PR author will re-request the review by clicking on the  on the top right corner and might ping the reviewer on a comment if necessary with <code>@github_name</code>.</li> <li>When the PR is approved by at least one reviewer you might want to merge it to dev (you should have that privileges), unless you want to make sure that such PR is reviewed by another reviewer (e.g. you are doing big changes or important changes or you want to make sure that other person is aware/updated about the changes in that PR).</li> </ul>"},{"location":"developing/github/#releases","title":"Releases","text":"<p>In to order to make a release, please follow these steps:</p> <ol> <li>Make sure that all new feature and bug fix PRs that should be part of the new release have been merged to <code>dev</code>.</li> <li>Checkout the <code>dev</code> branch and update it with <code>git pull</code>. Ensure that there are no uncommitted changes.</li> <li>Create a new branch off <code>dev</code>, naming it <code>release-vX.Y.Z</code> where <code>X.Y.Z</code> is the new version. Typically, patch version increments for bug fixes, minor version increments for new features that do not break backward compatibility with previous versions (i.e. no database schema changes), and major version increments for large changes or for changes that would break backward compatibility.</li> <li>Bump the version number of the Python package using Poetry, i.e. <code>poetry version X.Y.Z</code>. This will update the version number in <code>pyproject.toml</code>.</li> <li>Update the version in <code>package.json</code> and <code>vast_pipeline/_version.py</code> to match the new version number, then run <code>npm install</code> to update the <code>package-lock.json</code> file.</li> <li>Update the \"announcement bar\" in the documentation to refer to the new release. This can be found in <code>docs/theme/main.html</code> at line 37.</li> <li>Update the <code>CHANGELOG.md</code> by renaming the \"Unreleased\" heading to the new version. Include a link to the release - it won't exist yet, so just follow the format of the others. Also remove any empty sub-headings under the new release heading.</li> <li>Commit all the changes made above to the new branch and push it to GitHub.</li> <li>Open a PR to merge the new branch into <code>master</code>. Note that the default target branch is <code>dev</code> so you will need to change this to <code>master</code> when creating the PR.</li> <li>Once the PR has been reviewed and approved, merge the branch into <code>master</code>. This can only be done by administrators of the repository.</li> <li> <p>Tag the merge commit on <code>master</code> with the version, i.e. <code>git tag vX.Y.Z</code>, then push the tag to GitHub.</p> <p>Warning</p> <p>If you merged the release branch into <code>master</code> with the GitHub web UI, you will need to sync that merge to your local copy and checkout <code>master</code> before creating the tag. You cannot create tags with the GitHub web UI.</p> </li> <li> <p>Push the tag to GitHub, i.e. <code>git push origin vX.Y.Z</code>.</p> </li> <li>Merge the release branch into <code>dev</code>, resolving any conflicts.</li> <li>Append \"dev\" to the version numbers in <code>pyproject.toml</code>, <code>package.json</code> and <code>vast_pipeline/_version.py</code>, then run <code>npm install</code> to update <code>package-lock.json</code>. Add a new \"Unreleased\" heading to the <code>CHANGELOG.md</code> with all standard subheadings (\"Added\", \"Changed\", \"Fixed\", \"Removed\" and \"List of PRs\"). Commit all changes to <code>dev</code> either as a new commit, or while resolving merge conflicts in the previous step.</li> <li>Create a new release on GitHub that points to the tagged commit on master.</li> </ol>"},{"location":"developing/intro/","title":"Contributing and Developing Guidelines","text":"<p>This section explains how to contribute to the project code base and collaborate on GitHub platform. Please use the section navigator to left to visit pages for specific details on areas of development.</p>"},{"location":"developing/intro/#terminology","title":"Terminology","text":"<p>These below is the terminology used to identify pipeline objects.</p> <ul> <li>Pipeline run (or \"Run\") -&gt; Pipeline run instance, also referred as <code>run, p_run, piperun, pipe_run, ...</code> in the code</li> <li>Measurement -&gt; the extracted measurement from the source finder of a single astrophysical source from an image, referred in the code as <code>measurement(s), meas, ...</code></li> <li>Source -&gt; A collection of single measurements for the same astrophysical source, referred as <code>src, source, ...</code> in the code</li> </ul>"},{"location":"developing/intro/#docstrings","title":"Docstrings","text":"<p>All docstrings must be done using the Google format to ensure compatibility with the documentation.</p>"},{"location":"developing/localdevenv/","title":"Pipeline Local Development Environment Guidelines","text":"<p>This section describes how to set up a local development environment more in details.</p>"},{"location":"developing/localdevenv/#back-end","title":"Back End","text":""},{"location":"developing/localdevenv/#installation","title":"Installation","text":"<p>The installation instructions are the same as the ones describes in the Getting Started section with one key difference. Rather than installing the Python dependencies with pip, you will need to install and use Poetry. After installing Poetry, running the command below will install the pipeline dependencies defined in <code>poetry.lock</code> into a virtual environment. The main difference between using Poetry and pip is that pip will only install the dependencies necessary for using the pipeline, whereas Poetry will also install development dependencies required for contributing (e.g. tools to build the documentation).</p> <pre><code>poetry install\n</code></pre> <p>Note</p> <p>Poetry will automatically create a virtual environment if it detects that your shell isn't currently using one. This should be fine for most users. If you prefer to use an alternative virtual environment manager (e.g. Miniconda), you can prevent Poetry from creating virtual environments. However, even if you are using something like Miniconda, allowing Poetry to manage the virtualenv (the default behaviour) is fine. The development team only uses this option during our automated testing since our test runner machines only contain a single Python environment so using virtualenvs is redundant.</p>"},{"location":"developing/localdevenv/#changes-to-the-data-models","title":"Changes to the data models","text":"<p>When changes are made to the data models defined in <code>vast_pipeline/models.py</code>, the database schema needs to be updated to reflect those changes. Django can handle this for you by generating migration files that contain the necessary code to update the schema. Migration files are generated with the Django management command <code>python manage.py makemigrations</code> and applied to the database with <code>python manage.py migrate</code>. Depending on the nature of the changes, this may break backward compatibility, i.e. runs created with previous versions of the pipeline may not be compatible with your changes.</p> <p>Database migrations must be committed to source control so that others can pull in your model changes. They can sometimes be complex and require additional attention. If you have any difficulty with migrations, please contact the VAST development team for help. More information can be found in the Django documentation on migrations.</p>"},{"location":"developing/localdevenv/#removingclearing-data","title":"Removing/Clearing Data","text":"<p>The following sub-sections show how to completely drop every data in the database and how to remove only the data related to one or more pipeline runs.</p>"},{"location":"developing/localdevenv/#reset-the-database","title":"Reset the database","text":"<p>Make sure you installed the requirements <code>dev.txt</code>. And <code>django_extensions</code> is in <code>EXTRA_APPS</code> in your setting configuration file <code>.env</code> (e.g. <code>EXTRA_APPS=django_extensions,another_app,...</code>).</p> <pre><code>(pipeline_env)$ ./manage.py reset_db  &amp;&amp; ./manage.py migrate\n# use the following for no confirmation prompt\n(pipeline_env)$ ./manage.py reset_db --noinput  &amp;&amp; ./manage.py migrate\n</code></pre>"},{"location":"developing/localdevenv/#clearing-run-data","title":"Clearing Run Data","text":"<p>It is sometimes convenient to remove the data belonging to one or more pipeline runs while developing the code base. This is particularly useful to save time by not having to re-upload the image data along with the measurements. The data related to the pipeline are the Sources, Associations, Forced extractions entries in database and the parquet files in the respective folder. By default the command will keep the run folder with the config and the log files.</p> <pre><code>(pipeline_env)$ ./manage.py clearpiperun path/to/my-pipe-run\n</code></pre> <p>To clear more than one run:</p> <pre><code>(pipeline_env)$ ./manage.py clearpiperun path/to/my-pipe-run1 my-pipe-run2 path/to/my-pipe-run3\n</code></pre> <p>The command accept both a path or a name of the pipeline run(s). To remove all the runs, issue:</p> <pre><code>(pipeline_env)$ ./manage.py clearpiperun clearall\n</code></pre> <p>The command to keep the parquet files is:</p> <pre><code>(pipeline_env)$ ./manage.py clearpiperun path/to/my-pipe-run --keep-parquet\n</code></pre> <p>The remove completely the pipeline folder</p> <pre><code>(pipeline_env)$ ./manage.py clearpiperun path/to/my-pipe-run --remove-all\n</code></pre>"},{"location":"developing/localdevenv/#frontend-assets-management-and-guidelines","title":"FrontEnd Assets Management and Guidelines","text":"<p>This guide explain the installation, compilation and development of the front end assets (HTML, CSS, JS and relative modules). We make use of a <code>node</code> installation with <code>npm</code> and <code>gulp</code> tasks to build the front end assets.</p>"},{"location":"developing/localdevenv/#installation-of-node-packages","title":"Installation of node packages","text":"<p>After installing a <code>node</code> version and <code>npm</code>, install the node modules using from the base folder (<code>vast-pipeline</code>):</p> <pre><code>npm ci\n</code></pre> <p><code>Npm</code> will install the node packages in a <code>node_modules</code> folder under the main root.</p> <pre><code>...\n\u251c\u2500\u2500 node_modules\n...\n</code></pre> <p>For installing future additional dependencies you can run <code>npm install --save my-package</code> or <code>npm install --save-dev my-dev-package</code> (to save a development module), and after that commit both <code>package.json</code> and <code>package-lock.json</code> files. For details about the installed packages and npm scripts see <code>package.json</code>.</p>"},{"location":"developing/localdevenv/#frontend-tasks-with-gulp","title":"FrontEnd Tasks with <code>gulp</code>","text":"<p>Using <code>gulp</code> and <code>npm</code> scripts you can:</p> <ol> <li>Install dependencies under the <code>./static/vendor</code> folder.</li> <li>Building (e.g. minify/uglify) CSS and/or Javascript files.</li> <li>Run a development server that \"hot-reload\" your web page when any HTML, CSS or Javascript file is modified.</li> </ol> <p>The command to list all the <code>gulp</code> \"tasks\" and sub-tasks is (you might need <code>gulp-cli</code> installed globally, i.e. <code>npm i --global gulp-cli</code>, more info here):</p> <pre><code>gulp --tasks\n</code></pre> <p>Output:</p> <pre><code>[11:55:30] Tasks for ~/PATH/TO/REPO/vast-pipeline/gulpfile.js\n[11:55:30] \u251c\u2500\u2500 clean\n[11:55:30] \u251c\u2500\u252c js9\n[11:55:30] \u2502 \u2514\u2500\u252c &lt;series&gt;\n[11:55:30] \u2502   \u251c\u2500\u2500 js9Dir\n[11:55:30] \u2502   \u251c\u2500\u2500 js9MakeConfig\n[11:55:30] \u2502   \u251c\u2500\u2500 js9Make\n[11:55:30] \u2502   \u251c\u2500\u2500 js9MakeInst\n[11:55:30] \u2502   \u2514\u2500\u2500 js9Config\n[11:55:30] \u251c\u2500\u2500 css\n[11:55:30] \u251c\u2500\u2500 js\n[11:55:30] \u251c\u2500\u252c vendor\n[11:55:30] \u2502 \u2514\u2500\u252c &lt;parallel&gt;\n[11:55:30] \u2502   \u251c\u2500\u2500 modules\n[11:55:30] \u2502   \u2514\u2500\u252c &lt;series&gt;\n[11:55:30] \u2502     \u251c\u2500\u2500 js9Dir\n[11:55:30] \u2502     \u251c\u2500\u2500 js9MakeConfig\n[11:55:30] \u2502     \u251c\u2500\u2500 js9Make\n[11:55:30] \u2502     \u251c\u2500\u2500 js9MakeInst\n[11:55:30] \u2502     \u2514\u2500\u2500 js9Config\n[11:55:30] \u251c\u2500\u252c build\n[11:55:30] \u2502 \u2514\u2500\u252c &lt;series&gt;\n[11:55:30] \u2502   \u251c\u2500\u252c &lt;parallel&gt;\n[11:55:30] \u2502   \u2502 \u251c\u2500\u2500 modules\n[11:55:30] \u2502   \u2502 \u2514\u2500\u252c &lt;series&gt;\n[11:55:30] \u2502   \u2502   \u251c\u2500\u2500 js9Dir\n[11:55:30] \u2502   \u2502   \u251c\u2500\u2500 js9MakeConfig\n[11:55:30] \u2502   \u2502   \u251c\u2500\u2500 js9Make\n[11:55:30] \u2502   \u2502   \u251c\u2500\u2500 js9MakeInst\n[11:55:30] \u2502   \u2502   \u2514\u2500\u2500 js9Config\n[11:55:30] \u2502   \u2514\u2500\u252c &lt;parallel&gt;\n[11:55:30] \u2502     \u251c\u2500\u2500 cssTask\n[11:55:30] \u2502     \u2514\u2500\u2500 jsTask\n[11:55:30] \u251c\u2500\u252c watch\n[11:55:30] \u2502 \u2514\u2500\u252c &lt;series&gt;\n[11:55:30] \u2502   \u251c\u2500\u252c &lt;parallel&gt;\n[11:55:30] \u2502   \u2502 \u251c\u2500\u2500 cssTask\n[11:55:30] \u2502   \u2502 \u2514\u2500\u2500 jsTask\n[11:55:30] \u2502   \u2514\u2500\u252c &lt;parallel&gt;\n[11:55:30] \u2502     \u251c\u2500\u2500 watchFiles\n[11:55:30] \u2502     \u2514\u2500\u2500 browserSync\n[11:55:30] \u251c\u2500\u252c default\n[11:55:30] \u2502 \u2514\u2500\u252c &lt;series&gt;\n[11:55:30] \u2502   \u251c\u2500\u252c &lt;parallel&gt;\n[11:55:30] \u2502   \u2502 \u251c\u2500\u2500 modules\n[11:55:30] \u2502   \u2502 \u2514\u2500\u252c &lt;series&gt;\n[11:55:30] \u2502   \u2502   \u251c\u2500\u2500 js9Dir\n[11:55:30] \u2502   \u2502   \u251c\u2500\u2500 js9MakeConfig\n[11:55:30] \u2502   \u2502   \u251c\u2500\u2500 js9Make\n[11:55:30] \u2502   \u2502   \u251c\u2500\u2500 js9MakeInst\n[11:55:30] \u2502   \u2502   \u2514\u2500\u2500 js9Config\n[11:55:30] \u2502   \u2514\u2500\u252c &lt;parallel&gt;\n[11:55:30] \u2502     \u251c\u2500\u2500 cssTask\n[11:55:30] \u2502     \u2514\u2500\u2500 jsTask\n[11:55:30] \u2514\u2500\u2500 debug\n</code></pre> <p>Alternatively you can run gulp from the installed version in the <code>node_modules</code> folder with:</p> <pre><code>./node_modules/.bin/gulp --tasks\n</code></pre> <p>For further details about tasks, see <code>gulpfile</code>.</p>"},{"location":"developing/localdevenv/#1-install-dependencies-under-vendor-folder","title":"1. Install Dependencies under <code>vendor</code> Folder","text":"<p>Install the dependencies under the <code>./static/vendor</code> folder, with:</p> <pre><code>npm run vendor\n</code></pre> <p>Or, using global <code>gulp-cli</code>:</p> <pre><code>gulp vendor\n</code></pre> <p>As seen in the tasks diagram above, the <code>vendor</code> task run the module task in parallel with the js9 tasks. JS9 has many task as these run with manual command that involve <code>make/make install</code> and then writing configuration to <code>js9prefs.js</code> file. You can run manually the installation of JS9 with <code>gulp js9</code>.</p>"},{"location":"developing/localdevenv/#2-building-css-and-javascript-files","title":"2. Building CSS and Javascript files","text":"<pre><code>npm run build\n# or\nnpm start\n# or\ngulp build\n# or\ngulp default\n# or\ngulp\n</code></pre> <p>will run the vendor task and minify both CSS and Javascript files. By default, when no other tasks is specified, <code>gulp</code> runs the build task. You can run single tasks with:</p> <pre><code>gulp css\n</code></pre> <p>to run just the minification of the CSS files.</p>"},{"location":"developing/localdevenv/#3-run-development-server","title":"3. Run Development Server","text":"<p>Start your normal Django server with (NOTE: do not change the default port!):</p> <pre><code>(pipeline_env)$: ./manage.py runserver\n</code></pre> <p>In another terminal run:</p> <pre><code>npm run watch\n# or\ngulp watch\n</code></pre> <p>The latter will open your dev server, that will auto reload and apply your latest changes in any CSS, Javascript and/or HTML files. As pointed out in the gulp task tree above the <code>watch</code> task run both the vendor and build tasks.</p>"},{"location":"developing/localdevenv/#4-debug-task","title":"4. Debug Task","text":"<p>This task is for debugging the paths used in the others task, but also serve as a place holder to debug commands.</p> <pre><code>npm run debug\n# or\ngulp debug\n</code></pre>"},{"location":"developing/localdevenv/#5-clean-task","title":"5. Clean Task","text":"<p>This task delete the vendor folder (<code>/static/vendor</code>) along with all the files.</p> <pre><code>npm run clean\n# or\ngulp clean\n</code></pre>"},{"location":"developing/localdevenv/#frontend-assets-for-production","title":"FrontEnd assets for Production","text":"<p>In order to compile the frontend assets for production, activate the Python virtual environment, then run:</p> <pre><code>(pipeline_env)$ npm run js9staticprod &amp;&amp; ./manage.py collectstatic -c\n</code></pre> <p>This command will collect all static assets (Javascript and CSS files) and copy them to <code>STATIC_ROOT</code> path in setting.py, so make sure you have permission to write to that. <code>STATIC_ROOT</code> is assigned to <code>./staticfiles</code> by default, otherwise assigned to the path you defined in your <code>.env</code> file.</p> <p>The <code>js9staticprod</code> gulp task is necessary if you specify a <code>STATIC_URL</code> and a <code>BASE_URL</code> different than the default, for example if you need to prefix the site <code>/</code> with a base url because you are running another webserver (e.g. another web server is running on <code>https://my-astro-platform.com/</code> so you want to run the pipeline on the same server/domain <code>https://my-astro-platform.com/pipeline</code>, so you need to set <code>BASE_URL='/pipeline/'</code> and <code>STATIC_URL=/pipeline-static/</code> in <code>settings.py</code>). We recommend to run this in any case!</p> <p>Then you can move that folder to where it can be served by the production static files server (Ningx or Apache are usually good choices, in case refer to the Django documentation).</p>"},{"location":"developing/profiling/","title":"Benchmarks","text":""},{"location":"developing/profiling/#initial-profiling","title":"Initial Profiling","text":"<p>These profiling tests were run with the pipeline codebase correspondent to commit 373c2ce (some commits after the first release).</p> <p>Running on 12GB of data with 464MB peak memory usage takes 4 mins: performance: ~80% <code>final_operations</code>, ~10% <code>get_src_skyregion_merged_df</code>. <code>final_operations</code> calls other functions, out of these the largest is 50% in <code>make_upload_sources</code> which spends about 20% of time on <code>utils</code> <code>&lt;method 'execute' of 'psycopg2.extensions.cursor' objects&gt;</code>. The <code>get_src_skyregion_merged_df</code> time sink is in <code>threading</code> 15% of time is on <code>wait</code> (<code>final_operations</code> spends some time on <code>threading</code> as well, hence 15% &gt; 10%). memory: 40% <code>pyarrow</code> <code>parquet</code> <code>write_table</code>, rest is mostly fragmented, some more <code>pyarrow</code> and some <code>pandas</code></p> <p>Running on 3MB of data with peak memory usage 176MB, takes 1.5s: performance: ~30% goes to pipeline (about 9% of this is <code>pickle</code>), ~11% goes to <code>read</code>, rest goes to django I think memory: 30% memory is spent on <code>django</code>, 20% is spent on <code>astropy/coordinates/matrix_utilities</code>, 10% on importing other modules, rest is fragmented quite small</p> <p>Note that I didn't include the generation of the <code>images/*/measurements.parquet</code> or other files in these profiles.</p>"},{"location":"developing/profiling/#database-update-operations","title":"Database Update Operations","text":"<p>Delete (<code>Model.objects.all().delete()</code>) and reupload (<code>bulk_upload</code>) (in seconds)</p> columns\\rows 10<sup>3</sup> 10<sup>4</sup> 10<sup>5</sup> 4 0.15 1.24 12.95 8 0.26 1.64 19.11 12 0.31 2.18 21.49 <p>Per cell, 10<sup>3</sup> rows is slower than 10<sup>4</sup> and 10<sup>5</sup> rows, possibly due to overhead. Best to avoid uploading 10<sup>3</sup> rows each bulk_create call.</p> <p>Django <code>bulk_update</code></p> columns\\rows 10<sup>3</sup> 10<sup>4</sup> 10<sup>5</sup> 4 3.39 na na 8 4.38 na na 12 5.50 na na <p>I don't think there's any point testing 10<sup>4</sup> or 10<sup>5</sup> rows, it's obviously the worst performing function, and I've already had to force quit the terminal twice because keyboard interrupt didn't work.</p> <p>SQL join as (<code>SQL_update</code> in <code>vast_pipeline.pipeline.loading</code>)</p> columns\\rows 10<sup>3</sup> 10<sup>4</sup> 10<sup>5</sup> 4 0.016 0.11 3.08 8 0.019 0.32 4.31 12 0.027 0.38 5.39 <p>10<sup>5</sup> is slower per cell than 10<sup>4</sup> and 10<sup>3</sup>, not sure why. Recommend updating 10<sup>4</sup> rows each time.</p> <p>This timing info does vary a bit on randomness. Sometimes the SQL join as takes as long as 1 second to complete 10<sup>3</sup> rows, I'm not sure what's causing this.</p>"},{"location":"developing/tests/","title":"Tests Guidelines","text":"<p>This section describes how to run the test suite of the VAST Pipeline.</p>"},{"location":"developing/tests/#general-tests","title":"General Tests","text":"<p>Test are found under the folder tests. Have a look and feel free to include new tests.</p> <p>Run the tests with the following:</p> <p>To run all tests:</p> <pre><code>(pipeline_env)$ ./manage.py test\n</code></pre> <p>To run one test file or class, use:</p> <pre><code>(pipeline_env)$ ./manage.py test &lt;path.to.test&gt;\n</code></pre> <p>for example, to run the test class <code>CheckRunConfigValidationTest</code> located in <code>test_pipelineconfig.py</code>, use:</p> <pre><code>(pipeline_env)$ ./manage.py test vast_pipeline.tests.test_pipelineconfig.CheckRunConfigValidationTest\n</code></pre> <p>to run the tests located in <code>test_webserver.py</code>, use:</p> <pre><code>(pipeline_env)$ ./manage.py test vast_pipeline.tests.test_webserver\n</code></pre>"},{"location":"developing/tests/#regression-tests","title":"Regression Tests","text":"<p>Regression tests located in <code>test_regression</code> require the use of the VAST_2118-06A field test dataset which is not a part of the repository. This data is downloadable from the USyd machines. You can use the script located in tests/regression-data/:</p> <pre><code>cd vast_pipeline/tests/regression-data/ &amp;&amp; ./download.sh\n</code></pre> <p>to download the VAST_2118-06A field test dataset into the <code>regression-data</code> folder. Or manually by clicking the button below:</p> <p>Download data for test </p> <p>and place the VAST_2118-06A field test dataset into the <code>regression-data</code> folder. These regression tests are skipped if the dataset is not present.</p> <p>All tests should be run before pushing to master. Running all the tests takes a few minutes, so it is not recommended to run them for every change.</p> <p>If you have made a minor change and would like to only run unit tests, skipping regression tests, use:</p> <pre><code>(pipeline_env)$ ./manage.py test vast_pipeline.tests.test_pipeline\n</code></pre> <p>Note</p> <p>If changes are made to the default config keys, these changes need to be propagated to the test config files.</p>"},{"location":"exploringwebsite/admintools/","title":"Website Admin Tools","text":""},{"location":"exploringwebsite/admintools/#accessing-the-admin-tools","title":"Accessing the Admin Tools","text":"<p>Users designated as administrators of the pipeline instance being explored (controlled by GitHub team membership)  will be able to see the admin button at the top of the navbar as shown below.</p> <p></p> <p>Clicking this button and then selecting the <code>Django</code> open will take the user to the Django admin backend interface shown below.</p> <p></p>"},{"location":"exploringwebsite/admintools/#authentification-and-authorization","title":"Authentification and Authorization","text":"<p>This section allows for the management of the user accounts and groups. Here users can be made admins and details such as email address updated.</p>"},{"location":"exploringwebsite/admintools/#django-q-tasks","title":"Django Q Tasks","text":"<p>This area allows for the management of the <code>Django Q</code> processing queue. Admins are able to cancel scheduled tasks, view failed tasks or schedule new tasks.</p>"},{"location":"exploringwebsite/admintools/#python-social-auth","title":"Python Social Auth","text":"<p>The area for managing aspects of the authentification system that allows users to log in via GitHub.</p>"},{"location":"exploringwebsite/admintools/#vast_pipeline","title":"VAST_PIPELINE","text":"<p>Admins are able to interact with the pipeline results data that has been uploaded from pipeline runs. This includes editing and removing objects or fields in the data as well as tags and comments.</p>"},{"location":"exploringwebsite/datatables/","title":"DataTables","text":"<p>Much of the data is presented using tables that share consistent functionality across the website.  An example of a table is shown below, note the interactive features across the top of the table, these are explained after the screenshot.</p> <p></p> <ul> <li>Show 10 entries: A selectable limiter of how many rows to display at once (maximum 100).</li> <li>Column visibility: Enables the user to hide and show columns columns. In the screenshot below the <code>compactness</code> column is hidden by deselecting it in the presented list.      </li> <li>CSV: Will download a CSV file of the data currently shown on screen.</li> <li> <p>Excel: Will download an Excel file of the data currently shown on screen.</p> <p>Warning</p> <p>Note the statement currently shown on screen - only this data will be downloaded to the CSV and Excel files. All the records are not able to be downloaded in this manner - for this it is recommened to interact with the output parquet files.</p> </li> <li> <p>Search: A search bar for the user to filter the table to the row they require. The search will take into account all appropriate columns when searching.</p> </li> </ul>"},{"location":"exploringwebsite/imagepages/","title":"Image Pages","text":"<p>This page details the website pages for information on the images.</p>"},{"location":"exploringwebsite/imagepages/#list-of-images","title":"List of Images","text":"<p>Shown on this page is a list of images that have been ingested into the pipeline database from all pipeline runs, along with their statistics. From this page the full detail page of a specific image can be accessed by clicking on the image name.  Explanation of the table options can be found in the DataTables section.</p> <p></p>"},{"location":"exploringwebsite/imagepages/#image-detail-page","title":"Image Detail Page","text":"<p>This page presents all the information about the selected image.</p> <p></p>"},{"location":"exploringwebsite/imagepages/#previous-next-buttons","title":"Previous &amp; Next Buttons","text":"<p>These buttons do the following:</p> <ul> <li>Previous: Navigates to the previous image by <code>id</code> value.</li> <li>Next: Navigates to the next image by <code>id</code> value.</li> </ul>"},{"location":"exploringwebsite/imagepages/#details","title":"Details","text":"<p>A text representation of details of the image.</p>"},{"location":"exploringwebsite/imagepages/#aladin-lite-viewer","title":"Aladin Lite Viewer","text":"<p>Aladin Lite Documentation.</p> <p>The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey, centred on the image central coordinates of the image. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS. The red square shows the footprint of the image sky region on the sky.</p>"},{"location":"exploringwebsite/imagepages/#user-comments","title":"User Comments","text":"<p>Users are able to read and post comments on an image using this form.</p>"},{"location":"exploringwebsite/imagepages/#measurements-table","title":"Measurements Table","text":"<p>This table displays all the <code>Selavy</code> measurements that were ingested with the image (no forced measurements appear here as they are run specific).  The measurement detail page can be reached by clicking the measurement name.</p> <p></p>"},{"location":"exploringwebsite/imagepages/#pipeline-runs-table","title":"Pipeline Runs Table","text":"<p>This table displays all the pipeline runs that use the current image. The pipeline detail page can be reached by clicking the run name.</p> <p></p>"},{"location":"exploringwebsite/measurementdetail/","title":"Measurement Detail Page","text":"<p>This page presents all the information about the selected measurement, including a postage stamp cutout of the component.</p> <p></p>"},{"location":"exploringwebsite/measurementdetail/#simbad-ned-previous-next-buttons","title":"SIMBAD, NED, Previous &amp; Next Buttons","text":"<p>These buttons do the following:</p> <ul> <li>SIMBAD: Performs a cone search on SIMBAD with a radius of 10 arcmin centered on the measurement location.</li> <li>NED: Performs a cone search on NED with a radius of 10 arcmin centered on the measurement location.</li> <li>Previous: Navigates to the previous measurement by <code>id</code> value.</li> <li>Next: Navigates to the next measurement by <code>id</code> value.</li> </ul>"},{"location":"exploringwebsite/measurementdetail/#details","title":"Details","text":"<p>A text representation of details of the measurement.</p>"},{"location":"exploringwebsite/measurementdetail/#aladin-lite-viewer","title":"Aladin Lite Viewer","text":"<p>Aladin Lite Documentation.</p> <p>The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey, centred on the location of the measurement. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS.</p>"},{"location":"exploringwebsite/measurementdetail/#js9-viewer","title":"JS9 Viewer","text":"<p>JS9 website.</p> <p>The right panel contains a JS9 viewer showing the postage stamp FITS image of the measurement loaded from its respective image FITS file.</p> <p>Note</p> <p>If the image data is removed from its location when the pipeline run was processed the JS9 viewer will no longer work.</p>"},{"location":"exploringwebsite/measurementdetail/#user-comments","title":"User Comments","text":"<p>Users are able to read and post comments on a measurement using this form.</p> <p></p>"},{"location":"exploringwebsite/measurementdetail/#sources-table","title":"Sources Table","text":"<p>The sources table shows all the sources, from all pipeline runs, that the measurement is associated to. Explanation of the table options can be found on the overview page here.</p> <p></p>"},{"location":"exploringwebsite/measurementdetail/#siblings-table","title":"Siblings Table","text":"<p>The siblings table displays all other measurements that are a sibling of the current measurement, i.e., the measurements belong to the same island (as determined by <code>Selavy</code>).</p>"},{"location":"exploringwebsite/runpages/","title":"Pipeline Run Pages","text":"<p>This page details the website pages for information on the pipeline runs.</p>"},{"location":"exploringwebsite/runpages/#list-of-pipeline-runs","title":"List of Pipeline Runs","text":"<p>A list of the pipeline runs that have been processed or initialised are presented on this page along with basic statistics, including the run status. From this page the full detail page of a specific pipeline run can be accessed by clicking on the name of the pipeline run.  Explanation of the table options can be found in the DataTables section.</p> <p></p> Run Status Description Completed The run has successfully finished processing. Deleting The pipeline run is currently being deleted. Error The run has encountered an error during processing and has stopped. Initialised The run has been created but not yet run. Queued The run has been sent to the scheduler for running but has not started yet. Restoring The pipeline run is currently being restored. Running The run is currently processing."},{"location":"exploringwebsite/runpages/#pipeline-run-detail-page","title":"Pipeline Run Detail Page","text":"<p>This page presents all the information about the pipeline run, including options to edit the configuration file and to schedule the run for processing, restore the run, delete the run and generate the arrow measurement files.</p> <p></p>"},{"location":"exploringwebsite/runpages/#action-buttons","title":"Action Buttons","text":"<p>For admins and creators of runs there are four action buttons available:</p> <ul> <li>Generate Arrow Files      A process to generate the arrow measurement files.      See Generating Arrow Files.</li> <li>Delete Run      Delete the pipeline run.      See Deleting a Run.</li> <li>Restore Run      A process to restore the run to the previous successful state.      See Restoring a Run.</li> <li>Add Images or Re-Process Run/Process Run      Process the pipeline run.      See Processing a Run.</li> </ul>"},{"location":"exploringwebsite/runpages/#summary-cards","title":"Summary Cards","text":"<p>The cards at the top of the page give a summary of the total numbers of:</p> <ul> <li>Images in the pipeline run.</li> <li>Measurements in the pipeline run.</li> <li>Sources in the pipeline run.</li> <li>New sources in the pipeline run.</li> </ul> <p>Clicking on the total number of images or measurements will navigate the user to the Image and Measurements tables on this page,  where as the source cards will take the user to the Sources Query page.</p> <p>Warning</p> <p>When sent to the source query page, the user should make sure to click submit on the search.</p>"},{"location":"exploringwebsite/runpages/#details","title":"Details","text":"<p>A text representation of details of the pipeline run.</p>"},{"location":"exploringwebsite/runpages/#run-sky-regions","title":"Run Sky Regions","text":"<p>A sky map showing the area of sky covered by the images associated with the pipeline run.</p>"},{"location":"exploringwebsite/runpages/#configuration-file","title":"Configuration File","text":"<p>Here the pipeline run configuration file can be viewed, edited and validated.</p>"},{"location":"exploringwebsite/runpages/#editing-the-configuration-file","title":"Editing the Configuration File","text":"<p>To edit the configuration file first select the <code>Toggle on/off Config Edit</code> option, that is shown in the screenshot to the right.  This will enter edit mode on the configuration file as denoted by the <code>--Edit Mode--</code> message shown in the screenshot below. </p> <p>Warning</p> <p>Do not toggle off edit mode without first selecting <code>Wrtie Current Config</code> otherwise changes will be lost.</p> <p></p> <p>When all changes are applied, select the <code>Write Current Config</code> to save the changes.</p>"},{"location":"exploringwebsite/runpages/#validating-the-configuration-file","title":"Validating the Configuration File","text":"<p>From the configuration file menu select the <code>Validate Config</code> option.  A feedback modal will then appear with feedback stating whether the configuration validation was successful or failed. The feedback may take a moment to appear as the check is performed.</p> <p></p>"},{"location":"exploringwebsite/runpages/#user-comments","title":"User Comments","text":"<p>Users are able to read and post comments on a pipeline run using this form.</p>"},{"location":"exploringwebsite/runpages/#log-files","title":"Log Files","text":"<p>There are three log files available, which are present depending on the actions performed. All logs are timestamped with the run time, and by default the most recent log is shown. A dropdown menu of available log files to view is available at the right hand side of the header as shown in the examples below. If there are no logs to show for the respective task then the log window will display <code>No logs to show</code> and the dropdown menu will appear empty.</p>"},{"location":"exploringwebsite/runpages/#run-log-file","title":"Run Log File","text":"<p>The full log file of the pipeline run process.</p> <p></p>"},{"location":"exploringwebsite/runpages/#restore-log-file","title":"Restore Log File","text":"<p>The log file of the restore run action. </p> <p></p>"},{"location":"exploringwebsite/runpages/#generate-arrow-files-log-file","title":"Generate Arrow Files Log File","text":"<p>The log file of the generate arrow files action.</p> <p></p>"},{"location":"exploringwebsite/runpages/#image-and-measurements-tables","title":"Image and Measurements Tables","text":"<p>Two tables are on the pipeline run detail page displaying the images and measurements (including forced measurements) that are part of the pipeline run.</p>"},{"location":"exploringwebsite/sourceanalysis/","title":"Source \u03b7-V Analysis","text":"<p>This page details the interactive analysis tool of the \u03b7 and V metrics for a selection of sources.</p> <p>Further Reading</p> <p>Descriptions of the \u03b7 and V metrics can be found on the source statistics page.</p> <p>For a detailed overview of the method, please refer to Rowlinson et al., 2019.</p> <p>The analysis can be performed on the results of a source query.</p> <p>Tip: Sensible Querying!</p> <p>To get the most out of the tool is advised to design a query that will eliminate as many erroneous results as possible. For example, making sure sources are isolated and have no siblings or relations. The VAST Tools package should be used for more advanced queries.</p>"},{"location":"exploringwebsite/sourceanalysis/#accessing-the-analysis-page","title":"Accessing the Analysis Page","text":"<p>Once a query has been performed on the source query page the <code>Go to \u03b7-V analysis</code> button will be active as highlighted in the image below.</p> <p></p> <p>Click this button and the analysis page will open.</p> <p>Warning: Bad Sources</p> <p>Bad sources for analysis will be automatically removed. These include sources that only have one datapoint or where the \u03b7 and/or V are equal to 0 or have failed.</p> <p></p>"},{"location":"exploringwebsite/sourceanalysis/#-v-plot-description","title":"\u03b7-V Plot Description","text":"<p>Shown on the left side of the page is the the log-log plot of the peak flux \u03b7 and V metrics of the sources from the query. The distributions of the metrics are fitted with a Gaussian, and a sigma cut is displayed on the plot that by default is set to a value of \\(3\\sigma\\). The highlighted region represents the area of plot that is beyond both thresholds, and this is where transient sources will be found. The colour of the points represent how many detection datapoints the source contains.</p>"},{"location":"exploringwebsite/sourceanalysis/#plot-options","title":"Plot Options","text":"<p>At the bottom of the plot are options to change both the flux type and the multiplication factor of the sigma cut. Once the new settings are entered click the <code>Apply</code> button and the plot will reload with the new options.</p>"},{"location":"exploringwebsite/sourceanalysis/#viewing-source-light-curves-information","title":"Viewing Source Light Curves &amp; Information","text":"<p>Hovering over a source on the plot will show an information window that displays the source name, id and the \u03b7 and V values.</p> <p></p> <p>Clicking on the source will load the light curve, source information and external crossmatching search results into the panels on the right side of the page. Selecting another source will dynamically update these panels without having to leave the page.</p> <p></p> <p>The source information panel contains a link to go to the full source detail page and the ability to favourite a source directly from this page.</p>"},{"location":"exploringwebsite/sourceanalysis/#displaying-high-source-counts","title":"Displaying High Source Counts","text":"<p>When querying large pipeline runs it is possible that a query will return tens of thousands of results. Plotting such a high number of sources is very intensive and would take a significant amount of time to render. To solve this, when a high number of sources are requested to be plotted, all the sources outside of the threshold transient area are plotted as a static image that represents the distribution of the sources. These sources are not interactive.  By default the threshold is set to 20,000 datapoints and is configurable by the administrator.</p> <p>Any sources that fall within the transient threshold region are plotted as normal and are interactive as with the standard plot.</p> <p>Warning: Setting Low Thresholds</p> <p>Setting low thresholds with high source counts could cause a significant amount of candidates to be plotted with the normal method. This may result in the plot taking up to a few minutes to load within the browser.</p> <p>Note: Colour Bar</p> <p>The colour bar only applies to the interactive datapoints.</p> <p></p>"},{"location":"exploringwebsite/sourcedetail/","title":"Source Detail","text":"<p>This page presents all the information about the selected source, including a light curve and cutouts of all the measurements that are associated to the source.</p> <p></p>"},{"location":"exploringwebsite/sourcedetail/#star-simbad-ned-ztf-previous-next-buttons","title":"Star, SIMBAD, NED, ZTF, Previous &amp; Next Buttons","text":"<p>These buttons do the following:</p> <ul> <li>Star: Adds the source to the user's favourites, see Source Tags and Favourites.</li> <li>SIMBAD: Performs a cone search on SIMBAD with a radius of 10 arcmin centered on the source location.</li> <li>NED: Performs a cone search on NED with a radius of 10 arcmin centered on the source location.</li> <li>ZTF: Performs a cone search for optical transient alerts from the Zwicky Transient Facility (ZTF) with a radius of 10 arcsec centered on the source location.</li> <li>Previous: Navigates to the previous source that was returned in the source query.</li> <li>Next: Navigates to the next source that was returned in the source query</li> </ul>"},{"location":"exploringwebsite/sourcedetail/#details","title":"Details","text":"<p>A text representation of details of the source.</p>"},{"location":"exploringwebsite/sourcedetail/#first-detection-postage-stamp","title":"First Detection Postage Stamp","text":"<p>The JS9 viewer is used to show the postage stamp FITS image (cutout) of the first source-finder detection for this source. Cutouts for each measurement are shown further down the page.</p>"},{"location":"exploringwebsite/sourcedetail/#aladin-lite-viewer","title":"Aladin Lite Viewer","text":"<p>Aladin Lite Documentation.</p> <p>The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey, centred on the location of the source. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS.</p>"},{"location":"exploringwebsite/sourcedetail/#flux-variability-details","title":"Flux &amp; Variability Details","text":"<p>Aggregate flux density statistics and variability metrics for this source, separated by flux type (peak or integrated).</p>"},{"location":"exploringwebsite/sourcedetail/#light-curve","title":"Light Curve","text":"<p>The light curve of the source is shown. The peak or integrated flux can be selected by using the radio selection buttons.</p> <p></p> <p>Hovering over the data points on the light curve will show an information panel that contains the date of the measurement, the flux and the measurement number. It also contains a thumbnail image preview of the respective measurement.</p>"},{"location":"exploringwebsite/sourcedetail/#two-epoch-node-graph","title":"Two-epoch Node Graph","text":"<p>The node graph is a visual representation of what two-epoch pairings have significant variability metric values. If an epoch pairing is significant then they are joined by a line on the graph. Hovering over the line will display the pair metrics for the selected flux type (peak or integrated) and highlight the epoch pairing on the light curve plot.</p>"},{"location":"exploringwebsite/sourcedetail/#external-search-results-table","title":"External Search Results Table","text":"<p>This table shows the result of a query to the SIMBAD, NED, and TNS services for astronomical sources within 1 arcmin of the source location. Along with the name and coordinate of the matches, the on-sky separation between the source is shown along with the source type.</p> <p></p>"},{"location":"exploringwebsite/sourcedetail/#user-comments-tags","title":"User Comments &amp; Tags","text":"<p>Users are able to read and post comments on a source using this form, in addition to adding and removing tags, see Source Tags and Favourites.</p>"},{"location":"exploringwebsite/sourcedetail/#js9-viewer-postage-stamps","title":"JS9 Viewer Postage Stamps","text":"<p>JS9 website.</p> <p>More JS9 viewers are used to show the postage stamp FITS images of the measurements that are associated with the source, loaded from their respective image FITS files. The number of cutouts to display is configurable with the <code>MAX_CUTOUT_IMAGES</code> setting: only the first <code>MAX_CUTOUT_IMAGES</code> measurements will be displayed as cutout images. A warning will be displayed if the number of displayed cutouts has been truncated. Refer to the pipeline configuration documentation for more information.</p> <p>Note</p> <p>If the image data is removed from its location when the pipeline run was processed the JS9 viewer will no longer work.</p> <p></p>"},{"location":"exploringwebsite/sourcedetail/#source-measurements-table","title":"Source Measurements Table","text":"<p>This table displays the measurements that are associated with the source. The detail page for the measurement can be reached by clicking the name of the respective measurement.</p> <p></p>"},{"location":"exploringwebsite/sourcedetail/#related-sources-table","title":"Related Sources Table","text":"<p>This table displays the sources that are a relation of the source in question. For further information refer to the Relations section in the association documentation.</p>"},{"location":"exploringwebsite/sourcequery/","title":"Source Query","text":"<p>This page details the Source Query interface.</p>"},{"location":"exploringwebsite/sourcequery/#query-options","title":"Query Options","text":"<p>Users can filter and query the sources currently in the database by using the form located on this page. </p> <p>The form is submitted by clicking the blue :fontawesome-solid-search: button, the red  button  will reset the form by removing all entered values. Once the form is submitted the results are dynamically updated in the results table below the form (i.e. on the same page).</p> <p>The following sections provide further details on the form.</p> <p></p>"},{"location":"exploringwebsite/sourcequery/#data-source","title":"Data Source","text":"<p>Here a specific pipeline run can be selected from a dropdown list to filter sources to only those from the chosen run. By default sources from all runs are shown.</p> <p>Note</p> <p>Only successfully completed pipeline runs are available to select. This rule also applies when <code>all</code> is selected.</p>"},{"location":"exploringwebsite/sourcequery/#cone-search","title":"Cone Search","text":"<p>Users can choose whether to input their coordinates directly or use the object name resolver to attempt to fetch the coordinates.</p>"},{"location":"exploringwebsite/sourcequery/#manual-input","title":"Manual Input","text":"<p>The format of the coordinates should be in a standard format that is recognised by astropy, for example:</p> <ul> <li><code>21 29 45.29 -04 29 11.9</code></li> <li><code>21:29:45.29 -04:29:11.9</code></li> <li><code>322.4387083 -4.4866389</code></li> </ul> <p>Galactic coordinates can also be entered by selecting <code>Galactic</code> from the dropdown menu that is set to <code>ICRS</code> by default.  Feedback will be given immediately whether the coordinates are valid, as shown in the screenshots below:</p> <p> </p> <p>Once the coordinates have been entered the radius value must also be specified as shown in the screenshot above. Use the dropdown menu to change the radius unit to be <code>arcsec</code>, <code>arcmin</code> or <code>deg</code>.</p>"},{"location":"exploringwebsite/sourcequery/#name-resolver","title":"Name Resolver","text":"<p>To use the name resolver, the name of the source should be entered into the <code>Object Name</code> field (e.g. <code>PSR J2129-04</code>), select the name resolver service and then click the <code>Resolve</code> button. The coordinates will then be automatically filled on a successful match.</p> <p></p> <p>if no match is found then this will be communicated by the form as below:</p> <p></p>"},{"location":"exploringwebsite/sourcequery/#table-filters","title":"Table Filters","text":"<p>This section of the form allows the user to place thresholds and selections on specific metrics of the sources.  Please refer to the Source Statistics page for details on the different metrics. There are also tooltips located on the form to offer explanations.</p> <p>The following options are not standard source metrics:</p>"},{"location":"exploringwebsite/sourcequery/#include-and-exclude-tags","title":"Include and Exclude Tags","text":"<p>Users can attach tags to sources (see Source Tags and Favourites) and here tags can be selected to include or exclude in the source search.</p>"},{"location":"exploringwebsite/sourcequery/#source-selection","title":"Source Selection","text":"<p>Here specific sources can be searched for by entering the source names, or source database <code>id</code> values, in a comma-separated list. For example:</p> <ul> <li><code>J011816.05-730747.77,J011816.05-730747.77,J213221.21-040900.42</code></li> <li><code>1031,1280,52</code></li> </ul> <p>are valid entries to this search field. Use the dropdown menu to declare whether <code>name</code> (default) or <code>id</code> values are being searched.</p>"},{"location":"exploringwebsite/sourcequery/#results-table","title":"Results Table","text":"<p>Located directly below the form is the results table which is dynamically updated once the form is submitted. The full detail page of a specific source can be accessed by clicking on the source name in the table.  Explanation of the table options can be found in the DataTables section. The <code>Go to \u03b7-V analysis</code> button will launch the \u03b7-V analysis page for the sources contained in the query results. Please refer to the Source \u03b7-V Analysis Page section for the full details of this feature.</p> <p>Note</p> <p>The <code>Go to \u03b7-V analysis</code> button is only active after a query has been performed.</p> <p></p>"},{"location":"exploringwebsite/sourcetagsfavs/","title":"Source Tags and Favourites","text":"<p>Users are able to save a source as a favourite for later reference, in addition to adding <code>tags</code> to sources that can be used in source queries.</p>"},{"location":"exploringwebsite/sourcetagsfavs/#adding-a-source-to-favourites","title":"Adding a Source to Favourites","text":"<p>A source can be added to a user's favourites by:</p> <ol> <li> <p>Selecting the 'star' button at the top of the source detail page as shown below.</p> <p></p> </li> <li> <p>A modal window will open to confirm the saving of the source as a favourite. An optional comment can be entered.</p> <p></p> </li> <li> <p>Select <code>Add to Favourites</code> and a confirmation alert will be shown to signify the source has been added successfully.</p> <p></p> </li> </ol>"},{"location":"exploringwebsite/sourcetagsfavs/#viewing-favourite-sources","title":"Viewing Favourite Sources","text":"<p>A user can access their favourite sources by selecting the <code>Favourite Sources</code> option from the menu when clicking on their username at the top right-hand corner of the page.</p> <p></p> <p>The user will then be navigated to their favourite sources table as shown below.</p> <p></p>"},{"location":"exploringwebsite/sourcetagsfavs/#adding-a-tag","title":"Adding a Tag","text":"<p>Follow these steps to add a tag to a source:</p> <ol> <li>Type the tag to be added into the field tag field. </li> <li>If the tag has already been used it will appear in the dropdown text options and can be selected by clicking the text. To add a new tag, enter the complete text of the new tag and again click the text in the dropdown text. </li> <li>After clicking the text the tag will then show as a bordered tag in the input field.</li> <li>Finally, click the submit button (a comment is optional) and the tag will be saved as shown below. A comment will appear stating the addition of the tag.</li> </ol> <p> </p>"},{"location":"exploringwebsite/sourcetagsfavs/#removing-a-tag","title":"Removing a Tag","text":"<p>Click the <code>x</code> on the tag to remove it and then click the <code>Submit</code> button to save the removal.</p> <p></p>"},{"location":"exploringwebsite/websiteoverview/","title":"Website Overview","text":"<p>This page gives an overview of the pipeline website, with links to main pages on features where appropriate.</p> <p>Refer to Accessing the Pipeline for details on how to access the pipeline instance that is hosted by the VAST collaboration.</p> <p>For admins, refer to the following pages for details of the configuration and set up of the web server: Configuration, Deployment and Web App Admin Usage.</p>"},{"location":"exploringwebsite/websiteoverview/#homepage","title":"Homepage","text":"<p>The homepage contains a welcome message and links to popular pages of the list of pipeline runs, the source query page or the list of images.</p>"},{"location":"exploringwebsite/websiteoverview/#navbar","title":"Navbar","text":"<p>The navbar, shown to the right, acts as the main method in which to navigate around the website. The following sections link to the respective documentation pages explainging the features of each link.</p> <p>Note</p> <p>The admin button on the navbar is only seen when the user is designated as an administrator.</p> <p>Tip</p> <p>The navbar can be collapsed by pressing the menu (or hamburger) button next to it at the top of the page.</p>"},{"location":"exploringwebsite/websiteoverview/#admin","title":"Admin","text":"<p>See the website admin tools page.</p> <p>Allows for admins to manage users, Django Q schedules and the data itself.</p>"},{"location":"exploringwebsite/websiteoverview/#pipeline-runs","title":"Pipeline Runs","text":"<p>See the Pipeline Run Pages doc.</p> <p>Navigates the user to the list of pipeline runs available, which in turn link to the detail page for each respective run.</p>"},{"location":"exploringwebsite/websiteoverview/#sources-query","title":"Sources Query","text":"<p>See the Source Query section.</p> <p>Takes the user to the source query page, where users can search for sources by defining a set of thresholds and feature requirements.  From the results users can also access the detail page for individual sources.</p>"},{"location":"exploringwebsite/websiteoverview/#images","title":"Images","text":"<p>See the Image Pages section.</p> <p>Takes the user to the images page that features a table containing all the images currently held in the database.  From here users can also access the detail page for individual images.</p>"},{"location":"exploringwebsite/websiteoverview/#external-links","title":"External Links","text":"<ul> <li>Documentation: Links to this documentation website.</li> <li>Pipeline Repository: A link to the GitHub pipeline repository.</li> <li>Raise an Issue: A link to open a new issue on the GitHub repository.</li> <li>Start a Discussion: A link to open a new discussion on the GitHub repository.</li> <li>VAST Links<ul> <li>GitHub: A link to the VAST organisation GitHub page.</li> <li>JupyterHub: Links to the VAST hosted JupyterHub instance which includes access to the pipeline results and <code>vast-tools</code>.</li> <li>Website: Links to the VAST collaboration website.</li> <li>Wiki: Links to the VAST Wiki which is hosted on GitHub.</li> </ul> </li> </ul>"},{"location":"gettingstarted/configuration/","title":"Configuration","text":""},{"location":"gettingstarted/configuration/#configuration","title":"Configuration","text":"<p>This section describe how to configure your VAST Pipeline installation.</p>"},{"location":"gettingstarted/configuration/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>The following instructions, will get you started in setting up the database and pipeline configuration.</p> <p>Note</p> <p>The commands given in this section, unless otherwise stated, assume that the current directory is the pipeline root and that your pipeline Python environment has been activated.</p> <ol> <li> <p>Create a database for the pipeline. If you followed the installation process, you will have a PostgreSQL Docker container running on your system. Use the provided script <code>init-tools/init-db.py</code> script to create a new database for the pipeline. As a security precaution, this script will also create a new database user and set the pipeline database owner to this new user.</p> <p>The initialization script requires several input parameters. For usage information, run with the <code>--help</code> option:</p> <pre><code>python init-tools/init-db.py --help\nusage: init-db.py [-h] host port admin-username admin-password username password database-name\n\nInitialize a PostgreSQL database for VAST Pipeline use. Creates a new superuser and creates a new database owned by the new superuser.\n\npositional arguments:\nhost            database host\nport            database port\nadmin-username  database administrator username\nadmin-password  database administrator password\nusername        username for the new user/role to create for the VAST Pipeline\npassword        password for the new user/role to create for the VAST Pipeline\ndatabase-name   name of the new database to create for the VAST Pipeline\n\noptional arguments:\n-h, --help      show this help message and exit\n</code></pre> <p>Fill in the parameters as appropriate for your configuration. If you followed the installation instructions, these would be the details for your PostgreSQL Docker container. Following from the same example in the installation section:</p> <pre><code>python init-tools/init-db.py localhost 55002 postgres &lt;password&gt; vast &lt;vast-user-password&gt; vastdb\n</code></pre> <p>Info</p> <p>Where <code>&lt;password&gt;</code> is the superuser password that was passed to <code>docker run</code>, and <code>&lt;vast-user-password&gt;</code> is a new password of your choice for the new <code>vast</code> database user.</p> <p>You may change the values for the username and database-name, the above is just an example.</p> <p>If everything went well the output should be:</p> <pre><code>Creating new user/role vast ...\nCreating new database vastdb ...\nDone!\n</code></pre> </li> <li> <p>Copy the setting configuration file template and modify it with your desired settings. Please refer to the .env File section on this page for further details about the settings that are set in this file along with their defaults.</p> <pre><code>cp webinterface/.env.template webinterface/.env\n</code></pre> </li> <li> <p>Set the database connection settings in the <code>webinterface/.env</code> file by modifying <code>DATABASE_URL</code> (for URL syntax see this link). For example:</p> <p>.env</p> <pre><code>DATABASE_URL=psql://vast:&lt;vast-user-password&gt;@localhost:55002/vastdb\n</code></pre> <p>Note</p> <p>The connection details are the same that you setup during the installation. The database/user names must not contain any spaces or dashes, so use the underscore if you want, e.g. <code>this_is_my_db_name</code>.</p> </li> <li> <p>Create the pipeline database tables. The <code>createcachetable</code> command creates the cache tables required by DjangoQ.</p> <pre><code>python manage.py migrate\npython manage.py createcachetable\n</code></pre> </li> <li> <p>Create the pipeline data directories. The pipeline has several directories that can be configured in <code>webinterface/.env</code>:</p> <ul> <li><code>PIPELINE_WORKING_DIR</code>: location to store various pipeline output files.</li> <li><code>RAW_IMAGE_DIR</code>: default location that the pipeline will search for input images and catalogues to ingest during a pipeline run. Data inputs can also be defined as absolute paths in a pipeline run configuration file, so this setting only affects relative paths in the pipeline run configuration.</li> <li><code>HOME_DATA_DIR</code>: a path relative to a user's home directory to search for additional input images and catalogues. Intended for multi-user server deployments and unlikely to be useful for local installations. See below for some examples.</li> <li><code>HOME_DATA_ROOT</code>: path to the location of user home directories. Used together with <code>HOME_DATA_DIR</code>. If not supplied, the pipeline will search for the user's home directory using the default OS location. See below for some examples.</li> </ul> <p>.env \u2013 User data configuration examples</p> <p>In the following examples, assume that the user's name is <code>foo</code>.</p> <p><pre><code># HOME_DATA_ROOT=Uncomment to set a custom path to user data dirs\nHOME_DATA_DIR=vast-pipeline-extra-data\n</code></pre> Using the above settings, the pipeline will search for additional input data in the user's home directory as resolved by the OS. e.g. on an Ubuntu system, this would be <code>/home/foo/vast-pipeline-extra-data</code>.</p> <p><pre><code>HOME_DATA_ROOT=/data/home\nHOME_DATA_DIR=vast-pipeline-extra-data\n</code></pre> Using the above settings, the pipeline will search for additional input data in <code>/data/home/foo/vast-pipeline-extra-data</code>.</p> <p>While the default values for these settings are relative to the pipeline codebase root (i.e. within the repo), we recommend creating these directories outside of the repo and updating the <code>webinterface/.env</code> file appropriately with absolute paths. For example, assuming you wish to create these directories in <code>/data/vast-pipeline</code>:</p> <pre><code>mkdir -p /data/vast-pipeline\nmkdir /data/vast-pipeline/pipeline-runs\nmkdir /data/vast-pipeline/raw-images\n</code></pre> <p>and update the <code>webinterface/.env</code> file with:</p> <p>.env</p> <pre><code>PIPELINE_WORKING_DIR=/data/vast-pipeline/pipeline-runs\nRAW_IMAGE_DIR=/data/vast-pipeline/raw-images\n</code></pre> </li> </ol>"},{"location":"gettingstarted/configuration/#env-file","title":".env File","text":"<p>The <code>.env</code> file contains various top-level settings that apply to Django, authentication and the running of the pipeline itself. Shown below is the <code>.env.template</code> file which is provided to be able to copy in step 3 above.</p> <p>.env.template</p> <pre><code># Django\nDEBUG=True\nSECRET_KEY=FillMeUPWithSomeComplicatedString\n# see https://django-environ.readthedocs.io/en/latest/#tips\nDATABASE_URL=psql://FILLMYUSER:FILLMYPASSWORD@FILLMYHOST:FILLMYPORT/FILLMYDBNAME\n# BASE_URL=this for append a base url in a production deployment\nSTATIC_ROOT=./staticfiles/\nSTATIC_URL=/static/\n# STATICFILES_DIRS= uncomment and fill to use\n# EXTRA_APPS= uncomment and fill to use\n# EXTRA_MIDDLEWARE= uncomment and fill to use\nALLOWED_HOSTS=localhost\n\n# Github Authentication\nGITHUB_AUTH_TYPE='org'\nSOCIAL_AUTH_GITHUB_KEY=fillMeUp\nSOCIAL_AUTH_GITHUB_SECRET=fillMeUp\nSOCIAL_AUTH_GITHUB_ORG_NAME=fillMeUp\nSOCIAL_AUTH_GITHUB_ADMIN_TEAM=fillMeUp\n\n# External APIs\n# TNS_API_KEY= uncomment and fill to use\n# TNS_USER_AGENT= uncomment and fill to use\n\n# Pipeline\nPIPELINE_WORKING_DIR=pipeline-runs\nFLUX_DEFAULT_MIN_ERROR=0.001\nPOS_DEFAULT_MIN_ERROR=0.01\nRAW_IMAGE_DIR=raw-images\nHOME_DATA_DIR=vast-pipeline-extra-data\n# HOME_DATA_ROOT=Uncomment to set a custom path to user data dirs\n# PIPELINE_MAINTAINANCE_MESSAGE=Uncomment and fill to show\nMAX_PIPELINE_RUNS=3\nMAX_PIPERUN_IMAGES=200\nMAX_CUTOUT_IMAGES=30\n\n# Q_CLUSTER_TIMEOUT=86400\n# Q_CLUSTER_RETRY=86402\n# Q_CLUSTER_MAX_ATTEMPTS=1\n\nETA_V_DATASHADER_THRESHOLD=20000\n</code></pre> <p>The available settings are grouped into 4 distinct categories:</p>"},{"location":"gettingstarted/configuration/#django","title":"Django","text":"<p>These settings are standard Django settings that are commonly set in the <code>settings.py</code> file of Django projects.  Please see this page in the Django documentation for explanations on their meaning. Multiple entries for settings such as <code>EXTRA_APPS</code> or <code>EXTRA_MIDDLEWARE</code> can be entered as comma-separated strings like the following example:</p> <p>.env</p> <pre><code>EXTRA_APPS=django_extensions,debug_toolbar\n</code></pre>"},{"location":"gettingstarted/configuration/#github-authentication","title":"GitHub Authentication","text":"<p>The settings in this section control the GitHub organization authentication method.  Please refer to the Python Social Auth documentation for descriptions of the required settings.</p> <p>Note</p> <p>By default the pipeline is set up for authentication using GitHub organizations. Note that switching to teams will require changes to <code>settings.py</code>.  Please refer to the instructions in the Python Social Auth documentation.</p>"},{"location":"gettingstarted/configuration/#external-apis","title":"External APIs","text":"<p>The pipeline website interface supports querying some external APIs, e.g. SIMBAD, NED, VizieR, TNS. Some of these APIs require authentication which are described below.</p>"},{"location":"gettingstarted/configuration/#transient-name-server-tns","title":"Transient Name Server (TNS)","text":"<p>If you wish to enable TNS cone search results on the source detail page, you must first obtain an API key for TNS.</p> <ol> <li>Create a TNS account at https://www.wis-tns.org/user/register if you do not already have one.</li> <li>Once logged in, create a bot by navigating to https://www.wis-tns.org/bots and clicking \"Add bot\" near the top of the table.</li> <li>Fill in the create bot form. Ensure that you select \"Create new API key\".</li> <li> <p>Securely store the API key and paste it into your pipeline <code>webinterface/.env</code> file under <code>TNS_API_KEY</code>.</p> <p>Warning</p> <p>Do not lose the API key! It is not possible to retrieve it again past this point.</p> </li> <li> <p>Navigate to your account page on TNS and copy the User-Agent specification. Paste it into your pipeline <code>webinterface/.env</code> file under <code>TNS_USER_AGENT</code>.</p> <p>webinterface/.env</p> <pre><code>TNS_USER_AGENT='tns_marker{\"tns_id\": 0000, \"type\": \"user\", \"name\": \"your_username\"}'\n</code></pre> </li> </ol>"},{"location":"gettingstarted/configuration/#pipeline","title":"Pipeline","text":"<p>These settings apply to various aspects of the VAST pipeline itself. The table below provides descriptions of each setting.</p> Setting  Default Value Description <code>PIPELINE_WORKING_DIR</code> pipeline-runs The name of the working directory where pipeline run directories are created. The pipeline location acts as the root directory. <code>FLUX_DEFAULT_MIN_ERROR</code> 0.001 In the event a measurement is ingested with a flux error of 0 from Selavy, the error is replaced with this default value (mJy). <code>POS_DEFAULT_MIN_ERROR</code> 0.01 In the event a measurement is ingested with an positional error of 0 from Selavy, the error is replaced with this default value (arcsec). <code>RAW_IMAGE_DIR</code> raw-images Directory where the majority of raw ASKAP FITS images are expected to be stored. This directory is scanned to provide user with an image list when configuration a job using the website interface. <code>HOME_DATA_DIR</code> vast-pipeline-extra-data Directory relative to the user's home directory that contains additional input images and catalogues. Safe to ignore if you don't intend to use this functionality. <code>HOME_DATA_ROOT</code> Disabled Path to directory that contains user's home directories. Enable by uncommenting and setting the desired path. If left disabled (commented), the pipeline will assume the OS default home directory location. <code>PIPELINE_MAINTAINANCE_MESSAGE</code> Disabled The message to display at the top of the webserver. See image below this table for an example. Comment out the setting to disable. <code>MAX_PIPELINE_RUNS</code> 3 The allowed maximum number of concurrent pipeline runs. <code>MAX_PIPERUN_IMAGES</code> 200 The allowed maximum number of images in a single pipeline run (non-admins). <code>MAX_CUTOUT_IMAGES</code> 30 The maximum number of cutout images to display on the source detail pages. The first <code>MAX_CUTOUT_IMAGES</code> cutouts are displayed, in temporal order. <code>Q_CLUSTER_TIMEOUT</code> 86400 Number of seconds a Django-Q cluster worker may spend on a task before it is terminated. See the Django-Q documentation. <code>Q_CLUSTER_RETRY</code> 86402 Number of seconds a Django-Q broker will wait for a cluster to finish a task before it's presented again. See the Django-Q documentation. <code>Q_CLUSTER_MAX_ATTEMPTS</code> 1 Number of times a failed task is retried. See the Django-Q documentation. <code>ETA_V_DATASHADER_THRESHOLD</code> 20000 The number of datapoints above which the eta-V plot uses datashader to plot the non-threshold distribution."},{"location":"gettingstarted/configuration/#maintenance-message-example","title":"Maintenance Message Example","text":"<p>.env</p> <pre><code>PIPELINE_MAINTAINANCE_MESSAGE=This website is subject to rapid changes which may result in data loss and may go offline with minimal warning. Please be mindful of usage.\n</code></pre> <p></p>"},{"location":"gettingstarted/configuration/#authentication","title":"Authentication","text":"<p>The pipeline supports two authentication methods: GitHub Organizations, intended to multi-user server deployments; and local Django administrator. For a single-user local installation, we recommend creating a Django superuser account.</p>"},{"location":"gettingstarted/configuration/#github-organizations","title":"GitHub Organizations","text":"<p>Please refer to the Python Social Auth documentation for a complete description on this authentication method and how to set up the GitHub app used for authentication. All settings are entered into the <code>.env</code> file as detailed in the above section.</p>"},{"location":"gettingstarted/configuration/#django-superuser","title":"Django superuser","text":"<p>Create a Django superuser account with the following command and follow the interactive prompts.</p> <pre><code>python manage.py createsuperuser\n</code></pre> <p>This account can be used to log into the Django admin panel once the webserver is running (see Starting the Pipeline Web App) by navigating to https://localhost:8000/pipe-admin/. Once logged in, you will land on the Django admin page. Navigate back to the pipeline homepage http://localhost:8000/ and you should be authenticated.</p>"},{"location":"gettingstarted/configuration/#data-exploration-via-django-web-server","title":"Data Exploration via Django Web Server","text":"<p>You can start the web app/server via the instructions provided in Starting the Pipeline Web App.</p>"},{"location":"gettingstarted/deployment/","title":"Deployment","text":""},{"location":"gettingstarted/deployment/#production-system","title":"Production System","text":"<p>This section describes a simple deployment without using Docker containers, assuming the use of WhiteNoise to serve the static files. It is possible to serve the static files using other methods (e.g. Nginx). And in the future it is possible to upgrade the deployment stack using Docker container and Docker compose (we foresee 3 main containers: Django, Dask and Traefik/Nginx). We recommend in any case reading Django deployment documentation for general knowledge.</p> <p>Note</p> <p>We assume deployment to a UNIX server.</p> <p>The following steps describes how to set up the Django side of the production deployment, and can be of reference for a future Dockerization. They assumed you have <code>SSH</code> access to your remote server and have <code>sudo</code> priviledges.</p>"},{"location":"gettingstarted/deployment/#web-app-deployment","title":"Web App Deployment","text":"<ol> <li> <p>Clone the repo in a suitable path, e.g. <code>/opt/</code>.</p> <pre><code>$ cd /opt &amp;&amp; sudo git clone https://github.com/askap-vast/vast-pipeline\n</code></pre> </li> <li> <p>Follow the Installation Instructions. We recommend installing the Python virtual environment under the pipeline folder.</p> <pre><code>$ cd /opt/vast-pipeline &amp;&amp; virtualenv -p python3 pipeline_env\n</code></pre> </li> <li> <p>Configure your <code>.env</code> files with all the right settings.</p> </li> <li> <p>Check that your server is running fine by changing <code>DEBUG = True</code> in the <code>.env</code> file.</p> </li> <li> <p>Run Django deployment checklist command to see what are you missing. It is possible that some options are turned off, as implemented in the reverse proxy or load balancer of your server (e.g. <code>SECURE_SSL_REDIRECT = False</code> or not set, assumes your reverse proxy redirect HTTP to HTTPS).</p> <pre><code>(pipeline_env)$ ./manage.py check --deploy\n</code></pre> </li> <li> <p>Build up the static and fix url in JS9:</p> <pre><code>(pipeline_env)$ cd /opt/vast-pipeline &amp;&amp; npm ci &amp;&amp; npm start \\\n    &amp;&amp; npm run js9staticprod &amp;&amp; ./manage.py collectstatic -c --noinput\n</code></pre> </li> <li> <p>Set up a unit/systemd file as recommended in Gunicorn docs (feel free to use the socket or an IP and port). An example of command to write in the file is (assuming a virtual environment is installed in <code>venv</code> under the main pipeline folder):</p> <p><pre><code>ExecStart=/opt/vast-pipeline/venv/bin/gunicorn -w 3 -k gevent \\\n    --worker-connections=1000 --timeout 120 --limit-request-line 6500 \\\n    -b 127.0.0.1:8000 webinterface.wsgi\n</code></pre> NOTE: (for future development) the <code>--limit-request-line</code> parameter needs to be adjusted for the actual request length as that might change if more parameters are added to the query.</p> </li> <li> <p>Finalise the installation of the unit file. Some good instructions on where to put, link and install the unit file are described in the Jupyter Hub docs</p> </li> </ol>"},{"location":"gettingstarted/deployment/#extra-services-deployment","title":"Extra Service(s) Deployment","text":""},{"location":"gettingstarted/deployment/#django-q","title":"Django Q","text":"<p>In order to run a pipeline run from the Web App, the <code>Django Q</code> process needs to be started and managed as a service by the OS. In order to do so we recommend building a unit/systemd file to manage the <code>Django Q</code> process, in a similar way of the <code>gunicorn</code> process (following the Jupyter Hub docs):</p> <pre><code>...\nWorkingDirectory=/opt/vast-pipeline\nExecStart=/opt/vast-pipeline/venv/bin/python manage.py qcluster\n...\n</code></pre> <p>Tip</p> <p>In the examples above, the Python virtual enviroment used by the pipeline is installed in the <code>venv</code> folder under the cloned repository.</p>"},{"location":"gettingstarted/deployment/#security","title":"Security","text":"<p>By default the settings file has some security parameters that are set when you run the web app in production (<code>DEBUG = False</code>), but you can read more in the Django documentation or in this blog post in which they explain how to get an A+ rating for your web site.</p>"},{"location":"gettingstarted/installation/","title":"Installation","text":""},{"location":"gettingstarted/installation/#installation","title":"Installation","text":"<p>This document provides instructions on installing the VAST Pipeline for local use.</p> <p>The VAST Pipeline consists of 3 main components that require installation:</p> <ol> <li>a PostgreSQL database,</li> <li>a Django application,</li> <li>a front-end website.</li> </ol> <p>The instructions have been tested on Debian/Ubuntu and macOS.</p>"},{"location":"gettingstarted/installation/#postgresql","title":"PostgreSQL","text":"<p>We recommend using a Docker container for the database rather than installing the database system-wide.</p> <p>Steps:</p> <ol> <li> <p>Install Docker. Refer to the official documentation, and for Ubuntu users to this. Remember to add your user account to the <code>docker</code> group official docs, by running:</p> <pre><code>sudo groupadd docker\nsudo usermod -aG docker $USER\n</code></pre> </li> <li> <p>Create a PostgreSQL container. The VAST Pipeline requires a PostgreSQL database with the Q3C plugin to enable special indexing on coordinates and fast cone-search queries. We have prepared a Docker image based on the latest PostgreSQL image that includes Q3C . Start a container using this image by running the command below, replacing <code>&lt;container-name&gt;</code> with a name of your choice (e.g. vast-pipeline-db) and <code>&lt;password&gt;</code> with a password of your choice which will be set for the default <code>postgres</code> database superuser account. <pre><code>docker run --name &lt;container-name&gt; --env POSTGRES_PASSWORD=&lt;password&gt; --publish-all --detach ghcr.io/marxide/postgres-q3c:latest\n</code></pre> <p>The <code>--publish-all</code> option will make the PostgreSQL server port 5432 in the container accessible on a random available port on your system (the host). The <code>--detach</code> option instructs Docker to start the container in the background rather than taking over your current shell. Verify that the container is running and note the host port that <code>5432/tcp</code> is published on by running <code>docker ps</code>, e.g. in the example below, the host port is <code>55002</code>.</p> <pre><code>docker ps\nCONTAINER ID   IMAGE                                 COMMAND                  CREATED         STATUS         PORTS                     NAMES\n8ff553add2ed   ghcr.io/marxide/postgres-q3c:latest   \"docker-entrypoint.s\u2026\"   4 seconds ago   Up 3 seconds   0.0.0.0:55002-&gt;5432/tcp   vast-pipeline-db\n</code></pre> <p>The database server should now be running in a container on your machine.</p> <p>Tip</p> <p>To stop the database server, simply stop the container with the following command</p> <pre><code>docker stop &lt;container-name or container-id&gt;\n</code></pre> <p>You can start an existing stopped container with the following command</p> <pre><code>docker start &lt;container-name or container-id&gt;\n</code></pre> <p>Note that <code>docker run</code> and <code>docker start</code> are not the same. <code>docker run</code> will create and start a container from an image; <code>docker start</code> will start an existing stopped container. If you have previously created a VAST Pipeline database container and you wish to reuse it, you want to use <code>docker start</code>. You will likely need to restart the container after a system reboot.</p>"},{"location":"gettingstarted/installation/#python-environment","title":"Python Environment","text":"<p>We strongly recommend installing the VAST Pipeline in an isolated virtual environment (e.g. using Miniconda, Virtualenv, or venv). This will keep the rather complex set of dependencies separated from the system-wide Python installation.</p> <ol> <li> <p>Create a new Python environment using your chosen virtual environment manager and activate it. For example, Miniconda users should run the following command, replacing <code>&lt;environment-name&gt;</code> with an appropriate name (e.g. pipeline-env):</p> <pre><code>conda create --name &lt;environment-name&gt; python=3.8\nconda activate &lt;environment-name&gt;\n</code></pre> <p>Note</p> <p>All further installation instructions will assume you have activated your new virtual environment. Your environment manager will usually prepend the virtual environment name to the shell prompt, e.g.</p> <pre><code>(pipeline-env)$ ...\n</code></pre> </li> <li> <p>Clone the pipeline repository https://github.com/askap-vast/vast-pipeline and change into the repo directory.</p> <pre><code>git clone https://github.com/askap-vast/vast-pipeline.git\ncd vast-pipeline\n</code></pre> <p>Warning</p> <p>Do not change the the repo folder name, e.g. <code>git clone https://github.com/askap-vast/vast-pipeline.git my-pipeline-local-dev</code></p> </li> <li> <p>(Optional) Checkout the version you want to install. Currently, the repo will have cloned the latest code from the master branch. If you require a specific version, checkout the appropriate version tag into a new branch e.g. for version 0.2.0</p> <pre><code>git checkout -b &lt;new-branch-name&gt; 0.2.0\n</code></pre> </li> <li> <p>Install non-Python dependencies. Some of the Python dependencies required by the pipeline depend on some non-Python libraries. These can also be installed by Miniconda, otherwise they are best installed using an appropriate package manager for your operating system e.g. <code>apt</code> for Debian/Ubuntu, <code>dnf</code> for RHEL 8/CentOS 8, Homebrew for macOS. The dependencies are:</p> MinicondaDebian/UbuntuRHEL/CentOSHomebrew <ul> <li>libpq</li> <li>graphviz</li> </ul> <p>Both are available on the conda-forge channel. They are also specified in the environment file <code>requirements/environment.yml</code> which can be used to install the required packages into an activated conda environment with the following command</p> <pre><code>conda env update -f requirements/environment.yml\n</code></pre> <ul> <li>libpq-dev</li> <li>libgraphviz-dev</li> </ul> <ul> <li>libpq-devel</li> <li>graphviz-devel</li> </ul> <p>CentOS users</p> <p>You may need to enable the PowerTools repository to install <code>graphviz-devel</code>.</p> <pre><code>dnf install dnf-plugins-core\ndnf config-manager --set-enabled powertools\n</code></pre> <ul> <li>libpq</li> <li>graphviz</li> </ul> </li> <li> <p>Install the pipeline and it's Python dependencies.</p> <pre><code>pip install .\n</code></pre> <p>Warning</p> <p>Don't forget the <code>.</code> at the end of the above command. It instructs <code>pip</code> that the root directory of the package to install is the current directory.</p> <p>Tip</p> <p>If you are intending to deploy an instance of the pipeline onto a server, you may also want to install the recommended production extras with <code>pip install .[prod]</code>. However, note that these are recommendations only and there are other alternative packages that may work just as well.</p> <p>Tip</p> <p>If you intend to contribute to development of the pipeline, you will need the Python dependency management tool Poetry. See the development guidelines.</p> </li> </ol>"},{"location":"gettingstarted/installation/#front-end-assets-quickstart","title":"Front-End Assets Quickstart","text":"<p>In order to install and compile the front-end website assets (modules like js9 and bootstrap, as well as minification of JS and CSS files) you need a recent version of NodeJS installed.</p>"},{"location":"gettingstarted/installation/#installation-of-nodejs","title":"Installation of NodeJS","text":"<p>If you are using Miniconda and installed the <code>requirements/environment.yml</code> file as shown above, then NodeJS is already installed. Otherwise, we recommend following the instructions on the NodeJS downloads page for your OS (there are many installation options).</p>"},{"location":"gettingstarted/installation/#setting-up-the-front-end-assets","title":"Setting up the front-end assets","text":"<p>In order to set up the front end assets, run:</p> <pre><code>npm ci &amp;&amp; npm start\n</code></pre> <p>Note</p> <p>Ensure you are still in the root of the repo before running the command above. The <code>npm ci</code> command (\"clean install\") will remove all previous node modules and install all the dependencies from scratch. The <code>npm start</code> command will run the default <code>gulp</code> \"task\" which, among other things, compiles Sass into CSS, minifies CSS and JS files, and copies these files into the <code>static/vendor</code> folder. For more details of compilation of frontend assets (e.g. single tasks), and front-end developement set up read the Front End Developing Guidelines.</p> <p>Bug</p> <p>When <code>npm start</code> or <code>npm run start</code> was run in a Ubuntu 20.04 LTS (containerised environment), for some unknown reasons, both commands failed with the following error.</p> <pre><code>[12:48:19] 'js9Make' errored after 7.67 ms\n[12:48:19] Error: spawn make ENOENT\n    at Process.ChildProcess._handle.onexit (internal/child_process.js:267:19)\n    at onErrorNT (internal/child_process.js:469:16)\n    at processTicksAndRejections (internal/process/task_queues.js:84:21)\n[12:48:19] 'default' errored after 2.63 s\nnpm ERR! code ELIFECYCLE\nnpm ERR! errno 1\nnpm ERR! vast-pipeline@99.99.99-dev start: `gulp default`\nnpm ERR! Exit status 1\nnpm ERR!\nnpm ERR! Failed at the vast-pipeline@99.99.99-dev start script.\nnpm ERR! This is probably not a problem with npm. There is likely additional logging output above.\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     /home/vast/.npm/_logs/2020-10-06T01_48_19_215Z-debug.log\n</code></pre> <p>The way around for this issue is unorthodox. The following steps were followed to overcome the issue:</p> <pre><code>cd node_modules/js9/\n./configure\nmake\nmake install\ncd ~/vast-pipeline/  ## (to comeback to the root folder of the project)\nnpm install\n</code></pre> <p>That somehow solved the issue mentioned above.</p> <p>Done! Now go to Vast Pipeline Configuration file to see how to initialize and run the pipeline. Otherwise if you intend on developing the repo open the Contributing and Developing Guidelines file for instructions on how to contribute to the repo.</p>"},{"location":"outputs/coldesc/","title":"Column Descriptions","text":"<p>This page details the columns contained in each output file.</p>"},{"location":"outputs/coldesc/#associations","title":"associations","text":"Column Unit Description <code>source_id</code> n/a The database <code>id</code> of the source for the association. <code>meas_id</code> n/a The database <code>id</code> of the measurement for the association. <code>d2d</code> arcsec The on-sky separation of the measurement to the source at the iteration stage the association was created. <code>dr</code> n/a The de Ruiter radius of the measurement to the source at the iteration stage the association was created. Will be 0 if de Ruiter assocation is not being used."},{"location":"outputs/coldesc/#bands","title":"bands","text":"Column Unit Description <code>id</code> n/a The database <code>id</code> of the band. <code>name</code> n/a The string name of the band, equal to the frequency value. <code>frequency</code> MHz The band central frequency. <code>bandwidth</code> MHz The bandwidth of the frequency band, will be 0 if not known."},{"location":"outputs/coldesc/#images","title":"images","text":"Column  Unit Description <code>id</code> n/a The database <code>id</code> of the image. <code>band_id</code> n/a The database <code>id</code> of the associated band. <code>skyreg_id</code> n/a The database <code>id</code> of the associated sky region. <code>measurements_path</code> n/a The system path to the measurements parquet file. <code>polarisation</code> n/a The polarisation of the image. <code>name</code> n/a The name of the image, taken from the filename. <code>path</code> n/a The system path to the image FITS file. <code>noise_path</code> n/a The system path to the associated noise image FITS file. <code>background_path</code> n/a The system path to the associated background image FITS file. <code>datetime</code> n/a The date and time of the observation, read from the FITS header. <code>jd</code> days The date and time of the observation in Julian Days. <code>duration</code> s The duration of the observation taken from the FITS header, if available. <code>ra</code> deg The central Right Ascension coordinate of the image. <code>dec</code> deg The central Declination coordinate of the image. <code>fov_bmaj</code> deg The estimated major axis field-of-view value - the <code>radius_pixels</code> multipled by the major axis pixel size. <code>fov_bmin</code> deg The estimated minor axis field-of-view value - the <code>radius_pixels</code> multipled by the minor axis pixel size. <code>physical_bmaj</code> deg The actual major axis on-sky size - the number of pixels on the major axis multiplied by the major axis pixel size. <code>physical_bmin</code> deg The actual minor axis on-sky size - the number of pixels on the minor axis multiplied by the minor axis pixel size. <code>radius_pixels</code> pixels Estimated 'diameter' of the useable image area. <code>beam_bmaj</code> deg The size of the major axis of the image restoring beam. <code>beam_bmin</code> deg The size of the minor axis of the image restoring beam. <code>beam_bpa</code> deg The position angle of the image restoring beam. <code>rms_median</code> mJy/beam The median RMS value derrived from the RMS map. <code>rms_min</code> mJy/beam The minimum RMS value derrived from the RMS map (pixel value). <code>rms_max</code> mJy/beam The maximum RMS value derrived from the RMS map (pixel value)."},{"location":"outputs/coldesc/#measurements","title":"measurements","text":"<p>Tip</p> <p>Some columns are the same as that defined in the Selavy source finder output.</p> Column  Unit Description <code>island_id</code> n/a The Selavy assigned island_id. <code>component_id</code> n/a The Selavy assigned component_id. <code>local_rms</code> mJy The rms value at the location of the measurement. <code>ra</code> deg The right ascension coordinate of the measurement. <code>ra_err</code> deg The error of the right ascension coordinate of the measurement. <code>dec</code> deg The declination coordinate of the measurement. <code>dec_err</code> deg The error of the declination coordinate of the measurement. <code>flux_peak</code> mJy/beam The measured peak flux of the component. <code>flux_peak_err</code> mJy/beam The error of the measured peak flux of the component. <code>flux_int</code> mJy The measured integrated flux of the component. <code>flux_int_err</code> mJy The error of the measured integrated flux of the component. <code>bmaj</code> arcsec The major axis size of the fitted Gaussian (FWHM). <code>err_bmaj</code> deg The error of the major axis size of the fitted Gaussian (FWHM). <code>bmin</code> arcsec The minor axis size of the fitted Gaussian (FWHM). <code>err_bmin</code> deg The error of the minor axis size of the fitted Gaussian (FWHM). <code>pa</code> deg The position angle of the fitted Gaussian (FWHM). <code>err_pa</code> deg The error of the position angle of the fitted Gaussian (FWHM). <code>psf_bmaj</code> arcsec The Selavy deconvolved size of the major axis of the fitted Gaussian. <code>psf_bmin</code> arcsec The Selavy deconvolved size of the minor axis of the fitted Gaussian. <code>psf_pa</code> deg The Selavy deconvolved position angle of the fitted Gaussian. <code>flag_c4</code> n/a Selavy flag denoting whether the component is considered formally bad (doesn't meet chi-squared criterion). <code>chi_squared_fit</code> n/a The Selavy quality of the fit. <code>spectral_index</code> n/a The fitted Selavy spectral index of the component. <code>spectral_index_from_TT</code> n/a Selavy flag to denote if the spectral index has been derived from the Taylor-term images (<code>True</code>). <code>has_siblings</code> n/a Selavy flag to denote whether the component is one of many fitted to the same island. <code>image_id</code> n/a The database <code>id</code> of the image the measurement is from. <code>time</code> n/a The date and time of observation the measurement is from (obtained from the image). <code>name</code> n/a The string name of the measurement. <code>snr</code> n/a The signal-to-noise ratio of the measurement. <code>compactness</code> n/a The compactness of the measurement (<code>flux_int</code>/<code>flux_peak</code>). <code>ew_sys_err</code> deg The systematic right ascension error assigned to the measurement. <code>ns_sys_err</code> deg The systematic declination error assigned to the measurement. <code>error_radius</code> deg The pipeline estimated error radius of the measurement. <code>uncertainty_ew</code> deg Total RA positional error of the measurement. <code>uncertainty_ns</code> deg Total Dec positional error of the measurement. <code>weight_ew</code> deg\\(^{-1}\\) The weight of the RA error (1/e). <code>weight_ns</code> deg\\(^{-1}\\) The weight of the Dec error (1/e). <code>forced</code> n/a Flag to denote whether the measurement is produced from the forced fitting procedure (<code>True</code>). <code>flux_int_isl_ratio</code> n/a The ratio of the measurements integrated flux to the total island integrated flux. <code>flux_peak_isl_ratio</code> n/a The ratio of the measurements peak flux to the total island peak flux. <code>id</code> n/a The database <code>id</code> of the measurement."},{"location":"outputs/coldesc/#measurement_pairs","title":"measurement_pairs","text":"Column Unit Description <code>meas_id_a</code> n/a The database <code>id</code> of measurement <code>a</code> of the pair. <code>meas_id_b</code> n/a The database <code>id</code> of measurement <code>b</code> of the pair. <code>flux_int_a</code> mJy The integrated flux of measurement <code>a</code> of the pair. <code>flux_int_err_a</code> mJy The error of the integrated flux of measurement <code>a</code> of the pair. <code>flux_peak_a</code> mJy/beam The peak flux of measurement <code>a</code> of the pair. <code>flux_peak_err_a</code> mJy/beam The error of the peak flux of measurement <code>a</code> of the pair. <code>image_name_a</code> n/a The <code>image name</code> of measurement <code>a</code> of the pair. <code>flux_int_b</code> mJy The integrated flux of measurement <code>b</code> of the pair. <code>flux_int_err_b</code> mJy The error of the integrated flux of measurement <code>b</code> of the pair. <code>flux_peak_b</code> mJy/beam The peak flux of measurement <code>b</code> of the pair. <code>flux_peak_err_b</code> mJy/beam The error of the peak flux of measurement <code>b</code> of the pair. <code>image_name_b</code> n/a The <code>image name</code> of measurement <code>b</code> of the pair. <code>vs_peak</code> n/a The pair \\(V_s\\) value using the peak flux. <code>vs_int</code> n/a The pair \\(V_s\\) value using the integrated flux. <code>m_peak</code> n/a The pair \\(m\\) value using the peak flux. <code>m_int</code> n/a The pair \\(m\\) value using the integrated flux. <code>source_id</code> n/a The database <code>id</code> of the source the pair is associated to."},{"location":"outputs/coldesc/#relations","title":"relations","text":"Column Unit Description <code>from_source_id</code> n/a The database <code>id</code> of the first source in the relation pair. <code>to_source_id</code> n/a The database <code>id</code> of the second source in the relation pair."},{"location":"outputs/coldesc/#skyregions","title":"skyregions","text":"Column Unit Description <code>id</code> n/a The database <code>id</code> of the sky region. <code>centre_ra</code> deg The right ascension value of the sky region central coordinate. <code>centre_dec</code> deg The declination value of the sky region central coordinate. <code>width_ra</code> deg The width of the area covered by the sky region. <code>width_dec</code> deg The height of the area covered by the sky region. <code>xtr_radius</code> deg The hypotenuse radius of the sky region. <code>x</code> rad The central cartesian x coordinate of the sky region. <code>y</code> rad The central cartesian y coordinate of the sky region. <code>z</code> rad The central cartesian z coordinate of the sky region."},{"location":"outputs/coldesc/#sources","title":"sources","text":"<p>Note</p> <p>The index column of the sources parquet is set to the database <code>id</code> of the source.</p> Column  Unit Description <code>n_meas</code> n/a The total number of measurements associated to the source (selavy and forced). <code>n_meas_sel</code> n/a The total number of selavy measurements associated to the source. <code>n_meas_forced</code> n/a The total number of forced measurements associated to the source. <code>n_sibl</code> n/a The total number of measurements that have a <code>has_sibling</code> value of <code>True</code>. <code>n_rel</code> n/a The total number of relations the source has. <code>wavg_ra</code> deg The weighted average of the Right Ascension of the measurements, that acts as the source position. <code>wavg_dec</code> deg The weighted average of the Declination of the measurements, that acts as the source position. <code>wavg_uncertainty_ew</code> deg The error of the weighted average right ascension value. <code>wavg_uncertainty_ns</code> deg The error of the weighted average declination value. <code>new</code> n/a Flag to signify the source is classed as a <code>new source</code> (<code>True</code>). <code>new_high_sigma</code> n/a The largest sigma value a new source would have if it was placed at its location in the previous images it was not detected in. See New Sources for more information. Set to 0 for non-new sources. <code>n_neighbour_dist</code> deg The on-sky separation to the nearest source with in the same pipeline run. <code>avg_compactness</code> n/a The average compactness value of the associated measurements. <code>min_snr</code> n/a The minimum signal-to-noise ratio of the associated measurements. <code>max_snr</code> n/a The maximum signal-to-noise ratio of the associated measurements. <code>avg_flux_int</code> mJy The average integrated flux value of the measurements associated to the source (inc. forced measurements). <code>max_flux_int</code> mJy The maximum integrated flux value of the measurements associated to the source (inc. forced measurements). <code>min_flux_int</code> mJy The minimum integrated flux value of the measurements associated to the source (inc. forced measurements). <code>avg_flux_peak</code> mJy/beam The average peak flux value of the measurements associated to the source (inc. forced measurements). <code>max_flux_peak</code> mJy/beam The maximum peak flux value of the measurements associated to the source (inc. forced measurements). <code>min_flux_peak</code> mJy/beam The minimum peak flux value of the measurements associated to the source (inc. forced measurements). <code>min_flux_peak_isl_ratio</code> n/a The minimum ratio of the peak flux to the total island peak flux of the measurements associated to the source. <code>min_flux_int_isl_ratio</code> n/a The minimum ratio of the integrated flux to the total island integrated flux of the measurements associated to the source. <code>v_int</code> n/a The calculated variability \\(V\\) metric using the integrated flux values. See variability statistics. <code>v_peak</code> n/a The calculated variability \\(V\\) metric using the peak flux values. See variability statistics. <code>eta_int</code> n/a The calculated variability \\(\\eta\\) metric using the integrated flux. See variability statistics. <code>eta_peak</code> n/a The calculated variability \\(\\eta\\) metric using the peak flux. See variability statistics. <code>vs_abs_significant_max_peak</code> n/a The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be <code>0</code> if no significant pair. See variability statistics. <code>m_abs_significant_max_peak</code> n/a The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be <code>0</code> if no significant pair. See variability statistics. <code>vs_abs_significant_max_int</code> n/a The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be <code>0</code> if no significant pair. See variability statistics. <code>m_abs_significant_max_int</code> n/a The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be <code>0</code> if no significant pair. See variability statistics."},{"location":"outputs/outputs/","title":"Outputs Overview","text":""},{"location":"outputs/outputs/#outputs-overview","title":"Outputs Overview","text":"<p>This page gives details on the output files that the pipeline writes to disk.</p>"},{"location":"outputs/outputs/#pipeline-run-output-overview","title":"Pipeline Run Output Overview","text":"<p>The output for a pipeline run will be located in the pipeline working directory, which is defined at the pipeline configuration stage (see Pipeline Configuration). A sub-directory will exist for each pipeline run that contains the output products for the run.</p> <p>Note</p> <p>If you do not administrate your system or do not have access to a <code>vast-tools</code> notebook interface, please contact your system admin to confirm the working directory and how to best access the files.</p> <p>The pipeline uses the Apache Parquet file format to write results to disk. Details on how to read these files can be found below in Reading the Outputs.</p> <p>Below is the output structure for a pipeline run named <code>new-test-data</code> when the pipeline run option <code>measurements.write_arrow_files</code> has been set to <code>True</code> and the working directory is named <code>pipeline-runs</code> (see File Details for descriptions):</p> <pre><code>pipeline-runs\n\u251c\u2500\u2500 new-test-data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 associations.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bands.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 config.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 config_prev.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH01_I_cutout_fits.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH05x_I_cutout_fits.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH06x_I_cutout_fits.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH01_I_cutout_fits.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH02_I_cutout_fits.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH03x_I_cutout_fits.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH05x_I_cutout_fits.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH06x_I_cutout_fits.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH01_I_cutout_fits.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH02_I_cutout_fits.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH03x_I_cutout_fits.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH05x_I_cutout_fits.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH06x_I_cutout_fits.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH12_I_cutout_fits.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 images.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 YYYY-MM-DD-HH-MM-SS_log.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 measurements.arrow\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 measurement_pairs.arrow\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 measurement_pairs.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 relations.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 skyregions.parquet\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sources.parquet\n</code></pre>"},{"location":"outputs/outputs/#arrow-files","title":"Arrow Files","text":"<p>Large pipeline runs (hundreds of images) mean that to read the measurements, hundreds of parquet files need to be read in, and can contain millions of rows. This can be slow using libraries such as pandas, and also consumes a lot of system memory. A solution to this is to save all the measurements associated with the pipeline run into one single file in the Apache Arrow format.</p> <p>The library <code>vaex</code> is able to open <code>.arrow</code> files in an out-of-core context so the memory footprint is hugely reduced along with the reading of the file being very fast. The two-epoch measurement pairs are also saved to arrow format due to the same reasons. See Reading with vaex for further details on using <code>vaex</code>.</p> <p>Note</p> <p>At the time of development <code>vaex</code> could not open parquets in an out-of-core context. This will be reviewed in the future if such functionality is added to <code>vaex</code>.</p> <p>To enable the arrow files to be produced, the option <code>measurements.write_arrow_files</code> is required to be set to <code>True</code> in the pipeline run config. Alternatively, the arrow files can be generated after the completion of the run, see the Generating Arrow Files page for full details.</p>"},{"location":"outputs/outputs/#image-data","title":"Image Data","text":"<p>The data for the images ingested into the pipeline is also stored in the pipeline working directory under the subdirectory <code>images</code>:</p> <pre><code>pipeline-runs\n\u251c\u2500\u2500 images\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 VAST_0127-73A_EPOCH01_I_cutout_fits\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 measurements.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 VAST_0127-73A_EPOCH05x_I_cutout_fits\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 measurements.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 VAST_0127-73A_EPOCH06x_I_cutout_fits\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 measurements.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 VAST_2118+00A_EPOCH01_I_cutout_fits\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 measurements.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 VAST_2118+00A_EPOCH02_I_cutout_fits\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 measurements.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 VAST_2118+00A_EPOCH03x_I_cutout_fits\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 measurements.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 VAST_2118+00A_EPOCH05x_I_cutout_fits\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 measurements.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 VAST_2118+00A_EPOCH06x_I_cutout_fits\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 measurements.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 VAST_2118-06A_EPOCH01_I_cutout_fits\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 measurements.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 VAST_2118-06A_EPOCH02_I_cutout_fits\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 measurements.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 VAST_2118-06A_EPOCH03x_I_cutout_fits\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 measurements.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 VAST_2118-06A_EPOCH05x_I_cutout_fits\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 measurements.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 VAST_2118-06A_EPOCH06x_I_cutout_fits\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 measurements.parquet\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 VAST_2118-06A_EPOCH12_I_cutout_fits\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 measurements.parquet\n</code></pre> <p>Here, for each image, the selavy measurements that have been ingested are stored in the parquet format under a subdirectory of the respective image name.</p>"},{"location":"outputs/outputs/#file-details","title":"File Details","text":"File Description <code>associations.parquet</code> Contains the association information between sources and measurements. <code>bands.parquet</code> Contains the information of the bands associated with the pipeline run. <code>config.yaml</code> The pipeline run configuration file. <code>config_prev.yaml</code> The previous pipeline run configuration file used by the add image mode. <code>forced_measurements*.parquet</code> Multiple files that contain the forced measurements extracted from the respective image denoted in the filename. <code>images.parquet</code> Contains the information of the images processed in the pipeline run. <code>YYYY-MM-DD-HH-MM-SS_log.txt</code> The log file of the pipeline run. It is timestamped with the date and time of the run start. <code>measurements.arrow</code> An Apache Arrow format file containing all the measurements associated with the pipeline run (see Arrow Files). <code>measurement_pairs.arrow</code> An Apache Arrow format file containing all the measurement pair metrics (see Arrow Files). <code>measurement_pairs.parquet</code> Contains all the measurement pairs metrics. <code>relations.parquet</code> Contains the relation information between sources. <code>skyregions.parquet</code> Contains the sky region information of the pipeline run. <code>sources.parquet</code> Contains all the sources resulting from teh pipeline run."},{"location":"outputs/usingoutputs/","title":"Using the Outputs","text":"<p>This page gives details on how to open and use the pipeline output files.</p> <p>It is recommended to use <code>pandas</code> or <code>vaex</code> to read the pipeline results from the parquet files. See the sections below for more information on using each library.</p> <p>Note</p> <p>It is also possible to use <code>Dask</code> to read the parquets in an out-of-core context but the general performance can sometimes be poor with many parquet files.  <code>vaex</code> is the preferred out-of-core method.</p> <p>Tip</p> <p>Be sure to look at <code>vast-tools</code>, a ready-made library for exploring pipeline results!</p>"},{"location":"outputs/usingoutputs/#reading-with-pandas","title":"Reading with pandas","text":"<p>pandas documentation.</p> <p>Warning</p> <p><code>pyarrow</code> will be required to open parquets with <code>pandas</code>. We recommend using this instead of <code>fastparquet</code>.</p> <p>To open a parquet using <code>pandas</code> use the <code>read_parquet</code> method:</p> <pre><code>import pandas as pd\n\nsources = pd.read_parquet('pipeline-runs/new-test-data/sources.parquet')\n\nsources.head()\n    n_meas_forced  n_meas  ...  vs_abs_significant_max_int  m_abs_significant_max_int\nid                         ...\n1               0       3  ...                   55.050146                   0.191083\n2               1       3  ...                   29.367098                   0.525999\n3               0       3  ...                    4.388447                   0.199877\n4               1       3  ...                   20.000058                   1.047998\n5               0       3  ...                    0.000000                   0.000000\n\n[5 rows x 31 columns]\n</code></pre> <p>To read multiple parquets at once using <code>pandas</code> a loop must be used:</p> <pre><code>import glob\nimport pandas as pd\n\nfiles = glob.glob(\"pipeline-runs/images/*/measurements.parquet\")\ndata = [pd.read_parquet(f) for f in files]\nmeasurements = pd.concat(data, ignore_index=True)\n</code></pre> <p>Tip</p> <p>If you don't require all the columns you can specify which columes to read using the <code>columns</code> variable.</p> <pre><code>sources = pd.read_parquet('pipeline-runs/new-test-data/sources.parquet', columns=['id', 'n_meas'])\n</code></pre>"},{"location":"outputs/usingoutputs/#reading-with-vaex","title":"Reading with vaex","text":"<p>vaex documentation.</p> <p>Warning</p> <p>vaex is a young project so bugs may be expected along with frequent updates. It has currently been tested with version <code>3.0.0</code>.  Version <code>4.0.0</code> promises opening parquet files in an out-of-core context.</p> <p>Warning</p> <p>Some pipeline <code>parquet</code> format files do not open with vaex 3.0.0. <code>arrow</code> format files should open successfully.</p> <p>A parquet, or arrow file, can be opened using the <code>open()</code> method:</p> <pre><code>import vaex\n\nmeasurements = vaex.open('pipeline-runs/new-test-data/measurements.arrow')\n\nmeasurements.head()\n  #    source  island_id         component_id            local_rms       ra       ra_err       dec      dec_err    flux_peak    flux_peak_err    flux_int    flux_int_err    bmaj     err_bmaj    bmin     err_bmin      pa      err_pa    psf_bmaj    psf_bmin    psf_pa  flag_c4      chi_squared_fit    spectral_index  spectral_index_from_TT    has_siblings      image_id  time                           name                                    snr    compactness    ew_sys_err    ns_sys_err    error_radius    uncertainty_ew    uncertainty_ns    weight_ew    weight_ns  forced      flux_int_isl_ratio    flux_peak_isl_ratio    id\n  0       730  SB00004_island_1  SB00004_component_1a     0.463596  321.902  4.42819e-06  -4.20097  2.2637e-06       307.991         0.4742       425.175        1.04728    20.95  1.08838e-05   12.28  4.32359e-06  108.19  0.00160065       15.58        0       -73.86  False                3516.59               -99  True                      True                     2  2019-08-27 13:38:38.810000000  VAST_2118+00A_SB00004_component_1a  664.352       1.38048    0.000277778   0.000277778     5.05099e-06       0.000277824       0.000277824  1.29557e+07  1.29557e+07  False                 0.651019               0.709182   204\n  1       730  SB00009_island_1  SB00009_component_1a     0.463422  321.902  3.52062e-06  -4.20103  2.45968e-06      318.544         0.472982     349.471        0.87264    21.4   8.50698e-06   12.78  5.46908e-06  107.02  0.0020224         5.72        0        46.03  True                15427.1                -99  True                      True                     3  2019-08-27 18:52:00.556000000  VAST_2118-06A_SB00009_component_1a  687.374       1.09709    0.000277778   0.000277778     4.26887e-06       0.000277811       0.000277811  1.29569e+07  1.29569e+07  False                 0.721399               0.7483     352\n  2       730  SB00006_island_1  SB00006_component_1a     0.627357  321.901  4.58076e-06  -4.20086  2.92861e-06      310.503         0.662503     421.137        1.4171     17.05  1.07873e-05   12.42  6.89559e-06   90.71  0.00438684       10.51        4.24    -82.52  False                4483.74               -99  True                      True                     5  2019-10-29 10:28:07.911000000  VAST_2118+00A_SB00006_component_1a  494.938       1.35631    0.000277778   0.000277778     5.46682e-06       0.000277832       0.000277832  1.2955e+07   1.2955e+07   False                 0.677562               0.721427   670\n  3       730  SB00011_island_1  SB00011_component_1a     0.627496  321.901  3.92144e-06  -4.20087  3.21715e-06      310.998         0.643049     350.901        1.21083    17.06  9.23494e-06   12.43  7.57501e-06   91.19  0.00481863        6.43        0        27.62  False                4405.81               -99  True                      True                     4  2019-10-29 13:39:33.996000000  VAST_2118-06A_SB00011_component_1a  495.618       1.12831    0.000277778   0.000277778     5.05099e-06       0.000277824       0.000277824  1.29557e+07  1.29557e+07  False                 0.677576               0.721816   511\n  4       730  SB00005_island_1  SB00005_component_1a     0.346147  321.901  1.83032e-06  -4.20051  1.71797e-06      299.072         0.342783     288.462        0.579893   14.13  4.37963e-06   12.14  3.97013e-06   65.14  0.00546325        2.86        0         6.42  False                2661.49               -99  True                      True                     7  2019-10-30 09:10:04.340000000  VAST_2118+00A_SB00005_component_1a  864.004       0.964524   0.000277778   0.000277778     2.69987e-06       0.000277791       0.000277791  1.29588e+07  1.29588e+07  False                 0.659887               0.719556   976\n  5       730  SB00010_island_1  SB00010_component_1a     0.347692  321.901  1.97328e-06  -4.20052  1.75466e-06      300.969         0.360198     353.643        0.695754   14.16  4.76862e-06   12.16  3.99059e-06   65.77  0.00546515        6.12        3.95     49.18  False                2412.18               -99  True                      True                     6  2019-10-30 10:11:56.913000000  VAST_2118-06A_SB00010_component_1a  865.62        1.17502    0.000277778   0.000277778     2.56132e-06       0.00027779        0.00027779   1.29589e+07  1.29589e+07  False                 0.664605               0.723765   816\n  6       730  SB00007_island_1  SB00007_component_1a     0.387605  321.901  2.03947e-06  -4.20032  1.77854e-06      317.014         0.392106     332.662        0.701451   14.56  4.96382e-06   11.54  3.99573e-06   64.78  0.00375775        4.81        0        24.08  False                1486.99               -99  True                      True                    10  2020-01-11 05:27:24.605000000  VAST_2118+00A_SB00007_component_1a  817.88        1.04936    0.000277778   0.000277778     2.83165e-06       0.000277792       0.000277792  1.29587e+07  1.29587e+07  False                 0.666924               0.716339  1493\n  7       730  SB00012_island_1  SB00012_component_1a     0.391978  321.901  2.12442e-06  -4.20032  1.81129e-06      318.042         0.404457     365.987        0.770202   14.57  5.18203e-06   11.53  4.0454e-06    65.33  0.00378202        5.75        3.63     46.53  False                1328.58               -99  True                      True                     9  2020-01-11 05:40:11.007000000  VAST_2118-06A_SB00012_component_1a  811.376       1.15075    0.000277778   0.000277778     2.69987e-06       0.000277791       0.000277791  1.29588e+07  1.29588e+07  False                 0.667317               0.717746  1330\n  8       730  SB00008_island_1  SB00008_component_1a     0.432726  321.901  2.98443e-06  -4.20052  2.32201e-06      293.737         0.436863     309.072        0.784657   18.35  7.14713e-06   12.12  5.31097e-06  105.78  0.00261377        4.82        0        18.62  False                2448.82               -99  True                      True                    13  2020-01-12 05:23:07.478000000  VAST_2118+00A_SB00008_component_1a  678.807       1.05221    0.000277778   0.000277778     3.81819e-06       0.000277804       0.000277804  1.29576e+07  1.29576e+07  False                 0.63994                0.725001  1889\n  9       730  SB00013_island_1  SB00013_component_1a     0.437279  321.901  3.14407e-06  -4.20052  2.36161e-06      294.141         0.451346     340.92         0.864347   18.38  7.55055e-06   12.12  5.36009e-06  106.18  0.00262701        6.01        4        51.55  False                2368.93               -99  True                      True                    12  2020-01-12 05:36:03.834000000  VAST_2118-06A_SB00013_component_1a  672.663       1.15903    0.000277778   0.000277778     4.00455e-06       0.000277807       0.000277807  1.29573e+07  1.29573e+07  False                 0.640807               0.72508   1740\n</code></pre> <p>Multiple parquet files can be opened at once using the <code>open_many()</code> method:</p> <pre><code>import glob\nimport vaex\n\nfiles = glob.glob(\"pipeline-runs/images/*/measurements.parquet\")\nmeasurements = vaex.open_many(files)\n</code></pre> <p>Tip</p> <p>You can convert a vaex dataframe to pandas by using the <code>to_pandas_df()</code> method: <pre><code>import vaex\n\nsources = vaex.open('pipeline-runs/new-test-data/sources.parquet')\nsources = sources.to_pandas_df()\n</code></pre></p>"},{"location":"outputs/usingoutputs/#linking-the-results","title":"Linking the Results","text":"<p>The table below shows what parameters act as keys to link data from the different results tables.</p> <p>Tip</p> <p>If loading the measurements via the <code>.arrow</code> file, then the measurements already have the <code>source</code> column in-place.</p> <p>Tip</p> <p>The <code>images.parquet</code> file contains the column <code>measurements_path</code> which can be used to get the filepaths for all the selavy <code>parquet</code> files.</p> Data Column Linked to Column <code>associations.parquet</code> <code>meas_id</code> <code>measurements.parquet</code>, <code>forced_measurements*.parquet</code> <code>id</code> <code>associations.parquet</code> <code>source_id</code> <code>sources.parquet</code> <code>id</code> (index column) <code>measurements.parquet</code>, <code>forced_measurements*.parquet</code> <code>image_id</code> <code>images.parquet</code> <code>id</code> <code>images.parquet</code> <code>band_id</code> <code>bands.parquet</code> <code>id</code> <code>images.parquet</code> <code>skyreg_id</code> <code>skyregions.parquet</code> <code>id</code> <code>measurement_pairs.parquet</code> <code>meas_id_a</code>, <code>meas_id_b</code> <code>measurements.parquet</code>, <code>forced_measurements*.parquet</code> <code>id</code> <code>measurement_pairs.parquet</code> <code>source_id</code> <code>sources.parquet</code> <code>id</code> (index column) <code>relations.parquet</code> <code>from_source_id</code>, <code>to_source_id</code> <code>sources.parquet</code> <code>id</code> (index column)"},{"location":"outputs/usingoutputs/#vast-tools","title":"vast-tools","text":"<p>Link to the <code>vast-tools</code> documentation.</p> <p>VAST has developed a python library called <code>vast-tools</code> that makes the exploration of results from the pipeline simple and efficient, in addition to being designed to be used in a Jupyter Notebook environment. </p> <p>Full details can be found in the documentation linked above, which also includes example notebooks of how to interact with the data.</p>"},{"location":"reference/admin/","title":"admin.py","text":"<p>This module contains the admin classes that are registered with the Django Admin site.</p>"},{"location":"reference/admin/#vast_pipeline.admin.Association","title":"<code>Association</code>","text":"<p>               Bases: <code>Model</code></p> <p>model association between sources and measurements based on some parameters</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class Association(models.Model):\n    \"\"\"\n    model association between sources and measurements based on\n    some parameters\n    \"\"\"\n    source = models.ForeignKey(Source, on_delete=models.CASCADE)\n    meas = models.ForeignKey(Measurement, on_delete=models.CASCADE)\n\n    d2d = models.FloatField(\n        default=0.,\n        help_text='astronomical distance calculated by Astropy, arcsec.'\n    )\n    dr = models.FloatField(\n        default=0.,\n        help_text='De Ruiter radius calculated in advanced association.'\n    )\n\n    def __str__(self):\n        return (\n            f'distance: {self.d2d:.2f}' if self.dr == 0 else\n            f'distance: {self.dr:.2f}'\n        )\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.Band","title":"<code>Band</code>","text":"<p>               Bases: <code>Model</code></p> <p>A band on the frequency spectrum used for imaging. Each image is associated with one band.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class Band(models.Model):\n    \"\"\"\n    A band on the frequency spectrum used for imaging. Each image is\n    associated with one band.\n    \"\"\"\n    name = models.CharField(max_length=12, unique=True)\n    frequency = models.FloatField(\n        help_text='central frequency of band (integer MHz)'\n    )\n    bandwidth = models.FloatField(\n        help_text='bandwidth (MHz)'\n    )\n\n    class Meta:\n        ordering = ['frequency']\n\n    def __str__(self):\n        return self.name\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.Comment","title":"<code>Comment</code>","text":"<p>               Bases: <code>Model</code></p> <p>The model object for a comment.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class Comment(models.Model):\n    \"\"\"\n    The model object for a comment.\n    \"\"\"\n    author = models.ForeignKey(User, on_delete=models.CASCADE)\n    datetime = models.DateTimeField(auto_now_add=True)\n    comment = models.TextField()\n    content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n    object_id = models.PositiveIntegerField()\n    content_object = GenericForeignKey('content_type', 'object_id')\n\n    def get_avatar_url(self) -&gt; str:\n        \"\"\"Get the URL for the user's avatar from GitHub. If the user has\n        no associated GitHub account (e.g. a Django superuser), return the URL\n        to the default user avatar.\n\n        Returns:\n            The avatar URL.\n        \"\"\"\n        social = UserSocialAuth.get_social_auth_for_user(self.author).first()\n        if social and \"avatar_url\" in social.extra_data:\n            return social.extra_data[\"avatar_url\"]\n        else:\n            return static(\"img/user-32.png\")\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.Comment.get_avatar_url","title":"<code>get_avatar_url()</code>","text":"<p>Get the URL for the user's avatar from GitHub. If the user has no associated GitHub account (e.g. a Django superuser), return the URL to the default user avatar.</p> <p>Returns:</p> Type Description <code>str</code> <p>The avatar URL.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>def get_avatar_url(self) -&gt; str:\n    \"\"\"Get the URL for the user's avatar from GitHub. If the user has\n    no associated GitHub account (e.g. a Django superuser), return the URL\n    to the default user avatar.\n\n    Returns:\n        The avatar URL.\n    \"\"\"\n    social = UserSocialAuth.get_social_auth_for_user(self.author).first()\n    if social and \"avatar_url\" in social.extra_data:\n        return social.extra_data[\"avatar_url\"]\n    else:\n        return static(\"img/user-32.png\")\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.CommentableModel","title":"<code>CommentableModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>A class to provide a commentable model.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class CommentableModel(models.Model):\n    \"\"\"\n    A class to provide a commentable model.\n    \"\"\"\n    comment = GenericRelation(\n        Comment,\n        content_type_field=\"content_type\",\n        object_id_field=\"object_id\",\n        related_query_name=\"%(class)s\",\n    )\n\n    class Meta:\n        abstract = True\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.Image","title":"<code>Image</code>","text":"<p>               Bases: <code>CommentableModel</code></p> <p>An image is a 2D radio image from a cube</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class Image(CommentableModel):\n    \"\"\"An image is a 2D radio image from a cube\"\"\"\n    band = models.ForeignKey(Band, on_delete=models.CASCADE)\n    run = models.ManyToManyField(Run)\n    skyreg = models.ForeignKey(SkyRegion, on_delete=models.CASCADE)\n\n    measurements_path = models.FilePathField(\n        max_length=200,\n        db_column='meas_path',\n        help_text=(\n            'the path to the measurements parquet that belongs to this image'\n        )\n    )\n    POLARISATION_CHOICES = [\n        ('I', 'I'),\n        ('XX', 'XX'),\n        ('YY', 'YY'),\n        ('Q', 'Q'),\n        ('U', 'U'),\n        ('V', 'V'),\n    ]\n    polarisation = models.CharField(\n        max_length=2,\n        choices=POLARISATION_CHOICES,\n        help_text='Polarisation of the image one of I,XX,YY,Q,U,V.'\n    )\n    name = models.CharField(\n        unique=True,\n        max_length=200,\n        help_text='Name of the image.'\n    )\n    path = models.FilePathField(\n        max_length=500,\n        help_text='Path to the file containing the image.'\n    )\n    noise_path = models.FilePathField(\n        max_length=300,\n        blank=True,\n        default='',\n        help_text='Path to the file containing the RMS image.'\n    )\n    background_path = models.FilePathField(\n        max_length=300,\n        blank=True,\n        default='',\n        help_text='Path to the file containing the background image.'\n    )\n\n    datetime = models.DateTimeField(\n        help_text='Date/time of observation or epoch.'\n    )\n    jd = models.FloatField(\n        help_text='Julian date of the observation (days).'\n    )\n    duration = models.FloatField(\n        default=0.,\n        help_text='Duration of the observation.'\n    )\n\n    ra = models.FloatField(\n        help_text='RA of the image centre (Deg).'\n    )\n    dec = models.FloatField(\n        help_text='DEC of the image centre (Deg).'\n    )\n    fov_bmaj = models.FloatField(\n        help_text='Field of view major axis (Deg).'\n    )  # Major (Dec) radius of image (degrees)\n    fov_bmin = models.FloatField(\n        help_text='Field of view minor axis (Deg).'\n    )  # Minor (RA) radius of image (degrees)\n    physical_bmaj = models.FloatField(\n        help_text='The actual size of the image major axis (Deg).'\n    )  # Major (Dec) radius of image (degrees)\n    physical_bmin = models.FloatField(\n        help_text='The actual size of the image minor axis (Deg).'\n    )  # Minor (RA) radius of image (degrees)\n    radius_pixels = models.FloatField(\n        help_text='Radius of the useable region of the image (pixels).'\n    )\n\n    beam_bmaj = models.FloatField(\n        help_text='Major axis of image restoring beam (Deg).'\n    )\n    beam_bmin = models.FloatField(\n        help_text='Minor axis of image restoring beam (Deg).'\n    )\n    beam_bpa = models.FloatField(\n        help_text='Beam position angle (Deg).'\n    )\n    rms_median = models.FloatField(\n        help_text='Background average RMS from the provided RMS map (mJy).'\n    )\n    rms_min = models.FloatField(\n        help_text='Background minimum RMS from the provided RMS map (mJy).'\n    )\n    rms_max = models.FloatField(\n        help_text='Background maximum RMS from the provided RMS map (mJy).'\n    )\n\n    class Meta:\n        ordering = ['datetime']\n\n    def __str__(self):\n        return self.name\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.ImageAdmin","title":"<code>ImageAdmin</code>","text":"<p>               Bases: <code>ModelAdmin</code></p> <p>The ImageAdmin class.</p> Source code in <code>vast_pipeline/admin.py</code> <pre><code>class ImageAdmin(admin.ModelAdmin):\n    \"\"\"\n    The ImageAdmin class.\n    \"\"\"\n    list_display = ('name', 'ra', 'dec', 'datetime')\n    exclude = ('measurements_path', 'path', 'noise_path', 'background_path')\n    search_fields = ('name',)\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.Measurement","title":"<code>Measurement</code>","text":"<p>               Bases: <code>CommentableModel</code></p> <p>A Measurement is an object in the sky that has been detected at least once. Essentially a source single measurement in time.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class Measurement(CommentableModel):\n    \"\"\"\n    A Measurement is an object in the sky that has been detected at least once.\n    Essentially a source single measurement in time.\n    \"\"\"\n    image = models.ForeignKey(\n        Image,\n        null=True,\n        on_delete=models.CASCADE\n    )  # first image seen in\n    source = models.ManyToManyField(\n        'Source',\n        through='Association',\n        through_fields=('meas', 'source')\n    )\n\n    name = models.CharField(max_length=64)\n\n    ra = models.FloatField(help_text='RA of the source (Deg).')  # degrees\n    ra_err = models.FloatField(\n        help_text='RA error of the source (Deg).'\n    )\n    dec = models.FloatField(help_text='DEC of the source (Deg).')  # degrees\n    dec_err = models.FloatField(\n        help_text='DEC error of the source (Deg).'\n    )\n\n    bmaj = models.FloatField(\n        help_text=(\n            'The major axis of the Gaussian fit to the source (Deg).'\n        )\n    )\n    err_bmaj = models.FloatField(help_text='Error major axis (Deg).')\n    bmin = models.FloatField(\n        help_text=(\n            'The minor axis of the Gaussian fit to the source (Deg).'\n        )\n    )\n    err_bmin = models.FloatField(help_text='Error minor axis (Deg).')\n    pa = models.FloatField(\n        help_text=(\n            'Position angle of Gaussian fit east of north to bmaj '\n            '(Deg).'\n        )\n    )\n    err_pa = models.FloatField(help_text='Error position angle (Deg).')\n\n    # supplied by user via config\n    ew_sys_err = models.FloatField(\n        help_text='Systematic error in east-west (RA) direction (Deg).'\n    )\n    # supplied by user via config\n    ns_sys_err = models.FloatField(\n        help_text='Systematic error in north-south (dec) direction (Deg).'\n    )\n\n    # estimate of maximum error radius (from ra_err and dec_err)\n    # Used in advanced association.\n    error_radius = models.FloatField(\n        help_text=(\n            'Estimate of maximum error radius using ra_err'\n            ' and dec_err (Deg).'\n        )\n    )\n\n    # quadratic sum of error_radius and ew_sys_err\n    uncertainty_ew = models.FloatField(\n        help_text=(\n            'Total east-west (RA) uncertainty, quadratic sum of'\n            ' error_radius and ew_sys_err (Deg).'\n        )\n    )\n    # quadratic sum of error_radius and ns_sys_err\n    uncertainty_ns = models.FloatField(\n        help_text=(\n            'Total north-south (Dec) uncertainty, quadratic sum of '\n            'error_radius and ns_sys_err (Deg).'\n        )\n    )\n\n    flux_int = models.FloatField()  # mJy/beam\n    flux_int_err = models.FloatField()  # mJy/beam\n    flux_int_isl_ratio = models.FloatField(\n        help_text=(\n            'Ratio of the component integrated flux to the total'\n            ' island integrated flux.'\n        )\n    )\n    flux_peak = models.FloatField()  # mJy/beam\n    flux_peak_err = models.FloatField()  # mJy/beam\n    flux_peak_isl_ratio = models.FloatField(\n        help_text=(\n            'Ratio of the component peak flux to the total'\n            ' island peak flux.'\n        )\n    )\n    chi_squared_fit = models.FloatField(\n        db_column='chi2_fit',\n        help_text='Chi-squared of the Guassian fit to the source.'\n    )\n    spectral_index = models.FloatField(\n        db_column='spectr_idx',\n        help_text='In-band Selavy spectral index.'\n    )\n    spectral_index_from_TT = models.BooleanField(\n        default=False,\n        db_column='spectr_idx_tt',\n        help_text=(\n            'True/False if the spectral index came from the taylor '\n            'term.'\n        )\n    )\n\n    local_rms = models.FloatField(\n        help_text='Local rms in mJy from Selavy.'\n    )  # mJy/beam\n\n    snr = models.FloatField(\n        help_text='Signal-to-noise ratio of the measurement.'\n    )\n\n    flag_c4 = models.BooleanField(\n        default=False,\n        help_text='Fit flag from Selavy.'\n    )\n\n    compactness = models.FloatField(\n        help_text='Int flux over peak flux.'\n    )\n\n    has_siblings = models.BooleanField(\n        default=False,\n        help_text='True if the fit comes from an island that has more than 1 component.'\n    )\n    component_id = models.CharField(\n        max_length=64,\n        help_text=(\n            'The ID of the component from which the source comes from.'\n        )\n    )\n    island_id = models.CharField(\n        max_length=64,\n        help_text=(\n            'The ID of the island from which the source comes from.'\n        )\n    )\n\n    forced = models.BooleanField(\n        default=False,\n        help_text='True: the measurement is forced extracted.'\n    )\n\n    objects = MeasurementQuerySet.as_manager()\n\n    class Meta:\n        ordering = ['ra']\n\n    def __str__(self):\n        return self.name\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.MeasurementAdmin","title":"<code>MeasurementAdmin</code>","text":"<p>               Bases: <code>ModelAdmin</code></p> <p>The MeasurementAdmin class.</p> Source code in <code>vast_pipeline/admin.py</code> <pre><code>class MeasurementAdmin(admin.ModelAdmin):\n    \"\"\"\n    The MeasurementAdmin class.\n    \"\"\"\n    list_display = ('name', 'ra', 'dec', 'forced')\n    list_filter = ('forced',)\n    search_fields = ('name',)\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.MeasurementQuerySet","title":"<code>MeasurementQuerySet</code>","text":"<p>               Bases: <code>QuerySet</code></p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class MeasurementQuerySet(models.QuerySet):\n\n    def cone_search(\n        self, ra: float, dec: float, radius_deg: float\n    ) -&gt; models.QuerySet:\n        \"\"\"\n        Return all the Sources withing radius_deg of (ra,dec).\n        Returns a QuerySet of Sources, ordered by distance from\n        (ra,dec) ascending.\n\n        Args:\n            ra: The right ascension value of the cone search central\n                coordinate.\n            dec: The declination value of the cone search central coordinate.\n            radius_deg: The radius over which to perform the cone search.\n\n        Returns:\n            Measurements found withing the cone search area.\n        \"\"\"\n        return (\n            self.extra(\n                select={\n                    \"distance\": \"q3c_dist(ra, dec, %s, %s) * 3600\"\n                },\n                select_params=[ra, dec],\n                where=[\"q3c_radial_query(ra, dec, %s, %s, %s)\"],\n                params=[ra, dec, radius_deg],\n            )\n            .order_by(\"distance\")\n        )\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.MeasurementQuerySet.cone_search","title":"<code>cone_search(ra, dec, radius_deg)</code>","text":"<p>Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending.</p> <p>Parameters:</p> Name Type Description Default <code>ra</code> <code>float</code> <p>The right ascension value of the cone search central coordinate.</p> required <code>dec</code> <code>float</code> <p>The declination value of the cone search central coordinate.</p> required <code>radius_deg</code> <code>float</code> <p>The radius over which to perform the cone search.</p> required <p>Returns:</p> Type Description <code>QuerySet</code> <p>Measurements found withing the cone search area.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>def cone_search(\n    self, ra: float, dec: float, radius_deg: float\n) -&gt; models.QuerySet:\n    \"\"\"\n    Return all the Sources withing radius_deg of (ra,dec).\n    Returns a QuerySet of Sources, ordered by distance from\n    (ra,dec) ascending.\n\n    Args:\n        ra: The right ascension value of the cone search central\n            coordinate.\n        dec: The declination value of the cone search central coordinate.\n        radius_deg: The radius over which to perform the cone search.\n\n    Returns:\n        Measurements found withing the cone search area.\n    \"\"\"\n    return (\n        self.extra(\n            select={\n                \"distance\": \"q3c_dist(ra, dec, %s, %s) * 3600\"\n            },\n            select_params=[ra, dec],\n            where=[\"q3c_radial_query(ra, dec, %s, %s, %s)\"],\n            params=[ra, dec, radius_deg],\n        )\n        .order_by(\"distance\")\n    )\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.PipelineConfig","title":"<code>PipelineConfig</code>","text":"<p>Pipeline run configuration.</p> <p>Attributes:</p> Name Type Description <code>SCHEMA</code> <p>class attribute containing the YAML schema for the run config.</p> <code>TEMPLATE_PATH</code> <code>str</code> <p>class attribute containing the path to the default Jinja2 run config template file.</p> <code>epoch_based</code> <code>bool</code> <p>boolean indicating if the original run config inputs were provided with user-defined epochs.</p> <p>Raises:</p> Type Description <code>PipelineConfigError</code> <p>the input YAML config violates the schema.</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>class PipelineConfig:\n    \"\"\"Pipeline run configuration.\n\n    Attributes:\n        SCHEMA: class attribute containing the YAML schema for the run config.\n        TEMPLATE_PATH: class attribute containing the path to the default Jinja2 run\n            config template file.\n        epoch_based: boolean indicating if the original run config inputs were provided\n            with user-defined epochs.\n\n    Raises:\n        PipelineConfigError: the input YAML config violates the schema.\n    \"\"\"\n\n    # key: config input type, value: boolean indicating if it is required\n    _REQUIRED_INPUT_TYPES: Dict[str, bool] = {\n        \"image\": True,\n        \"selavy\": True,\n        \"noise\": True,\n        \"background\": False,\n    }\n    # Inputs may be optional. All inputs will be either a unique list or a mapping (epoch\n    # mode and/or glob expressions). These possibilities cannot be validated at once, so\n    # it will accept Any and then revalidate later.\n    _SCHEMA_INPUTS = {\n        (k if v else yaml.Optional(k)): yaml.MapPattern(yaml.Str(), yaml.Any())\n        | yaml.UniqueSeq(yaml.Str())\n        for k, v in _REQUIRED_INPUT_TYPES.items()\n    }\n    _SCHEMA_GLOB_INPUTS = {\"glob\": yaml.Str() | yaml.Seq(yaml.Str())}\n    _VALID_ASSOC_METHODS: List[str] = [\"basic\", \"advanced\", \"deruiter\"]\n    SCHEMA = yaml.Map(\n        {\n            \"run\": yaml.Map(\n                {\n                    \"path\": yaml.Str(),\n                    \"suppress_astropy_warnings\": yaml.Bool(),\n                }\n            ),\n            \"inputs\": yaml.Map(_SCHEMA_INPUTS),\n            \"source_monitoring\": yaml.Map(\n                {\n                    \"monitor\": yaml.Bool(),\n                    \"min_sigma\": yaml.Float(),\n                    \"edge_buffer_scale\": yaml.Float(),\n                    \"cluster_threshold\": yaml.Float(),\n                    \"allow_nan\": yaml.Bool(),\n                }\n            ),\n            \"source_association\": yaml.Map(\n                {\n                    \"method\": yaml.Enum(_VALID_ASSOC_METHODS),\n                    \"radius\": yaml.Float(),\n                    \"deruiter_radius\": yaml.Float(),\n                    \"deruiter_beamwidth_limit\": yaml.Float(),\n                    \"parallel\": yaml.Bool(),\n                    \"epoch_duplicate_radius\": yaml.Float(),\n                }\n            ),\n            \"new_sources\": yaml.Map(\n                {\n                    \"min_sigma\": yaml.Float(),\n                }\n            ),\n            \"measurements\": yaml.Map(\n                {\n                    \"source_finder\": yaml.Enum([\"selavy\"]),\n                    \"flux_fractional_error\": yaml.Float(),\n                    \"condon_errors\": yaml.Bool(),\n                    \"selavy_local_rms_fill_value\": yaml.Float(),\n                    \"write_arrow_files\": yaml.Bool(),\n                    \"ra_uncertainty\": yaml.Float(),\n                    \"dec_uncertainty\": yaml.Float(),\n                }\n            ),\n            \"variability\": yaml.Map(\n                {\n                    \"pair_metrics\": yaml.Bool(),\n                    \"source_aggregate_pair_metrics_min_abs_vs\": yaml.Float(),\n                }\n            ),\n            yaml.Optional(\"processing\"): yaml.Map(\n                {\n                    yaml.Optional(\n                        \"num_workers\",\n                        default=settings.PIPE_RUN_CONFIG_DEFAULTS['num_workers']):\n                        yaml.NullNone() | yaml.Int() | yaml.Str(),\n                    yaml.Optional(\n                        \"num_workers_io\",\n                        default=settings.PIPE_RUN_CONFIG_DEFAULTS['num_workers_io']):\n                        yaml.NullNone() | yaml.Int() | yaml.Str(),\n                    yaml.Optional(\n                        \"max_partition_mb\",\n                        default=settings.PIPE_RUN_CONFIG_DEFAULTS['max_partition_mb']):\n                        yaml.Int()\n                }\n            )\n        }\n    )\n    # path to default run config template\n    TEMPLATE_PATH: str = os.path.join(\n        settings.BASE_DIR, \"vast_pipeline\", \"config_template.yaml.j2\"\n    )\n\n    def __init__(self, config_yaml: yaml.YAML, validate_inputs: bool = True):\n        \"\"\"Initialises PipelineConfig with parsed (but not necessarily validated) YAML.\n\n        Args:\n            config_yaml (yaml.YAML): Input YAML, usually the output of `strictyaml.load`.\n            validate_inputs (bool, optional): Validate the config input files. Ensures\n                that the inputs match (e.g. each image has a catalogue), and that each\n                path exists. Set to False to skip these checks. Defaults to True.\n\n        Raises:\n            PipelineConfigError: The input YAML config violates the schema.\n        \"\"\"\n        self._yaml: yaml.YAML = config_yaml\n        # The epoch_based parameter below is for if the user has entered just lists we\n        # don't have access to the dates until the Image instances are created. So we\n        # flag this as true so that we can reorder the epochs once the date information\n        # is available. It is also recorded in the database such that there is a record\n        # of the fact that the run was processed in an epoch based mode.\n        self.epoch_based: bool\n\n        # Determine if epoch-based association should be used based on input files.\n        # If inputs have been parsed to dicts, then the user has defined their own epochs.\n        # If inputs have been parsed to lists, we must convert to dicts and auto-fill\n        # the epochs.\n\n        # ensure the inputs are valid in case .from_file(..., validate=False) was used\n        if not validate_inputs:\n            return\n\n        try:\n            self._validate_inputs()\n        except yaml.YAMLValidationError as e:\n            raise PipelineConfigError(e)\n\n        # detect simple list inputs and convert them to epoch-mode inputs\n        yaml_inputs = self._yaml[\"inputs\"]\n        inputs = yaml_inputs.data\n        for input_file_type in self._REQUIRED_INPUT_TYPES:\n            # skip missing optional input types, e.g. background\n            if (\n                not self._REQUIRED_INPUT_TYPES[input_file_type]\n                and input_file_type not in self[\"inputs\"]\n            ):\n                continue\n\n            input_files = inputs[input_file_type]\n\n            # resolve glob expressions if present\n            if isinstance(input_files, dict):\n                # must be either a glob expression, list of glob expressions, or epoch-mode\n                if \"glob\" in input_files:\n                    # resolve the glob expressions\n                    self.epoch_based = False\n                    file_list = self._resolve_glob_expressions(\n                        yaml_inputs[input_file_type]\n                    )\n                    inputs[input_file_type] = self._create_input_epochs(file_list)\n                else:\n                    # epoch-mode with either a list of files or glob expressions\n                    self.epoch_based = True\n                    for epoch in input_files:\n                        if \"glob\" in input_files[epoch]:\n                            # resolve the glob expressions\n                            file_list = self._resolve_glob_expressions(\n                                yaml_inputs[input_file_type][epoch]\n                            )\n                            inputs[input_file_type][epoch] = file_list\n            else:\n                # Epoch-based association not requested and no globs present. Replace\n                # input lists with dicts where each input file has it's own epoch.\n                self.epoch_based = False\n                inputs[input_file_type] = self._create_input_epochs(\n                    input_files\n                )\n        self._yaml[\"inputs\"] = inputs\n\n    def __getitem__(self, name: str):\n        \"\"\"Retrieves the requested YAML chunk as a native Python object.\"\"\"\n        return self._yaml[name].data\n\n    @staticmethod\n    def _create_input_epochs(input_files: List[str]) -&gt; Dict[str, List[str]]:\n        \"\"\"Convert a list of input files into a dict where each list element is placed\n        into its own list of length 1 and mapped to by a unique key, a string that is a\n        0-padded integer. For example, [\"A\", \"B\", \"C\", ..., \"Z\"] would be converted to\n        {\n            \"01\": [\"A\"],\n            \"02\": [\"B\"],\n            \"03\": [\"C\"],\n            ...\n            \"26\": [\"Z\"],\n        }\n        The keys are 0-padded to ensure the strings are sortable regardless of the\n        length of `input_files`.\n        This conversion is required for run configs that are not defined in \"epoch mode\"\n        as after config validation, the pipeline assumes that there will be defined\n        epochs.\n\n        Args:\n            input_files: the list of input file paths.\n\n        Returns:\n            The input file paths mapped to by unique epoch keys.\n        \"\"\"\n        pad_width = len(str(len(input_files)))\n        input_files_dict = {\n            f\"{i + 1:0{pad_width}}\": [val] for i, val in enumerate(input_files)\n        }\n        return input_files_dict\n\n    @classmethod\n    def from_file(\n        cls,\n        yaml_path: str,\n        label: str = \"run config\",\n        validate: bool = True,\n        validate_inputs: bool = True,\n        add_defaults: bool = True,\n    ) -&gt; \"PipelineConfig\":\n        \"\"\"Create a PipelineConfig object from a run configuration YAML file.\n\n        Args:\n            yaml_path: Path to the run config YAML file.\n            label: A label for the config object that will be used in error messages.\n                Default is \"run config\".\n            validate: Perform config schema validation immediately after loading\n                the config file. If set to False, the full schema validation\n                will not be performed until PipelineConfig.validate() is\n                explicitly called. The inputs are always validated regardless.\n                Defaults to True.\n            validate_inputs: Validate the config input files. Ensures that the inputs\n                match (e.g. each image has a catalogue), and that each path exists. Set\n                to False to skip these checks. Defaults to True.\n            add_defaults: Add missing configuration parameters using configured\n                defaults. The defaults are read from the Django settings file.\n                Defaults to True.\n\n        Raises:\n            PipelineConfigError: The run config YAML file fails schema validation.\n\n        \"\"\"\n        schema = cls.SCHEMA if validate else yaml.Any()\n        with open(yaml_path) as fh:\n            config_str = fh.read()\n        try:\n            config_yaml = yaml.load(config_str, schema=schema, label=label)\n        except yaml.YAMLValidationError as e:\n            raise PipelineConfigError(e)\n\n        if add_defaults:\n            # make a template config based on defaults\n            config_defaults_str = make_config_template(\n                cls.TEMPLATE_PATH,\n                **settings.PIPE_RUN_CONFIG_DEFAULTS,\n            )\n            config_defaults_dict: Dict[str, Any] = yaml.load(config_defaults_str).data\n\n            # merge configs\n            config_dict = dict_merge(config_defaults_dict, config_yaml.data)\n            config_yaml = yaml.as_document(config_dict, schema=schema, label=label)\n        return cls(config_yaml, validate_inputs=validate_inputs)\n\n    @staticmethod\n    def _resolve_glob_expressions(input_files: yaml.YAML) -&gt; List[str]:\n        \"\"\"Resolve glob expressions in a YAML chunk, returning a list of sorted file\n        paths.\n\n        Args:\n            input_files (yaml.YAML): A validated YAML chunk of input files that is a\n                mapping of \"glob\" to either a single glob expression or a sequence of\n                glob expressions. e.g.\n                ---\n                glob: /foo/*.fits\n                ---\n                or\n                ---\n                glob:\n                - /foo/A/*.fits\n                - /foo/B/*.fits\n                ---\n\n        Returns:\n            The resolved file paths in lexicographical order.\n        \"\"\"\n        file_list: List[str] = []\n        if input_files[\"glob\"].is_sequence():\n            for glob_expr in input_files[\"glob\"]:\n                file_list.extend(sorted(list(glob(glob_expr.data))))\n        else:\n            file_list.extend(sorted(list(glob(input_files[\"glob\"].data))))\n        return file_list\n\n    def _validate_inputs(self):\n        \"\"\"Validate the input files. Each input type (i.e. image, selavy, noise,\n        background) may be given as one of the following:\n            1. A list of files.\n            2. A glob expression.\n            3. A list of glob expressions.\n            4. A mapping of epochs to any of the above.\n        Each input type is validated individually. Extra input validation steps, e.g. to\n        ensure each input type has the same number of files, are performed in\n        `validate()`.\n\n        Raises:\n            PipelineConfigError: The run config inputs fail schema validation.\n        \"\"\"\n        try:\n            # first pass validation\n            self._yaml[\"inputs\"].revalidate(yaml.Map(self._SCHEMA_INPUTS))\n\n            for input_type in self._yaml[\"inputs\"]:\n                input_yaml = self._yaml[\"inputs\"][input_type]\n                if input_yaml.is_mapping():\n                    # inputs are either epoch-mode, glob expressions, or both\n                    if \"glob\" in input_yaml:\n                        # validate globs\n                        input_yaml.revalidate(yaml.Map(self._SCHEMA_GLOB_INPUTS))\n                    else:\n                        # validate epoch mode which may also contain glob expressions\n                        input_yaml.revalidate(\n                            yaml.MapPattern(\n                                yaml.Str(),\n                                yaml.UniqueSeq(yaml.Str())\n                                | yaml.Map(self._SCHEMA_GLOB_INPUTS),\n                            )\n                        )\n        except yaml.YAMLValidationError as e:\n            raise PipelineConfigError(e)\n\n    def validate(self, user: User = None):\n        \"\"\"Perform extra validation steps not covered by the default schema validation.\n        The following checks are performed in order. If a check fails, an exception is\n        raised and no further checks are performed.\n\n        1. All input files have the same number of epochs and the same number of files\n            per epoch.\n        2. The number of input files does not exceed the configured pipeline maximum.\n            This is only enforced if a regular user (not staff/admin) created the run.\n        3. There are at least two input images.\n        4. Background input images are required if source monitoring is turned on.\n        5. All input files exist.\n\n        Args:\n            user: Optional. The User of the request if made through the UI. Defaults to\n                None.\n\n        Raises:\n            PipelineConfigError: a validation check failed.\n        \"\"\"\n        # run standard base schema validation\n        try:\n            self._yaml.revalidate(self.SCHEMA)\n        except yaml.YAMLValidationError as e:\n            raise PipelineConfigError(e)\n\n        inputs = self[\"inputs\"]\n\n        # epochs defined for images only, used as the reference list of epochs\n        epochs_image = inputs[\"image\"].keys()\n        # map input type to a set of epochs\n        epochs_by_input_type = {\n            input_type: set(inputs[input_type].keys())\n            for input_type in inputs.keys()\n        }\n        # map input type to total number of files from all epochs\n        n_files_by_input_type: Dict[str, int] = {}\n        epoch_n_files: Dict[str, Dict[str, int]] = {}\n        n_files = 0\n        for input_type, epochs_set in epochs_by_input_type.items():\n            epoch_n_files[input_type] = {}\n            n_files_by_input_type[input_type] = 0\n            for epoch in epochs_set:\n                n = len(inputs[input_type][epoch])\n                n_files_by_input_type[input_type] += n\n                epoch_n_files[input_type][epoch] = n\n                n_files += n\n\n        # Note by this point the input files have been converted to a mapping regardless\n        # of the user's input format.\n        # Ensure all input file types have the same epochs.\n        try:\n            schema = yaml.Map({epoch: yaml.Seq(yaml.Str()) for epoch in epochs_image})\n            for input_type in inputs.keys():\n                # Generate a new YAML object on-the-fly per input to avoid saving\n                # a validation schema per file in the PipelineConfig object\n                # (These can consume a lot of RAM for long lists of input files).\n                yaml.load(self._yaml[\"inputs\"][input_type].as_yaml(), schema=schema)\n        except yaml.YAMLValidationError:\n            # number of epochs could be different or the name of the epochs may not match\n            # find out which by counting the number of unique epochs per input type\n            n_epochs_per_input_type = [\n                len(epochs_set) for epochs_set in epochs_by_input_type.values()\n            ]\n            if len(set(n_epochs_per_input_type)) &gt; 1:\n                if self.epoch_based:\n                    error_msg = \"The number of epochs must match for all input types.\\n\"\n                else:\n                    error_msg = \"The number of files must match for all input types.\\n\"\n            else:\n                error_msg = \"The name of the epochs must match for all input types.\\n\"\n            counts_str = \"\"\n            if self.epoch_based:\n                for input_type in epoch_n_files.keys():\n                    n = len(epoch_n_files[input_type])\n                    counts_str += (\n                        f\"{input_type} has {n} epoch{'s' if n &gt; 1 else ''}:\"\n                        f\" {', '.join(epoch_n_files[input_type].keys())}\\n\"\n                    )\n            else:\n                for input_type, n in n_files_by_input_type.items():\n                    counts_str += f\"{input_type} has {n} file{'s' if n &gt; 1 else ''}\\n\"\n\n            counts_str = counts_str[:-1]\n            raise PipelineConfigError(error_msg + counts_str)\n\n        # Ensure all input file type epochs have the same number of files per epoch.\n        # This could be combined with the number of epochs validation above, but we want\n        # to give specific feedback to the user on failure.\n        try:\n            schema = yaml.Map(\n                {epoch: yaml.FixedSeq([yaml.Str()] * epoch_n_files[\"image\"][epoch])\n                for epoch in epochs_image})\n            for input_type in inputs.keys():\n                yaml.load(self._yaml[\"inputs\"][input_type].as_yaml(), schema=schema)\n        except yaml.YAMLValidationError:\n            # map input type to a mapping of epoch to file count\n            file_counts_str = \"\"\n            for input_type in inputs.keys():\n                file_counts_str += f\"{input_type}:\\n\"\n                for epoch in sorted(inputs[input_type].keys()):\n                    file_counts_str += (\n                        f\"  {epoch}: {len(inputs[input_type][epoch])}\\n\"\n                    )\n            file_counts_str = file_counts_str[:-1]\n            raise PipelineConfigError(\n                \"The number of files per epoch does not match between input types.\\n\"\n                + file_counts_str\n            )\n\n        # ensure the number of input files is less than the user limit\n        if user and n_files &gt; settings.MAX_PIPERUN_IMAGES:\n            if user.is_staff:\n                logger.warning(\n                    \"Maximum number of images\"\n                    f\" ({settings.MAX_PIPERUN_IMAGES}) rule bypassed with\"\n                    \" admin status.\"\n                )\n            else:\n                raise PipelineConfigError(\n                    f\"The number of images entered ({n_files})\"\n                    \" exceeds the maximum number of images currently\"\n                    f\" allowed ({settings.MAX_PIPERUN_IMAGES}). Please ask\"\n                    \" an administrator for advice on processing your run.\"\n                )\n\n        # ensure at least two inputs are provided\n        check = [n_files_by_input_type[input_type] &lt; 2 for input_type in inputs.keys()]\n        if any(check):\n            raise PipelineConfigError(\n                \"Number of image files must to be larger than 1\"\n            )\n\n        # ensure background files are provided if source monitoring is requested\n        try:\n            monitor = self[\"source_monitoring\"][\"monitor\"]\n        except KeyError:\n            monitor = False\n\n        if monitor:\n            inputs_schema = yaml.Map(\n                {\n                    k: yaml.UniqueSeq(yaml.Str())\n                    | yaml.MapPattern(yaml.Str(), yaml.UniqueSeq(yaml.Str()))\n                    for k in self._REQUIRED_INPUT_TYPES\n                }\n            )\n            try:\n                self._yaml[\"inputs\"].revalidate(inputs_schema)\n            except yaml.YAMLValidationError:\n                raise PipelineConfigError(\n                    \"Background files must be provided if source monitoring is enabled.\"\n                )\n\n        # ensure the input files all exist\n        for input_type in inputs.keys():\n            for epoch, file_list in inputs[input_type].items():\n                for file in file_list:\n                    if not os.path.exists(file):\n                        raise PipelineConfigError(f\"{file} does not exist.\")\n\n        # ensure num_workers and num_workers_io are\n        # either None (from null in config yaml) or an integer\n        for param_name in ('num_workers', 'num_workers_io'):\n            param_value = self['processing'][param_name]\n            if (param_value is not None) and (type(param_value) is not int):\n                raise PipelineConfigError(f\"{param_name} can only be an integer or 'null'\")\n\n    def check_prev_config_diff(self) -&gt; bool:\n        \"\"\"\n        Checks if the previous config file differs from the current config file. Used in\n        add mode. Only returns true if the images are different and the other general\n        settings are the same (the requirement for add mode). Otherwise False is returned.\n\n        Returns:\n            `True` if images are different but general settings are the same,\n                otherwise `False` is returned.\n        \"\"\"\n        prev_config = PipelineConfig.from_file(\n            os.path.join(self[\"run\"][\"path\"], \"config_prev.yaml\"),\n            label=\"previous run config\",\n        )\n        if self._yaml == prev_config._yaml:\n            return True\n\n        # are the input image files different?\n        images_changed = self[\"inputs\"][\"image\"] != prev_config[\"inputs\"][\"image\"]\n\n        # are all the non-input file configs the same?\n        config_dict = self._yaml.data\n        prev_config_dict = prev_config._yaml.data\n        _ = config_dict.pop(\"inputs\")\n        _ = prev_config_dict.pop(\"inputs\")\n        settings_check = config_dict == prev_config_dict\n\n        if images_changed and settings_check:\n            return False\n        return True\n\n    def image_opts(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the config options required for image ingestion only.\n        Namely:\n            - selavy_local_rms_fill_value\n            - condon_errors\n            - ra_uncertainty\n            - dec_uncertainty\n\n        Returns:\n            The relevant key value pairs\n        \"\"\"\n        keys = [\n            \"selavy_local_rms_fill_value\",\n            \"condon_errors\",\n            \"ra_uncertainty\",\n            \"dec_uncertainty\"\n        ]\n        return {key: self[\"measurements\"][key] for key in keys}\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.PipelineConfig.__getitem__","title":"<code>__getitem__(name)</code>","text":"<p>Retrieves the requested YAML chunk as a native Python object.</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>def __getitem__(self, name: str):\n    \"\"\"Retrieves the requested YAML chunk as a native Python object.\"\"\"\n    return self._yaml[name].data\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.PipelineConfig.__init__","title":"<code>__init__(config_yaml, validate_inputs=True)</code>","text":"<p>Initialises PipelineConfig with parsed (but not necessarily validated) YAML.</p> <p>Parameters:</p> Name Type Description Default <code>config_yaml</code> <code>YAML</code> <p>Input YAML, usually the output of <code>strictyaml.load</code>.</p> required <code>validate_inputs</code> <code>bool</code> <p>Validate the config input files. Ensures that the inputs match (e.g. each image has a catalogue), and that each path exists. Set to False to skip these checks. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>PipelineConfigError</code> <p>The input YAML config violates the schema.</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>def __init__(self, config_yaml: yaml.YAML, validate_inputs: bool = True):\n    \"\"\"Initialises PipelineConfig with parsed (but not necessarily validated) YAML.\n\n    Args:\n        config_yaml (yaml.YAML): Input YAML, usually the output of `strictyaml.load`.\n        validate_inputs (bool, optional): Validate the config input files. Ensures\n            that the inputs match (e.g. each image has a catalogue), and that each\n            path exists. Set to False to skip these checks. Defaults to True.\n\n    Raises:\n        PipelineConfigError: The input YAML config violates the schema.\n    \"\"\"\n    self._yaml: yaml.YAML = config_yaml\n    # The epoch_based parameter below is for if the user has entered just lists we\n    # don't have access to the dates until the Image instances are created. So we\n    # flag this as true so that we can reorder the epochs once the date information\n    # is available. It is also recorded in the database such that there is a record\n    # of the fact that the run was processed in an epoch based mode.\n    self.epoch_based: bool\n\n    # Determine if epoch-based association should be used based on input files.\n    # If inputs have been parsed to dicts, then the user has defined their own epochs.\n    # If inputs have been parsed to lists, we must convert to dicts and auto-fill\n    # the epochs.\n\n    # ensure the inputs are valid in case .from_file(..., validate=False) was used\n    if not validate_inputs:\n        return\n\n    try:\n        self._validate_inputs()\n    except yaml.YAMLValidationError as e:\n        raise PipelineConfigError(e)\n\n    # detect simple list inputs and convert them to epoch-mode inputs\n    yaml_inputs = self._yaml[\"inputs\"]\n    inputs = yaml_inputs.data\n    for input_file_type in self._REQUIRED_INPUT_TYPES:\n        # skip missing optional input types, e.g. background\n        if (\n            not self._REQUIRED_INPUT_TYPES[input_file_type]\n            and input_file_type not in self[\"inputs\"]\n        ):\n            continue\n\n        input_files = inputs[input_file_type]\n\n        # resolve glob expressions if present\n        if isinstance(input_files, dict):\n            # must be either a glob expression, list of glob expressions, or epoch-mode\n            if \"glob\" in input_files:\n                # resolve the glob expressions\n                self.epoch_based = False\n                file_list = self._resolve_glob_expressions(\n                    yaml_inputs[input_file_type]\n                )\n                inputs[input_file_type] = self._create_input_epochs(file_list)\n            else:\n                # epoch-mode with either a list of files or glob expressions\n                self.epoch_based = True\n                for epoch in input_files:\n                    if \"glob\" in input_files[epoch]:\n                        # resolve the glob expressions\n                        file_list = self._resolve_glob_expressions(\n                            yaml_inputs[input_file_type][epoch]\n                        )\n                        inputs[input_file_type][epoch] = file_list\n        else:\n            # Epoch-based association not requested and no globs present. Replace\n            # input lists with dicts where each input file has it's own epoch.\n            self.epoch_based = False\n            inputs[input_file_type] = self._create_input_epochs(\n                input_files\n            )\n    self._yaml[\"inputs\"] = inputs\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.PipelineConfig.check_prev_config_diff","title":"<code>check_prev_config_diff()</code>","text":"<p>Checks if the previous config file differs from the current config file. Used in add mode. Only returns true if the images are different and the other general settings are the same (the requirement for add mode). Otherwise False is returned.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if images are different but general settings are the same, otherwise <code>False</code> is returned.</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>def check_prev_config_diff(self) -&gt; bool:\n    \"\"\"\n    Checks if the previous config file differs from the current config file. Used in\n    add mode. Only returns true if the images are different and the other general\n    settings are the same (the requirement for add mode). Otherwise False is returned.\n\n    Returns:\n        `True` if images are different but general settings are the same,\n            otherwise `False` is returned.\n    \"\"\"\n    prev_config = PipelineConfig.from_file(\n        os.path.join(self[\"run\"][\"path\"], \"config_prev.yaml\"),\n        label=\"previous run config\",\n    )\n    if self._yaml == prev_config._yaml:\n        return True\n\n    # are the input image files different?\n    images_changed = self[\"inputs\"][\"image\"] != prev_config[\"inputs\"][\"image\"]\n\n    # are all the non-input file configs the same?\n    config_dict = self._yaml.data\n    prev_config_dict = prev_config._yaml.data\n    _ = config_dict.pop(\"inputs\")\n    _ = prev_config_dict.pop(\"inputs\")\n    settings_check = config_dict == prev_config_dict\n\n    if images_changed and settings_check:\n        return False\n    return True\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.PipelineConfig.from_file","title":"<code>from_file(yaml_path, label='run config', validate=True, validate_inputs=True, add_defaults=True)</code>  <code>classmethod</code>","text":"<p>Create a PipelineConfig object from a run configuration YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>str</code> <p>Path to the run config YAML file.</p> required <code>label</code> <code>str</code> <p>A label for the config object that will be used in error messages. Default is \"run config\".</p> <code>'run config'</code> <code>validate</code> <code>bool</code> <p>Perform config schema validation immediately after loading the config file. If set to False, the full schema validation will not be performed until PipelineConfig.validate() is explicitly called. The inputs are always validated regardless. Defaults to True.</p> <code>True</code> <code>validate_inputs</code> <code>bool</code> <p>Validate the config input files. Ensures that the inputs match (e.g. each image has a catalogue), and that each path exists. Set to False to skip these checks. Defaults to True.</p> <code>True</code> <code>add_defaults</code> <code>bool</code> <p>Add missing configuration parameters using configured defaults. The defaults are read from the Django settings file. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>PipelineConfigError</code> <p>The run config YAML file fails schema validation.</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>@classmethod\ndef from_file(\n    cls,\n    yaml_path: str,\n    label: str = \"run config\",\n    validate: bool = True,\n    validate_inputs: bool = True,\n    add_defaults: bool = True,\n) -&gt; \"PipelineConfig\":\n    \"\"\"Create a PipelineConfig object from a run configuration YAML file.\n\n    Args:\n        yaml_path: Path to the run config YAML file.\n        label: A label for the config object that will be used in error messages.\n            Default is \"run config\".\n        validate: Perform config schema validation immediately after loading\n            the config file. If set to False, the full schema validation\n            will not be performed until PipelineConfig.validate() is\n            explicitly called. The inputs are always validated regardless.\n            Defaults to True.\n        validate_inputs: Validate the config input files. Ensures that the inputs\n            match (e.g. each image has a catalogue), and that each path exists. Set\n            to False to skip these checks. Defaults to True.\n        add_defaults: Add missing configuration parameters using configured\n            defaults. The defaults are read from the Django settings file.\n            Defaults to True.\n\n    Raises:\n        PipelineConfigError: The run config YAML file fails schema validation.\n\n    \"\"\"\n    schema = cls.SCHEMA if validate else yaml.Any()\n    with open(yaml_path) as fh:\n        config_str = fh.read()\n    try:\n        config_yaml = yaml.load(config_str, schema=schema, label=label)\n    except yaml.YAMLValidationError as e:\n        raise PipelineConfigError(e)\n\n    if add_defaults:\n        # make a template config based on defaults\n        config_defaults_str = make_config_template(\n            cls.TEMPLATE_PATH,\n            **settings.PIPE_RUN_CONFIG_DEFAULTS,\n        )\n        config_defaults_dict: Dict[str, Any] = yaml.load(config_defaults_str).data\n\n        # merge configs\n        config_dict = dict_merge(config_defaults_dict, config_yaml.data)\n        config_yaml = yaml.as_document(config_dict, schema=schema, label=label)\n    return cls(config_yaml, validate_inputs=validate_inputs)\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.PipelineConfig.image_opts","title":"<code>image_opts()</code>","text":"<p>Get the config options required for image ingestion only. Namely:     - selavy_local_rms_fill_value     - condon_errors     - ra_uncertainty     - dec_uncertainty</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The relevant key value pairs</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>def image_opts(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get the config options required for image ingestion only.\n    Namely:\n        - selavy_local_rms_fill_value\n        - condon_errors\n        - ra_uncertainty\n        - dec_uncertainty\n\n    Returns:\n        The relevant key value pairs\n    \"\"\"\n    keys = [\n        \"selavy_local_rms_fill_value\",\n        \"condon_errors\",\n        \"ra_uncertainty\",\n        \"dec_uncertainty\"\n    ]\n    return {key: self[\"measurements\"][key] for key in keys}\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.PipelineConfig.validate","title":"<code>validate(user=None)</code>","text":"<p>Perform extra validation steps not covered by the default schema validation. The following checks are performed in order. If a check fails, an exception is raised and no further checks are performed.</p> <ol> <li>All input files have the same number of epochs and the same number of files     per epoch.</li> <li>The number of input files does not exceed the configured pipeline maximum.     This is only enforced if a regular user (not staff/admin) created the run.</li> <li>There are at least two input images.</li> <li>Background input images are required if source monitoring is turned on.</li> <li>All input files exist.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>User</code> <p>Optional. The User of the request if made through the UI. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>PipelineConfigError</code> <p>a validation check failed.</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>def validate(self, user: User = None):\n    \"\"\"Perform extra validation steps not covered by the default schema validation.\n    The following checks are performed in order. If a check fails, an exception is\n    raised and no further checks are performed.\n\n    1. All input files have the same number of epochs and the same number of files\n        per epoch.\n    2. The number of input files does not exceed the configured pipeline maximum.\n        This is only enforced if a regular user (not staff/admin) created the run.\n    3. There are at least two input images.\n    4. Background input images are required if source monitoring is turned on.\n    5. All input files exist.\n\n    Args:\n        user: Optional. The User of the request if made through the UI. Defaults to\n            None.\n\n    Raises:\n        PipelineConfigError: a validation check failed.\n    \"\"\"\n    # run standard base schema validation\n    try:\n        self._yaml.revalidate(self.SCHEMA)\n    except yaml.YAMLValidationError as e:\n        raise PipelineConfigError(e)\n\n    inputs = self[\"inputs\"]\n\n    # epochs defined for images only, used as the reference list of epochs\n    epochs_image = inputs[\"image\"].keys()\n    # map input type to a set of epochs\n    epochs_by_input_type = {\n        input_type: set(inputs[input_type].keys())\n        for input_type in inputs.keys()\n    }\n    # map input type to total number of files from all epochs\n    n_files_by_input_type: Dict[str, int] = {}\n    epoch_n_files: Dict[str, Dict[str, int]] = {}\n    n_files = 0\n    for input_type, epochs_set in epochs_by_input_type.items():\n        epoch_n_files[input_type] = {}\n        n_files_by_input_type[input_type] = 0\n        for epoch in epochs_set:\n            n = len(inputs[input_type][epoch])\n            n_files_by_input_type[input_type] += n\n            epoch_n_files[input_type][epoch] = n\n            n_files += n\n\n    # Note by this point the input files have been converted to a mapping regardless\n    # of the user's input format.\n    # Ensure all input file types have the same epochs.\n    try:\n        schema = yaml.Map({epoch: yaml.Seq(yaml.Str()) for epoch in epochs_image})\n        for input_type in inputs.keys():\n            # Generate a new YAML object on-the-fly per input to avoid saving\n            # a validation schema per file in the PipelineConfig object\n            # (These can consume a lot of RAM for long lists of input files).\n            yaml.load(self._yaml[\"inputs\"][input_type].as_yaml(), schema=schema)\n    except yaml.YAMLValidationError:\n        # number of epochs could be different or the name of the epochs may not match\n        # find out which by counting the number of unique epochs per input type\n        n_epochs_per_input_type = [\n            len(epochs_set) for epochs_set in epochs_by_input_type.values()\n        ]\n        if len(set(n_epochs_per_input_type)) &gt; 1:\n            if self.epoch_based:\n                error_msg = \"The number of epochs must match for all input types.\\n\"\n            else:\n                error_msg = \"The number of files must match for all input types.\\n\"\n        else:\n            error_msg = \"The name of the epochs must match for all input types.\\n\"\n        counts_str = \"\"\n        if self.epoch_based:\n            for input_type in epoch_n_files.keys():\n                n = len(epoch_n_files[input_type])\n                counts_str += (\n                    f\"{input_type} has {n} epoch{'s' if n &gt; 1 else ''}:\"\n                    f\" {', '.join(epoch_n_files[input_type].keys())}\\n\"\n                )\n        else:\n            for input_type, n in n_files_by_input_type.items():\n                counts_str += f\"{input_type} has {n} file{'s' if n &gt; 1 else ''}\\n\"\n\n        counts_str = counts_str[:-1]\n        raise PipelineConfigError(error_msg + counts_str)\n\n    # Ensure all input file type epochs have the same number of files per epoch.\n    # This could be combined with the number of epochs validation above, but we want\n    # to give specific feedback to the user on failure.\n    try:\n        schema = yaml.Map(\n            {epoch: yaml.FixedSeq([yaml.Str()] * epoch_n_files[\"image\"][epoch])\n            for epoch in epochs_image})\n        for input_type in inputs.keys():\n            yaml.load(self._yaml[\"inputs\"][input_type].as_yaml(), schema=schema)\n    except yaml.YAMLValidationError:\n        # map input type to a mapping of epoch to file count\n        file_counts_str = \"\"\n        for input_type in inputs.keys():\n            file_counts_str += f\"{input_type}:\\n\"\n            for epoch in sorted(inputs[input_type].keys()):\n                file_counts_str += (\n                    f\"  {epoch}: {len(inputs[input_type][epoch])}\\n\"\n                )\n        file_counts_str = file_counts_str[:-1]\n        raise PipelineConfigError(\n            \"The number of files per epoch does not match between input types.\\n\"\n            + file_counts_str\n        )\n\n    # ensure the number of input files is less than the user limit\n    if user and n_files &gt; settings.MAX_PIPERUN_IMAGES:\n        if user.is_staff:\n            logger.warning(\n                \"Maximum number of images\"\n                f\" ({settings.MAX_PIPERUN_IMAGES}) rule bypassed with\"\n                \" admin status.\"\n            )\n        else:\n            raise PipelineConfigError(\n                f\"The number of images entered ({n_files})\"\n                \" exceeds the maximum number of images currently\"\n                f\" allowed ({settings.MAX_PIPERUN_IMAGES}). Please ask\"\n                \" an administrator for advice on processing your run.\"\n            )\n\n    # ensure at least two inputs are provided\n    check = [n_files_by_input_type[input_type] &lt; 2 for input_type in inputs.keys()]\n    if any(check):\n        raise PipelineConfigError(\n            \"Number of image files must to be larger than 1\"\n        )\n\n    # ensure background files are provided if source monitoring is requested\n    try:\n        monitor = self[\"source_monitoring\"][\"monitor\"]\n    except KeyError:\n        monitor = False\n\n    if monitor:\n        inputs_schema = yaml.Map(\n            {\n                k: yaml.UniqueSeq(yaml.Str())\n                | yaml.MapPattern(yaml.Str(), yaml.UniqueSeq(yaml.Str()))\n                for k in self._REQUIRED_INPUT_TYPES\n            }\n        )\n        try:\n            self._yaml[\"inputs\"].revalidate(inputs_schema)\n        except yaml.YAMLValidationError:\n            raise PipelineConfigError(\n                \"Background files must be provided if source monitoring is enabled.\"\n            )\n\n    # ensure the input files all exist\n    for input_type in inputs.keys():\n        for epoch, file_list in inputs[input_type].items():\n            for file in file_list:\n                if not os.path.exists(file):\n                    raise PipelineConfigError(f\"{file} does not exist.\")\n\n    # ensure num_workers and num_workers_io are\n    # either None (from null in config yaml) or an integer\n    for param_name in ('num_workers', 'num_workers_io'):\n        param_value = self['processing'][param_name]\n        if (param_value is not None) and (type(param_value) is not int):\n            raise PipelineConfigError(f\"{param_name} can only be an integer or 'null'\")\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.RelatedSource","title":"<code>RelatedSource</code>","text":"<p>               Bases: <code>Model</code></p> <p>Association table for the many to many Source relationship with itself Django doc https://docs.djangoproject.com/en/3.1/ref/models/fields/#django.db.models.ManyToManyField.through</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class RelatedSource(models.Model):\n    '''\n    Association table for the many to many Source relationship with itself\n    Django doc\n    https://docs.djangoproject.com/en/3.1/ref/models/fields/#django.db.models.ManyToManyField.through\n    '''\n    from_source = models.ForeignKey(Source, on_delete=models.CASCADE)\n    to_source = models.ForeignKey(\n        Source,\n        on_delete=models.CASCADE,\n        related_name='related_sources'\n    )\n\n    class Meta:\n        constraints = [\n            models.UniqueConstraint(\n                name='%(app_label)s_%(class)s_unique_pair',\n                fields=['from_source', 'to_source']\n            )\n        ]\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.Run","title":"<code>Run</code>","text":"<p>               Bases: <code>CommentableModel</code></p> <p>A Run is essentially a pipeline run/processing istance over a set of images</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class Run(CommentableModel):\n    \"\"\"\n    A Run is essentially a pipeline run/processing istance over a set of\n    images\n    \"\"\"\n    user = models.ForeignKey(\n        User,\n        on_delete=models.SET_NULL,\n        null=True,\n        blank=True\n    )\n\n    name = models.CharField(\n        max_length=64,\n        unique=True,\n        validators=[\n            RegexValidator(\n                regex=r'[\\[@!#$%^&amp;*()&lt;&gt;?/\\|}{~:\\] ]',\n                message='Name contains not allowed characters!',\n                inverse_match=True\n            ),\n        ],\n        help_text='name of the pipeline run'\n    )\n    description = models.CharField(\n        max_length=240,\n        blank=True,\n        help_text=\"A short description of the pipeline run.\"\n    )\n    time = models.DateTimeField(\n        auto_now=True,\n        help_text='Datetime of a pipeline run.'\n    )\n    path = models.FilePathField(\n        max_length=200,\n        help_text='path to the pipeline run'\n    )\n    STATUS_CHOICES = [\n        ('INI', 'Initialised'),\n        ('QUE', 'Queued'),\n        ('RUN', 'Running'),\n        ('END', 'Completed'),\n        ('ERR', 'Error'),\n        ('RES', 'Restoring'),\n        ('DEL', 'Deleting'),\n    ]\n    status = models.CharField(\n        max_length=3,\n        choices=STATUS_CHOICES,\n        default='INI',\n        help_text='Status of the pipeline run.'\n    )\n    n_images = models.IntegerField(\n        default=0,\n        help_text='number of images processed in this run'\n    )\n    n_sources = models.IntegerField(\n        default=0,\n        help_text='number of sources extracted in this run'\n    )\n    n_selavy_measurements = models.IntegerField(\n        default=0,\n        help_text='number of selavy measurements in this run'\n    )\n    n_forced_measurements = models.IntegerField(\n        default=0,\n        help_text='number of forced measurements in this run'\n    )\n    n_new_sources = models.IntegerField(\n        default=0,\n        help_text='number of new sources in this run'\n    )\n    epoch_based = models.BooleanField(\n        default=False,\n        help_text=(\n            'Whether the run was processed using epoch based association'\n            ', i.e. the user passed in groups of images defining epochs'\n            ' rather than every image being treated individually.'\n        )\n    )\n\n    objects = RunManager()  # used instead of RunQuerySet.as_manager() so mypy checks work\n\n    class Meta:\n        ordering = ['name']\n\n    def __str__(self):\n        return self.name\n\n    def save(self, *args, **kwargs):\n        # enforce the full model validation on save\n        self.full_clean()\n        super(Run, self).save(*args, **kwargs)\n\n    def get_config(\n        self, validate: bool = True, validate_inputs: bool = True, prev: bool = False\n    ) -&gt; PipelineConfig:\n        \"\"\"Read, parse, and optionally validate the run configuration file.\n\n        Args:\n            validate: Validate the run configuration. Defaults to False.\n            validate_inputs: Validate the config input files. Ensures\n                that the inputs match (e.g. each image has a catalogue), and that each\n                path exists. Set to False to skip these checks. Defaults to True.\n            prev: Get the previous config file instead of the current config. The\n                previous config is the one used for the last successfully completed run.\n                The current config may have been modified since the run was executed.\n\n        Returns:\n            PipelineConfig: The run configuration object.\n        \"\"\"\n        config_name = \"config_prev.yaml\" if prev else \"config.yaml\"\n        config = PipelineConfig.from_file(\n            str(Path(self.path) / config_name),\n            validate=validate,\n            validate_inputs=validate_inputs,\n        )\n        return config\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.Run.get_config","title":"<code>get_config(validate=True, validate_inputs=True, prev=False)</code>","text":"<p>Read, parse, and optionally validate the run configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>validate</code> <code>bool</code> <p>Validate the run configuration. Defaults to False.</p> <code>True</code> <code>validate_inputs</code> <code>bool</code> <p>Validate the config input files. Ensures that the inputs match (e.g. each image has a catalogue), and that each path exists. Set to False to skip these checks. Defaults to True.</p> <code>True</code> <code>prev</code> <code>bool</code> <p>Get the previous config file instead of the current config. The previous config is the one used for the last successfully completed run. The current config may have been modified since the run was executed.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>PipelineConfig</code> <code>PipelineConfig</code> <p>The run configuration object.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>def get_config(\n    self, validate: bool = True, validate_inputs: bool = True, prev: bool = False\n) -&gt; PipelineConfig:\n    \"\"\"Read, parse, and optionally validate the run configuration file.\n\n    Args:\n        validate: Validate the run configuration. Defaults to False.\n        validate_inputs: Validate the config input files. Ensures\n            that the inputs match (e.g. each image has a catalogue), and that each\n            path exists. Set to False to skip these checks. Defaults to True.\n        prev: Get the previous config file instead of the current config. The\n            previous config is the one used for the last successfully completed run.\n            The current config may have been modified since the run was executed.\n\n    Returns:\n        PipelineConfig: The run configuration object.\n    \"\"\"\n    config_name = \"config_prev.yaml\" if prev else \"config.yaml\"\n    config = PipelineConfig.from_file(\n        str(Path(self.path) / config_name),\n        validate=validate,\n        validate_inputs=validate_inputs,\n    )\n    return config\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.RunAdmin","title":"<code>RunAdmin</code>","text":"<p>               Bases: <code>ModelAdmin</code></p> <p>The RunAdmin class.</p> Source code in <code>vast_pipeline/admin.py</code> <pre><code>class RunAdmin(admin.ModelAdmin):\n    \"\"\"\n    The RunAdmin class.\n    \"\"\"\n    list_display = ('name', 'time', 'status')\n    list_filter = ('time', 'status')\n    exclude = ('path',)\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.RunQuerySet","title":"<code>RunQuerySet</code>","text":"<p>               Bases: <code>QuerySet</code></p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class RunQuerySet(models.QuerySet):\n\n    def check_max_runs(self, max_runs: int = 5) -&gt; int:\n        \"\"\"\n        Check if number of running pipeline runs is above threshold.\n\n        Args:\n            max_runs: The maximum number of processing runs allowed.\n\n        Returns:\n            The count of the current pipeline runs with a status of `RUN`.\n        \"\"\"\n        return self.filter(status='RUN').count() &gt;= max_runs\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.RunQuerySet.check_max_runs","title":"<code>check_max_runs(max_runs=5)</code>","text":"<p>Check if number of running pipeline runs is above threshold.</p> <p>Parameters:</p> Name Type Description Default <code>max_runs</code> <code>int</code> <p>The maximum number of processing runs allowed.</p> <code>5</code> <p>Returns:</p> Type Description <code>int</code> <p>The count of the current pipeline runs with a status of <code>RUN</code>.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>def check_max_runs(self, max_runs: int = 5) -&gt; int:\n    \"\"\"\n    Check if number of running pipeline runs is above threshold.\n\n    Args:\n        max_runs: The maximum number of processing runs allowed.\n\n    Returns:\n        The count of the current pipeline runs with a status of `RUN`.\n    \"\"\"\n    return self.filter(status='RUN').count() &gt;= max_runs\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.SkyRegionAdmin","title":"<code>SkyRegionAdmin</code>","text":"<p>               Bases: <code>ModelAdmin</code></p> <p>The SkyRegionAdmin class.</p> Source code in <code>vast_pipeline/admin.py</code> <pre><code>class SkyRegionAdmin(admin.ModelAdmin):\n    \"\"\"\n    The SkyRegionAdmin class.\n    \"\"\"\n    list_display = ('__str__', 'centre_ra', 'centre_dec')\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.Source","title":"<code>Source</code>","text":"<p>               Bases: <code>CommentableModel</code></p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class Source(CommentableModel):\n    run = models.ForeignKey(Run, on_delete=models.CASCADE, null=True,)\n    related = models.ManyToManyField(\n        'self',\n        through='RelatedSource',\n        symmetrical=False,\n        through_fields=('from_source', 'to_source')\n    )\n\n    name = models.CharField(max_length=100)\n    new = models.BooleanField(default=False, help_text='New Source.')\n    tags = TagField(\n        space_delimiter=False,\n        autocomplete_view=\"vast_pipeline:source_tags_autocomplete\",\n        autocomplete_settings={\"width\": \"100%\"},\n    )\n\n    # average fields calculated from the source measurements\n    wavg_ra = models.FloatField(\n        help_text='The weighted average right ascension (Deg).'\n    )\n    wavg_dec = models.FloatField(\n        help_text='The weighted average declination (Deg).'\n    )\n    wavg_uncertainty_ew = models.FloatField(\n        help_text=(\n            'The weighted average uncertainty in the east-'\n            'west (RA) direction (Deg).'\n        )\n    )\n    wavg_uncertainty_ns = models.FloatField(\n        help_text=(\n            'The weighted average uncertainty in the north-'\n            'south (Dec) direction (Deg).'\n        )\n    )\n    avg_flux_int = models.FloatField(\n        help_text='The average integrated flux value.'\n    )\n    avg_flux_peak = models.FloatField(\n        help_text='The average peak flux value.'\n    )\n    max_flux_peak = models.FloatField(\n        help_text='The maximum peak flux value.'\n    )\n    min_flux_peak = models.FloatField(\n        help_text='The minimum peak flux value.'\n    )\n    max_flux_int = models.FloatField(\n        help_text='The maximum integrated flux value.'\n    )\n    min_flux_int = models.FloatField(\n        help_text='The minimum integrated flux value.'\n    )\n    min_flux_int_isl_ratio = models.FloatField(\n        help_text='The minimum integrated island flux ratio value.'\n    )\n    min_flux_peak_isl_ratio = models.FloatField(\n        help_text='The minimum peak island flux ratio value.'\n    )\n    avg_compactness = models.FloatField(\n        help_text='The average compactness.'\n    )\n    min_snr = models.FloatField(\n        help_text='The minimum signal-to-noise ratio value of the detections.'\n    )\n    max_snr = models.FloatField(\n        help_text='The maximum signal-to-noise ratio value of the detections.'\n    )\n\n    # metrics\n    v_int = models.FloatField(\n        help_text='V metric for int flux.'\n    )\n    v_peak = models.FloatField(\n        help_text='V metric for peak flux.'\n    )\n    eta_int = models.FloatField(\n        help_text='Eta metric for int flux.'\n    )\n    eta_peak = models.FloatField(\n        help_text='Eta metric for peak flux.'\n    )\n    new_high_sigma = models.FloatField(\n        help_text=(\n            'The largest sigma value for the new source'\n            ' if it was placed in previous image.'\n        )\n    )\n    n_neighbour_dist = models.FloatField(\n        help_text='Distance to the nearest neighbour (deg)'\n    )\n    vs_abs_significant_max_int = models.FloatField(\n        default=0.0,\n        help_text=(\n            'Maximum value of all measurement pair variability t-statistics for int'\n            ' flux that exceed variability.source_aggregate_pair_metrics_min_abs_vs in'\n            ' the pipeline run configuration.'\n        )\n    )\n    m_abs_significant_max_int = models.FloatField(\n        default=0.0,\n        help_text=(\n            'Maximum absolute value of all measurement pair modulation indices for int'\n            ' flux that exceed variability.source_aggregate_pair_metrics_min_abs_vs in'\n            ' the pipeline run configuration.'\n        )\n    )\n    vs_abs_significant_max_peak = models.FloatField(\n        default=0.0,\n        help_text=(\n            'Maximum absolute value of all measurement pair variability t-statistics for'\n            ' peak flux that exceed variability.source_aggregate_pair_metrics_min_abs_vs'\n            ' in the pipeline run configuration.'\n        )\n    )\n    m_abs_significant_max_peak = models.FloatField(\n        default=0.0,\n        help_text=(\n            'Maximum absolute value of all measurement pair modulation indices for '\n            ' peak flux that exceed variability.source_aggregate_pair_metrics_min_abs_vs'\n            ' in the pipeline run configuration.'\n        )\n    )\n\n    # total metrics to report in UI\n    n_meas = models.IntegerField(\n        help_text='total measurements of the source'\n    )\n    n_meas_sel = models.IntegerField(\n        help_text='total selavy extracted measurements of the source'\n    )\n    n_meas_forced = models.IntegerField(\n        help_text='total force extracted measurements of the source'\n    )\n    n_rel = models.IntegerField(\n        help_text='total relations of the source with other sources'\n    )\n    n_sibl = models.IntegerField(\n        help_text='total siblings of the source'\n    )\n\n    objects = SourceQuerySet.as_manager()\n\n    def __str__(self):\n        return self.name\n\n    def get_measurement_pairs(self) -&gt; List[MeasurementPair]:\n        \"\"\"Calculate the measurement pair metrics for the source. If the run config\n        set variability.pair_metrics to false, then no pairs are calculated and an empty\n        list is returned.\n\n        Returns:\n            List[MeasurementPair]: The list of measurement pairs and their metrics.\n        \"\"\"\n        # do not calculate pair metrics if it was disabled in the run config\n        config = self.run.get_config(validate=False, validate_inputs=False, prev=True)\n        # validate the config schema only, not the full validation executed by\n        # PipelineConfig.validate.\n        config._yaml.revalidate(PipelineConfig.SCHEMA)\n        if not config[\"variability\"][\"pair_metrics\"]:\n            return []\n\n        measurements = (\n            Measurement.objects.filter(source=self)\n            .select_related(\"image\")\n            .order_by(\"image__datetime\")\n        )\n        measurement_pairs: List[MeasurementPair] = []\n        for meas_a, meas_b in combinations(measurements, 2):\n            # ensure the measurements are in time order\n            if meas_a.image.datetime &gt; meas_b.image.datetime:\n                meas_a, meas_b = meas_b, meas_a\n            # calculate metrics\n            vs_peak = calculate_vs_metric(\n                meas_a.flux_peak, meas_b.flux_peak, meas_a.flux_peak_err, meas_b.flux_peak_err\n            )\n            m_int = calculate_m_metric(meas_a.flux_int, meas_b.flux_int)\n            vs_int = calculate_vs_metric(\n                meas_a.flux_int, meas_b.flux_int, meas_a.flux_int_err, meas_b.flux_int_err\n            )\n            m_peak = calculate_m_metric(meas_a.flux_peak, meas_b.flux_peak)\n            measurement_pairs.append(\n                MeasurementPair(self.id, meas_a.id, meas_b.id, vs_peak, m_peak, vs_int, m_int)\n            )\n        return measurement_pairs\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.Source.get_measurement_pairs","title":"<code>get_measurement_pairs()</code>","text":"<p>Calculate the measurement pair metrics for the source. If the run config set variability.pair_metrics to false, then no pairs are calculated and an empty list is returned.</p> <p>Returns:</p> Type Description <code>List[MeasurementPair]</code> <p>List[MeasurementPair]: The list of measurement pairs and their metrics.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>def get_measurement_pairs(self) -&gt; List[MeasurementPair]:\n    \"\"\"Calculate the measurement pair metrics for the source. If the run config\n    set variability.pair_metrics to false, then no pairs are calculated and an empty\n    list is returned.\n\n    Returns:\n        List[MeasurementPair]: The list of measurement pairs and their metrics.\n    \"\"\"\n    # do not calculate pair metrics if it was disabled in the run config\n    config = self.run.get_config(validate=False, validate_inputs=False, prev=True)\n    # validate the config schema only, not the full validation executed by\n    # PipelineConfig.validate.\n    config._yaml.revalidate(PipelineConfig.SCHEMA)\n    if not config[\"variability\"][\"pair_metrics\"]:\n        return []\n\n    measurements = (\n        Measurement.objects.filter(source=self)\n        .select_related(\"image\")\n        .order_by(\"image__datetime\")\n    )\n    measurement_pairs: List[MeasurementPair] = []\n    for meas_a, meas_b in combinations(measurements, 2):\n        # ensure the measurements are in time order\n        if meas_a.image.datetime &gt; meas_b.image.datetime:\n            meas_a, meas_b = meas_b, meas_a\n        # calculate metrics\n        vs_peak = calculate_vs_metric(\n            meas_a.flux_peak, meas_b.flux_peak, meas_a.flux_peak_err, meas_b.flux_peak_err\n        )\n        m_int = calculate_m_metric(meas_a.flux_int, meas_b.flux_int)\n        vs_int = calculate_vs_metric(\n            meas_a.flux_int, meas_b.flux_int, meas_a.flux_int_err, meas_b.flux_int_err\n        )\n        m_peak = calculate_m_metric(meas_a.flux_peak, meas_b.flux_peak)\n        measurement_pairs.append(\n            MeasurementPair(self.id, meas_a.id, meas_b.id, vs_peak, m_peak, vs_int, m_int)\n        )\n    return measurement_pairs\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.SourceAdmin","title":"<code>SourceAdmin</code>","text":"<p>               Bases: <code>ModelAdmin</code></p> <p>The SourceAdmin class.</p> Source code in <code>vast_pipeline/admin.py</code> <pre><code>class SourceAdmin(admin.ModelAdmin):\n    \"\"\"\n    The SourceAdmin class.\n    \"\"\"\n    list_display = ('name', 'wavg_ra', 'wavg_dec', 'new')\n    list_filter = ('new',)\n    search_fields = ('name',)\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.SourceFavAdmin","title":"<code>SourceFavAdmin</code>","text":"<p>               Bases: <code>ModelAdmin</code></p> <p>The SourceFavAdmin class.</p> Source code in <code>vast_pipeline/admin.py</code> <pre><code>class SourceFavAdmin(admin.ModelAdmin):\n    \"\"\"\n    The SourceFavAdmin class.\n    \"\"\"\n    list_display = ('user', 'source', 'comment')\n    list_filter = ('user',)\n    search_fields = ('user','source')\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.SourceQuerySet","title":"<code>SourceQuerySet</code>","text":"<p>               Bases: <code>QuerySet</code></p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class SourceQuerySet(models.QuerySet):\n\n    def cone_search(\n        self, ra: float, dec: float, radius_deg: float\n    ) -&gt; models.QuerySet:\n        \"\"\"\n        Return all the Sources withing radius_deg of (ra,dec).\n        Returns a QuerySet of Sources, ordered by distance from\n        (ra,dec) ascending.\n\n        Args:\n            ra: The right ascension value of the cone search central\n                coordinate.\n            dec: The declination value of the cone search central coordinate.\n            radius_deg: The radius over which to perform the cone search.\n\n        Returns:\n            Sources found withing the cone search area.\n        \"\"\"\n        return (\n            self.extra(\n                select={\n                    \"distance\": \"q3c_dist(wavg_ra, wavg_dec, %s, %s) * 3600\"\n                },\n                select_params=[ra, dec],\n                where=[\"q3c_radial_query(wavg_ra, wavg_dec, %s, %s, %s)\"],\n                params=[ra, dec, radius_deg],\n            )\n            .order_by(\"distance\")\n        )\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.SourceQuerySet.cone_search","title":"<code>cone_search(ra, dec, radius_deg)</code>","text":"<p>Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending.</p> <p>Parameters:</p> Name Type Description Default <code>ra</code> <code>float</code> <p>The right ascension value of the cone search central coordinate.</p> required <code>dec</code> <code>float</code> <p>The declination value of the cone search central coordinate.</p> required <code>radius_deg</code> <code>float</code> <p>The radius over which to perform the cone search.</p> required <p>Returns:</p> Type Description <code>QuerySet</code> <p>Sources found withing the cone search area.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>def cone_search(\n    self, ra: float, dec: float, radius_deg: float\n) -&gt; models.QuerySet:\n    \"\"\"\n    Return all the Sources withing radius_deg of (ra,dec).\n    Returns a QuerySet of Sources, ordered by distance from\n    (ra,dec) ascending.\n\n    Args:\n        ra: The right ascension value of the cone search central\n            coordinate.\n        dec: The declination value of the cone search central coordinate.\n        radius_deg: The radius over which to perform the cone search.\n\n    Returns:\n        Sources found withing the cone search area.\n    \"\"\"\n    return (\n        self.extra(\n            select={\n                \"distance\": \"q3c_dist(wavg_ra, wavg_dec, %s, %s) * 3600\"\n            },\n            select_params=[ra, dec],\n            where=[\"q3c_radial_query(wavg_ra, wavg_dec, %s, %s, %s)\"],\n            params=[ra, dec, radius_deg],\n        )\n        .order_by(\"distance\")\n    )\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.calculate_m_metric","title":"<code>calculate_m_metric(flux_a, flux_b)</code>","text":"<p>Calculate the m variability metric which is the modulation index between two fluxes. This is proportional to the fractional variability. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105.</p> <p>Parameters:</p> Name Type Description Default <code>flux_a</code> <code>float</code> <p>flux value \"A\".</p> required <code>flux_b</code> <code>float</code> <p>flux value \"B\".</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>the m metric for flux values \"A\" and \"B\".</p> Source code in <code>vast_pipeline/pipeline/pairs.py</code> <pre><code>def calculate_m_metric(flux_a: float, flux_b: float) -&gt; float:\n    \"\"\"Calculate the m variability metric which is the modulation index between two fluxes.\n    This is proportional to the fractional variability.\n    See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105.\n\n    Args:\n        flux_a (float): flux value \"A\".\n        flux_b (float): flux value \"B\".\n\n    Returns:\n        float: the m metric for flux values \"A\" and \"B\".\n    \"\"\"\n    return 2 * ((flux_a - flux_b) / (flux_a + flux_b))\n</code></pre>"},{"location":"reference/admin/#vast_pipeline.admin.calculate_vs_metric","title":"<code>calculate_vs_metric(flux_a, flux_b, flux_err_a, flux_err_b)</code>","text":"<p>Calculate the Vs variability metric which is the t-statistic that the provided fluxes are variable. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105.</p> <p>Parameters:</p> Name Type Description Default <code>flux_a</code> <code>float</code> <p>flux value \"A\".</p> required <code>flux_b</code> <code>float</code> <p>flux value \"B\".</p> required <code>flux_err_a</code> <code>float</code> <p>error of <code>flux_a</code>.</p> required <code>flux_err_b</code> <code>float</code> <p>error of <code>flux_b</code>.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>the Vs metric for flux values \"A\" and \"B\".</p> Source code in <code>vast_pipeline/pipeline/pairs.py</code> <pre><code>def calculate_vs_metric(\n    flux_a: float, flux_b: float, flux_err_a: float, flux_err_b: float\n) -&gt; float:\n    \"\"\"Calculate the Vs variability metric which is the t-statistic that the provided\n    fluxes are variable. See Section 5 of Mooley et al. (2016) for details,\n    DOI: 10.3847/0004-637X/818/2/105.\n\n    Args:\n        flux_a (float): flux value \"A\".\n        flux_b (float): flux value \"B\".\n        flux_err_a (float): error of `flux_a`.\n        flux_err_b (float): error of `flux_b`.\n\n    Returns:\n        float: the Vs metric for flux values \"A\" and \"B\".\n    \"\"\"\n    return (flux_a - flux_b) / np.hypot(flux_err_a, flux_err_b)\n</code></pre>"},{"location":"reference/apps/","title":"apps.py","text":""},{"location":"reference/apps/#vast_pipeline.apps.PipelineConfig","title":"<code>PipelineConfig</code>","text":"<p>               Bases: <code>AppConfig</code></p> <p>Class representing the configuration for the vast_pipeline app.</p> Source code in <code>vast_pipeline/apps.py</code> <pre><code>class PipelineConfig(AppConfig):\n    \"\"\"Class representing the configuration for the vast_pipeline app.\"\"\"\n    name = 'vast_pipeline'\n\n    def ready(self) -&gt; None:\n        \"\"\"Initialization tasks performed as soon as the app registry is populated.\n        See &lt;https://docs.djangoproject.com/en/3.1/ref/applications/#django.apps.AppConfig.ready&gt;.\n        \"\"\"\n        # import the signals to register them with the application\n        import vast_pipeline.signals  # noqa: F401\n</code></pre>"},{"location":"reference/apps/#vast_pipeline.apps.PipelineConfig.ready","title":"<code>ready()</code>","text":"<p>Initialization tasks performed as soon as the app registry is populated. See https://docs.djangoproject.com/en/3.1/ref/applications/#django.apps.AppConfig.ready.</p> Source code in <code>vast_pipeline/apps.py</code> <pre><code>def ready(self) -&gt; None:\n    \"\"\"Initialization tasks performed as soon as the app registry is populated.\n    See &lt;https://docs.djangoproject.com/en/3.1/ref/applications/#django.apps.AppConfig.ready&gt;.\n    \"\"\"\n    # import the signals to register them with the application\n    import vast_pipeline.signals  # noqa: F401\n</code></pre>"},{"location":"reference/context_processors/","title":"context_processors.py","text":""},{"location":"reference/context_processors/#vast_pipeline.context_processors.maintainance_banner","title":"<code>maintainance_banner(request)</code>","text":"<p>Generates the maintainance banner for the web server if a message has been set in the Django settings.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>HttpRequest</code> <p>The web server request.</p> required <p>Returns:</p> Type Description <code>Dict[str, Optional[str]]</code> <p>Dictionary representing the JSON object with the maintainance message.</p> Source code in <code>vast_pipeline/context_processors.py</code> <pre><code>def maintainance_banner(request: HttpRequest) -&gt; Dict[str, Optional[str]]:\n    \"\"\"\n    Generates the maintainance banner for the web server if a message has been\n    set in the Django settings.\n\n    Args:\n        request (HttpRequest): The web server request.\n\n    Returns:\n        Dictionary representing the JSON object with the maintainance message.\n    \"\"\"\n    if settings.PIPELINE_MAINTAINANCE_MESSAGE:\n        return {\"maintainance_message\": settings.PIPELINE_MAINTAINANCE_MESSAGE}\n\n    return {\"maintainance_message\": None}\n</code></pre>"},{"location":"reference/context_processors/#vast_pipeline.context_processors.pipeline_version","title":"<code>pipeline_version(request)</code>","text":"<p>Adds the pipeline version to the template context.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>HttpRequest</code> <p>The web server request.</p> required <p>Returns:</p> Type Description <code>Dict[str, Optional[str]]</code> <p>Key-value pairs to add to the template context.</p> Source code in <code>vast_pipeline/context_processors.py</code> <pre><code>def pipeline_version(request: HttpRequest) -&gt; Dict[str, Optional[str]]:\n    \"\"\"Adds the pipeline version to the template context.\n\n    Args:\n        request (HttpRequest): The web server request.\n\n    Returns:\n        Key-value pairs to add to the template context.\n    \"\"\"\n    url: Optional[str] = None\n    if not __version__.endswith(\"dev\"):\n        url = f\"https://github.com/askap-vast/vast-pipeline/releases/tag/v{__version__}\"\n\n    return {\n        \"pipeline_version\": __version__,\n        \"pipeline_version_url\": url,\n    }\n</code></pre>"},{"location":"reference/converters/","title":"converters.py","text":""},{"location":"reference/converters/#vast_pipeline.converters.AngleConverter","title":"<code>AngleConverter</code>","text":"<p>Accept any valid input value for an astropy.coordinates.Angle and ensure the returned value is a float in decimal degrees. The unit should be included in the input value.</p> Source code in <code>vast_pipeline/converters.py</code> <pre><code>class AngleConverter:\n    \"\"\"Accept any valid input value for an astropy.coordinates.Angle and ensure the\n    returned value is a float in decimal degrees. The unit should be included in the input\n    value.\"\"\"\n    regex = r\"\\d+(\\.\\d+)?\\s?\\w+\"\n\n    def to_python(self, value: Angle) -&gt; float:\n        \"\"\"\n        Return the decimal degrees from the coordinate input as an Angle object.\n\n        Args:\n            value: The value of the angle input.\n\n        Returns:\n            The angle returned as an Angle object.\n        \"\"\"\n        return Angle(value).deg\n\n    def to_url(self, value: Angle) -&gt; str:\n        \"\"\"\n        Return the string format of an Angle object from the coordinate input.\n\n        Args:\n            value: The value of the angle input.\n\n        Returns:\n            The string representation of the Angle object created from the input.\n        \"\"\"\n        return value.to_string()\n</code></pre>"},{"location":"reference/converters/#vast_pipeline.converters.AngleConverter.to_python","title":"<code>to_python(value)</code>","text":"<p>Return the decimal degrees from the coordinate input as an Angle object.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Angle</code> <p>The value of the angle input.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The angle returned as an Angle object.</p> Source code in <code>vast_pipeline/converters.py</code> <pre><code>def to_python(self, value: Angle) -&gt; float:\n    \"\"\"\n    Return the decimal degrees from the coordinate input as an Angle object.\n\n    Args:\n        value: The value of the angle input.\n\n    Returns:\n        The angle returned as an Angle object.\n    \"\"\"\n    return Angle(value).deg\n</code></pre>"},{"location":"reference/converters/#vast_pipeline.converters.AngleConverter.to_url","title":"<code>to_url(value)</code>","text":"<p>Return the string format of an Angle object from the coordinate input.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Angle</code> <p>The value of the angle input.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the Angle object created from the input.</p> Source code in <code>vast_pipeline/converters.py</code> <pre><code>def to_url(self, value: Angle) -&gt; str:\n    \"\"\"\n    Return the string format of an Angle object from the coordinate input.\n\n    Args:\n        value: The value of the angle input.\n\n    Returns:\n        The string representation of the Angle object created from the input.\n    \"\"\"\n    return value.to_string()\n</code></pre>"},{"location":"reference/converters/#vast_pipeline.converters.DeclinationConverter","title":"<code>DeclinationConverter</code>","text":"<p>Accept both decimal and sexigesimal representations of Dec and ensure the returned value is a float in decimal degrees. The input units are always assumed to be degrees.</p> Source code in <code>vast_pipeline/converters.py</code> <pre><code>class DeclinationConverter:\n    \"\"\"Accept both decimal and sexigesimal representations of Dec and ensure the returned\n    value is a float in decimal degrees. The input units are always assumed to be degrees.\"\"\"\n    regex = r\"(?:\\+|-)?(?:\\d{1,2}:\\d{1,2}:\\d{1,2}(?:\\.\\d+)?|\\d+(?:\\.\\d+)?)\"\n\n    def to_python(self, value: str) -&gt; float:\n        \"\"\"\n        Return the decimal degrees from the coordinate input as a python float\n        object.\n\n        Args:\n            value: The value of the declination input.\n\n        Returns:\n            The decimal degrees value.\n        \"\"\"\n        return Latitude(value, unit=\"deg\").deg\n\n    def to_url(self, value: str) -&gt; str:\n        \"\"\"\n        Return the decimal degrees from the coordinate input in a URL format.\n\n        Args:\n            value: The value of the declination input.\n\n        Returns:\n            The decimal degrees value as a string.\n        \"\"\"\n        return value.to_string(unit=\"deg\", decimal=True)\n</code></pre>"},{"location":"reference/converters/#vast_pipeline.converters.DeclinationConverter.to_python","title":"<code>to_python(value)</code>","text":"<p>Return the decimal degrees from the coordinate input as a python float object.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value of the declination input.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The decimal degrees value.</p> Source code in <code>vast_pipeline/converters.py</code> <pre><code>def to_python(self, value: str) -&gt; float:\n    \"\"\"\n    Return the decimal degrees from the coordinate input as a python float\n    object.\n\n    Args:\n        value: The value of the declination input.\n\n    Returns:\n        The decimal degrees value.\n    \"\"\"\n    return Latitude(value, unit=\"deg\").deg\n</code></pre>"},{"location":"reference/converters/#vast_pipeline.converters.DeclinationConverter.to_url","title":"<code>to_url(value)</code>","text":"<p>Return the decimal degrees from the coordinate input in a URL format.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value of the declination input.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The decimal degrees value as a string.</p> Source code in <code>vast_pipeline/converters.py</code> <pre><code>def to_url(self, value: str) -&gt; str:\n    \"\"\"\n    Return the decimal degrees from the coordinate input in a URL format.\n\n    Args:\n        value: The value of the declination input.\n\n    Returns:\n        The decimal degrees value as a string.\n    \"\"\"\n    return value.to_string(unit=\"deg\", decimal=True)\n</code></pre>"},{"location":"reference/converters/#vast_pipeline.converters.RightAscensionConverter","title":"<code>RightAscensionConverter</code>","text":"<p>Accept both decimal and sexigesimal representations of RA and ensure the returned value is a float in decimal degrees. If the input is in sexigesimal format, assume it is in units of hourangle.</p> Source code in <code>vast_pipeline/converters.py</code> <pre><code>class RightAscensionConverter:\n    \"\"\"Accept both decimal and sexigesimal representations of RA and ensure the returned\n    value is a float in decimal degrees. If the input is in sexigesimal format, assume\n    it is in units of hourangle.\"\"\"\n    regex = r\"(?:\\d+(?:\\.\\d+)?|\\d{1,2}:\\d{1,2}:\\d{1,2}(?:\\.\\d+)?)\"\n\n    def to_python(self, value: str) -&gt; float:\n        \"\"\"\n        Return the decimal degrees from the coordinate input as a python float\n        object.\n\n        Args:\n            value: The value of the RA input.\n\n        Returns:\n            The decimal degrees value.\n        \"\"\"\n        unit = \"hourangle\" if \":\" in value else \"deg\"\n        return Longitude(value, unit=unit).deg\n\n    def to_url(self, value: str) -&gt; str:\n        \"\"\"\n        Return the decimal degrees from the coordinate input in a URL format.\n\n        Args:\n            value: The value of the RA input.\n\n        Returns:\n            The decimal degrees value as a string.\n        \"\"\"\n        return value.to_string(unit=\"deg\", decimal=True)\n</code></pre>"},{"location":"reference/converters/#vast_pipeline.converters.RightAscensionConverter.to_python","title":"<code>to_python(value)</code>","text":"<p>Return the decimal degrees from the coordinate input as a python float object.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value of the RA input.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The decimal degrees value.</p> Source code in <code>vast_pipeline/converters.py</code> <pre><code>def to_python(self, value: str) -&gt; float:\n    \"\"\"\n    Return the decimal degrees from the coordinate input as a python float\n    object.\n\n    Args:\n        value: The value of the RA input.\n\n    Returns:\n        The decimal degrees value.\n    \"\"\"\n    unit = \"hourangle\" if \":\" in value else \"deg\"\n    return Longitude(value, unit=unit).deg\n</code></pre>"},{"location":"reference/converters/#vast_pipeline.converters.RightAscensionConverter.to_url","title":"<code>to_url(value)</code>","text":"<p>Return the decimal degrees from the coordinate input in a URL format.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value of the RA input.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The decimal degrees value as a string.</p> Source code in <code>vast_pipeline/converters.py</code> <pre><code>def to_url(self, value: str) -&gt; str:\n    \"\"\"\n    Return the decimal degrees from the coordinate input in a URL format.\n\n    Args:\n        value: The value of the RA input.\n\n    Returns:\n        The decimal degrees value as a string.\n    \"\"\"\n    return value.to_string(unit=\"deg\", decimal=True)\n</code></pre>"},{"location":"reference/forms/","title":"forms.py","text":""},{"location":"reference/forms/#vast_pipeline.forms.CommentForm","title":"<code>CommentForm</code>","text":"<p>               Bases: <code>ModelForm</code></p> <p>The form used for users to leave comments on objects.</p> Source code in <code>vast_pipeline/forms.py</code> <pre><code>class CommentForm(forms.ModelForm):\n    \"\"\"\n    The form used for users to leave comments on objects.\n    \"\"\"\n    class Meta:\n        model = Comment\n        fields = [\"comment\"]\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        \"\"\"\n        Initialise a CommentForm.\n\n        Returns:\n            None.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.helper = FormHelper()\n        self.helper.layout = Layout(\n            Field(\"comment\", rows=2),\n        )\n        self.helper.add_input(Submit(\"submit\", \"Submit\"))\n</code></pre>"},{"location":"reference/forms/#vast_pipeline.forms.CommentForm.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialise a CommentForm.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>vast_pipeline/forms.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    \"\"\"\n    Initialise a CommentForm.\n\n    Returns:\n        None.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.helper = FormHelper()\n    self.helper.layout = Layout(\n        Field(\"comment\", rows=2),\n    )\n    self.helper.add_input(Submit(\"submit\", \"Submit\"))\n</code></pre>"},{"location":"reference/forms/#vast_pipeline.forms.PipelineRunForm","title":"<code>PipelineRunForm</code>","text":"<p>               Bases: <code>Form</code></p> <p>Class for the form used in the creation of a new pipeline run through the webserver.</p> Source code in <code>vast_pipeline/forms.py</code> <pre><code>class PipelineRunForm(forms.Form):\n    \"\"\"\n    Class for the form used in the creation of a new pipeline run through the\n    webserver.\n    \"\"\"\n    run_name = forms.CharField(\n        max_length=Run._meta.get_field('name').max_length\n    )\n    run_description = forms.CharField(widget=forms.Textarea(), required=False)\n    monitor = forms.BooleanField(required=False)\n    monitor_min_sigma = forms.FloatField()\n    monitor_edge_buffer_scale = forms.FloatField()\n    monitor_cluster_threshold = forms.FloatField()\n    monitor_allow_nan = forms.BooleanField(required=False)\n    association_method = forms.CharField()\n    association_radius = forms.FloatField()\n    association_de_ruiter_radius = forms.FloatField()\n    association_parallel = forms.BooleanField(required=False)\n    association_epoch_duplicate_radius = forms.FloatField()\n    astrometric_uncertainty_ra = forms.FloatField()\n    astrometric_uncertainty_dec = forms.FloatField()\n    new_source_min_sigma = forms.FloatField()\n    association_beamwidth_limit = forms.FloatField()\n    flux_perc_error = forms.FloatField()\n    selavy_local_rms_zero_fill_value = forms.FloatField()\n    source_aggregate_pair_metrics_min_abs_vs = forms.FloatField()\n    pair_metrics = forms.BooleanField(required=False)\n    use_condon_errors = forms.BooleanField(required=False)\n    create_measurements_arrow_files = forms.BooleanField(required=False)\n    suppress_astropy_warnings = forms.BooleanField(required=False)\n</code></pre>"},{"location":"reference/forms/#vast_pipeline.forms.TagWithCommentsForm","title":"<code>TagWithCommentsForm</code>","text":"<p>               Bases: <code>Form</code></p> <p>Class to combined tags with the CommentsForm.</p> Source code in <code>vast_pipeline/forms.py</code> <pre><code>class TagWithCommentsForm(forms.Form):\n    \"\"\"\n    Class to combined tags with the CommentsForm.\n    \"\"\"\n    comment = forms.CharField(required=False, widget=forms.Textarea())\n    tags = tagulous.forms.TagField(\n        required=False,\n        tag_options=tagulous.models.TagOptions(**Source.tags.tag_options.items()),\n    )\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        \"\"\"\n        Initialise a TagWithCommentsForm.\n\n        Returns:\n            None.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.helper = FormHelper()\n        self.helper.layout = Layout(\n            Field(\"tags\"),\n            Field(\n                \"comment\",\n                rows=2,\n                placeholder=(\n                    \"Optional. If changing the tags, you should provide justification here.\"\n                ),\n            ),\n            Submit(\"submit\", \"Submit\", css_class=\"btn-block\"),\n        )\n</code></pre>"},{"location":"reference/forms/#vast_pipeline.forms.TagWithCommentsForm.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialise a TagWithCommentsForm.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>vast_pipeline/forms.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    \"\"\"\n    Initialise a TagWithCommentsForm.\n\n    Returns:\n        None.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.helper = FormHelper()\n    self.helper.layout = Layout(\n        Field(\"tags\"),\n        Field(\n            \"comment\",\n            rows=2,\n            placeholder=(\n                \"Optional. If changing the tags, you should provide justification here.\"\n            ),\n        ),\n        Submit(\"submit\", \"Submit\", css_class=\"btn-block\"),\n    )\n</code></pre>"},{"location":"reference/models/","title":"models.py","text":""},{"location":"reference/models/#vast_pipeline.models.Association","title":"<code>Association</code>","text":"<p>               Bases: <code>Model</code></p> <p>model association between sources and measurements based on some parameters</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class Association(models.Model):\n    \"\"\"\n    model association between sources and measurements based on\n    some parameters\n    \"\"\"\n    source = models.ForeignKey(Source, on_delete=models.CASCADE)\n    meas = models.ForeignKey(Measurement, on_delete=models.CASCADE)\n\n    d2d = models.FloatField(\n        default=0.,\n        help_text='astronomical distance calculated by Astropy, arcsec.'\n    )\n    dr = models.FloatField(\n        default=0.,\n        help_text='De Ruiter radius calculated in advanced association.'\n    )\n\n    def __str__(self):\n        return (\n            f'distance: {self.d2d:.2f}' if self.dr == 0 else\n            f'distance: {self.dr:.2f}'\n        )\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.Band","title":"<code>Band</code>","text":"<p>               Bases: <code>Model</code></p> <p>A band on the frequency spectrum used for imaging. Each image is associated with one band.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class Band(models.Model):\n    \"\"\"\n    A band on the frequency spectrum used for imaging. Each image is\n    associated with one band.\n    \"\"\"\n    name = models.CharField(max_length=12, unique=True)\n    frequency = models.FloatField(\n        help_text='central frequency of band (integer MHz)'\n    )\n    bandwidth = models.FloatField(\n        help_text='bandwidth (MHz)'\n    )\n\n    class Meta:\n        ordering = ['frequency']\n\n    def __str__(self):\n        return self.name\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.Comment","title":"<code>Comment</code>","text":"<p>               Bases: <code>Model</code></p> <p>The model object for a comment.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class Comment(models.Model):\n    \"\"\"\n    The model object for a comment.\n    \"\"\"\n    author = models.ForeignKey(User, on_delete=models.CASCADE)\n    datetime = models.DateTimeField(auto_now_add=True)\n    comment = models.TextField()\n    content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n    object_id = models.PositiveIntegerField()\n    content_object = GenericForeignKey('content_type', 'object_id')\n\n    def get_avatar_url(self) -&gt; str:\n        \"\"\"Get the URL for the user's avatar from GitHub. If the user has\n        no associated GitHub account (e.g. a Django superuser), return the URL\n        to the default user avatar.\n\n        Returns:\n            The avatar URL.\n        \"\"\"\n        social = UserSocialAuth.get_social_auth_for_user(self.author).first()\n        if social and \"avatar_url\" in social.extra_data:\n            return social.extra_data[\"avatar_url\"]\n        else:\n            return static(\"img/user-32.png\")\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.Comment.get_avatar_url","title":"<code>get_avatar_url()</code>","text":"<p>Get the URL for the user's avatar from GitHub. If the user has no associated GitHub account (e.g. a Django superuser), return the URL to the default user avatar.</p> <p>Returns:</p> Type Description <code>str</code> <p>The avatar URL.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>def get_avatar_url(self) -&gt; str:\n    \"\"\"Get the URL for the user's avatar from GitHub. If the user has\n    no associated GitHub account (e.g. a Django superuser), return the URL\n    to the default user avatar.\n\n    Returns:\n        The avatar URL.\n    \"\"\"\n    social = UserSocialAuth.get_social_auth_for_user(self.author).first()\n    if social and \"avatar_url\" in social.extra_data:\n        return social.extra_data[\"avatar_url\"]\n    else:\n        return static(\"img/user-32.png\")\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.CommentableModel","title":"<code>CommentableModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>A class to provide a commentable model.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class CommentableModel(models.Model):\n    \"\"\"\n    A class to provide a commentable model.\n    \"\"\"\n    comment = GenericRelation(\n        Comment,\n        content_type_field=\"content_type\",\n        object_id_field=\"object_id\",\n        related_query_name=\"%(class)s\",\n    )\n\n    class Meta:\n        abstract = True\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.Image","title":"<code>Image</code>","text":"<p>               Bases: <code>CommentableModel</code></p> <p>An image is a 2D radio image from a cube</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class Image(CommentableModel):\n    \"\"\"An image is a 2D radio image from a cube\"\"\"\n    band = models.ForeignKey(Band, on_delete=models.CASCADE)\n    run = models.ManyToManyField(Run)\n    skyreg = models.ForeignKey(SkyRegion, on_delete=models.CASCADE)\n\n    measurements_path = models.FilePathField(\n        max_length=200,\n        db_column='meas_path',\n        help_text=(\n            'the path to the measurements parquet that belongs to this image'\n        )\n    )\n    POLARISATION_CHOICES = [\n        ('I', 'I'),\n        ('XX', 'XX'),\n        ('YY', 'YY'),\n        ('Q', 'Q'),\n        ('U', 'U'),\n        ('V', 'V'),\n    ]\n    polarisation = models.CharField(\n        max_length=2,\n        choices=POLARISATION_CHOICES,\n        help_text='Polarisation of the image one of I,XX,YY,Q,U,V.'\n    )\n    name = models.CharField(\n        unique=True,\n        max_length=200,\n        help_text='Name of the image.'\n    )\n    path = models.FilePathField(\n        max_length=500,\n        help_text='Path to the file containing the image.'\n    )\n    noise_path = models.FilePathField(\n        max_length=300,\n        blank=True,\n        default='',\n        help_text='Path to the file containing the RMS image.'\n    )\n    background_path = models.FilePathField(\n        max_length=300,\n        blank=True,\n        default='',\n        help_text='Path to the file containing the background image.'\n    )\n\n    datetime = models.DateTimeField(\n        help_text='Date/time of observation or epoch.'\n    )\n    jd = models.FloatField(\n        help_text='Julian date of the observation (days).'\n    )\n    duration = models.FloatField(\n        default=0.,\n        help_text='Duration of the observation.'\n    )\n\n    ra = models.FloatField(\n        help_text='RA of the image centre (Deg).'\n    )\n    dec = models.FloatField(\n        help_text='DEC of the image centre (Deg).'\n    )\n    fov_bmaj = models.FloatField(\n        help_text='Field of view major axis (Deg).'\n    )  # Major (Dec) radius of image (degrees)\n    fov_bmin = models.FloatField(\n        help_text='Field of view minor axis (Deg).'\n    )  # Minor (RA) radius of image (degrees)\n    physical_bmaj = models.FloatField(\n        help_text='The actual size of the image major axis (Deg).'\n    )  # Major (Dec) radius of image (degrees)\n    physical_bmin = models.FloatField(\n        help_text='The actual size of the image minor axis (Deg).'\n    )  # Minor (RA) radius of image (degrees)\n    radius_pixels = models.FloatField(\n        help_text='Radius of the useable region of the image (pixels).'\n    )\n\n    beam_bmaj = models.FloatField(\n        help_text='Major axis of image restoring beam (Deg).'\n    )\n    beam_bmin = models.FloatField(\n        help_text='Minor axis of image restoring beam (Deg).'\n    )\n    beam_bpa = models.FloatField(\n        help_text='Beam position angle (Deg).'\n    )\n    rms_median = models.FloatField(\n        help_text='Background average RMS from the provided RMS map (mJy).'\n    )\n    rms_min = models.FloatField(\n        help_text='Background minimum RMS from the provided RMS map (mJy).'\n    )\n    rms_max = models.FloatField(\n        help_text='Background maximum RMS from the provided RMS map (mJy).'\n    )\n\n    class Meta:\n        ordering = ['datetime']\n\n    def __str__(self):\n        return self.name\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.Measurement","title":"<code>Measurement</code>","text":"<p>               Bases: <code>CommentableModel</code></p> <p>A Measurement is an object in the sky that has been detected at least once. Essentially a source single measurement in time.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class Measurement(CommentableModel):\n    \"\"\"\n    A Measurement is an object in the sky that has been detected at least once.\n    Essentially a source single measurement in time.\n    \"\"\"\n    image = models.ForeignKey(\n        Image,\n        null=True,\n        on_delete=models.CASCADE\n    )  # first image seen in\n    source = models.ManyToManyField(\n        'Source',\n        through='Association',\n        through_fields=('meas', 'source')\n    )\n\n    name = models.CharField(max_length=64)\n\n    ra = models.FloatField(help_text='RA of the source (Deg).')  # degrees\n    ra_err = models.FloatField(\n        help_text='RA error of the source (Deg).'\n    )\n    dec = models.FloatField(help_text='DEC of the source (Deg).')  # degrees\n    dec_err = models.FloatField(\n        help_text='DEC error of the source (Deg).'\n    )\n\n    bmaj = models.FloatField(\n        help_text=(\n            'The major axis of the Gaussian fit to the source (Deg).'\n        )\n    )\n    err_bmaj = models.FloatField(help_text='Error major axis (Deg).')\n    bmin = models.FloatField(\n        help_text=(\n            'The minor axis of the Gaussian fit to the source (Deg).'\n        )\n    )\n    err_bmin = models.FloatField(help_text='Error minor axis (Deg).')\n    pa = models.FloatField(\n        help_text=(\n            'Position angle of Gaussian fit east of north to bmaj '\n            '(Deg).'\n        )\n    )\n    err_pa = models.FloatField(help_text='Error position angle (Deg).')\n\n    # supplied by user via config\n    ew_sys_err = models.FloatField(\n        help_text='Systematic error in east-west (RA) direction (Deg).'\n    )\n    # supplied by user via config\n    ns_sys_err = models.FloatField(\n        help_text='Systematic error in north-south (dec) direction (Deg).'\n    )\n\n    # estimate of maximum error radius (from ra_err and dec_err)\n    # Used in advanced association.\n    error_radius = models.FloatField(\n        help_text=(\n            'Estimate of maximum error radius using ra_err'\n            ' and dec_err (Deg).'\n        )\n    )\n\n    # quadratic sum of error_radius and ew_sys_err\n    uncertainty_ew = models.FloatField(\n        help_text=(\n            'Total east-west (RA) uncertainty, quadratic sum of'\n            ' error_radius and ew_sys_err (Deg).'\n        )\n    )\n    # quadratic sum of error_radius and ns_sys_err\n    uncertainty_ns = models.FloatField(\n        help_text=(\n            'Total north-south (Dec) uncertainty, quadratic sum of '\n            'error_radius and ns_sys_err (Deg).'\n        )\n    )\n\n    flux_int = models.FloatField()  # mJy/beam\n    flux_int_err = models.FloatField()  # mJy/beam\n    flux_int_isl_ratio = models.FloatField(\n        help_text=(\n            'Ratio of the component integrated flux to the total'\n            ' island integrated flux.'\n        )\n    )\n    flux_peak = models.FloatField()  # mJy/beam\n    flux_peak_err = models.FloatField()  # mJy/beam\n    flux_peak_isl_ratio = models.FloatField(\n        help_text=(\n            'Ratio of the component peak flux to the total'\n            ' island peak flux.'\n        )\n    )\n    chi_squared_fit = models.FloatField(\n        db_column='chi2_fit',\n        help_text='Chi-squared of the Guassian fit to the source.'\n    )\n    spectral_index = models.FloatField(\n        db_column='spectr_idx',\n        help_text='In-band Selavy spectral index.'\n    )\n    spectral_index_from_TT = models.BooleanField(\n        default=False,\n        db_column='spectr_idx_tt',\n        help_text=(\n            'True/False if the spectral index came from the taylor '\n            'term.'\n        )\n    )\n\n    local_rms = models.FloatField(\n        help_text='Local rms in mJy from Selavy.'\n    )  # mJy/beam\n\n    snr = models.FloatField(\n        help_text='Signal-to-noise ratio of the measurement.'\n    )\n\n    flag_c4 = models.BooleanField(\n        default=False,\n        help_text='Fit flag from Selavy.'\n    )\n\n    compactness = models.FloatField(\n        help_text='Int flux over peak flux.'\n    )\n\n    has_siblings = models.BooleanField(\n        default=False,\n        help_text='True if the fit comes from an island that has more than 1 component.'\n    )\n    component_id = models.CharField(\n        max_length=64,\n        help_text=(\n            'The ID of the component from which the source comes from.'\n        )\n    )\n    island_id = models.CharField(\n        max_length=64,\n        help_text=(\n            'The ID of the island from which the source comes from.'\n        )\n    )\n\n    forced = models.BooleanField(\n        default=False,\n        help_text='True: the measurement is forced extracted.'\n    )\n\n    objects = MeasurementQuerySet.as_manager()\n\n    class Meta:\n        ordering = ['ra']\n\n    def __str__(self):\n        return self.name\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.MeasurementQuerySet","title":"<code>MeasurementQuerySet</code>","text":"<p>               Bases: <code>QuerySet</code></p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class MeasurementQuerySet(models.QuerySet):\n\n    def cone_search(\n        self, ra: float, dec: float, radius_deg: float\n    ) -&gt; models.QuerySet:\n        \"\"\"\n        Return all the Sources withing radius_deg of (ra,dec).\n        Returns a QuerySet of Sources, ordered by distance from\n        (ra,dec) ascending.\n\n        Args:\n            ra: The right ascension value of the cone search central\n                coordinate.\n            dec: The declination value of the cone search central coordinate.\n            radius_deg: The radius over which to perform the cone search.\n\n        Returns:\n            Measurements found withing the cone search area.\n        \"\"\"\n        return (\n            self.extra(\n                select={\n                    \"distance\": \"q3c_dist(ra, dec, %s, %s) * 3600\"\n                },\n                select_params=[ra, dec],\n                where=[\"q3c_radial_query(ra, dec, %s, %s, %s)\"],\n                params=[ra, dec, radius_deg],\n            )\n            .order_by(\"distance\")\n        )\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.MeasurementQuerySet.cone_search","title":"<code>cone_search(ra, dec, radius_deg)</code>","text":"<p>Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending.</p> <p>Parameters:</p> Name Type Description Default <code>ra</code> <code>float</code> <p>The right ascension value of the cone search central coordinate.</p> required <code>dec</code> <code>float</code> <p>The declination value of the cone search central coordinate.</p> required <code>radius_deg</code> <code>float</code> <p>The radius over which to perform the cone search.</p> required <p>Returns:</p> Type Description <code>QuerySet</code> <p>Measurements found withing the cone search area.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>def cone_search(\n    self, ra: float, dec: float, radius_deg: float\n) -&gt; models.QuerySet:\n    \"\"\"\n    Return all the Sources withing radius_deg of (ra,dec).\n    Returns a QuerySet of Sources, ordered by distance from\n    (ra,dec) ascending.\n\n    Args:\n        ra: The right ascension value of the cone search central\n            coordinate.\n        dec: The declination value of the cone search central coordinate.\n        radius_deg: The radius over which to perform the cone search.\n\n    Returns:\n        Measurements found withing the cone search area.\n    \"\"\"\n    return (\n        self.extra(\n            select={\n                \"distance\": \"q3c_dist(ra, dec, %s, %s) * 3600\"\n            },\n            select_params=[ra, dec],\n            where=[\"q3c_radial_query(ra, dec, %s, %s, %s)\"],\n            params=[ra, dec, radius_deg],\n        )\n        .order_by(\"distance\")\n    )\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.RelatedSource","title":"<code>RelatedSource</code>","text":"<p>               Bases: <code>Model</code></p> <p>Association table for the many to many Source relationship with itself Django doc https://docs.djangoproject.com/en/3.1/ref/models/fields/#django.db.models.ManyToManyField.through</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class RelatedSource(models.Model):\n    '''\n    Association table for the many to many Source relationship with itself\n    Django doc\n    https://docs.djangoproject.com/en/3.1/ref/models/fields/#django.db.models.ManyToManyField.through\n    '''\n    from_source = models.ForeignKey(Source, on_delete=models.CASCADE)\n    to_source = models.ForeignKey(\n        Source,\n        on_delete=models.CASCADE,\n        related_name='related_sources'\n    )\n\n    class Meta:\n        constraints = [\n            models.UniqueConstraint(\n                name='%(app_label)s_%(class)s_unique_pair',\n                fields=['from_source', 'to_source']\n            )\n        ]\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.Run","title":"<code>Run</code>","text":"<p>               Bases: <code>CommentableModel</code></p> <p>A Run is essentially a pipeline run/processing istance over a set of images</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class Run(CommentableModel):\n    \"\"\"\n    A Run is essentially a pipeline run/processing istance over a set of\n    images\n    \"\"\"\n    user = models.ForeignKey(\n        User,\n        on_delete=models.SET_NULL,\n        null=True,\n        blank=True\n    )\n\n    name = models.CharField(\n        max_length=64,\n        unique=True,\n        validators=[\n            RegexValidator(\n                regex=r'[\\[@!#$%^&amp;*()&lt;&gt;?/\\|}{~:\\] ]',\n                message='Name contains not allowed characters!',\n                inverse_match=True\n            ),\n        ],\n        help_text='name of the pipeline run'\n    )\n    description = models.CharField(\n        max_length=240,\n        blank=True,\n        help_text=\"A short description of the pipeline run.\"\n    )\n    time = models.DateTimeField(\n        auto_now=True,\n        help_text='Datetime of a pipeline run.'\n    )\n    path = models.FilePathField(\n        max_length=200,\n        help_text='path to the pipeline run'\n    )\n    STATUS_CHOICES = [\n        ('INI', 'Initialised'),\n        ('QUE', 'Queued'),\n        ('RUN', 'Running'),\n        ('END', 'Completed'),\n        ('ERR', 'Error'),\n        ('RES', 'Restoring'),\n        ('DEL', 'Deleting'),\n    ]\n    status = models.CharField(\n        max_length=3,\n        choices=STATUS_CHOICES,\n        default='INI',\n        help_text='Status of the pipeline run.'\n    )\n    n_images = models.IntegerField(\n        default=0,\n        help_text='number of images processed in this run'\n    )\n    n_sources = models.IntegerField(\n        default=0,\n        help_text='number of sources extracted in this run'\n    )\n    n_selavy_measurements = models.IntegerField(\n        default=0,\n        help_text='number of selavy measurements in this run'\n    )\n    n_forced_measurements = models.IntegerField(\n        default=0,\n        help_text='number of forced measurements in this run'\n    )\n    n_new_sources = models.IntegerField(\n        default=0,\n        help_text='number of new sources in this run'\n    )\n    epoch_based = models.BooleanField(\n        default=False,\n        help_text=(\n            'Whether the run was processed using epoch based association'\n            ', i.e. the user passed in groups of images defining epochs'\n            ' rather than every image being treated individually.'\n        )\n    )\n\n    objects = RunManager()  # used instead of RunQuerySet.as_manager() so mypy checks work\n\n    class Meta:\n        ordering = ['name']\n\n    def __str__(self):\n        return self.name\n\n    def save(self, *args, **kwargs):\n        # enforce the full model validation on save\n        self.full_clean()\n        super(Run, self).save(*args, **kwargs)\n\n    def get_config(\n        self, validate: bool = True, validate_inputs: bool = True, prev: bool = False\n    ) -&gt; PipelineConfig:\n        \"\"\"Read, parse, and optionally validate the run configuration file.\n\n        Args:\n            validate: Validate the run configuration. Defaults to False.\n            validate_inputs: Validate the config input files. Ensures\n                that the inputs match (e.g. each image has a catalogue), and that each\n                path exists. Set to False to skip these checks. Defaults to True.\n            prev: Get the previous config file instead of the current config. The\n                previous config is the one used for the last successfully completed run.\n                The current config may have been modified since the run was executed.\n\n        Returns:\n            PipelineConfig: The run configuration object.\n        \"\"\"\n        config_name = \"config_prev.yaml\" if prev else \"config.yaml\"\n        config = PipelineConfig.from_file(\n            str(Path(self.path) / config_name),\n            validate=validate,\n            validate_inputs=validate_inputs,\n        )\n        return config\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.Run.get_config","title":"<code>get_config(validate=True, validate_inputs=True, prev=False)</code>","text":"<p>Read, parse, and optionally validate the run configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>validate</code> <code>bool</code> <p>Validate the run configuration. Defaults to False.</p> <code>True</code> <code>validate_inputs</code> <code>bool</code> <p>Validate the config input files. Ensures that the inputs match (e.g. each image has a catalogue), and that each path exists. Set to False to skip these checks. Defaults to True.</p> <code>True</code> <code>prev</code> <code>bool</code> <p>Get the previous config file instead of the current config. The previous config is the one used for the last successfully completed run. The current config may have been modified since the run was executed.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>PipelineConfig</code> <code>PipelineConfig</code> <p>The run configuration object.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>def get_config(\n    self, validate: bool = True, validate_inputs: bool = True, prev: bool = False\n) -&gt; PipelineConfig:\n    \"\"\"Read, parse, and optionally validate the run configuration file.\n\n    Args:\n        validate: Validate the run configuration. Defaults to False.\n        validate_inputs: Validate the config input files. Ensures\n            that the inputs match (e.g. each image has a catalogue), and that each\n            path exists. Set to False to skip these checks. Defaults to True.\n        prev: Get the previous config file instead of the current config. The\n            previous config is the one used for the last successfully completed run.\n            The current config may have been modified since the run was executed.\n\n    Returns:\n        PipelineConfig: The run configuration object.\n    \"\"\"\n    config_name = \"config_prev.yaml\" if prev else \"config.yaml\"\n    config = PipelineConfig.from_file(\n        str(Path(self.path) / config_name),\n        validate=validate,\n        validate_inputs=validate_inputs,\n    )\n    return config\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.RunQuerySet","title":"<code>RunQuerySet</code>","text":"<p>               Bases: <code>QuerySet</code></p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class RunQuerySet(models.QuerySet):\n\n    def check_max_runs(self, max_runs: int = 5) -&gt; int:\n        \"\"\"\n        Check if number of running pipeline runs is above threshold.\n\n        Args:\n            max_runs: The maximum number of processing runs allowed.\n\n        Returns:\n            The count of the current pipeline runs with a status of `RUN`.\n        \"\"\"\n        return self.filter(status='RUN').count() &gt;= max_runs\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.RunQuerySet.check_max_runs","title":"<code>check_max_runs(max_runs=5)</code>","text":"<p>Check if number of running pipeline runs is above threshold.</p> <p>Parameters:</p> Name Type Description Default <code>max_runs</code> <code>int</code> <p>The maximum number of processing runs allowed.</p> <code>5</code> <p>Returns:</p> Type Description <code>int</code> <p>The count of the current pipeline runs with a status of <code>RUN</code>.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>def check_max_runs(self, max_runs: int = 5) -&gt; int:\n    \"\"\"\n    Check if number of running pipeline runs is above threshold.\n\n    Args:\n        max_runs: The maximum number of processing runs allowed.\n\n    Returns:\n        The count of the current pipeline runs with a status of `RUN`.\n    \"\"\"\n    return self.filter(status='RUN').count() &gt;= max_runs\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.Source","title":"<code>Source</code>","text":"<p>               Bases: <code>CommentableModel</code></p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class Source(CommentableModel):\n    run = models.ForeignKey(Run, on_delete=models.CASCADE, null=True,)\n    related = models.ManyToManyField(\n        'self',\n        through='RelatedSource',\n        symmetrical=False,\n        through_fields=('from_source', 'to_source')\n    )\n\n    name = models.CharField(max_length=100)\n    new = models.BooleanField(default=False, help_text='New Source.')\n    tags = TagField(\n        space_delimiter=False,\n        autocomplete_view=\"vast_pipeline:source_tags_autocomplete\",\n        autocomplete_settings={\"width\": \"100%\"},\n    )\n\n    # average fields calculated from the source measurements\n    wavg_ra = models.FloatField(\n        help_text='The weighted average right ascension (Deg).'\n    )\n    wavg_dec = models.FloatField(\n        help_text='The weighted average declination (Deg).'\n    )\n    wavg_uncertainty_ew = models.FloatField(\n        help_text=(\n            'The weighted average uncertainty in the east-'\n            'west (RA) direction (Deg).'\n        )\n    )\n    wavg_uncertainty_ns = models.FloatField(\n        help_text=(\n            'The weighted average uncertainty in the north-'\n            'south (Dec) direction (Deg).'\n        )\n    )\n    avg_flux_int = models.FloatField(\n        help_text='The average integrated flux value.'\n    )\n    avg_flux_peak = models.FloatField(\n        help_text='The average peak flux value.'\n    )\n    max_flux_peak = models.FloatField(\n        help_text='The maximum peak flux value.'\n    )\n    min_flux_peak = models.FloatField(\n        help_text='The minimum peak flux value.'\n    )\n    max_flux_int = models.FloatField(\n        help_text='The maximum integrated flux value.'\n    )\n    min_flux_int = models.FloatField(\n        help_text='The minimum integrated flux value.'\n    )\n    min_flux_int_isl_ratio = models.FloatField(\n        help_text='The minimum integrated island flux ratio value.'\n    )\n    min_flux_peak_isl_ratio = models.FloatField(\n        help_text='The minimum peak island flux ratio value.'\n    )\n    avg_compactness = models.FloatField(\n        help_text='The average compactness.'\n    )\n    min_snr = models.FloatField(\n        help_text='The minimum signal-to-noise ratio value of the detections.'\n    )\n    max_snr = models.FloatField(\n        help_text='The maximum signal-to-noise ratio value of the detections.'\n    )\n\n    # metrics\n    v_int = models.FloatField(\n        help_text='V metric for int flux.'\n    )\n    v_peak = models.FloatField(\n        help_text='V metric for peak flux.'\n    )\n    eta_int = models.FloatField(\n        help_text='Eta metric for int flux.'\n    )\n    eta_peak = models.FloatField(\n        help_text='Eta metric for peak flux.'\n    )\n    new_high_sigma = models.FloatField(\n        help_text=(\n            'The largest sigma value for the new source'\n            ' if it was placed in previous image.'\n        )\n    )\n    n_neighbour_dist = models.FloatField(\n        help_text='Distance to the nearest neighbour (deg)'\n    )\n    vs_abs_significant_max_int = models.FloatField(\n        default=0.0,\n        help_text=(\n            'Maximum value of all measurement pair variability t-statistics for int'\n            ' flux that exceed variability.source_aggregate_pair_metrics_min_abs_vs in'\n            ' the pipeline run configuration.'\n        )\n    )\n    m_abs_significant_max_int = models.FloatField(\n        default=0.0,\n        help_text=(\n            'Maximum absolute value of all measurement pair modulation indices for int'\n            ' flux that exceed variability.source_aggregate_pair_metrics_min_abs_vs in'\n            ' the pipeline run configuration.'\n        )\n    )\n    vs_abs_significant_max_peak = models.FloatField(\n        default=0.0,\n        help_text=(\n            'Maximum absolute value of all measurement pair variability t-statistics for'\n            ' peak flux that exceed variability.source_aggregate_pair_metrics_min_abs_vs'\n            ' in the pipeline run configuration.'\n        )\n    )\n    m_abs_significant_max_peak = models.FloatField(\n        default=0.0,\n        help_text=(\n            'Maximum absolute value of all measurement pair modulation indices for '\n            ' peak flux that exceed variability.source_aggregate_pair_metrics_min_abs_vs'\n            ' in the pipeline run configuration.'\n        )\n    )\n\n    # total metrics to report in UI\n    n_meas = models.IntegerField(\n        help_text='total measurements of the source'\n    )\n    n_meas_sel = models.IntegerField(\n        help_text='total selavy extracted measurements of the source'\n    )\n    n_meas_forced = models.IntegerField(\n        help_text='total force extracted measurements of the source'\n    )\n    n_rel = models.IntegerField(\n        help_text='total relations of the source with other sources'\n    )\n    n_sibl = models.IntegerField(\n        help_text='total siblings of the source'\n    )\n\n    objects = SourceQuerySet.as_manager()\n\n    def __str__(self):\n        return self.name\n\n    def get_measurement_pairs(self) -&gt; List[MeasurementPair]:\n        \"\"\"Calculate the measurement pair metrics for the source. If the run config\n        set variability.pair_metrics to false, then no pairs are calculated and an empty\n        list is returned.\n\n        Returns:\n            List[MeasurementPair]: The list of measurement pairs and their metrics.\n        \"\"\"\n        # do not calculate pair metrics if it was disabled in the run config\n        config = self.run.get_config(validate=False, validate_inputs=False, prev=True)\n        # validate the config schema only, not the full validation executed by\n        # PipelineConfig.validate.\n        config._yaml.revalidate(PipelineConfig.SCHEMA)\n        if not config[\"variability\"][\"pair_metrics\"]:\n            return []\n\n        measurements = (\n            Measurement.objects.filter(source=self)\n            .select_related(\"image\")\n            .order_by(\"image__datetime\")\n        )\n        measurement_pairs: List[MeasurementPair] = []\n        for meas_a, meas_b in combinations(measurements, 2):\n            # ensure the measurements are in time order\n            if meas_a.image.datetime &gt; meas_b.image.datetime:\n                meas_a, meas_b = meas_b, meas_a\n            # calculate metrics\n            vs_peak = calculate_vs_metric(\n                meas_a.flux_peak, meas_b.flux_peak, meas_a.flux_peak_err, meas_b.flux_peak_err\n            )\n            m_int = calculate_m_metric(meas_a.flux_int, meas_b.flux_int)\n            vs_int = calculate_vs_metric(\n                meas_a.flux_int, meas_b.flux_int, meas_a.flux_int_err, meas_b.flux_int_err\n            )\n            m_peak = calculate_m_metric(meas_a.flux_peak, meas_b.flux_peak)\n            measurement_pairs.append(\n                MeasurementPair(self.id, meas_a.id, meas_b.id, vs_peak, m_peak, vs_int, m_int)\n            )\n        return measurement_pairs\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.Source.get_measurement_pairs","title":"<code>get_measurement_pairs()</code>","text":"<p>Calculate the measurement pair metrics for the source. If the run config set variability.pair_metrics to false, then no pairs are calculated and an empty list is returned.</p> <p>Returns:</p> Type Description <code>List[MeasurementPair]</code> <p>List[MeasurementPair]: The list of measurement pairs and their metrics.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>def get_measurement_pairs(self) -&gt; List[MeasurementPair]:\n    \"\"\"Calculate the measurement pair metrics for the source. If the run config\n    set variability.pair_metrics to false, then no pairs are calculated and an empty\n    list is returned.\n\n    Returns:\n        List[MeasurementPair]: The list of measurement pairs and their metrics.\n    \"\"\"\n    # do not calculate pair metrics if it was disabled in the run config\n    config = self.run.get_config(validate=False, validate_inputs=False, prev=True)\n    # validate the config schema only, not the full validation executed by\n    # PipelineConfig.validate.\n    config._yaml.revalidate(PipelineConfig.SCHEMA)\n    if not config[\"variability\"][\"pair_metrics\"]:\n        return []\n\n    measurements = (\n        Measurement.objects.filter(source=self)\n        .select_related(\"image\")\n        .order_by(\"image__datetime\")\n    )\n    measurement_pairs: List[MeasurementPair] = []\n    for meas_a, meas_b in combinations(measurements, 2):\n        # ensure the measurements are in time order\n        if meas_a.image.datetime &gt; meas_b.image.datetime:\n            meas_a, meas_b = meas_b, meas_a\n        # calculate metrics\n        vs_peak = calculate_vs_metric(\n            meas_a.flux_peak, meas_b.flux_peak, meas_a.flux_peak_err, meas_b.flux_peak_err\n        )\n        m_int = calculate_m_metric(meas_a.flux_int, meas_b.flux_int)\n        vs_int = calculate_vs_metric(\n            meas_a.flux_int, meas_b.flux_int, meas_a.flux_int_err, meas_b.flux_int_err\n        )\n        m_peak = calculate_m_metric(meas_a.flux_peak, meas_b.flux_peak)\n        measurement_pairs.append(\n            MeasurementPair(self.id, meas_a.id, meas_b.id, vs_peak, m_peak, vs_int, m_int)\n        )\n    return measurement_pairs\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.SourceQuerySet","title":"<code>SourceQuerySet</code>","text":"<p>               Bases: <code>QuerySet</code></p> Source code in <code>vast_pipeline/models.py</code> <pre><code>class SourceQuerySet(models.QuerySet):\n\n    def cone_search(\n        self, ra: float, dec: float, radius_deg: float\n    ) -&gt; models.QuerySet:\n        \"\"\"\n        Return all the Sources withing radius_deg of (ra,dec).\n        Returns a QuerySet of Sources, ordered by distance from\n        (ra,dec) ascending.\n\n        Args:\n            ra: The right ascension value of the cone search central\n                coordinate.\n            dec: The declination value of the cone search central coordinate.\n            radius_deg: The radius over which to perform the cone search.\n\n        Returns:\n            Sources found withing the cone search area.\n        \"\"\"\n        return (\n            self.extra(\n                select={\n                    \"distance\": \"q3c_dist(wavg_ra, wavg_dec, %s, %s) * 3600\"\n                },\n                select_params=[ra, dec],\n                where=[\"q3c_radial_query(wavg_ra, wavg_dec, %s, %s, %s)\"],\n                params=[ra, dec, radius_deg],\n            )\n            .order_by(\"distance\")\n        )\n</code></pre>"},{"location":"reference/models/#vast_pipeline.models.SourceQuerySet.cone_search","title":"<code>cone_search(ra, dec, radius_deg)</code>","text":"<p>Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending.</p> <p>Parameters:</p> Name Type Description Default <code>ra</code> <code>float</code> <p>The right ascension value of the cone search central coordinate.</p> required <code>dec</code> <code>float</code> <p>The declination value of the cone search central coordinate.</p> required <code>radius_deg</code> <code>float</code> <p>The radius over which to perform the cone search.</p> required <p>Returns:</p> Type Description <code>QuerySet</code> <p>Sources found withing the cone search area.</p> Source code in <code>vast_pipeline/models.py</code> <pre><code>def cone_search(\n    self, ra: float, dec: float, radius_deg: float\n) -&gt; models.QuerySet:\n    \"\"\"\n    Return all the Sources withing radius_deg of (ra,dec).\n    Returns a QuerySet of Sources, ordered by distance from\n    (ra,dec) ascending.\n\n    Args:\n        ra: The right ascension value of the cone search central\n            coordinate.\n        dec: The declination value of the cone search central coordinate.\n        radius_deg: The radius over which to perform the cone search.\n\n    Returns:\n        Sources found withing the cone search area.\n    \"\"\"\n    return (\n        self.extra(\n            select={\n                \"distance\": \"q3c_dist(wavg_ra, wavg_dec, %s, %s) * 3600\"\n            },\n            select_params=[ra, dec],\n            where=[\"q3c_radial_query(wavg_ra, wavg_dec, %s, %s, %s)\"],\n            params=[ra, dec, radius_deg],\n        )\n        .order_by(\"distance\")\n    )\n</code></pre>"},{"location":"reference/plots/","title":"plots.py","text":"<p>Contains plotting code used by the web server.</p>"},{"location":"reference/plots/#vast_pipeline.plots.fit_eta_v","title":"<code>fit_eta_v(df, use_peak_flux=False)</code>","text":"<p>Fits the eta and v distributions with Gaussians. Used from within the 'run_eta_v_analysis' method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the sources from the pipeline run. A <code>pandas.core.frame.DataFrame</code> instance.</p> required <code>use_peak_flux</code> <code>bool</code> <p>Use peak fluxes for the analysis instead of integrated fluxes, defaults to 'False'.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>The mean of the eta fit.</p> <code>float</code> <p>The sigma of the eta fit.</p> <code>float</code> <p>The mean of the v fit.</p> <code>float</code> <p>The sigma of the v fit.</p> Source code in <code>vast_pipeline/plots.py</code> <pre><code>def fit_eta_v(\n    df: pd.DataFrame, use_peak_flux: bool = False\n) -&gt; Tuple[float, float, float, float]:\n    \"\"\"\n    Fits the eta and v distributions with Gaussians. Used from\n    within the 'run_eta_v_analysis' method.\n\n    Args:\n        df: DataFrame containing the sources from the pipeline run. A\n            `pandas.core.frame.DataFrame` instance.\n        use_peak_flux: Use peak fluxes for the analysis instead of\n            integrated fluxes, defaults to 'False'.\n\n    Returns:\n        The mean of the eta fit.\n        The sigma of the eta fit.\n        The mean of the v fit.\n        The sigma of the v fit.\n    \"\"\"\n\n    if use_peak_flux:\n        eta_label = 'eta_peak'\n        v_label = 'v_peak'\n    else:\n        eta_label = 'eta_int'\n        v_label = 'v_int'\n\n    eta_log = np.log10(df[eta_label])\n    v_log = np.log10(df[v_label])\n\n    eta_log_clipped = sigma_clip(\n        eta_log, masked=False, stdfunc=mad_std, sigma=3\n    )\n    v_log_clipped = sigma_clip(\n        v_log, masked=False, stdfunc=mad_std, sigma=3\n    )\n\n    eta_fit_mean, eta_fit_sigma = norm.fit(eta_log_clipped)\n    v_fit_mean, v_fit_sigma = norm.fit(v_log_clipped)\n\n    return (eta_fit_mean, eta_fit_sigma, v_fit_mean, v_fit_sigma)\n</code></pre>"},{"location":"reference/plots/#vast_pipeline.plots.plot_eta_v_bokeh","title":"<code>plot_eta_v_bokeh(source, eta_sigma, v_sigma, use_peak_flux=True)</code>","text":"<p>Adapted from code written by Andrew O'Brien. Produces the eta, V candidates plot (see Rowlinson et al., 2018, https://ui.adsabs.harvard.edu/abs/2019A%26C....27..111R/abstract). Returns a bokeh version.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Source</code> <p>The source model object containing the result of the query.</p> required <code>eta_sigma</code> <code>float</code> <p>The log10 eta_cutoff from the analysis.</p> required <code>v_sigma</code> <code>float</code> <p>The log10 v_cutoff from the analysis.</p> required <code>use_peak_flux</code> <code>bool</code> <p>Use peak fluxes for the analysis instead of integrated fluxes, defaults to 'True'.</p> <code>True</code> <p>Returns:</p> Type Description <code>gridplot</code> <p>Bokeh grid object containing figure.</p> Source code in <code>vast_pipeline/plots.py</code> <pre><code>def plot_eta_v_bokeh(\n    source: Source,\n    eta_sigma: float,\n    v_sigma: float,\n    use_peak_flux: bool = True\n) -&gt; gridplot:\n    \"\"\"\n    Adapted from code written by Andrew O'Brien.\n    Produces the eta, V candidates plot\n    (see Rowlinson et al., 2018,\n    https://ui.adsabs.harvard.edu/abs/2019A%26C....27..111R/abstract).\n    Returns a bokeh version.\n\n    Args:\n        source: The source model object containing the result of the query.\n        eta_sigma: The log10 eta_cutoff from the analysis.\n        v_sigma: The log10 v_cutoff from the analysis.\n        use_peak_flux: Use peak fluxes for the analysis instead of\n            integrated fluxes, defaults to 'True'.\n\n    Returns:\n        Bokeh grid object containing figure.\n    \"\"\"\n\n    df = pd.DataFrame(source.values(\n        \"id\", \"name\", \"eta_peak\", \"eta_int\", \"v_peak\", \"v_int\", \"n_meas_sel\"\n    ))\n\n    (\n        eta_fit_mean, eta_fit_sigma,\n        v_fit_mean, v_fit_sigma\n    ) = fit_eta_v(df, use_peak_flux=use_peak_flux)\n\n    eta_cutoff_log10 = eta_fit_mean + eta_sigma * eta_fit_sigma\n    v_cutoff_log10 = v_fit_mean + v_sigma * v_fit_sigma\n\n    eta_cutoff = 10 ** eta_cutoff_log10\n    v_cutoff = 10 ** v_cutoff_log10\n\n    # generate fitted curve data for plotting\n    eta_x = np.linspace(\n        norm.ppf(0.001, loc=eta_fit_mean, scale=eta_fit_sigma),\n        norm.ppf(0.999, loc=eta_fit_mean, scale=eta_fit_sigma),\n    )\n    eta_y = norm.pdf(eta_x, loc=eta_fit_mean, scale=eta_fit_sigma)\n\n    v_x = np.linspace(\n        norm.ppf(0.001, loc=v_fit_mean, scale=v_fit_sigma),\n        norm.ppf(0.999, loc=v_fit_mean, scale=v_fit_sigma),\n    )\n    v_y = norm.pdf(v_x, loc=v_fit_mean, scale=v_fit_sigma)\n\n    if use_peak_flux:\n        x_label = 'eta_peak'\n        y_label = 'v_peak'\n        title = 'Peak Flux'\n    else:\n        x_label = 'eta_int'\n        y_label = 'v_int'\n        title = \"Int. Flux\"\n\n    # PLOTTING NOTE!\n    # Datashader does not play nice with setting the axis to log-log, in fact\n    # it just doesn't work as of writing.\n    # See https://github.com/holoviz/holoviews/issues/2195.\n    # So this is why the actual log10 values are plotted instead on a linear\n    # axis. This could be revisited if the probelm with datashader and\n    # holoviews is resolved.\n    for i in [x_label, y_label]:\n        df[f\"{i}_log10\"] = np.log10(df[i])\n\n    PLOT_WIDTH = 700\n    PLOT_HEIGHT = PLOT_WIDTH\n\n    x_axis_label = \"log \\u03B7\"\n    y_axis_label = \"log V\"\n\n    cmap = linear_cmap(\n        \"n_meas_sel\",\n        cc.kb,\n        df[\"n_meas_sel\"].min(),\n        df[\"n_meas_sel\"].max(),\n    )\n\n    cb_title = \"Number of Selavy Measurements\"\n\n    if df.shape[0] &gt; settings.ETA_V_DATASHADER_THRESHOLD:\n\n        hv.extension('bokeh')\n\n        # create dfs for bokeh and datashader\n        mask = ((df[x_label] &gt;= eta_cutoff) &amp; (df[y_label] &gt;= v_cutoff))\n\n        bokeh_df = df.loc[mask]\n        ds_df = df.loc[~mask]\n\n        # create datashader version first\n        points = spread(\n            datashade(\n                hv.Points(ds_df[[f\"{x_label}_log10\", f\"{y_label}_log10\"]]),\n                cmap=\"Blues\"\n            ),\n            px=1,\n            shape='square'\n        ).opts(height=PLOT_HEIGHT, width=PLOT_WIDTH)\n\n        fig = hv.render(points)\n\n        fig.xaxis.axis_label = x_axis_label\n        fig.yaxis.axis_label = y_axis_label\n        fig.aspect_scale = 1\n        fig.sizing_mode = 'stretch_width'\n        fig.output_backend = \"webgl\"\n        # update the y axis default range\n        if bokeh_df.shape[0] &gt; 0:\n            fig.y_range.end = bokeh_df[f'{y_label}_log10'].max() + 0.2\n\n        cb_title += \" (interactive points only)\"\n    else:\n        bokeh_df = df\n\n        fig = figure(\n            output_backend=\"webgl\",\n            plot_width=PLOT_WIDTH,\n            plot_height=PLOT_HEIGHT,\n            aspect_scale=1,\n            x_axis_label=x_axis_label,\n            y_axis_label=y_axis_label,\n            sizing_mode=\"stretch_width\",\n        )\n\n    # activate scroll wheel zoom by default\n    fig.toolbar.active_scroll = fig.select_one(WheelZoomTool)\n\n    source = ColumnDataSource(data=bokeh_df)\n\n    bokeh_points = Scatter(\n        x=f\"{x_label}_log10\",\n        y=f\"{y_label}_log10\",\n        fill_color=cmap,\n        line_color=cmap,\n        marker=\"circle\",\n        size=5\n    )\n\n    bokeh_g1 = fig.add_glyph(source_or_glyph=source, glyph=bokeh_points)\n\n    hover = HoverTool(\n        renderers=[bokeh_g1],\n        tooltips=[\n            (\"source\", \"@name\"),\n            (\"\\u03B7\", f\"@{x_label}\"),\n            (\"V\", f\"@{y_label}\"),\n            (\"id\", \"@id\")\n        ],\n        mode='mouse'\n    )\n\n    fig.add_tools(hover)\n\n    color_bar = ColorBar(\n        color_mapper=cmap['transform'],\n        title=cb_title\n    )\n\n    fig.add_layout(color_bar, 'below')\n\n    # axis histograms\n    # filter out any forced-phot points for these\n    x_hist = figure(\n        plot_width=PLOT_WIDTH,\n        plot_height=100,\n        x_range=fig.x_range,\n        y_axis_type=None,\n        x_axis_type=\"linear\",\n        x_axis_location=\"above\",\n        sizing_mode=\"stretch_width\",\n        title=\"VAST eta-V {}\".format(title),\n        tools=\"\",\n        output_backend=\"webgl\",\n    )\n    x_hist_data, x_hist_edges = np.histogram(\n        df[f\"{x_label}_log10\"], density=True, bins=50,\n    )\n    x_hist.quad(\n        top=x_hist_data,\n        bottom=0,\n        left=x_hist_edges[:-1],\n        right=x_hist_edges[1:],\n    )\n    x_hist.line(eta_x, eta_y, color=\"black\")\n    x_hist_sigma_span = Span(\n        location=eta_cutoff_log10,\n        dimension=\"height\",\n        line_color=\"black\",\n        line_dash=\"dashed\",\n    )\n    x_hist.add_layout(x_hist_sigma_span)\n    fig.add_layout(x_hist_sigma_span)\n\n    y_hist = figure(\n        plot_height=PLOT_HEIGHT,\n        plot_width=100,\n        y_range=fig.y_range,\n        x_axis_type=None,\n        y_axis_type=\"linear\",\n        y_axis_location=\"right\",\n        sizing_mode=\"stretch_height\",\n        tools=\"\",\n        output_backend=\"webgl\",\n    )\n    y_hist_data, y_hist_edges = np.histogram(\n        (df[f\"{y_label}_log10\"]), density=True, bins=50,\n    )\n    y_hist.quad(\n        right=y_hist_data,\n        left=0,\n        top=y_hist_edges[:-1],\n        bottom=y_hist_edges[1:],\n    )\n    y_hist.line(v_y, v_x, color=\"black\")\n    y_hist_sigma_span = Span(\n        location=v_cutoff_log10,\n        dimension=\"width\",\n        line_color=\"black\",\n        line_dash=\"dashed\",\n    )\n    y_hist.add_layout(y_hist_sigma_span)\n    fig.add_layout(y_hist_sigma_span)\n\n    variable_region = BoxAnnotation(\n        left=eta_cutoff_log10,\n        bottom=v_cutoff_log10,\n        fill_color=\"orange\",\n        fill_alpha=0.3,\n        level=\"underlay\",\n    )\n    fig.add_layout(variable_region)\n\n    eta_slider = Slider(\n        start=0,\n        end=10,\n        step=0.1,\n        value=eta_sigma,\n        title=\"\\u03B7 sigma value\",\n        sizing_mode='stretch_width'\n    )\n    v_slider = Slider(\n        start=0,\n        end=10,\n        step=0.1,\n        value=v_sigma,\n        title=\"V sigma value\",\n        sizing_mode='stretch_width'\n    )\n\n    labels = ['Peak', 'Integrated']\n    active = 0 if use_peak_flux else 1\n    flux_choice_radio = RadioButtonGroup(\n        labels=labels,\n        active=active,\n        sizing_mode='stretch_width'\n    )\n\n    button = Button(\n        label=\"Apply\",\n        button_type=\"primary\",\n        sizing_mode='stretch_width'\n    )\n    button.js_on_click(\n        CustomJS(\n            args=dict(\n                eta_slider=eta_slider,\n                v_slider=v_slider,\n                button=button,\n                flux_choice_radio=flux_choice_radio\n            ),\n            code=\"\"\"\n            button.label = \"Loading...\"\n            var e = eta_slider.value;\n            var v = v_slider.value;\n            const peak = [\"peak\", \"int\"];\n            var fluxType = peak[flux_choice_radio.active];\n            getEtaVPlot(e, v, fluxType);\n            \"\"\"\n        )\n    )\n\n    grid = gridplot(\n        [\n            [x_hist, Spacer(width=100, height=100)],\n            [fig, y_hist],\n        ]\n    )\n\n    plot_column = column(\n        grid,\n        flux_choice_radio,\n        eta_slider,\n        v_slider,\n        button,\n        sizing_mode='stretch_width'\n    )\n\n    plot_column.css_classes.append(\"mx-auto\")\n\n    source = ColumnDataSource(data=bokeh_df)\n    callback = CustomJS(\n        args=dict(source=source, flux_choice_radio=flux_choice_radio),\n        code=\"\"\"\n        const d1 = source.data;\n        const i = cb_data.source.selected.indices[0];\n        const id = d1['id'][i];\n        const peak = [\"peak\", \"int\"];\n        var fluxType = peak[flux_choice_radio.active];\n\n        $(document).ready(function () {\n          update_card(id);\n          getLightcurvePlot(id, fluxType);\n        });\n        \"\"\"\n    )\n\n    tap = TapTool(callback=callback, renderers=[bokeh_g1])\n    fig.tools.append(tap)\n\n    plot_row = row(plot_column, sizing_mode=\"stretch_width\")\n    plot_row.css_classes.append(\"mx-auto\")\n\n    return plot_row\n</code></pre>"},{"location":"reference/plots/#vast_pipeline.plots.plot_lightcurve","title":"<code>plot_lightcurve(source, vs_abs_min=4.3, m_abs_min=0.26, use_peak_flux=True)</code>","text":"<p>Create the lightcurve and 2-epoch metric graph for a source with Bokeh.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Source</code> <p>Source object.</p> required <code>vs_abs_min</code> <code>float</code> <p>pairs of Measurement objects with an absolute vs metric greater than <code>vs_abs_min</code> and m metric greater than <code>m_abs_min</code> will be connected in the metric graph. Defaults to 4.3.</p> <code>4.3</code> <code>m_abs_min</code> <code>float</code> <p>See <code>vs_abs_min</code>. Defaults to 0.26.</p> <code>0.26</code> <code>use_peak_flux</code> <code>bool</code> <p>If True, use peak fluxes, otherwise use integrated fluxes. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Row</code> <code>Row</code> <p>Bokeh Row layout object containing the lightcurve and graph plots.</p> Source code in <code>vast_pipeline/plots.py</code> <pre><code>def plot_lightcurve(\n    source: Source,\n    vs_abs_min: float = 4.3,\n    m_abs_min: float = 0.26,\n    use_peak_flux: bool = True,\n) -&gt; Row:\n    \"\"\"Create the lightcurve and 2-epoch metric graph for a source with Bokeh.\n\n    Args:\n        source (Source): Source object.\n        vs_abs_min (float, optional): pairs of Measurement objects with an absolute vs metric\n            greater than `vs_abs_min` and m metric greater than `m_abs_min` will be connected\n            in the metric graph. Defaults to 4.3.\n        m_abs_min (float, optional): See `vs_abs_min`. Defaults to 0.26.\n        use_peak_flux (bool, optional): If True, use peak fluxes, otherwise use integrated\n            fluxes. Defaults to True.\n\n    Returns:\n        Row: Bokeh Row layout object containing the lightcurve and graph plots.\n    \"\"\"\n    PLOT_WIDTH = 800\n    PLOT_HEIGHT = 300\n    flux_column = \"flux_peak\" if use_peak_flux else \"flux_int\"\n    metric_suffix = \"peak\" if use_peak_flux else \"int\"\n    measurements_qs = (\n        Measurement.objects.filter(source__id=source.id)\n        .annotate(\n            taustart_ts=F(\"image__datetime\"),\n            flux=F(flux_column),\n            flux_err_lower=F(flux_column) - F(f\"{flux_column}_err\"),\n            flux_err_upper=F(flux_column) + F(f\"{flux_column}_err\"),\n        )\n        .values(\n            \"id\",\n            \"pk\",\n            \"taustart_ts\",\n            \"flux\",\n            \"flux_err_upper\",\n            \"flux_err_lower\",\n            \"forced\",\n            \"name\"\n        )\n        .order_by(\"taustart_ts\")\n    )\n\n    # lightcurve required cols: taustart_ts, flux, flux_err_upper, flux_err_lower, forced\n    lightcurve = pd.DataFrame(measurements_qs)\n    # remap method values to labels to make a better legend\n    lightcurve[\"method\"] = lightcurve.forced.map(\n        {True: \"Forced\", False: \"Selavy\"}\n    )\n    lightcurve['cutout'] = lightcurve['id'].apply(\n        lambda x: f'/cutout/{x}/normal/?img_type=png'\n    )\n    lc_source = ColumnDataSource(lightcurve)\n\n    method_mapper = factor_cmap(\n        \"method\", palette=\"Colorblind3\", factors=[\"Selavy\", \"Forced\"]\n    )\n\n    min_y = min(0, lightcurve.flux_err_lower.min())\n    max_y = lightcurve.flux_err_upper.max()\n    y_padding = (max_y - min_y) * 0.1\n    fig_lc = figure(\n        plot_width=PLOT_WIDTH,\n        plot_height=PLOT_HEIGHT,\n        sizing_mode=\"stretch_width\",\n        x_axis_type=\"datetime\",\n        x_range=DataRange1d(default_span=timedelta(days=1)),\n        y_range=DataRange1d(start=min_y, end=max_y + y_padding),\n    )\n    # line source must be a COPY of the data for the scatter source for the hover and\n    # selection to work properly, using the same ColumnDataSource will break it\n    fig_lc.line(\"taustart_ts\", \"flux\", source=lightcurve)\n    lc_scatter = fig_lc.scatter(\n        \"taustart_ts\",\n        \"flux\",\n        marker=\"circle\",\n        size=6,\n        color=method_mapper,\n        nonselection_color=method_mapper,\n        selection_color=\"red\",\n        nonselection_alpha=1.0,\n        hover_color=\"red\",\n        alpha=1.0,\n        source=lc_source,\n        legend_group=\"method\",\n    )\n    fig_lc.add_layout(\n        Whisker(\n            base=\"taustart_ts\",\n            upper=\"flux_err_upper\",\n            lower=\"flux_err_lower\",\n            source=lc_source,\n        )\n    )\n    fig_lc.xaxis.axis_label = \"Datetime\"\n    fig_lc.xaxis[0].formatter = DatetimeTickFormatter(days=\"%F\", hours='%H:%M')\n    fig_lc.yaxis.axis_label = (\n        \"Peak flux (mJy/beam)\" if use_peak_flux else \"Integrated flux (mJy)\"\n    )\n\n    # determine legend location: either bottom_left or top_left\n    legend_location = (\n        \"top_left\"\n        if lightcurve.sort_values(\"taustart_ts\").iloc[0].flux &lt; (max_y - min_y) / 2\n        else \"bottom_left\"\n    )\n    fig_lc.legend.location = legend_location\n\n    # TODO add vs and m metrics to graph edges\n    # create plot\n    fig_graph = figure(\n        plot_width=PLOT_HEIGHT,\n        plot_height=PLOT_HEIGHT,\n        x_range=Range1d(-1.1, 1.1),\n        y_range=Range1d(-1.1, 1.1),\n        x_axis_type=None,\n        y_axis_type=None,\n        sizing_mode=\"fixed\",\n    )\n    hover_tool_lc_callback = None\n    measurement_pairs = source.get_measurement_pairs()\n    if len(measurement_pairs) &gt; 0:\n        candidate_measurement_pairs_df = pd.DataFrame(measurement_pairs).query(\n            f\"m_{metric_suffix}.abs() &gt;= {m_abs_min} and vs_{metric_suffix}.abs() &gt;= {vs_abs_min}\"\n        ).reset_index()\n        g = nx.Graph()\n        for _row in candidate_measurement_pairs_df.itertuples(index=False):\n            g.add_edge(_row.measurement_a_id, _row.measurement_b_id)\n        node_layout = nx.circular_layout(g, scale=1, center=(0, 0))\n\n        # add node positions to dataframe\n        for suffix in [\"a\", \"b\"]:\n            pos_df = pd.DataFrame(\n                candidate_measurement_pairs_df[f\"measurement_{suffix}_id\"]\n                .map(node_layout)\n                .to_list(),\n                columns=[f\"measurement_{suffix}_x\", f\"measurement_{suffix}_y\"],\n            )\n            candidate_measurement_pairs_df = candidate_measurement_pairs_df.join(pos_df)\n        candidate_measurement_pairs_df[\"measurement_x\"] = list(\n            zip(\n                candidate_measurement_pairs_df.measurement_a_x.values,\n                candidate_measurement_pairs_df.measurement_b_x.values,\n            )\n        )\n        candidate_measurement_pairs_df[\"measurement_y\"] = list(\n            zip(\n                candidate_measurement_pairs_df.measurement_a_y.values,\n                candidate_measurement_pairs_df.measurement_b_y.values,\n            )\n        )\n        node_positions_df = pd.DataFrame.from_dict(\n            node_layout, orient=\"index\", columns=[\"x\", \"y\"]\n        )\n        node_positions_df[\"lc_index\"] = node_positions_df.index.map(\n            {v: k for k, v in lightcurve.id.to_dict().items()}\n        ).values.astype(str)\n        node_source = ColumnDataSource(node_positions_df)\n        edge_source = ColumnDataSource(candidate_measurement_pairs_df)\n\n        # add edges to plot\n        edge_renderer = fig_graph.multi_line(\n            \"measurement_x\",\n            \"measurement_y\",\n            line_width=5,\n            hover_color=\"red\",\n            source=edge_source,\n            name=\"edges\",\n        )\n        # add nodes to plot\n        node_renderer = fig_graph.circle(\n            \"x\",\n            \"y\",\n            size=20,\n            hover_color=\"red\",\n            selection_color=\"red\",\n            nonselection_alpha=1.0,\n            source=node_source,\n            name=\"nodes\",\n        )\n\n        # create hover tool for node edges\n        edge_callback_code = \"\"\"\n            // get edge index\n            let indices_a = cb_data.index.indices.map(i =&gt; edge_data.data.measurement_a_id[i]);\n            let indices_b = cb_data.index.indices.map(i =&gt; edge_data.data.measurement_b_id[i]);\n            let indices = indices_a.concat(indices_b);\n            let lightcurve_indices = indices.map(i =&gt; lightcurve_data.data.id.indexOf(i));\n            lightcurve_data.selected.indices = lightcurve_indices;\n        \"\"\"\n        hover_tool_edges = HoverTool(\n            tooltips=[\n                (f\"Vs {metric_suffix}\", f\"@vs_{metric_suffix}\"),\n                (f\"m {metric_suffix}\", f\"@m_{metric_suffix}\"),\n            ],\n            renderers=[edge_renderer],\n            callback=CustomJS(\n                args={\n                    \"lightcurve_data\": lc_scatter.data_source,\n                    \"edge_data\": edge_renderer.data_source,\n                },\n                code=edge_callback_code,\n            ),\n        )\n        fig_graph.add_tools(hover_tool_edges)\n        # create labels for nodes\n        graph_source = ColumnDataSource(node_positions_df)\n        labels = LabelSet(\n            x=\"x\",\n            y=\"y\",\n            text=\"lc_index\",\n            source=graph_source,\n            text_align=\"center\",\n            text_baseline=\"middle\",\n            text_font_size=\"1em\",\n            text_color=\"white\",\n        )\n        fig_graph.renderers.append(labels)\n\n        # prepare a JS callback for the lightcurve hover tool to mark the associated nodes\n        hover_tool_lc_callback = CustomJS(\n            args={\n                \"node_data\": node_renderer.data_source,\n                \"lightcurve_data\": lc_scatter.data_source,\n            },\n            code=\"\"\"\n                let ids = cb_data.index.indices.map(i =&gt; lightcurve_data.data.id[i]);\n                let node_indices = ids.map(i =&gt; node_data.data.index.indexOf(i));\n                node_data.selected.indices = node_indices;\n            \"\"\",\n        )\n\n    # create hover tool for lightcurve\n    hover_tool_lc = HoverTool(\n        # tooltips=[\n        #     (\"Index\", \"@index\"),\n        #     (\"Date\", \"@taustart_ts{%F}\"),\n        #     (f\"Flux {metric_suffix}\", \"@flux mJy\"),\n        #     ('Cutout', \"@cutout\")\n        # ],\n        tooltips=\"\"\"\n        &lt;div style=\"width:200;\"&gt;\n            &lt;div&gt;\n                &lt;img\n                    src=@cutout height=\"100\" alt=@cutout width=\"100\"\n                    style=\"float: left; margin: 0px 15px 15px 0px;\"\n                    border=\"2\"\n                &gt;&lt;/img&gt;\n            &lt;/div&gt;\n            &lt;div&gt;\n                &lt;div style=\"font-size: 12px; font-weight: bold;\"&gt;Date: &lt;/div&gt;\n                &lt;div style=\"font-size: 12px; color: #966;\"&gt;@taustart_ts{%F}&lt;/div&gt;\n            &lt;/div&gt;\n            &lt;div&gt;\n                &lt;div style=\"font-size: 12px; font-weight: bold;\"&gt;Flux:&lt;/div&gt;\n                &lt;div style=\"font-size: 12px; color: #966;\"&gt;@flux mJy&lt;/div&gt;\n            &lt;/div&gt;\n            &lt;div&gt;\n                &lt;div style=\"font-size: 12px; font-weight: bold;\"&gt;Index:&lt;/div&gt;\n                &lt;div style=\"font-size: 12px; color: #966;\"&gt;@index&lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n        \"\"\",\n        formatters={\"@taustart_ts\": \"datetime\", },\n        mode=\"mouse\",\n        callback=hover_tool_lc_callback,\n    )\n    fig_lc.add_tools(hover_tool_lc)\n\n    plot_row = row(fig_lc, fig_graph, sizing_mode=\"stretch_width\")\n    plot_row.css_classes.append(\"mx-auto\")\n    return plot_row\n</code></pre>"},{"location":"reference/serializers/","title":"serializers.py","text":""},{"location":"reference/serializers/#vast_pipeline.serializers.ExternalSearchSerializer","title":"<code>ExternalSearchSerializer</code>","text":"<p>               Bases: <code>Serializer</code></p> <p>Serializer for external database cone search results, i.e. SIMBAD and NED.</p> Source code in <code>vast_pipeline/serializers.py</code> <pre><code>class ExternalSearchSerializer(serializers.Serializer):\n    \"\"\"Serializer for external database cone search results, i.e. SIMBAD and NED.\n    \"\"\"\n    object_name = serializers.CharField()\n    database = serializers.CharField(\n        help_text=\"Result origin database, e.g. SIMBAD or NED.\"\n    )\n    separation_arcsec = serializers.FloatField()\n    otype = serializers.CharField(allow_blank=True, help_text=\"Object type, e.g. QSO.\")\n    otype_long = serializers.CharField(\n        allow_blank=True,\n        help_text=\"Longer form of object type, e.g. quasar. Only supplied for SIMBAD results.\",\n    )\n    ra_hms = serializers.CharField()\n    dec_dms = serializers.CharField()\n</code></pre>"},{"location":"reference/signals/","title":"signals.py","text":"<p>Functions that are executed upon receiving an application signal.</p>"},{"location":"reference/signals/#vast_pipeline.signals.delete_orphans_for_run","title":"<code>delete_orphans_for_run(sender, instance, using, **kwargs)</code>","text":"<p>Delete any Image and SkyRegion objects that would be orphaned by deleting the given Run. Expects to recieve the arguments sent by the pre_delete signal. See https://docs.djangoproject.com/en/3.1/ref/signals/#pre-delete.</p> <p>Parameters:</p> Name Type Description Default <code>sender</code> <code>Type[Run]</code> <p>Model class that sent the signal.</p> required <code>instance</code> <code>Run</code> <p>Model instance to be deleted.</p> required <code>using</code> <code>str</code> <p>Database alias.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/signals.py</code> <pre><code>@receiver(pre_delete, sender=Run, dispatch_uid=\"delete_orphans_for_run\")\ndef delete_orphans_for_run(sender: Type[Run], instance: Run, using: str, **kwargs) -&gt; None:\n    \"\"\"Delete any Image and SkyRegion objects that would be orphaned by deleting the\n    given Run. Expects to recieve the arguments sent by the pre_delete signal. See\n    &lt;https://docs.djangoproject.com/en/3.1/ref/signals/#pre-delete&gt;.\n\n    Args:\n        sender:\n            Model class that sent the signal.\n        instance:\n            Model instance to be deleted.\n        using:\n            Database alias.\n\n    Returns:\n        None\n    \"\"\"\n    image_orphans = (\n        Image.objects.annotate(num_runs=Count(\"run\"))\n        .filter(run=instance, num_runs=1)\n    )\n    n_obj_deleted, deleted_dict = image_orphans.delete()\n    logger.info(\n        \"Deleted %d objects: %s\",\n        n_obj_deleted,\n        \", \".join([f\"{model}: {n}\" for model, n in deleted_dict.items()]),\n    )\n\n    skyreg_orphans = (\n        SkyRegion.objects.annotate(num_runs=Count(\"run\"))\n        .filter(run=instance, num_runs=1)\n    )\n    n_obj_deleted, deleted_dict = skyreg_orphans.delete()\n    logger.info(\n        \"Deleted %d objects: %s\",\n        n_obj_deleted,\n        \", \".join([f\"{model}: {n}\" for model, n in deleted_dict.items()]),\n    )\n</code></pre>"},{"location":"reference/urls/","title":"urls.py","text":"<p>This module contains the urls used by the Django web server.</p>"},{"location":"reference/views/","title":"views.py","text":""},{"location":"reference/views/#vast_pipeline.views.MeasurementQuery","title":"<code>MeasurementQuery</code>","text":"<p>               Bases: <code>APIView</code></p> Source code in <code>vast_pipeline/views.py</code> <pre><code>class MeasurementQuery(APIView):\n    authentication_classes = [SessionAuthentication, BasicAuthentication]\n    permission_classes = [IsAuthenticated]\n\n    def get(\n        self,\n        request: Request,\n        ra_deg: float,\n        dec_deg: float,\n        image_id: int,\n        radius_deg: float,\n    ) -&gt; FileResponse:\n        \"\"\"Return a DS9/JS9 region file for all Measurement objects for a given cone search\n        on an Image. Optionally highlight sources based on a Measurement or Source ID.\n\n        Args:\n            request: Django HTTPRequest. Supports 4 URL GET parameters:\n                - selection_model: either \"measurement\" or \"source\" (defaults to \"measurement\").\n                - selection_id: the id for the given `selection_model`.\n                - run_id: (optional) only return measurements for sources with the given pipeline\n                    run id (defaults to None).\n                - no_forced: (optional) If true, exclude forced-photometry measurements (defaults\n                    to False).\n                Measurement objects that match the given selection criterion will be\n                highlighted. e.g. ?selection_model=measurement&amp;selection_id=100 will highlight\n                the Measurement object with id=100. ?selection_model=source&amp;selection_id=5\n                will highlight all Measurement objects associated with the Source object with\n                id=5.\n            ra_deg: Cone search RA in decimal degrees.\n            dec_deg: Cone search Dec in decimal degrees.\n            image_id: Primary key (id) of the Image object to search.\n            radius_deg: Cone search radius in decimal degrees.\n\n        Returns:\n            FileResponse: Django FileReponse containing a DS9/JS9 region file.\n        \"\"\"\n        columns = [\"id\", \"name\", \"ra\", \"dec\", \"bmaj\", \"bmin\", \"pa\", \"forced\", \"source\", \"source__name\"]\n        selection_model = request.GET.get(\"selection_model\", \"measurement\")\n        selection_id = request.GET.get(\"selection_id\", None)\n        run_id = request.GET.get(\"run_id\", None)\n        no_forced = request.GET.get(\"forced\", False)\n\n        # validate selection query params\n        if selection_id is not None:\n            if selection_model not in (\"measurement\", \"source\"):\n                raise Http404(\"GET param selection_model must be either 'measurement' or 'source'.\")\n            if selection_model == \"measurement\":\n                selection_attr = \"id\"\n                selection_name = \"name\"\n            else:\n                selection_attr = selection_model\n                selection_name = \"source__name\"\n            try:\n                selection_id = int(selection_id)\n            except ValueError:\n                raise Http404(\"GET param selection_id must be an integer.\")\n\n        measurements = (\n            Measurement.objects.filter(image=image_id)\n            .cone_search(ra_deg, dec_deg, radius_deg)\n            .values(*columns, __name=F(selection_name))\n        )\n        if run_id:\n            measurements = measurements.filter(source__run__id=run_id)\n        if no_forced:\n            measurements = measurements.filter(forced=False)\n        measurement_region_file = io.StringIO()\n        for meas in measurements:\n            if selection_id is not None:\n                color = \"#FF0000\" if meas[selection_attr] == selection_id else \"#0000FF\"\n            shape = (\n                f\"ellipse({meas['ra']}d, {meas['dec']}d, {meas['bmaj']}\\\", {meas['bmin']}\\\", \"\n                f\"{meas['pa']+90+180}d)\"\n            )\n            properties: Dict[str, Any] = {\n                \"color\": color,\n                \"data\": {\n                    \"text\": f\"{selection_model} ID: {meas[selection_attr]}\",\n                    \"link\": reverse(f\"vast_pipeline:{selection_model}_detail\", args=[selection_id]),\n                }\n            }\n            if meas[\"forced\"]:\n                properties.update(strokeDashArray=[3, 2])\n            region = f\"{shape} {json.dumps(properties)}\\n\"\n            measurement_region_file.write(region)\n        measurement_region_file.seek(0)\n        f = io.BytesIO(bytes(measurement_region_file.read(), encoding=\"utf8\"))\n        response = FileResponse(\n            f,\n            as_attachment=False,\n            filename=f\"image-{image_id}_{ra_deg:.5f}_{dec_deg:+.5f}_radius-{radius_deg:.3f}.reg\",\n        )\n        return response\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.MeasurementQuery.get","title":"<code>get(request, ra_deg, dec_deg, image_id, radius_deg)</code>","text":"<p>Return a DS9/JS9 region file for all Measurement objects for a given cone search on an Image. Optionally highlight sources based on a Measurement or Source ID.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Django HTTPRequest. Supports 4 URL GET parameters: - selection_model: either \"measurement\" or \"source\" (defaults to \"measurement\"). - selection_id: the id for the given <code>selection_model</code>. - run_id: (optional) only return measurements for sources with the given pipeline     run id (defaults to None). - no_forced: (optional) If true, exclude forced-photometry measurements (defaults     to False). Measurement objects that match the given selection criterion will be highlighted. e.g. ?selection_model=measurement&amp;selection_id=100 will highlight the Measurement object with id=100. ?selection_model=source&amp;selection_id=5 will highlight all Measurement objects associated with the Source object with id=5.</p> required <code>ra_deg</code> <code>float</code> <p>Cone search RA in decimal degrees.</p> required <code>dec_deg</code> <code>float</code> <p>Cone search Dec in decimal degrees.</p> required <code>image_id</code> <code>int</code> <p>Primary key (id) of the Image object to search.</p> required <code>radius_deg</code> <code>float</code> <p>Cone search radius in decimal degrees.</p> required <p>Returns:</p> Name Type Description <code>FileResponse</code> <code>FileResponse</code> <p>Django FileReponse containing a DS9/JS9 region file.</p> Source code in <code>vast_pipeline/views.py</code> <pre><code>def get(\n    self,\n    request: Request,\n    ra_deg: float,\n    dec_deg: float,\n    image_id: int,\n    radius_deg: float,\n) -&gt; FileResponse:\n    \"\"\"Return a DS9/JS9 region file for all Measurement objects for a given cone search\n    on an Image. Optionally highlight sources based on a Measurement or Source ID.\n\n    Args:\n        request: Django HTTPRequest. Supports 4 URL GET parameters:\n            - selection_model: either \"measurement\" or \"source\" (defaults to \"measurement\").\n            - selection_id: the id for the given `selection_model`.\n            - run_id: (optional) only return measurements for sources with the given pipeline\n                run id (defaults to None).\n            - no_forced: (optional) If true, exclude forced-photometry measurements (defaults\n                to False).\n            Measurement objects that match the given selection criterion will be\n            highlighted. e.g. ?selection_model=measurement&amp;selection_id=100 will highlight\n            the Measurement object with id=100. ?selection_model=source&amp;selection_id=5\n            will highlight all Measurement objects associated with the Source object with\n            id=5.\n        ra_deg: Cone search RA in decimal degrees.\n        dec_deg: Cone search Dec in decimal degrees.\n        image_id: Primary key (id) of the Image object to search.\n        radius_deg: Cone search radius in decimal degrees.\n\n    Returns:\n        FileResponse: Django FileReponse containing a DS9/JS9 region file.\n    \"\"\"\n    columns = [\"id\", \"name\", \"ra\", \"dec\", \"bmaj\", \"bmin\", \"pa\", \"forced\", \"source\", \"source__name\"]\n    selection_model = request.GET.get(\"selection_model\", \"measurement\")\n    selection_id = request.GET.get(\"selection_id\", None)\n    run_id = request.GET.get(\"run_id\", None)\n    no_forced = request.GET.get(\"forced\", False)\n\n    # validate selection query params\n    if selection_id is not None:\n        if selection_model not in (\"measurement\", \"source\"):\n            raise Http404(\"GET param selection_model must be either 'measurement' or 'source'.\")\n        if selection_model == \"measurement\":\n            selection_attr = \"id\"\n            selection_name = \"name\"\n        else:\n            selection_attr = selection_model\n            selection_name = \"source__name\"\n        try:\n            selection_id = int(selection_id)\n        except ValueError:\n            raise Http404(\"GET param selection_id must be an integer.\")\n\n    measurements = (\n        Measurement.objects.filter(image=image_id)\n        .cone_search(ra_deg, dec_deg, radius_deg)\n        .values(*columns, __name=F(selection_name))\n    )\n    if run_id:\n        measurements = measurements.filter(source__run__id=run_id)\n    if no_forced:\n        measurements = measurements.filter(forced=False)\n    measurement_region_file = io.StringIO()\n    for meas in measurements:\n        if selection_id is not None:\n            color = \"#FF0000\" if meas[selection_attr] == selection_id else \"#0000FF\"\n        shape = (\n            f\"ellipse({meas['ra']}d, {meas['dec']}d, {meas['bmaj']}\\\", {meas['bmin']}\\\", \"\n            f\"{meas['pa']+90+180}d)\"\n        )\n        properties: Dict[str, Any] = {\n            \"color\": color,\n            \"data\": {\n                \"text\": f\"{selection_model} ID: {meas[selection_attr]}\",\n                \"link\": reverse(f\"vast_pipeline:{selection_model}_detail\", args=[selection_id]),\n            }\n        }\n        if meas[\"forced\"]:\n            properties.update(strokeDashArray=[3, 2])\n        region = f\"{shape} {json.dumps(properties)}\\n\"\n        measurement_region_file.write(region)\n    measurement_region_file.seek(0)\n    f = io.BytesIO(bytes(measurement_region_file.read(), encoding=\"utf8\"))\n    response = FileResponse(\n        f,\n        as_attachment=False,\n        filename=f\"image-{image_id}_{ra_deg:.5f}_{dec_deg:+.5f}_radius-{radius_deg:.3f}.reg\",\n    )\n    return response\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.RawImageListSet","title":"<code>RawImageListSet</code>","text":"<p>               Bases: <code>ViewSet</code></p> Source code in <code>vast_pipeline/views.py</code> <pre><code>class RawImageListSet(ViewSet):\n    authentication_classes = [SessionAuthentication, BasicAuthentication]\n    permission_classes = [IsAuthenticated]\n\n    @staticmethod\n    def gen_title_data_tokens(list_of_paths):\n        '''\n        generate a dataframe with extra columns for HTML tags title\n        and data-tokens to generate something like:\n        &lt;option title={{title}} data-tokens={{datatokens}}&gt;{{path}}&lt;/option&gt;\n        this assume a path like this:\n        EPOCH06x/COMBINED/STOKESI_SELAVY/VAST_2118-06A.EPOCH06x.I.selavy.components.txt\n        For the following dataframe columns\n                                                     path\n        EPOCH06x/COMBINED/STOKESI_SELAVY/VAST_2118-06A...\n                                                 title\n        VAST_2118-06A.EPOCH06x.I.selavy.components.txt\n                                               datatokens\n        EPOCH06x VAST_2118-06A.EPOCH06x.I.selavy.compo...\n        '''\n        df = pd.DataFrame(list_of_paths, columns=['path'])\n        df = df.sort_values('path')\n        df['title'] = df['path'].str.split(pat=os.sep).str.get(-1)\n        df['datatokens'] = (\n            df['path'].str.split(pat=os.sep).str.get(0)\n            .str.cat(df['title'], sep=' ')\n        )\n\n        return df.to_dict(orient='records')\n\n    def list(self, request):\n        # generate the folders path regex, e.g. /path/to/images/**/*.fits\n        # first generate the list of main subfolders, e.g. [EPOCH01, ... ]\n        img_root = settings.RAW_IMAGE_DIR\n        if not os.path.exists(img_root):\n            msg = 'Raw image folder does not exists'\n            messages.error(request, msg)\n            raise Http404(msg)\n\n        img_subfolders_gen = filter(\n            lambda x: os.path.isdir(os.path.join(img_root, x)),\n            os.listdir(img_root)\n        )\n        img_subfolders1, img_subfolders2 = tee(img_subfolders_gen)\n        img_regex_list = list(map(\n            lambda x: os.path.join(img_root, x, '**' + os.sep + '*.fits'),\n            img_subfolders1\n        ))\n        selavy_regex_list = list(map(\n            lambda x: os.path.join(img_root, x, '**' + os.sep + '*.txt'),\n            img_subfolders2\n        ))\n        # add home directory user data for user and jupyter-user (user = github name)\n        req_user = request.user.username\n        for user in [f'{req_user}', f'jupyter-{req_user}']:\n            if settings.HOME_DATA_ROOT is not None:\n                user_home_data = os.path.join(\n                    settings.HOME_DATA_ROOT, user, settings.HOME_DATA_DIR\n                )\n            else:\n                user_home_data = os.path.join(\n                    os.path.expanduser(f'~{user}'), settings.HOME_DATA_DIR\n                )\n            if settings.HOME_DATA_DIR and os.path.exists(user_home_data):\n                img_regex_list.append(os.path.join(user_home_data, '**' + os.sep + '*.fits'))\n                selavy_regex_list.append(os.path.join(user_home_data, '**' + os.sep + '*.txt'))\n\n        # generate raw image list in parallel\n        dask_list = db.from_sequence(img_regex_list)\n        fits_files = (\n            dask_list.map(lambda x: glob(x, recursive=True))\n            .flatten()\n            .compute()\n        )\n        if not fits_files:\n            messages.info(request, 'no fits files found')\n\n        # generate raw image list in parallel\n        dask_list = db.from_sequence(selavy_regex_list)\n        selavy_files = (\n            dask_list.map(lambda x: glob(x, recursive=True))\n            .flatten()\n            .compute()\n        )\n        if not fits_files:\n            messages.info(request, 'no selavy files found')\n\n        # generate response datastructure\n        data = {\n            'fits': self.gen_title_data_tokens(fits_files),\n            'selavy': self.gen_title_data_tokens(selavy_files)\n        }\n        serializer = RawImageSelavyListSerializer(data)\n\n        return Response(serializer.data)\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.RawImageListSet.gen_title_data_tokens","title":"<code>gen_title_data_tokens(list_of_paths)</code>  <code>staticmethod</code>","text":"<p>generate a dataframe with extra columns for HTML tags title and data-tokens to generate something like:</p> {{path}} <p>this assume a path like this: EPOCH06x/COMBINED/STOKESI_SELAVY/VAST_2118-06A.EPOCH06x.I.selavy.components.txt For the following dataframe columns                                              path EPOCH06x/COMBINED/STOKESI_SELAVY/VAST_2118-06A...                                          title VAST_2118-06A.EPOCH06x.I.selavy.components.txt                                        datatokens EPOCH06x VAST_2118-06A.EPOCH06x.I.selavy.compo...</p> Source code in <code>vast_pipeline/views.py</code> <pre><code>@staticmethod\ndef gen_title_data_tokens(list_of_paths):\n    '''\n    generate a dataframe with extra columns for HTML tags title\n    and data-tokens to generate something like:\n    &lt;option title={{title}} data-tokens={{datatokens}}&gt;{{path}}&lt;/option&gt;\n    this assume a path like this:\n    EPOCH06x/COMBINED/STOKESI_SELAVY/VAST_2118-06A.EPOCH06x.I.selavy.components.txt\n    For the following dataframe columns\n                                                 path\n    EPOCH06x/COMBINED/STOKESI_SELAVY/VAST_2118-06A...\n                                             title\n    VAST_2118-06A.EPOCH06x.I.selavy.components.txt\n                                           datatokens\n    EPOCH06x VAST_2118-06A.EPOCH06x.I.selavy.compo...\n    '''\n    df = pd.DataFrame(list_of_paths, columns=['path'])\n    df = df.sort_values('path')\n    df['title'] = df['path'].str.split(pat=os.sep).str.get(-1)\n    df['datatokens'] = (\n        df['path'].str.split(pat=os.sep).str.get(0)\n        .str.cat(df['title'], sep=' ')\n    )\n\n    return df.to_dict(orient='records')\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.RunViewSet","title":"<code>RunViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> Source code in <code>vast_pipeline/views.py</code> <pre><code>class RunViewSet(ModelViewSet):\n    authentication_classes = [SessionAuthentication, BasicAuthentication]\n    permission_classes = [IsAuthenticated]\n    queryset = Run.objects.all()\n    serializer_class = RunSerializer\n\n    @rest_framework.decorators.action(detail=True, methods=['get'])\n    def images(self, request, pk=None):\n        qs = Image.objects.filter(run__id=pk).order_by('id')\n        qs = self.filter_queryset(qs)\n        page = self.paginate_queryset(qs)\n        if page is not None:\n            serializer = ImageSerializer(page, many=True)\n            return self.get_paginated_response(serializer.data)\n\n        serializer = ImageSerializer(qs, many=True)\n        return Response(serializer.data)\n\n    @rest_framework.decorators.action(detail=True, methods=['get'])\n    def measurements(self, request, pk=None):\n        qs = Measurement.objects.filter(image__run__in=[pk]).order_by('id')\n        qs = self.filter_queryset(qs)\n        page = self.paginate_queryset(qs)\n        if page is not None:\n            serializer = MeasurementSerializer(page, many=True)\n            return self.get_paginated_response(serializer.data)\n\n        serializer = MeasurementSerializer(qs, many=True)\n        return Response(serializer.data)\n\n    @rest_framework.decorators.action(detail=True, methods=['post'])\n    def run(\n        self, request: Request, pk: Optional[int] = None\n    ) -&gt; HttpResponseRedirect:\n        \"\"\"\n        Launches a pipeline run using a Django Q cluster. Includes a check\n        on ownership or admin stataus of the user to make sure processing\n        is allowed.\n\n        Args:\n            request: Django REST Framework request object.\n            pk: Run object primary key. Defaults to None.\n\n        Raises:\n            Http404: if a Source with the given `pk` cannot be found.\n\n        Returns:\n            Response: Returns to the orignal request page (the pipeline run\n                detail).\n        \"\"\"\n        if not pk:\n            messages.error(\n                request,\n                'Error in config write: Run pk parameter null or not passed'\n            )\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        try:\n            p_run = get_object_or_404(self.queryset, pk=pk)\n        except Exception as e:\n            messages.error(request, f'Error in config write: {e}')\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        # make sure that only the run creator or an admin can request the run\n        # to be processed.\n        if p_run.user != request.user and not request.user.is_staff:\n            msg = 'You do not have permission to process this pipeline run!'\n            messages.error(request, msg)\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        # check that it's not already running or queued\n        if p_run.status in [\"RUN\", \"QUE\", \"RES\"]:\n            msg = (\n                f'{p_run.name} is already running, queued or restoring.'\n                ' Please wait for the run to complete before trying to'\n                ' submit again.'\n            )\n            messages.error(\n                request,\n                msg\n            )\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        if Run.objects.check_max_runs(settings.MAX_PIPELINE_RUNS):\n            msg = (\n                'The maximum number of simultaneous pipeline runs has been'\n                f' reached ({settings.MAX_PIPELINE_RUNS})! Please try again'\n                ' when other jobs have finished.'\n            )\n            messages.error(\n                request,\n                msg\n            )\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        prev_status = p_run.status\n        try:\n            with transaction.atomic():\n                p_run.status = 'QUE'\n                p_run.save()\n\n            debug_flag = True if request.POST.get('debug', None) else False\n            full_rerun = True if request.POST.get('fullReRun', None) else False\n\n            async_task(\n                'vast_pipeline.management.commands.runpipeline.run_pipe',\n                p_run.name, p_run.path, p_run, False, debug_flag,\n                task_name=p_run.name, ack_failure=True, user=request.user,\n                full_rerun=full_rerun, prev_ui_status=prev_status\n            )\n            msg = mark_safe(\n                f'&lt;b&gt;{p_run.name}&lt;/b&gt; successfully sent to the queue!&lt;br&gt;&lt;br&gt;Refresh the'\n                ' page to check the status.'\n            )\n            messages.success(\n                request,\n                msg\n            )\n        except Exception as e:\n            with transaction.atomic():\n                p_run.status = 'ERR'\n                p_run.save()\n            messages.error(request, f'Error in running pipeline: {e}')\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        return HttpResponseRedirect(\n            reverse('vast_pipeline:run_detail', args=[p_run.id])\n        )\n\n    @rest_framework.decorators.action(detail=True, methods=['post'])\n    def restore(\n        self, request: Request, pk: Optional[int] = None\n    ) -&gt; HttpResponseRedirect:\n        \"\"\"\n        Launches a restore pipeline run using a Django Q cluster. Includes a\n        check on ownership or admin status of the user to make sure\n        processing is allowed.\n\n        Args:\n            request: Django REST Framework request object.\n            pk: Run object primary key. Defaults to None.\n\n        Raises:\n            Http404: if a Source with the given `pk` cannot be found.\n\n        Returns:\n            Response: Returns to the orignal request page (the pipeline run\n                detail).\n        \"\"\"\n        if not pk:\n            messages.error(\n                request,\n                'Error in config write: Run pk parameter null or not passed'\n            )\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        try:\n            p_run = get_object_or_404(self.queryset, pk=pk)\n        except Exception as e:\n            messages.error(request, f'Error in run fetch: {e}')\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        # make sure that only the run creator or an admin can request the run\n        # to be processed.\n        if p_run.user != request.user and not request.user.is_staff:\n            msg = 'You do not have permission to process this pipeline run!'\n            messages.error(request, msg)\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        # check that it's not already running or queued\n        if p_run.status in [\"RUN\", \"QUE\", \"RES\", \"INI\"]:\n            msg = (\n                f'{p_run.name} is already running, queued, restoring or is '\n                'only initialised. It cannot be restored at this time.'\n            )\n            messages.error(\n                request,\n                msg\n            )\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        if Run.objects.check_max_runs(settings.MAX_PIPELINE_RUNS):\n            msg = (\n                'The maximum number of simultaneous pipeline runs has been'\n                f' reached ({settings.MAX_PIPELINE_RUNS})! Please try again'\n                ' when other jobs have finished.'\n            )\n            messages.error(\n                request,\n                msg\n            )\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        prev_status = p_run.status\n        try:\n            debug_flag = 3 if request.POST.get('restoreDebug', None) else 1\n\n            async_task(\n                'django.core.management.call_command',\n                'restorepiperun',\n                p_run.path,\n                no_confirm=True,\n                verbosity=debug_flag\n            )\n\n            msg = mark_safe(\n                f'Restore &lt;b&gt;{p_run.name}&lt;/b&gt; successfully sent to the queue!&lt;br&gt;&lt;br&gt;Refresh the'\n                ' page to check the status.'\n            )\n            messages.success(\n                request,\n                msg\n            )\n        except Exception as e:\n            with transaction.atomic():\n                p_run.status = 'ERR'\n                p_run.save()\n            messages.error(request, f'Error in restoring run: {e}')\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        return HttpResponseRedirect(\n            reverse('vast_pipeline:run_detail', args=[p_run.id])\n        )\n\n    @rest_framework.decorators.action(detail=True, methods=['post'])\n    def delete(\n        self, request: Request, pk: Optional[int] = None\n    ) -&gt; HttpResponseRedirect:\n        \"\"\"\n        Launches the remove pipeline run using a Django Q cluster. Includes a\n        check on ownership or admin status of the user to make sure\n        deletion is allowed.\n\n        Args:\n            request (Request): Django REST Framework request object.\n            pk (int, optional): Run object primary key. Defaults to None.\n\n        Raises:\n            Http404: if a Source with the given `pk` cannot be found.\n\n        Returns:\n            Response: Returns to the run index page (list of pipeline runs).\n        \"\"\"\n        if not pk:\n            messages.error(\n                request,\n                'Error in config write: Run pk parameter null or not passed'\n            )\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        try:\n            p_run = get_object_or_404(self.queryset, pk=pk)\n        except Exception as e:\n            messages.error(request, f'Error in run fetch: {e}')\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        # make sure that only the run creator or an admin can request the run\n        # to be processed.\n        if p_run.user != request.user and not request.user.is_staff:\n            msg = 'You do not have permission to process this pipeline run!'\n            messages.error(request, msg)\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        # check that it's not already running or queued\n        if p_run.status in [\"RUN\", \"QUE\", \"RES\"]:\n            msg = (\n                f'{p_run.name} is already running, queued or restoring. '\n                'It cannot be deleted at this time.'\n            )\n            messages.error(\n                request,\n                msg\n            )\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        try:\n            async_task(\n                'django.core.management.call_command',\n                'clearpiperun',\n                p_run.path,\n                remove_all=True,\n            )\n\n            msg = mark_safe(\n                f'Delete &lt;b&gt;{p_run.name}&lt;/b&gt; successfully requested!&lt;br&gt;&lt;br&gt;'\n                ' Refresh the Pipeline Runs page for the deletion to take effect.'\n            )\n            messages.success(\n                request,\n                msg\n            )\n        except Exception as e:\n            with transaction.atomic():\n                p_run.status = 'ERR'\n                p_run.save()\n            messages.error(request, f'Error in deleting run: {e}')\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        return HttpResponseRedirect(\n            reverse('vast_pipeline:run_index')\n        )\n\n    @rest_framework.decorators.action(detail=True, methods=['post'])\n    def genarrow(\n        self, request: Request, pk: Optional[int] = None\n    ) -&gt;HttpResponseRedirect:\n        \"\"\"\n        Launches the create arrow files process for a pipeline run using\n        a Django Q cluster. Includes a check on ownership or admin status of\n        the user to make sure the creation is allowed.\n\n        Args:\n            request: Django REST Framework request object.\n            pk: Run object primary key. Defaults to None.\n\n        Raises:\n            Http404: if a Source with the given `pk` cannot be found.\n\n        Returns:\n            Response: Returns to the orignal request page (the pipeline run\n                detail).\n        \"\"\"\n        if not pk:\n            messages.error(\n                request,\n                'Error in config write: Run pk parameter null or not passed'\n            )\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        try:\n            p_run = get_object_or_404(self.queryset, pk=pk)\n        except Exception as e:\n            messages.error(request, f'Error in run fetch: {e}')\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        # make sure that only the run creator or an admin can request the run\n        # to be processed.\n        if p_run.user != request.user and not request.user.is_staff:\n            msg = 'You do not have permission to process this pipeline run!'\n            messages.error(request, msg)\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        # check that it's not already running or queued\n        if p_run.status != \"END\":\n            msg = (\n                f'{p_run.name} has not completed successfully.'\n                ' The arrow files can only be generated after the run is'\n                ' successful.'\n            )\n            messages.error(\n                request,\n                msg\n            )\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        try:\n            overwrite_flag = True if request.POST.get('arrowOverwrite', None) else False\n\n            async_task(\n                'django.core.management.call_command',\n                'createmeasarrow',\n                p_run.path,\n                overwrite=overwrite_flag,\n                verbosity=3\n            )\n\n            msg = mark_safe(\n                f'Generate the arrow files for &lt;b&gt;{p_run.name}&lt;/b&gt; successfully requested!&lt;br&gt;&lt;br&gt;'\n                ' Refresh the page and check the generate arrow log output for the status of the process.'\n            )\n            messages.success(\n                request,\n                msg\n            )\n        except Exception as e:\n            with transaction.atomic():\n                p_run.status = 'ERR'\n                p_run.save()\n            messages.error(request, f'Error in deleting run: {e}')\n            return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n        return HttpResponseRedirect(\n            reverse('vast_pipeline:run_detail', args=[p_run.id])\n        )\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.RunViewSet.delete","title":"<code>delete(request, pk=None)</code>","text":"<p>Launches the remove pipeline run using a Django Q cluster. Includes a check on ownership or admin status of the user to make sure deletion is allowed.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Django REST Framework request object.</p> required <code>pk</code> <code>int</code> <p>Run object primary key. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Http404</code> <p>if a Source with the given <code>pk</code> cannot be found.</p> <p>Returns:</p> Name Type Description <code>Response</code> <code>HttpResponseRedirect</code> <p>Returns to the run index page (list of pipeline runs).</p> Source code in <code>vast_pipeline/views.py</code> <pre><code>@rest_framework.decorators.action(detail=True, methods=['post'])\ndef delete(\n    self, request: Request, pk: Optional[int] = None\n) -&gt; HttpResponseRedirect:\n    \"\"\"\n    Launches the remove pipeline run using a Django Q cluster. Includes a\n    check on ownership or admin status of the user to make sure\n    deletion is allowed.\n\n    Args:\n        request (Request): Django REST Framework request object.\n        pk (int, optional): Run object primary key. Defaults to None.\n\n    Raises:\n        Http404: if a Source with the given `pk` cannot be found.\n\n    Returns:\n        Response: Returns to the run index page (list of pipeline runs).\n    \"\"\"\n    if not pk:\n        messages.error(\n            request,\n            'Error in config write: Run pk parameter null or not passed'\n        )\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    try:\n        p_run = get_object_or_404(self.queryset, pk=pk)\n    except Exception as e:\n        messages.error(request, f'Error in run fetch: {e}')\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    # make sure that only the run creator or an admin can request the run\n    # to be processed.\n    if p_run.user != request.user and not request.user.is_staff:\n        msg = 'You do not have permission to process this pipeline run!'\n        messages.error(request, msg)\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    # check that it's not already running or queued\n    if p_run.status in [\"RUN\", \"QUE\", \"RES\"]:\n        msg = (\n            f'{p_run.name} is already running, queued or restoring. '\n            'It cannot be deleted at this time.'\n        )\n        messages.error(\n            request,\n            msg\n        )\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    try:\n        async_task(\n            'django.core.management.call_command',\n            'clearpiperun',\n            p_run.path,\n            remove_all=True,\n        )\n\n        msg = mark_safe(\n            f'Delete &lt;b&gt;{p_run.name}&lt;/b&gt; successfully requested!&lt;br&gt;&lt;br&gt;'\n            ' Refresh the Pipeline Runs page for the deletion to take effect.'\n        )\n        messages.success(\n            request,\n            msg\n        )\n    except Exception as e:\n        with transaction.atomic():\n            p_run.status = 'ERR'\n            p_run.save()\n        messages.error(request, f'Error in deleting run: {e}')\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    return HttpResponseRedirect(\n        reverse('vast_pipeline:run_index')\n    )\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.RunViewSet.genarrow","title":"<code>genarrow(request, pk=None)</code>","text":"<p>Launches the create arrow files process for a pipeline run using a Django Q cluster. Includes a check on ownership or admin status of the user to make sure the creation is allowed.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Django REST Framework request object.</p> required <code>pk</code> <code>Optional[int]</code> <p>Run object primary key. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Http404</code> <p>if a Source with the given <code>pk</code> cannot be found.</p> <p>Returns:</p> Name Type Description <code>Response</code> <code>HttpResponseRedirect</code> <p>Returns to the orignal request page (the pipeline run detail).</p> Source code in <code>vast_pipeline/views.py</code> <pre><code>@rest_framework.decorators.action(detail=True, methods=['post'])\ndef genarrow(\n    self, request: Request, pk: Optional[int] = None\n) -&gt;HttpResponseRedirect:\n    \"\"\"\n    Launches the create arrow files process for a pipeline run using\n    a Django Q cluster. Includes a check on ownership or admin status of\n    the user to make sure the creation is allowed.\n\n    Args:\n        request: Django REST Framework request object.\n        pk: Run object primary key. Defaults to None.\n\n    Raises:\n        Http404: if a Source with the given `pk` cannot be found.\n\n    Returns:\n        Response: Returns to the orignal request page (the pipeline run\n            detail).\n    \"\"\"\n    if not pk:\n        messages.error(\n            request,\n            'Error in config write: Run pk parameter null or not passed'\n        )\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    try:\n        p_run = get_object_or_404(self.queryset, pk=pk)\n    except Exception as e:\n        messages.error(request, f'Error in run fetch: {e}')\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    # make sure that only the run creator or an admin can request the run\n    # to be processed.\n    if p_run.user != request.user and not request.user.is_staff:\n        msg = 'You do not have permission to process this pipeline run!'\n        messages.error(request, msg)\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    # check that it's not already running or queued\n    if p_run.status != \"END\":\n        msg = (\n            f'{p_run.name} has not completed successfully.'\n            ' The arrow files can only be generated after the run is'\n            ' successful.'\n        )\n        messages.error(\n            request,\n            msg\n        )\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    try:\n        overwrite_flag = True if request.POST.get('arrowOverwrite', None) else False\n\n        async_task(\n            'django.core.management.call_command',\n            'createmeasarrow',\n            p_run.path,\n            overwrite=overwrite_flag,\n            verbosity=3\n        )\n\n        msg = mark_safe(\n            f'Generate the arrow files for &lt;b&gt;{p_run.name}&lt;/b&gt; successfully requested!&lt;br&gt;&lt;br&gt;'\n            ' Refresh the page and check the generate arrow log output for the status of the process.'\n        )\n        messages.success(\n            request,\n            msg\n        )\n    except Exception as e:\n        with transaction.atomic():\n            p_run.status = 'ERR'\n            p_run.save()\n        messages.error(request, f'Error in deleting run: {e}')\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    return HttpResponseRedirect(\n        reverse('vast_pipeline:run_detail', args=[p_run.id])\n    )\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.RunViewSet.restore","title":"<code>restore(request, pk=None)</code>","text":"<p>Launches a restore pipeline run using a Django Q cluster. Includes a check on ownership or admin status of the user to make sure processing is allowed.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Django REST Framework request object.</p> required <code>pk</code> <code>Optional[int]</code> <p>Run object primary key. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Http404</code> <p>if a Source with the given <code>pk</code> cannot be found.</p> <p>Returns:</p> Name Type Description <code>Response</code> <code>HttpResponseRedirect</code> <p>Returns to the orignal request page (the pipeline run detail).</p> Source code in <code>vast_pipeline/views.py</code> <pre><code>@rest_framework.decorators.action(detail=True, methods=['post'])\ndef restore(\n    self, request: Request, pk: Optional[int] = None\n) -&gt; HttpResponseRedirect:\n    \"\"\"\n    Launches a restore pipeline run using a Django Q cluster. Includes a\n    check on ownership or admin status of the user to make sure\n    processing is allowed.\n\n    Args:\n        request: Django REST Framework request object.\n        pk: Run object primary key. Defaults to None.\n\n    Raises:\n        Http404: if a Source with the given `pk` cannot be found.\n\n    Returns:\n        Response: Returns to the orignal request page (the pipeline run\n            detail).\n    \"\"\"\n    if not pk:\n        messages.error(\n            request,\n            'Error in config write: Run pk parameter null or not passed'\n        )\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    try:\n        p_run = get_object_or_404(self.queryset, pk=pk)\n    except Exception as e:\n        messages.error(request, f'Error in run fetch: {e}')\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    # make sure that only the run creator or an admin can request the run\n    # to be processed.\n    if p_run.user != request.user and not request.user.is_staff:\n        msg = 'You do not have permission to process this pipeline run!'\n        messages.error(request, msg)\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    # check that it's not already running or queued\n    if p_run.status in [\"RUN\", \"QUE\", \"RES\", \"INI\"]:\n        msg = (\n            f'{p_run.name} is already running, queued, restoring or is '\n            'only initialised. It cannot be restored at this time.'\n        )\n        messages.error(\n            request,\n            msg\n        )\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    if Run.objects.check_max_runs(settings.MAX_PIPELINE_RUNS):\n        msg = (\n            'The maximum number of simultaneous pipeline runs has been'\n            f' reached ({settings.MAX_PIPELINE_RUNS})! Please try again'\n            ' when other jobs have finished.'\n        )\n        messages.error(\n            request,\n            msg\n        )\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    prev_status = p_run.status\n    try:\n        debug_flag = 3 if request.POST.get('restoreDebug', None) else 1\n\n        async_task(\n            'django.core.management.call_command',\n            'restorepiperun',\n            p_run.path,\n            no_confirm=True,\n            verbosity=debug_flag\n        )\n\n        msg = mark_safe(\n            f'Restore &lt;b&gt;{p_run.name}&lt;/b&gt; successfully sent to the queue!&lt;br&gt;&lt;br&gt;Refresh the'\n            ' page to check the status.'\n        )\n        messages.success(\n            request,\n            msg\n        )\n    except Exception as e:\n        with transaction.atomic():\n            p_run.status = 'ERR'\n            p_run.save()\n        messages.error(request, f'Error in restoring run: {e}')\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    return HttpResponseRedirect(\n        reverse('vast_pipeline:run_detail', args=[p_run.id])\n    )\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.RunViewSet.run","title":"<code>run(request, pk=None)</code>","text":"<p>Launches a pipeline run using a Django Q cluster. Includes a check on ownership or admin stataus of the user to make sure processing is allowed.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Django REST Framework request object.</p> required <code>pk</code> <code>Optional[int]</code> <p>Run object primary key. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Http404</code> <p>if a Source with the given <code>pk</code> cannot be found.</p> <p>Returns:</p> Name Type Description <code>Response</code> <code>HttpResponseRedirect</code> <p>Returns to the orignal request page (the pipeline run detail).</p> Source code in <code>vast_pipeline/views.py</code> <pre><code>@rest_framework.decorators.action(detail=True, methods=['post'])\ndef run(\n    self, request: Request, pk: Optional[int] = None\n) -&gt; HttpResponseRedirect:\n    \"\"\"\n    Launches a pipeline run using a Django Q cluster. Includes a check\n    on ownership or admin stataus of the user to make sure processing\n    is allowed.\n\n    Args:\n        request: Django REST Framework request object.\n        pk: Run object primary key. Defaults to None.\n\n    Raises:\n        Http404: if a Source with the given `pk` cannot be found.\n\n    Returns:\n        Response: Returns to the orignal request page (the pipeline run\n            detail).\n    \"\"\"\n    if not pk:\n        messages.error(\n            request,\n            'Error in config write: Run pk parameter null or not passed'\n        )\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    try:\n        p_run = get_object_or_404(self.queryset, pk=pk)\n    except Exception as e:\n        messages.error(request, f'Error in config write: {e}')\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    # make sure that only the run creator or an admin can request the run\n    # to be processed.\n    if p_run.user != request.user and not request.user.is_staff:\n        msg = 'You do not have permission to process this pipeline run!'\n        messages.error(request, msg)\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    # check that it's not already running or queued\n    if p_run.status in [\"RUN\", \"QUE\", \"RES\"]:\n        msg = (\n            f'{p_run.name} is already running, queued or restoring.'\n            ' Please wait for the run to complete before trying to'\n            ' submit again.'\n        )\n        messages.error(\n            request,\n            msg\n        )\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    if Run.objects.check_max_runs(settings.MAX_PIPELINE_RUNS):\n        msg = (\n            'The maximum number of simultaneous pipeline runs has been'\n            f' reached ({settings.MAX_PIPELINE_RUNS})! Please try again'\n            ' when other jobs have finished.'\n        )\n        messages.error(\n            request,\n            msg\n        )\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    prev_status = p_run.status\n    try:\n        with transaction.atomic():\n            p_run.status = 'QUE'\n            p_run.save()\n\n        debug_flag = True if request.POST.get('debug', None) else False\n        full_rerun = True if request.POST.get('fullReRun', None) else False\n\n        async_task(\n            'vast_pipeline.management.commands.runpipeline.run_pipe',\n            p_run.name, p_run.path, p_run, False, debug_flag,\n            task_name=p_run.name, ack_failure=True, user=request.user,\n            full_rerun=full_rerun, prev_ui_status=prev_status\n        )\n        msg = mark_safe(\n            f'&lt;b&gt;{p_run.name}&lt;/b&gt; successfully sent to the queue!&lt;br&gt;&lt;br&gt;Refresh the'\n            ' page to check the status.'\n        )\n        messages.success(\n            request,\n            msg\n        )\n    except Exception as e:\n        with transaction.atomic():\n            p_run.status = 'ERR'\n            p_run.save()\n        messages.error(request, f'Error in running pipeline: {e}')\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER'))\n\n    return HttpResponseRedirect(\n        reverse('vast_pipeline:run_detail', args=[p_run.id])\n    )\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.SourcePlotsSet","title":"<code>SourcePlotsSet</code>","text":"<p>               Bases: <code>ViewSet</code></p> Source code in <code>vast_pipeline/views.py</code> <pre><code>class SourcePlotsSet(ViewSet):\n    authentication_classes = [SessionAuthentication, BasicAuthentication]\n    permission_classes = [IsAuthenticated]\n\n    @rest_framework.decorators.action(methods=['get'], detail=True)\n    def lightcurve(self, request: Request, pk: int = None) -&gt; Response:\n        \"\"\"Create lightcurve and 2-epoch metric graph plots for a source.\n\n        Args:\n            request (Request): Django REST Framework request object.\n            pk (int, optional): Source object primary key. Defaults to None.\n\n        Raises:\n            Http404: if a Source with the given `pk` cannot be found.\n\n        Returns:\n            Response: Django REST Framework response object containing the Bokeh plot in\n                JSON format to be embedded in the HTML template.\n        \"\"\"\n        try:\n            source = Source.objects.get(pk=pk)\n        except Source.DoesNotExist:\n            raise Http404\n        # TODO raster plots version for Slack posts\n        use_peak_flux = request.query_params.get(\"peak_flux\", \"true\").lower() == \"true\"\n        plot_document = plot_lightcurve(source, use_peak_flux=use_peak_flux)\n        return Response(json_item(plot_document))\n\n    @rest_framework.decorators.action(methods=['get'], detail=False)\n    def etavplot(self, request: Request) -&gt; Response:\n        \"\"\"Create the eta-V plot.\n\n        Args:\n            request (Request): Django REST Framework request object.\n\n        Raises:\n            Http404: if no sources are found.\n\n        Returns:\n            Response: Django REST Framework response object containing the Bokeh plot in\n                JSON format to be embedded in the HTML template.\n        \"\"\"\n        source_query_result_id_list = request.session.get(\"source_query_result_ids\", [])\n        try:\n            source = Source.objects.filter(pk__in=source_query_result_id_list)\n        except Source.DoesNotExist:\n            raise Http404\n        # TODO raster plots version for Slack posts\n        use_peak_flux = request.query_params.get(\"peak_flux\", \"true\").lower() == \"true\"\n\n        eta_sigma = float(request.query_params.get(\"eta_sigma\", 3.0))\n        v_sigma = float(request.query_params.get(\"v_sigma\", 3.0))\n\n        plot_document = plot_eta_v_bokeh(\n            source, eta_sigma=eta_sigma, v_sigma=v_sigma, use_peak_flux=use_peak_flux)\n        return Response(json_item(plot_document))\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.SourcePlotsSet.etavplot","title":"<code>etavplot(request)</code>","text":"<p>Create the eta-V plot.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Django REST Framework request object.</p> required <p>Raises:</p> Type Description <code>Http404</code> <p>if no sources are found.</p> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>Django REST Framework response object containing the Bokeh plot in JSON format to be embedded in the HTML template.</p> Source code in <code>vast_pipeline/views.py</code> <pre><code>@rest_framework.decorators.action(methods=['get'], detail=False)\ndef etavplot(self, request: Request) -&gt; Response:\n    \"\"\"Create the eta-V plot.\n\n    Args:\n        request (Request): Django REST Framework request object.\n\n    Raises:\n        Http404: if no sources are found.\n\n    Returns:\n        Response: Django REST Framework response object containing the Bokeh plot in\n            JSON format to be embedded in the HTML template.\n    \"\"\"\n    source_query_result_id_list = request.session.get(\"source_query_result_ids\", [])\n    try:\n        source = Source.objects.filter(pk__in=source_query_result_id_list)\n    except Source.DoesNotExist:\n        raise Http404\n    # TODO raster plots version for Slack posts\n    use_peak_flux = request.query_params.get(\"peak_flux\", \"true\").lower() == \"true\"\n\n    eta_sigma = float(request.query_params.get(\"eta_sigma\", 3.0))\n    v_sigma = float(request.query_params.get(\"v_sigma\", 3.0))\n\n    plot_document = plot_eta_v_bokeh(\n        source, eta_sigma=eta_sigma, v_sigma=v_sigma, use_peak_flux=use_peak_flux)\n    return Response(json_item(plot_document))\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.SourcePlotsSet.lightcurve","title":"<code>lightcurve(request, pk=None)</code>","text":"<p>Create lightcurve and 2-epoch metric graph plots for a source.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Django REST Framework request object.</p> required <code>pk</code> <code>int</code> <p>Source object primary key. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Http404</code> <p>if a Source with the given <code>pk</code> cannot be found.</p> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>Django REST Framework response object containing the Bokeh plot in JSON format to be embedded in the HTML template.</p> Source code in <code>vast_pipeline/views.py</code> <pre><code>@rest_framework.decorators.action(methods=['get'], detail=True)\ndef lightcurve(self, request: Request, pk: int = None) -&gt; Response:\n    \"\"\"Create lightcurve and 2-epoch metric graph plots for a source.\n\n    Args:\n        request (Request): Django REST Framework request object.\n        pk (int, optional): Source object primary key. Defaults to None.\n\n    Raises:\n        Http404: if a Source with the given `pk` cannot be found.\n\n    Returns:\n        Response: Django REST Framework response object containing the Bokeh plot in\n            JSON format to be embedded in the HTML template.\n    \"\"\"\n    try:\n        source = Source.objects.get(pk=pk)\n    except Source.DoesNotExist:\n        raise Http404\n    # TODO raster plots version for Slack posts\n    use_peak_flux = request.query_params.get(\"peak_flux\", \"true\").lower() == \"true\"\n    plot_document = plot_lightcurve(source, use_peak_flux=use_peak_flux)\n    return Response(json_item(plot_document))\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.SourceViewSet","title":"<code>SourceViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> Source code in <code>vast_pipeline/views.py</code> <pre><code>class SourceViewSet(ModelViewSet):\n    authentication_classes = [SessionAuthentication, BasicAuthentication]\n    permission_classes = [IsAuthenticated]\n    serializer_class = SourceSerializer\n\n    def get_queryset(self):\n        qs = Source.objects.all().filter(run__status='END')\n\n        radius_conversions = {\n            \"arcsec\": 3600.,\n            \"arcmin\": 60.,\n            \"deg\": 1.\n        }\n\n        qry_dict = {}\n        p_run = self.request.query_params.get('run')\n        if p_run:\n            qry_dict['run__name'] = p_run\n\n        flux_qry_flds = [\n            'avg_flux_int',\n            'avg_flux_peak',\n            'min_flux_peak',\n            'max_flux_peak',\n            'min_flux_int',\n            'max_flux_int',\n            'min_flux_peak_isl_ratio',\n            'min_flux_int_isl_ratio',\n            'v_int',\n            'v_peak',\n            'eta_int',\n            'eta_peak',\n            'vs_abs_significant_max_int',\n            'vs_abs_significant_max_peak',\n            'm_abs_significant_max_int',\n            'm_abs_significant_max_peak',\n            'n_meas',\n            'n_meas_sel',\n            'n_meas_forced',\n            'n_rel',\n            'new_high_sigma',\n            'avg_compactness',\n            'min_snr',\n            'max_snr',\n            'n_neighbour_dist',\n            'source_selection',\n            'source_selection_type'\n        ]\n\n        neighbour_unit = self.request.query_params.get('NeighbourUnit')\n\n        for fld in flux_qry_flds:\n            for limit in ['max', 'min']:\n                val = self.request.query_params.get(limit + '_' + fld)\n                if val:\n                    ky = fld + '__lte' if limit == 'max' else fld + '__gte'\n                    if fld == 'n_neighbour_dist':\n                        val = float(val) / radius_conversions[neighbour_unit]\n                    qry_dict[ky] = val\n\n        measurements = self.request.query_params.get('meas')\n        if measurements:\n            qry_dict['measurements'] = measurements\n\n        if 'source_selection' in self.request.query_params:\n            selection_type = self.request.query_params['source_selection_type']\n            selection: List[str] = (\n                self.request.query_params['source_selection']\n                .replace(\" \", \"\")\n                .replace(\"VAST\", \"\")  # remove published source prefix if present\n                .split(\",\")\n            )\n            if selection_type == 'name':\n                qry_dict['name__in'] = selection\n            else:\n                try:\n                    selection = [int(i) for i in selection]\n                    qry_dict['id__in'] = selection\n                except:\n                    # this avoids an error on the check if the user has\n                    # accidentally entered names with a 'id' selection type.\n                    qry_dict['id'] = -1\n\n        if 'newsrc' in self.request.query_params:\n            qry_dict['new'] = True\n\n        if 'no_siblings' in self.request.query_params:\n            qry_dict['n_sibl'] = 0\n\n        if 'tags_include' in self.request.query_params:\n            qry_dict['tags'] = self.request.query_params['tags_include']\n\n        if 'tags_exclude' in self.request.query_params:\n            qs = qs.exclude(tags=self.request.query_params['tags_exclude'])\n\n        if qry_dict:\n            qs = qs.filter(**qry_dict)\n\n        radius = self.request.query_params.get('radius')\n        radiusUnit = self.request.query_params.get('radiusunit')\n        coordsys = self.request.query_params.get('coordsys')\n        coord_string = self.request.query_params.get('coord')\n        wavg_ra, wavg_dec = None, None\n        if coord_string:\n            coord = parse_coord(coord_string, coord_frame=coordsys).transform_to(\"icrs\")\n            wavg_ra = coord.ra.deg\n            wavg_dec = coord.dec.deg\n\n        if None not in (wavg_ra, wavg_dec, radius):\n            radius = float(radius) / radius_conversions[radiusUnit]\n            qs = qs.cone_search(wavg_ra, wavg_dec, radius)\n\n        return qs\n\n    def list(self, request, *args, **kwargs):\n        \"\"\"Override the DRF ModelViewSet.list function to store the source ID order in the\n        user session to retain the source order for source detail view next and previous\n        button links. Then, call the original list function.\n        \"\"\"\n        queryset = self.filter_queryset(self.get_queryset())\n        self.request.session[\"source_query_result_ids\"] = list(\n            queryset.values_list(\"id\", flat=True)\n        )\n        return super().list(request, *args, **kwargs)\n\n    @rest_framework.decorators.action(detail=True, methods=['get'])\n    def related(self, request, pk=None):\n        qs = Source.objects.filter(related__id=pk).order_by('id')\n        qs = self.filter_queryset(qs)\n        page = self.paginate_queryset(qs)\n        if page is not None:\n            serializer = self.get_serializer(page, many=True)\n            return self.get_paginated_response(serializer.data)\n\n        serializer = self.get_serializer(qs, many=True)\n        return Response(serializer.data)\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.SourceViewSet.list","title":"<code>list(request, *args, **kwargs)</code>","text":"<p>Override the DRF ModelViewSet.list function to store the source ID order in the user session to retain the source order for source detail view next and previous button links. Then, call the original list function.</p> Source code in <code>vast_pipeline/views.py</code> <pre><code>def list(self, request, *args, **kwargs):\n    \"\"\"Override the DRF ModelViewSet.list function to store the source ID order in the\n    user session to retain the source order for source detail view next and previous\n    button links. Then, call the original list function.\n    \"\"\"\n    queryset = self.filter_queryset(self.get_queryset())\n    self.request.session[\"source_query_result_ids\"] = list(\n        queryset.values_list(\"id\", flat=True)\n    )\n    return super().list(request, *args, **kwargs)\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.UtilitiesSet","title":"<code>UtilitiesSet</code>","text":"<p>               Bases: <code>ViewSet</code></p> Source code in <code>vast_pipeline/views.py</code> <pre><code>class UtilitiesSet(ViewSet):\n    authentication_classes = [SessionAuthentication, BasicAuthentication]\n    permission_classes = [IsAuthenticated]\n\n    @staticmethod\n    def _external_search_error_handler(\n        external_query_func,\n        coord: SkyCoord,\n        radius: Angle,\n        service_name: str,\n        request: Request,\n    ) -&gt; List[Dict[str, Any]]:\n        try:\n            results = external_query_func(coord, radius)\n        except requests.HTTPError:\n            messages.error(request, f\"Unable to get {service_name} query results.\")\n            results = []\n        return results\n\n    @rest_framework.decorators.action(methods=['get'], detail=False)\n    def sesame_search(self, request: Request) -&gt; Response:\n        \"\"\"Query the Sesame name resolver service and return a coordinate.\n\n        Args:\n            request (Request): Django REST framework Request object with GET parameters:\n                - object_name (str): Object name to query.\n                - service (str, optional): Sesame service to query (all, simbad, ned, vizier).\n                    Defaults to \"all\".\n\n        Returns:\n            A Django REST framework Response. Will return JSON with status code:\n                - 400 if the query params fail validation (i.e. if an invalid Sesame service\n                    or no object name is provided) or if the name resolution fails. Error\n                    messages are returned as an array of strings under the relevant query\n                    parameter key. e.g. {\"object_name\": [\"This field may not be blank.\"]}.\n                - 200 if successful. Response data contains the passed in query parameters and\n                    the resolved coordinate as a sexagesimal string with units hourangle, deg\n                    under the key `coord`.\n        \"\"\"\n        object_name = request.query_params.get(\"object_name\", \"\")\n        service = request.query_params.get(\"service\", \"all\")\n\n        serializer = SesameResultSerializer(data=dict(object_name=object_name, service=service))\n        serializer.is_valid(raise_exception=True)\n\n        return Response(serializer.data)\n\n    @rest_framework.decorators.action(methods=['get'], detail=False)\n    def coordinate_validator(self, request: Request) -&gt; Response:\n        \"\"\"Validate a coordinate string.\n\n        Args:\n            request (Request): Django REST framework Request object with GET parameters:\n                - coord (str): the coordinate string to validate.\n                - frame (str): the frame for the given coordinate string e.g. icrs, galactic.\n\n        Returns:\n            A Django REST framework Response. Will return JSON with status code:\n                - 400 if the query params fail validation, i.e. if a frame unknown to Astropy\n                    is given, or the coordinate string fails to parse. Error messages are\n                    returned as an array of strings under the relevant query parameter key.\n                    e.g. {\"coord\": [\"This field may not be blank.\"]}.\n                - 200 if the coordinate string successfully validates. No other data is returned.\n        \"\"\"\n        coord_string = request.query_params.get(\"coord\", \"\")\n        frame = request.query_params.get(\"frame\", \"\")\n\n        serializer = CoordinateValidatorSerializer(data=dict(coord=coord_string, frame=frame))\n        serializer.is_valid(raise_exception=True)\n        return Response()\n\n    @rest_framework.decorators.action(methods=[\"get\"], detail=False)\n    def external_search(self, request: Request) -&gt; Response:\n        \"\"\"Perform a cone search with external providers (e.g. SIMBAD, NED, TNS) and\n        return the combined results.\n\n        Args:\n            request (Request): Django REST Framework Request object get GET parameters:\n                - coord (str): the coordinate string to validate. Interpreted by\n                    `astropy.coordiantes.SkyCoord`.\n                - radius (str): the cone search radius with unit, e.g. \"1arcmin\".\n                    Interpreted by `astropy.coordinates.Angle`\n\n        Raises:\n            serializers.ValidationError: if either the coordinate or radius parameters\n                cannot be interpreted by `astropy.coordiantes.SkyCoord` or\n                `astropy.coordinates.Angle`, respectively.\n\n        Returns:\n            A Django REST framework Response containing result records as a list\n                under the data object key. Each record contains the properties:\n                    - object_name: the name of the astronomical object.\n                    - database: the source of the result, e.g. SIMBAD or NED.\n                    - separation_arcsec: separation to the query coordinate in arcsec.\n                    - otype: object type.\n                    - otype_long: long form of the object type (only available for SIMBAD).\n                    - ra_hms: RA coordinate string in &lt;HH&gt;h&lt;MM&gt;m&lt;SS.SSS&gt;s format.\n                    - dec_dms: Dec coordinate string in \u00b1&lt;DD&gt;d&lt;MM&gt;m&lt;SS.SSS&gt;s format.\n        \"\"\"\n        coord_string = request.query_params.get(\"coord\", \"\")\n        radius_string = request.query_params.get(\"radius\", \"1arcmin\")\n\n        # validate inputs\n        try:\n            coord = parse_coord(coord_string)\n        except ValueError as e:\n            raise serializers.ValidationError({\"coord\": str(e.args[0])})\n\n        try:\n            radius = Angle(radius_string)\n        except ValueError as e:\n            raise serializers.ValidationError({\"radius\": str(e.args[0])})\n\n        simbad_results = self._external_search_error_handler(\n            external_query.simbad, coord, radius, \"SIMBAD\", request\n        )\n        ned_results = self._external_search_error_handler(\n            external_query.ned, coord, radius, \"NED\", request\n        )\n        tns_results = self._external_search_error_handler(\n            external_query.tns, coord, radius, \"TNS\", request\n        )\n\n        results = simbad_results + ned_results + tns_results\n        serializer = ExternalSearchSerializer(data=results, many=True)\n        serializer.is_valid(raise_exception=True)\n        return Response(serializer.data)\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.UtilitiesSet.coordinate_validator","title":"<code>coordinate_validator(request)</code>","text":"<p>Validate a coordinate string.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Django REST framework Request object with GET parameters: - coord (str): the coordinate string to validate. - frame (str): the frame for the given coordinate string e.g. icrs, galactic.</p> required <p>Returns:</p> Type Description <code>Response</code> <p>A Django REST framework Response. Will return JSON with status code: - 400 if the query params fail validation, i.e. if a frame unknown to Astropy     is given, or the coordinate string fails to parse. Error messages are     returned as an array of strings under the relevant query parameter key.     e.g. {\"coord\": [\"This field may not be blank.\"]}. - 200 if the coordinate string successfully validates. No other data is returned.</p> Source code in <code>vast_pipeline/views.py</code> <pre><code>@rest_framework.decorators.action(methods=['get'], detail=False)\ndef coordinate_validator(self, request: Request) -&gt; Response:\n    \"\"\"Validate a coordinate string.\n\n    Args:\n        request (Request): Django REST framework Request object with GET parameters:\n            - coord (str): the coordinate string to validate.\n            - frame (str): the frame for the given coordinate string e.g. icrs, galactic.\n\n    Returns:\n        A Django REST framework Response. Will return JSON with status code:\n            - 400 if the query params fail validation, i.e. if a frame unknown to Astropy\n                is given, or the coordinate string fails to parse. Error messages are\n                returned as an array of strings under the relevant query parameter key.\n                e.g. {\"coord\": [\"This field may not be blank.\"]}.\n            - 200 if the coordinate string successfully validates. No other data is returned.\n    \"\"\"\n    coord_string = request.query_params.get(\"coord\", \"\")\n    frame = request.query_params.get(\"frame\", \"\")\n\n    serializer = CoordinateValidatorSerializer(data=dict(coord=coord_string, frame=frame))\n    serializer.is_valid(raise_exception=True)\n    return Response()\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.UtilitiesSet.external_search","title":"<code>external_search(request)</code>","text":"<p>Perform a cone search with external providers (e.g. SIMBAD, NED, TNS) and return the combined results.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Django REST Framework Request object get GET parameters: - coord (str): the coordinate string to validate. Interpreted by     <code>astropy.coordiantes.SkyCoord</code>. - radius (str): the cone search radius with unit, e.g. \"1arcmin\".     Interpreted by <code>astropy.coordinates.Angle</code></p> required <p>Raises:</p> Type Description <code>ValidationError</code> <p>if either the coordinate or radius parameters cannot be interpreted by <code>astropy.coordiantes.SkyCoord</code> or <code>astropy.coordinates.Angle</code>, respectively.</p> <p>Returns:</p> Type Description <code>Response</code> <p>A Django REST framework Response containing result records as a list under the data object key. Each record contains the properties:     - object_name: the name of the astronomical object.     - database: the source of the result, e.g. SIMBAD or NED.     - separation_arcsec: separation to the query coordinate in arcsec.     - otype: object type.     - otype_long: long form of the object type (only available for SIMBAD).     - ra_hms: RA coordinate string in hms format.     - dec_dms: Dec coordinate string in \u00b1dms format. Source code in <code>vast_pipeline/views.py</code> <pre><code>@rest_framework.decorators.action(methods=[\"get\"], detail=False)\ndef external_search(self, request: Request) -&gt; Response:\n    \"\"\"Perform a cone search with external providers (e.g. SIMBAD, NED, TNS) and\n    return the combined results.\n\n    Args:\n        request (Request): Django REST Framework Request object get GET parameters:\n            - coord (str): the coordinate string to validate. Interpreted by\n                `astropy.coordiantes.SkyCoord`.\n            - radius (str): the cone search radius with unit, e.g. \"1arcmin\".\n                Interpreted by `astropy.coordinates.Angle`\n\n    Raises:\n        serializers.ValidationError: if either the coordinate or radius parameters\n            cannot be interpreted by `astropy.coordiantes.SkyCoord` or\n            `astropy.coordinates.Angle`, respectively.\n\n    Returns:\n        A Django REST framework Response containing result records as a list\n            under the data object key. Each record contains the properties:\n                - object_name: the name of the astronomical object.\n                - database: the source of the result, e.g. SIMBAD or NED.\n                - separation_arcsec: separation to the query coordinate in arcsec.\n                - otype: object type.\n                - otype_long: long form of the object type (only available for SIMBAD).\n                - ra_hms: RA coordinate string in &lt;HH&gt;h&lt;MM&gt;m&lt;SS.SSS&gt;s format.\n                - dec_dms: Dec coordinate string in \u00b1&lt;DD&gt;d&lt;MM&gt;m&lt;SS.SSS&gt;s format.\n    \"\"\"\n    coord_string = request.query_params.get(\"coord\", \"\")\n    radius_string = request.query_params.get(\"radius\", \"1arcmin\")\n\n    # validate inputs\n    try:\n        coord = parse_coord(coord_string)\n    except ValueError as e:\n        raise serializers.ValidationError({\"coord\": str(e.args[0])})\n\n    try:\n        radius = Angle(radius_string)\n    except ValueError as e:\n        raise serializers.ValidationError({\"radius\": str(e.args[0])})\n\n    simbad_results = self._external_search_error_handler(\n        external_query.simbad, coord, radius, \"SIMBAD\", request\n    )\n    ned_results = self._external_search_error_handler(\n        external_query.ned, coord, radius, \"NED\", request\n    )\n    tns_results = self._external_search_error_handler(\n        external_query.tns, coord, radius, \"TNS\", request\n    )\n\n    results = simbad_results + ned_results + tns_results\n    serializer = ExternalSearchSerializer(data=results, many=True)\n    serializer.is_valid(raise_exception=True)\n    return Response(serializer.data)\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.UtilitiesSet.sesame_search","title":"<code>sesame_search(request)</code>","text":"<p>Query the Sesame name resolver service and return a coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Django REST framework Request object with GET parameters: - object_name (str): Object name to query. - service (str, optional): Sesame service to query (all, simbad, ned, vizier).     Defaults to \"all\".</p> required <p>Returns:</p> Type Description <code>Response</code> <p>A Django REST framework Response. Will return JSON with status code: - 400 if the query params fail validation (i.e. if an invalid Sesame service     or no object name is provided) or if the name resolution fails. Error     messages are returned as an array of strings under the relevant query     parameter key. e.g. {\"object_name\": [\"This field may not be blank.\"]}. - 200 if successful. Response data contains the passed in query parameters and     the resolved coordinate as a sexagesimal string with units hourangle, deg     under the key <code>coord</code>.</p> Source code in <code>vast_pipeline/views.py</code> <pre><code>@rest_framework.decorators.action(methods=['get'], detail=False)\ndef sesame_search(self, request: Request) -&gt; Response:\n    \"\"\"Query the Sesame name resolver service and return a coordinate.\n\n    Args:\n        request (Request): Django REST framework Request object with GET parameters:\n            - object_name (str): Object name to query.\n            - service (str, optional): Sesame service to query (all, simbad, ned, vizier).\n                Defaults to \"all\".\n\n    Returns:\n        A Django REST framework Response. Will return JSON with status code:\n            - 400 if the query params fail validation (i.e. if an invalid Sesame service\n                or no object name is provided) or if the name resolution fails. Error\n                messages are returned as an array of strings under the relevant query\n                parameter key. e.g. {\"object_name\": [\"This field may not be blank.\"]}.\n            - 200 if successful. Response data contains the passed in query parameters and\n                the resolved coordinate as a sexagesimal string with units hourangle, deg\n                under the key `coord`.\n    \"\"\"\n    object_name = request.query_params.get(\"object_name\", \"\")\n    service = request.query_params.get(\"service\", \"all\")\n\n    serializer = SesameResultSerializer(data=dict(object_name=object_name, service=service))\n    serializer.is_valid(raise_exception=True)\n\n    return Response(serializer.data)\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.SourceEtaVPlot","title":"<code>SourceEtaVPlot(request)</code>","text":"<p>The view for the main eta-V plot page.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Django REST Framework request object.</p> required <p>Returns:</p> Type Description <code>Response</code> <p>The response for the main eta-V page.</p> Source code in <code>vast_pipeline/views.py</code> <pre><code>@login_required\ndef SourceEtaVPlot(request: Request) -&gt; Response:\n    \"\"\"The view for the main eta-V plot page.\n\n    Args:\n        request (Request): Django REST Framework request object.\n\n    Returns:\n        The response for the main eta-V page.\n    \"\"\"\n    min_sources = 50\n\n    source_query_result_id_list = request.session.get(\"source_query_result_ids\", [])\n\n    sources_query_len = len(source_query_result_id_list)\n\n    if sources_query_len &lt; min_sources:\n        messages.error(\n            request,\n            (\n                f'The query has returned only {sources_query_len} sources.'\n                f' A minimum of {min_sources} sources must be used to produce'\n                ' the plot.'\n            )\n        )\n\n        plot_ok = 0\n\n    else:\n        sources = Source.objects.filter(\n            id__in=source_query_result_id_list,\n            n_meas__gt=1,\n            eta_peak__gt=0,\n            eta_int__gt=0,\n            v_peak__gt=0,\n            v_int__gt=0\n        )\n\n        new_sources_ids_list = list(sources.values_list(\"id\", flat=True))\n\n        new_sources_query_len = len(new_sources_ids_list)\n\n        diff = sources_query_len - new_sources_query_len\n\n        if diff &gt; 0:\n            messages.warning(\n                request,\n                (\n                    f'Removed {diff} sources that either had'\n                    ' only one datapoint, or, an \\u03B7 or V value of 0.'\n                    ' Change the query options to avoid these sources.'\n                )\n            )\n\n            request.session[\"source_query_result_ids\"] = new_sources_ids_list\n\n        if new_sources_query_len &lt; min_sources:\n            messages.error(\n                request,\n                (\n                    'After filtering, the query has returned only'\n                    f' {sources_query_len} sources. A minimum of {min_sources}'\n                    ' sources must be used to produce the plot.'\n                )\n            )\n\n            plot_ok = 0\n\n        else:\n            if new_sources_query_len &gt; settings.ETA_V_DATASHADER_THRESHOLD:\n                messages.info(\n                    request,\n                    (\n                        \"Sources outside of the selected sigma area\"\n                        \" are displayed as a non-interactive averaged\"\n                        \" distribution.\"\n                    )\n                )\n\n            plot_ok = 1\n\n    context = {\n        'plot_ok': plot_ok,\n    }\n\n    return render(request, 'sources_etav_plot.html', context)\n</code></pre>"},{"location":"reference/views/#vast_pipeline.views.SourceEtaVPlotUpdate","title":"<code>SourceEtaVPlotUpdate(request, pk)</code>","text":"<p>The view to perform the update on the eta-V plot page.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Django REST Framework request object.</p> required <code>pk</code> <code>int</code> <p>Source object primary key. Defaults to None.</p> required <p>Raises:</p> Type Description <code>Http404</code> <p>if a Source with the given <code>pk</code> cannot be found.</p> <p>Returns:</p> Type Description <code>Response</code> <p>The response for the update page.</p> Source code in <code>vast_pipeline/views.py</code> <pre><code>@login_required\ndef SourceEtaVPlotUpdate(request: Request, pk: int) -&gt; Response:\n    \"\"\"The view to perform the update on the eta-V plot page.\n\n    Args:\n        request (Request): Django REST Framework request object.\n        pk (int, optional): Source object primary key. Defaults to None.\n\n    Raises:\n        Http404: if a Source with the given `pk` cannot be found.\n\n    Returns:\n        The response for the update page.\n    \"\"\"\n    try:\n        source = Source.objects.values().get(pk=pk)\n    except Source.DoesNotExist:\n        raise Http404\n\n    source['wavg_ra_hms'] = deg2hms(source['wavg_ra'], hms_format=True)\n    source['wavg_dec_dms'] = deg2dms(source['wavg_dec'], dms_format=True)\n    source['wavg_l'], source['wavg_b'] = equ2gal(source['wavg_ra'], source['wavg_dec'])\n\n    context = {\n        'source': source,\n        'sourcefav': (\n            SourceFav.objects.filter(\n                user__id=request.user.id,\n                source__id=source['id']\n            )\n            .exists()\n        ),\n        'datatables': []\n    }\n\n    return render(request, 'sources_etav_plot_update.html', context)\n</code></pre>"},{"location":"reference/image/main/","title":"main.py","text":"<p>This module contains the relevant classes for the image ingestion.</p>"},{"location":"reference/image/main/#vast_pipeline.image.main.FitsImage","title":"<code>FitsImage</code>","text":"<p>               Bases: <code>Image</code></p> <p>FitsImage class to model FITS files</p> <p>Attributes:</p> Name Type Description <code>beam_bmaj</code> <code>float</code> <p>Major axis size of the restoring beam (degrees).</p> <code>beam_bmin</code> <code>float</code> <p>Minor axis size of the restoring beam (degrees).</p> <code>beam_bpa</code> <code>float</code> <p>Position angle of the restoring beam (degrees).</p> <code>datetime</code> <code>Timestamp</code> <p>Date of the observation.</p> <code>duration</code> <code>float</code> <p>Duration of the observation in seconds. Is set to 0 if duration is not in header.</p> <code>fov_bmaj</code> <code>float</code> <p>Estimate of the field of view in the north-south direction (degrees).</p> <code>fov_bmin</code> <code>float</code> <p>Estimate of the field of view in the east-west direction (degrees).</p> <code>ra</code> <code>float</code> <p>Right ascension coordinate of the image centre (degrees).</p> <code>dec</code> <code>float</code> <p>Declination coordinate of the image centre (degrees).</p> <code>polarisation</code> <code>str</code> <p>The polarisation of the image.</p> Source code in <code>vast_pipeline/image/main.py</code> <pre><code>class FitsImage(Image):\n    \"\"\"\n    FitsImage class to model FITS files\n\n    Attributes:\n        beam_bmaj (float): Major axis size of the restoring beam (degrees).\n        beam_bmin (float): Minor axis size of the restoring beam (degrees).\n        beam_bpa (float): Position angle of the restoring beam (degrees).\n        datetime (pd.Timestamp): Date of the observation.\n        duration (float): Duration of the observation in seconds. Is set to\n            0 if duration is not in header.\n        fov_bmaj (float): Estimate of the field of view in the north-south\n            direction (degrees).\n        fov_bmin (float): Estimate of the field of view in the east-west\n            direction (degrees).\n        ra (float): Right ascension coordinate of the image centre (degrees).\n        dec (float): Declination coordinate of the image centre (degrees).\n        polarisation (str): The polarisation of the image.\n    \"\"\"\n\n    entire_image = True\n\n    def __init__(self, path: str, hdu_index: int = 0) -&gt; None:\n        \"\"\"\n        Initialise a FitsImage object.\n\n        Args:\n            path:\n                The system path of the FITS image.\n            hdu_index:\n                The index to use on the hdu to fetch the FITS header.\n\n        Returns:\n            None.\n        \"\"\"\n        # inherit from parent\n        super().__init__(path)\n\n        # set other attributes\n        header = self.__get_header(hdu_index)\n\n        # set the rest of the attributes\n        self.__set_img_attr_for_telescope(header)\n\n        # get the frequency\n        self.__get_frequency(header)\n\n    def __get_header(self, hdu_index: int) -&gt; fits.Header:\n        \"\"\"\n        Retrieves the header from the FITS image.\n\n        Args:\n            hdu_index:\n                The index to use on the hdu to fetch the FITS header.\n\n        Returns:\n            The FITS header as an astropy.io.fits.Header object.\n        \"\"\"\n\n        try:\n            with open_fits(self.path) as hdulist:\n                hdu = hdulist[hdu_index]\n        except Exception:\n            raise IOError((\n                'Could not read FITS file: '\n                f'{self.path}'\n            ))\n\n        return hdu.header.copy()\n\n    def __set_img_attr_for_telescope(self, header):\n        '''\n        Set the image attributes depending on the telescope type\n        '''\n        self.polarisation = header.get('STOKES', 'I')\n        self.duration = float(header.get('DURATION', 0.))\n        self.beam_bmaj = 0.\n        self.beam_bmin = 0.\n        self.beam_bpa = 0.\n        self.ra = None\n        self.dec = None\n        self.fov_bmaj = None\n        self.fov_bmin = None\n\n        if header.get('TELESCOP', None) == 'ASKAP':\n            try:\n                self.datetime = pd.Timestamp(\n                    header['DATE-OBS'], tz=header['TIMESYS']\n                )\n                self.beam_bmaj = header['BMAJ']\n                self.beam_bmin = header['BMIN']\n                self.beam_bpa = header['BPA']\n            except KeyError as e:\n                logger.exception(\n                    \"Image %s does not contain expected FITS header keywords.\",\n                    self.name,\n                )\n                raise e\n\n            params = {\n                'header': header,\n                'fits_naxis1': 'NAXIS1',\n                'fits_naxis2': 'NAXIS2',\n            }\n\n            # set the coordinate attributes\n            self.__get_img_coordinates(**params)\n\n        # get the time as Julian Datetime using Pandas function\n        self.jd = self.datetime.to_julian_date()\n\n    def __get_img_coordinates(\n        self, header: fits.Header, fits_naxis1: str, fits_naxis2: str\n    ) -&gt; None:\n        \"\"\"\n        Set the image attributes ra, dec, fov_bmin and fov_bmaj, radius\n        from the image file header.\n\n        Args:\n            header: The FITS header object.\n            fits_naxis1: The header keyword of the NAXIS1 to use.\n            fits_naxis2: The header keyword of the NAXIS2 to use.\n\n        Returns:\n            None\n        \"\"\"\n        wcs = WCS(header, naxis=2)\n        pix_centre = [[header[fits_naxis1] / 2., header[fits_naxis2] / 2.]]\n        self.ra, self.dec = wcs.wcs_pix2world(pix_centre, 1)[0]\n\n        # The field-of-view (in pixels) is assumed to be a circle in the centre\n        # of the image. This may be an ellipse on the sky, eg MOST images.\n        # We leave a pixel margin at the edge that we don't use.\n        # TODO: move unused pixel as argument\n        unusedpix = 0.\n        usable_radius_pix = self.__get_radius_pixels(\n            header, fits_naxis1, fits_naxis2\n        ) - unusedpix\n        cdelt1, cdelt2 = proj_plane_pixel_scales(WCS(header).celestial)\n        self.fov_bmin = usable_radius_pix * abs(cdelt1)\n        self.fov_bmaj = usable_radius_pix * abs(cdelt2)\n        self.physical_bmin = header[fits_naxis1] * abs(cdelt1)\n        self.physical_bmaj = header[fits_naxis2] * abs(cdelt2)\n\n        # set the pixels radius\n        # TODO: check calcs\n        self.radius_pixels = usable_radius_pix\n\n    def __get_radius_pixels(\n        self, header: fits.Header, fits_naxis1: str, fits_naxis2: str\n    ) -&gt; float:\n        \"\"\"\n        Return the radius (pixels) of the full image.\n\n        If the image is not a square/circle then the shortest radius will be\n        returned.\n\n        Args:\n            header: The FITS header object.\n            fits_naxis1: The header keyword of the NAXIS1 to use.\n            fits_naxis2: The header keyword of the NAXIS2 to use.\n\n        Returns:\n            The radius of the image in pixels.\n        \"\"\"\n        if self.entire_image:\n            # a large circle that *should* include the whole image\n            # (and then some)\n            diameter = np.hypot(header[fits_naxis1], header[fits_naxis2])\n        else:\n            # We simply place the largest circle we can in the centre.\n            diameter = min(header[fits_naxis1], header[fits_naxis2])\n        return diameter / 2.\n\n    def __get_frequency(self, header: fits.Header) -&gt; None:\n        \"\"\"\n        Set some 'shortcut' variables for access to the frequency parameters\n        in the FITS file header.\n\n        Args:\n            header: The FITS header object.\n\n        Returns:\n            None\n        \"\"\"\n        self.freq_eff = None\n        self.freq_bw = None\n        try:\n            freq_keys = ('FREQ', 'VOPT')\n            if ('ctype3' in header) and (header['ctype3'] in freq_keys):\n                self.freq_eff = header['crval3']\n                self.freq_bw = header['cdelt3'] if 'cdelt3' in header else 0.0\n            elif ('ctype4' in header) and (header['ctype4'] in freq_keys):\n                self.freq_eff = header['crval4']\n                self.freq_bw = header['cdelt4'] if 'cdelt4' in header else 0.0\n            else:\n                self.freq_eff = header['restfreq']\n                self.freq_bw = header['restbw'] if 'restbw' in header else 0.0\n        except Exception:\n            msg = f\"Frequency not specified in headers for {self.name}\"\n            logger.error(msg)\n            raise TypeError(msg)\n</code></pre>"},{"location":"reference/image/main/#vast_pipeline.image.main.FitsImage.__get_frequency","title":"<code>__get_frequency(header)</code>","text":"<p>Set some 'shortcut' variables for access to the frequency parameters in the FITS file header.</p> <p>Parameters:</p> Name Type Description Default <code>header</code> <code>Header</code> <p>The FITS header object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/image/main.py</code> <pre><code>def __get_frequency(self, header: fits.Header) -&gt; None:\n    \"\"\"\n    Set some 'shortcut' variables for access to the frequency parameters\n    in the FITS file header.\n\n    Args:\n        header: The FITS header object.\n\n    Returns:\n        None\n    \"\"\"\n    self.freq_eff = None\n    self.freq_bw = None\n    try:\n        freq_keys = ('FREQ', 'VOPT')\n        if ('ctype3' in header) and (header['ctype3'] in freq_keys):\n            self.freq_eff = header['crval3']\n            self.freq_bw = header['cdelt3'] if 'cdelt3' in header else 0.0\n        elif ('ctype4' in header) and (header['ctype4'] in freq_keys):\n            self.freq_eff = header['crval4']\n            self.freq_bw = header['cdelt4'] if 'cdelt4' in header else 0.0\n        else:\n            self.freq_eff = header['restfreq']\n            self.freq_bw = header['restbw'] if 'restbw' in header else 0.0\n    except Exception:\n        msg = f\"Frequency not specified in headers for {self.name}\"\n        logger.error(msg)\n        raise TypeError(msg)\n</code></pre>"},{"location":"reference/image/main/#vast_pipeline.image.main.FitsImage.__get_header","title":"<code>__get_header(hdu_index)</code>","text":"<p>Retrieves the header from the FITS image.</p> <p>Parameters:</p> Name Type Description Default <code>hdu_index</code> <code>int</code> <p>The index to use on the hdu to fetch the FITS header.</p> required <p>Returns:</p> Type Description <code>Header</code> <p>The FITS header as an astropy.io.fits.Header object.</p> Source code in <code>vast_pipeline/image/main.py</code> <pre><code>def __get_header(self, hdu_index: int) -&gt; fits.Header:\n    \"\"\"\n    Retrieves the header from the FITS image.\n\n    Args:\n        hdu_index:\n            The index to use on the hdu to fetch the FITS header.\n\n    Returns:\n        The FITS header as an astropy.io.fits.Header object.\n    \"\"\"\n\n    try:\n        with open_fits(self.path) as hdulist:\n            hdu = hdulist[hdu_index]\n    except Exception:\n        raise IOError((\n            'Could not read FITS file: '\n            f'{self.path}'\n        ))\n\n    return hdu.header.copy()\n</code></pre>"},{"location":"reference/image/main/#vast_pipeline.image.main.FitsImage.__get_img_coordinates","title":"<code>__get_img_coordinates(header, fits_naxis1, fits_naxis2)</code>","text":"<p>Set the image attributes ra, dec, fov_bmin and fov_bmaj, radius from the image file header.</p> <p>Parameters:</p> Name Type Description Default <code>header</code> <code>Header</code> <p>The FITS header object.</p> required <code>fits_naxis1</code> <code>str</code> <p>The header keyword of the NAXIS1 to use.</p> required <code>fits_naxis2</code> <code>str</code> <p>The header keyword of the NAXIS2 to use.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/image/main.py</code> <pre><code>def __get_img_coordinates(\n    self, header: fits.Header, fits_naxis1: str, fits_naxis2: str\n) -&gt; None:\n    \"\"\"\n    Set the image attributes ra, dec, fov_bmin and fov_bmaj, radius\n    from the image file header.\n\n    Args:\n        header: The FITS header object.\n        fits_naxis1: The header keyword of the NAXIS1 to use.\n        fits_naxis2: The header keyword of the NAXIS2 to use.\n\n    Returns:\n        None\n    \"\"\"\n    wcs = WCS(header, naxis=2)\n    pix_centre = [[header[fits_naxis1] / 2., header[fits_naxis2] / 2.]]\n    self.ra, self.dec = wcs.wcs_pix2world(pix_centre, 1)[0]\n\n    # The field-of-view (in pixels) is assumed to be a circle in the centre\n    # of the image. This may be an ellipse on the sky, eg MOST images.\n    # We leave a pixel margin at the edge that we don't use.\n    # TODO: move unused pixel as argument\n    unusedpix = 0.\n    usable_radius_pix = self.__get_radius_pixels(\n        header, fits_naxis1, fits_naxis2\n    ) - unusedpix\n    cdelt1, cdelt2 = proj_plane_pixel_scales(WCS(header).celestial)\n    self.fov_bmin = usable_radius_pix * abs(cdelt1)\n    self.fov_bmaj = usable_radius_pix * abs(cdelt2)\n    self.physical_bmin = header[fits_naxis1] * abs(cdelt1)\n    self.physical_bmaj = header[fits_naxis2] * abs(cdelt2)\n\n    # set the pixels radius\n    # TODO: check calcs\n    self.radius_pixels = usable_radius_pix\n</code></pre>"},{"location":"reference/image/main/#vast_pipeline.image.main.FitsImage.__get_radius_pixels","title":"<code>__get_radius_pixels(header, fits_naxis1, fits_naxis2)</code>","text":"<p>Return the radius (pixels) of the full image.</p> <p>If the image is not a square/circle then the shortest radius will be returned.</p> <p>Parameters:</p> Name Type Description Default <code>header</code> <code>Header</code> <p>The FITS header object.</p> required <code>fits_naxis1</code> <code>str</code> <p>The header keyword of the NAXIS1 to use.</p> required <code>fits_naxis2</code> <code>str</code> <p>The header keyword of the NAXIS2 to use.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The radius of the image in pixels.</p> Source code in <code>vast_pipeline/image/main.py</code> <pre><code>def __get_radius_pixels(\n    self, header: fits.Header, fits_naxis1: str, fits_naxis2: str\n) -&gt; float:\n    \"\"\"\n    Return the radius (pixels) of the full image.\n\n    If the image is not a square/circle then the shortest radius will be\n    returned.\n\n    Args:\n        header: The FITS header object.\n        fits_naxis1: The header keyword of the NAXIS1 to use.\n        fits_naxis2: The header keyword of the NAXIS2 to use.\n\n    Returns:\n        The radius of the image in pixels.\n    \"\"\"\n    if self.entire_image:\n        # a large circle that *should* include the whole image\n        # (and then some)\n        diameter = np.hypot(header[fits_naxis1], header[fits_naxis2])\n    else:\n        # We simply place the largest circle we can in the centre.\n        diameter = min(header[fits_naxis1], header[fits_naxis2])\n    return diameter / 2.\n</code></pre>"},{"location":"reference/image/main/#vast_pipeline.image.main.FitsImage.__init__","title":"<code>__init__(path, hdu_index=0)</code>","text":"<p>Initialise a FitsImage object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The system path of the FITS image.</p> required <code>hdu_index</code> <code>int</code> <p>The index to use on the hdu to fetch the FITS header.</p> <code>0</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>vast_pipeline/image/main.py</code> <pre><code>def __init__(self, path: str, hdu_index: int = 0) -&gt; None:\n    \"\"\"\n    Initialise a FitsImage object.\n\n    Args:\n        path:\n            The system path of the FITS image.\n        hdu_index:\n            The index to use on the hdu to fetch the FITS header.\n\n    Returns:\n        None.\n    \"\"\"\n    # inherit from parent\n    super().__init__(path)\n\n    # set other attributes\n    header = self.__get_header(hdu_index)\n\n    # set the rest of the attributes\n    self.__set_img_attr_for_telescope(header)\n\n    # get the frequency\n    self.__get_frequency(header)\n</code></pre>"},{"location":"reference/image/main/#vast_pipeline.image.main.FitsImage.__set_img_attr_for_telescope","title":"<code>__set_img_attr_for_telescope(header)</code>","text":"<p>Set the image attributes depending on the telescope type</p> Source code in <code>vast_pipeline/image/main.py</code> <pre><code>def __set_img_attr_for_telescope(self, header):\n    '''\n    Set the image attributes depending on the telescope type\n    '''\n    self.polarisation = header.get('STOKES', 'I')\n    self.duration = float(header.get('DURATION', 0.))\n    self.beam_bmaj = 0.\n    self.beam_bmin = 0.\n    self.beam_bpa = 0.\n    self.ra = None\n    self.dec = None\n    self.fov_bmaj = None\n    self.fov_bmin = None\n\n    if header.get('TELESCOP', None) == 'ASKAP':\n        try:\n            self.datetime = pd.Timestamp(\n                header['DATE-OBS'], tz=header['TIMESYS']\n            )\n            self.beam_bmaj = header['BMAJ']\n            self.beam_bmin = header['BMIN']\n            self.beam_bpa = header['BPA']\n        except KeyError as e:\n            logger.exception(\n                \"Image %s does not contain expected FITS header keywords.\",\n                self.name,\n            )\n            raise e\n\n        params = {\n            'header': header,\n            'fits_naxis1': 'NAXIS1',\n            'fits_naxis2': 'NAXIS2',\n        }\n\n        # set the coordinate attributes\n        self.__get_img_coordinates(**params)\n\n    # get the time as Julian Datetime using Pandas function\n    self.jd = self.datetime.to_julian_date()\n</code></pre>"},{"location":"reference/image/main/#vast_pipeline.image.main.Image","title":"<code>Image</code>","text":"<p>               Bases: <code>object</code></p> <p>Generic abstract class for an image.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The image name taken from the file name.</p> <code>path</code> <code>str</code> <p>The system path to the image.</p> Source code in <code>vast_pipeline/image/main.py</code> <pre><code>class Image(object):\n    \"\"\"Generic abstract class for an image.\n\n    Attributes:\n        name (str): The image name taken from the file name.\n        path (str): The system path to the image.\n\n    \"\"\"\n\n    def __init__(self, path: str) -&gt; None:\n        \"\"\"\n        Initiliase an image object.\n\n        Args:\n            path:\n                The system path to the FITS image. The name of the image is\n                taken from the filename in the given path.\n\n        Returns:\n            None.\n        \"\"\"\n        self.name = os.path.basename(path)\n        self.path = path\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Defines the printable representation.\n\n        Returns:\n            Printable representation which is the pipeline run name.\n        \"\"\"\n        return self.name\n</code></pre>"},{"location":"reference/image/main/#vast_pipeline.image.main.Image.__init__","title":"<code>__init__(path)</code>","text":"<p>Initiliase an image object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The system path to the FITS image. The name of the image is taken from the filename in the given path.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>vast_pipeline/image/main.py</code> <pre><code>def __init__(self, path: str) -&gt; None:\n    \"\"\"\n    Initiliase an image object.\n\n    Args:\n        path:\n            The system path to the FITS image. The name of the image is\n            taken from the filename in the given path.\n\n    Returns:\n        None.\n    \"\"\"\n    self.name = os.path.basename(path)\n    self.path = path\n</code></pre>"},{"location":"reference/image/main/#vast_pipeline.image.main.Image.__repr__","title":"<code>__repr__()</code>","text":"<p>Defines the printable representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>Printable representation which is the pipeline run name.</p> Source code in <code>vast_pipeline/image/main.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Defines the printable representation.\n\n    Returns:\n        Printable representation which is the pipeline run name.\n    \"\"\"\n    return self.name\n</code></pre>"},{"location":"reference/image/main/#vast_pipeline.image.main.SelavyImage","title":"<code>SelavyImage</code>","text":"<p>               Bases: <code>FitsImage</code></p> <p>Fits images that have a selavy catalogue.</p> <p>Attributes:</p> Name Type Description <code>selavy_path</code> <code>str</code> <p>The system path to the Selavy file.</p> <code>noise_path</code> <code>str</code> <p>The system path to the noise image associated with the image.</p> <code>background_path</code> <code>str</code> <p>The system path to the background image associated with the image.</p> <code>config</code> <code>Dict</code> <p>The image configuration settings.</p> Source code in <code>vast_pipeline/image/main.py</code> <pre><code>class SelavyImage(FitsImage):\n    \"\"\"\n    Fits images that have a selavy catalogue.\n\n    Attributes:\n        selavy_path (str): The system path to the Selavy file.\n        noise_path (str): The system path to the noise image associated\n            with the image.\n        background_path (str): The system path to the background image\n            associated with the image.\n        config (Dict): The image configuration settings.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        paths: Dict[str, Dict[str, str]],\n        config: Dict,\n        hdu_index: int = 0,\n    ) -&gt; None:\n        \"\"\"\n        Initialise the SelavyImage.\n\n        Args:\n            path: The system path to the FITS image.\n            paths: Dictionary containing the system paths to the associated\n                image products and selavy catalogue. The keys are 'selavy',\n                'noise', 'background'.\n            config: Configuration settings for the image ingestion.\n            hdu_index: The index number to use to access the header from the\n                hdu object.\n\n        Returns:\n            None.\n        \"\"\"\n        # inherit from parent\n        self.selavy_path = paths['selavy'][path]\n        self.noise_path = paths['noise'].get(path, '')\n        self.background_path = paths['background'].get(path, '')\n        self.config: Dict = config\n        super().__init__(path, hdu_index)\n\n    def read_selavy(self, dj_image: models.Image) -&gt; pd.DataFrame:\n        \"\"\"\n        Read the sources from the selavy catalogue, select wanted columns\n        and remap them to correct names, followed by filtering and Condon\n        error calculations.\n\n        Args:\n            dj_image: The image model object.\n\n        Returns:\n            Dataframe containing the cleaned and processed Selavy components.\n        \"\"\"\n        # TODO: improve with loading only the cols we need and set datatype\n        if self.selavy_path.endswith(\n                \".xml\") or self.selavy_path.endswith(\".vot\"):\n            df = Table.read(\n                self.selavy_path, format=\"votable\", use_names_over_ids=True\n            ).to_pandas()\n        elif self.selavy_path.endswith(\".csv\"):\n            # CSVs from CASDA have all lowercase column names\n            df = pd.read_csv(self.selavy_path).rename(\n                columns={\"spectral_index_from_tt\": \"spectral_index_from_TT\"}\n            )\n        else:\n            df = pd.read_fwf(self.selavy_path, skiprows=[1])\n        # drop first line with unit of measure, select only wanted\n        # columns and rename them\n        df = df.loc[:, tr_selavy.keys()].rename(\n            columns={x: tr_selavy[x][\"name\"] for x in tr_selavy}\n        )\n\n        # fix dtype of columns\n        for ky in tr_selavy:\n            key = tr_selavy[ky]\n            if df[key['name']].dtype != key['dtype']:\n                df[key['name']] = df[key['name']].astype(key['dtype'])\n\n        # do checks and fill in missing field for uploading sources\n        # in DB (see fields in models.py -&gt; Source model)\n        if df['component_id'].duplicated().any():\n            raise Exception('Found duplicated names in sources')\n\n        # drop unrealistic sources\n        cols_to_check = [\n            'bmaj',\n            'bmin',\n            'flux_peak',\n            'flux_int',\n        ]\n\n        bad_sources = df[(df[cols_to_check] == 0).any(axis=1)]\n        if bad_sources.shape[0] &gt; 0:\n            logger.debug(\"Dropping %i bad sources.\", bad_sources.shape[0])\n            df = df.drop(bad_sources.index)\n\n        # dropping tiny sources\n        nr_sources_old = df.shape[0]\n        df = df.loc[\n            (df['bmaj'] &gt; dj_image.beam_bmaj * 500) &amp;\n            (df['bmin'] &gt; dj_image.beam_bmin * 500)\n        ]\n        if df.shape[0] != nr_sources_old:\n            logger.info(\n                'Dropped %i tiny sources.', nr_sources_old - df.shape[0]\n            )\n\n        # add fields from image and fix name column\n        df['image_id'] = dj_image.id\n        df['time'] = dj_image.datetime\n\n        # append img prefix to source name\n        img_prefix = dj_image.name.split('.i.', 1)[-1].split('.', 1)[0] + '_'\n        df['name'] = img_prefix + df['component_id']\n\n        # # fix error fluxes\n        for col in ['flux_int_err', 'flux_peak_err']:\n            sel = df[col] &lt; settings.FLUX_DEFAULT_MIN_ERROR\n            if sel.any():\n                df.loc[sel, col] = settings.FLUX_DEFAULT_MIN_ERROR\n\n        # # fix error ra dec\n        for col in ['ra_err', 'dec_err']:\n            sel = df[col] &lt; settings.POS_DEFAULT_MIN_ERROR\n            if sel.any():\n                df.loc[sel, col] = settings.POS_DEFAULT_MIN_ERROR\n            df[col] = df[col] / 3600.\n\n        # replace 0 local_rms values using user config value\n        df.loc[\n            df['local_rms'] == 0., 'local_rms'\n        ] = self.config[\"selavy_local_rms_fill_value\"]\n\n        df['snr'] = df['flux_peak'].values / df['local_rms'].values\n        df['compactness'] = df['flux_int'].values / df['flux_peak'].values\n\n        if self.config[\"condon_errors\"]:\n            logger.debug(\"Calculating Condon '97 errors...\")\n            theta_B = dj_image.beam_bmaj\n            theta_b = dj_image.beam_bmin\n\n            df[[\n                'flux_peak_err',\n                'flux_int_err',\n                'err_bmaj',\n                'err_bmin',\n                'err_pa',\n                'ra_err',\n                'dec_err',\n            ]] = df[[\n                'flux_peak',\n                'flux_int',\n                'bmaj',\n                'bmin',\n                'pa',\n                'snr',\n                'local_rms',\n            ]].apply(\n                calc_condon_flux_errors,\n                args=(theta_B, theta_b),\n                axis=1,\n                result_type='expand'\n            )\n\n            logger.debug(\"Condon errors done.\")\n\n        logger.debug(\"Calculating positional errors...\")\n        # TODO: avoid extra column given that it is a single value\n        df['ew_sys_err'] = self.config[\"ra_uncertainty\"] / 3600.\n        df['ns_sys_err'] = self.config[\"dec_uncertainty\"] / 3600.\n\n        df['error_radius'] = calc_error_radius(\n            df['ra'].values,\n            df['ra_err'].values,\n            df['dec'].values,\n            df['dec_err'].values,\n        )\n\n        df['uncertainty_ew'] = np.hypot(\n            df['ew_sys_err'].values, df['error_radius'].values\n        )\n\n        df['uncertainty_ns'] = np.hypot(\n            df['ns_sys_err'].values, df['error_radius'].values\n        )\n\n        # weight calculations to use later\n        df['weight_ew'] = 1. / df['uncertainty_ew'].values**2\n        df['weight_ns'] = 1. / df['uncertainty_ns'].values**2\n\n        logger.debug('Positional errors done.')\n\n        # Initialise the forced column as False\n        df['forced'] = False\n\n        # Calculate island flux fractions\n        island_flux_totals = (\n            df[['island_id', 'flux_int', 'flux_peak']]\n            .groupby('island_id')\n            .agg('sum')\n        )\n\n        df['flux_int_isl_ratio'] = (\n            df['flux_int'].values\n            / island_flux_totals.loc[df['island_id']]['flux_int'].values\n        )\n\n        df['flux_peak_isl_ratio'] = (\n            df['flux_peak'].values\n            / island_flux_totals.loc[df['island_id']]['flux_peak'].values\n        )\n\n        return df\n</code></pre>"},{"location":"reference/image/main/#vast_pipeline.image.main.SelavyImage.__init__","title":"<code>__init__(path, paths, config, hdu_index=0)</code>","text":"<p>Initialise the SelavyImage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The system path to the FITS image.</p> required <code>paths</code> <code>Dict[str, Dict[str, str]]</code> <p>Dictionary containing the system paths to the associated image products and selavy catalogue. The keys are 'selavy', 'noise', 'background'.</p> required <code>config</code> <code>Dict</code> <p>Configuration settings for the image ingestion.</p> required <code>hdu_index</code> <code>int</code> <p>The index number to use to access the header from the hdu object.</p> <code>0</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>vast_pipeline/image/main.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    paths: Dict[str, Dict[str, str]],\n    config: Dict,\n    hdu_index: int = 0,\n) -&gt; None:\n    \"\"\"\n    Initialise the SelavyImage.\n\n    Args:\n        path: The system path to the FITS image.\n        paths: Dictionary containing the system paths to the associated\n            image products and selavy catalogue. The keys are 'selavy',\n            'noise', 'background'.\n        config: Configuration settings for the image ingestion.\n        hdu_index: The index number to use to access the header from the\n            hdu object.\n\n    Returns:\n        None.\n    \"\"\"\n    # inherit from parent\n    self.selavy_path = paths['selavy'][path]\n    self.noise_path = paths['noise'].get(path, '')\n    self.background_path = paths['background'].get(path, '')\n    self.config: Dict = config\n    super().__init__(path, hdu_index)\n</code></pre>"},{"location":"reference/image/main/#vast_pipeline.image.main.SelavyImage.read_selavy","title":"<code>read_selavy(dj_image)</code>","text":"<p>Read the sources from the selavy catalogue, select wanted columns and remap them to correct names, followed by filtering and Condon error calculations.</p> <p>Parameters:</p> Name Type Description Default <code>dj_image</code> <code>Image</code> <p>The image model object.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe containing the cleaned and processed Selavy components.</p> Source code in <code>vast_pipeline/image/main.py</code> <pre><code>def read_selavy(self, dj_image: models.Image) -&gt; pd.DataFrame:\n    \"\"\"\n    Read the sources from the selavy catalogue, select wanted columns\n    and remap them to correct names, followed by filtering and Condon\n    error calculations.\n\n    Args:\n        dj_image: The image model object.\n\n    Returns:\n        Dataframe containing the cleaned and processed Selavy components.\n    \"\"\"\n    # TODO: improve with loading only the cols we need and set datatype\n    if self.selavy_path.endswith(\n            \".xml\") or self.selavy_path.endswith(\".vot\"):\n        df = Table.read(\n            self.selavy_path, format=\"votable\", use_names_over_ids=True\n        ).to_pandas()\n    elif self.selavy_path.endswith(\".csv\"):\n        # CSVs from CASDA have all lowercase column names\n        df = pd.read_csv(self.selavy_path).rename(\n            columns={\"spectral_index_from_tt\": \"spectral_index_from_TT\"}\n        )\n    else:\n        df = pd.read_fwf(self.selavy_path, skiprows=[1])\n    # drop first line with unit of measure, select only wanted\n    # columns and rename them\n    df = df.loc[:, tr_selavy.keys()].rename(\n        columns={x: tr_selavy[x][\"name\"] for x in tr_selavy}\n    )\n\n    # fix dtype of columns\n    for ky in tr_selavy:\n        key = tr_selavy[ky]\n        if df[key['name']].dtype != key['dtype']:\n            df[key['name']] = df[key['name']].astype(key['dtype'])\n\n    # do checks and fill in missing field for uploading sources\n    # in DB (see fields in models.py -&gt; Source model)\n    if df['component_id'].duplicated().any():\n        raise Exception('Found duplicated names in sources')\n\n    # drop unrealistic sources\n    cols_to_check = [\n        'bmaj',\n        'bmin',\n        'flux_peak',\n        'flux_int',\n    ]\n\n    bad_sources = df[(df[cols_to_check] == 0).any(axis=1)]\n    if bad_sources.shape[0] &gt; 0:\n        logger.debug(\"Dropping %i bad sources.\", bad_sources.shape[0])\n        df = df.drop(bad_sources.index)\n\n    # dropping tiny sources\n    nr_sources_old = df.shape[0]\n    df = df.loc[\n        (df['bmaj'] &gt; dj_image.beam_bmaj * 500) &amp;\n        (df['bmin'] &gt; dj_image.beam_bmin * 500)\n    ]\n    if df.shape[0] != nr_sources_old:\n        logger.info(\n            'Dropped %i tiny sources.', nr_sources_old - df.shape[0]\n        )\n\n    # add fields from image and fix name column\n    df['image_id'] = dj_image.id\n    df['time'] = dj_image.datetime\n\n    # append img prefix to source name\n    img_prefix = dj_image.name.split('.i.', 1)[-1].split('.', 1)[0] + '_'\n    df['name'] = img_prefix + df['component_id']\n\n    # # fix error fluxes\n    for col in ['flux_int_err', 'flux_peak_err']:\n        sel = df[col] &lt; settings.FLUX_DEFAULT_MIN_ERROR\n        if sel.any():\n            df.loc[sel, col] = settings.FLUX_DEFAULT_MIN_ERROR\n\n    # # fix error ra dec\n    for col in ['ra_err', 'dec_err']:\n        sel = df[col] &lt; settings.POS_DEFAULT_MIN_ERROR\n        if sel.any():\n            df.loc[sel, col] = settings.POS_DEFAULT_MIN_ERROR\n        df[col] = df[col] / 3600.\n\n    # replace 0 local_rms values using user config value\n    df.loc[\n        df['local_rms'] == 0., 'local_rms'\n    ] = self.config[\"selavy_local_rms_fill_value\"]\n\n    df['snr'] = df['flux_peak'].values / df['local_rms'].values\n    df['compactness'] = df['flux_int'].values / df['flux_peak'].values\n\n    if self.config[\"condon_errors\"]:\n        logger.debug(\"Calculating Condon '97 errors...\")\n        theta_B = dj_image.beam_bmaj\n        theta_b = dj_image.beam_bmin\n\n        df[[\n            'flux_peak_err',\n            'flux_int_err',\n            'err_bmaj',\n            'err_bmin',\n            'err_pa',\n            'ra_err',\n            'dec_err',\n        ]] = df[[\n            'flux_peak',\n            'flux_int',\n            'bmaj',\n            'bmin',\n            'pa',\n            'snr',\n            'local_rms',\n        ]].apply(\n            calc_condon_flux_errors,\n            args=(theta_B, theta_b),\n            axis=1,\n            result_type='expand'\n        )\n\n        logger.debug(\"Condon errors done.\")\n\n    logger.debug(\"Calculating positional errors...\")\n    # TODO: avoid extra column given that it is a single value\n    df['ew_sys_err'] = self.config[\"ra_uncertainty\"] / 3600.\n    df['ns_sys_err'] = self.config[\"dec_uncertainty\"] / 3600.\n\n    df['error_radius'] = calc_error_radius(\n        df['ra'].values,\n        df['ra_err'].values,\n        df['dec'].values,\n        df['dec_err'].values,\n    )\n\n    df['uncertainty_ew'] = np.hypot(\n        df['ew_sys_err'].values, df['error_radius'].values\n    )\n\n    df['uncertainty_ns'] = np.hypot(\n        df['ns_sys_err'].values, df['error_radius'].values\n    )\n\n    # weight calculations to use later\n    df['weight_ew'] = 1. / df['uncertainty_ew'].values**2\n    df['weight_ns'] = 1. / df['uncertainty_ns'].values**2\n\n    logger.debug('Positional errors done.')\n\n    # Initialise the forced column as False\n    df['forced'] = False\n\n    # Calculate island flux fractions\n    island_flux_totals = (\n        df[['island_id', 'flux_int', 'flux_peak']]\n        .groupby('island_id')\n        .agg('sum')\n    )\n\n    df['flux_int_isl_ratio'] = (\n        df['flux_int'].values\n        / island_flux_totals.loc[df['island_id']]['flux_int'].values\n    )\n\n    df['flux_peak_isl_ratio'] = (\n        df['flux_peak'].values\n        / island_flux_totals.loc[df['island_id']]['flux_peak'].values\n    )\n\n    return df\n</code></pre>"},{"location":"reference/image/utils/","title":"utils.py","text":"<p>This module contains utility functions used by the image ingestion section of the pipeline.</p>"},{"location":"reference/image/utils/#vast_pipeline.image.utils.calc_condon_flux_errors","title":"<code>calc_condon_flux_errors(row, theta_B, theta_b, alpha_maj1=2.5, alpha_min1=0.5, alpha_maj2=0.5, alpha_min2=2.5, alpha_maj3=1.5, alpha_min3=1.5, clean_bias=0.0, clean_bias_error=0.0, frac_flux_cal_error=0.0)</code>","text":"<p>The following code for this function taken from the TraP with a few modifications.</p> <p>Returns the errors on parameters from Gaussian fits according to the Condon (PASP 109, 166 (1997)) formulae. These formulae are not perfect, but we'll use them for the time being.  (See Refregier and Brown (astro-ph/9803279v1) for a more rigorous approach.) It also returns the corrected peak. The peak can be corrected for the overestimate due to the local noise gradient, but this is currently not used in the function.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>The row containing the component information from the Selavy component catalogue.</p> required <code>theta_B</code> <code>float</code> <p>The major axis size of the restoring beam of the image (degrees).</p> required <code>theta_b</code> <code>float</code> <p>The minor axis size of the restoring beam of the image (degrees).</p> required <code>alpha_maj1</code> <code>float</code> <p>The alpha_M exponent value for x_0.</p> <code>2.5</code> <code>alpha_min1</code> <code>float</code> <p>The alpha_m exponent value for x_0.</p> <code>0.5</code> <code>alpha_maj2</code> <code>float</code> <p>The alpha_M exponent value for y_0.</p> <code>0.5</code> <code>alpha_min2</code> <code>float</code> <p>The alpha_m exponent value for y_0.</p> <code>2.5</code> <code>alpha_maj3</code> <code>float</code> <p>The alpha_M exponent value for the amplitude error.</p> <code>1.5</code> <code>alpha_min3</code> <code>float</code> <p>The alpha_m exponent value for the amplitude error.</p> <code>1.5</code> <code>clean_bias</code> <code>float</code> <p>Clean bias value used in the peak flux correction (not currently  used).</p> <code>0.0</code> <code>clean_bias_error</code> <code>float</code> <p>The error of the clean bias value used in the peak flux correction (not currently used).</p> <code>0.0</code> <code>frac_flux_cal_error</code> <code>float</code> <p>Flux calibration error value. (Unsure of exact meaning, refer to TraP).</p> <code>0.0</code> <p>Returns:</p> Type Description <code>float</code> <p>Peak flux error (Jy).</p> <code>float</code> <p>Integrated flux error (Jy).</p> <code>float</code> <p>Major axis error (deg).</p> <code>float</code> <p>Minor axis error (deg).</p> <code>float</code> <p>Position angle error (deg).</p> <code>float</code> <p>Right ascension error (deg).</p> <code>float</code> <p>Declination error (deg).</p> Source code in <code>vast_pipeline/image/utils.py</code> <pre><code>def calc_condon_flux_errors(\n    row: pd.Series,\n    theta_B: float,\n    theta_b: float,\n    alpha_maj1: float = 2.5,\n    alpha_min1: float = 0.5,\n    alpha_maj2: float = 0.5,\n    alpha_min2: float = 2.5,\n    alpha_maj3: float = 1.5,\n    alpha_min3: float = 1.5,\n    clean_bias: float = 0.0,\n    clean_bias_error: float = 0.0,\n    frac_flux_cal_error: float = 0.0\n) -&gt; Tuple[float, float, float, float, float, float, float]:\n    \"\"\"\n    The following code for this function taken from the TraP with a few\n    modifications.\n\n    Returns the errors on parameters from Gaussian fits according to\n    the Condon (PASP 109, 166 (1997)) formulae.\n    These formulae are not perfect, but we'll use them for the\n    time being.  (See Refregier and Brown (astro-ph/9803279v1) for\n    a more rigorous approach.) It also returns the corrected peak.\n    The peak can be corrected for the overestimate due to the local\n    noise gradient, but this is currently not used in the function.\n\n    Args:\n        row (pd.Series):\n            The row containing the component information from the Selavy\n            component catalogue.\n        theta_B (float):\n            The major axis size of the restoring beam of the image (degrees).\n        theta_b (float):\n            The minor axis size of the restoring beam of the image (degrees).\n        alpha_maj1 (float):\n            The alpha_M exponent value for x_0.\n        alpha_min1 (float):\n            The alpha_m exponent value for x_0.\n        alpha_maj2 (float):\n            The alpha_M exponent value for y_0.\n        alpha_min2 (float):\n            The alpha_m exponent value for y_0.\n        alpha_maj3 (float):\n            The alpha_M exponent value for the amplitude error.\n        alpha_min3 (float):\n            The alpha_m exponent value for the amplitude error.\n        clean_bias (float):\n            Clean bias value used in the peak flux correction (not currently\n             used).\n        clean_bias_error (float):\n            The error of the clean bias value used in the peak flux correction\n            (not currently used).\n        frac_flux_cal_error (float):\n            Flux calibration error value. (Unsure of exact meaning, refer to\n            TraP).\n\n    Returns:\n        Peak flux error (Jy).\n        Integrated flux error (Jy).\n        Major axis error (deg).\n        Minor axis error (deg).\n        Position angle error (deg).\n        Right ascension error (deg).\n        Declination error (deg).\n    \"\"\"\n\n    major = row.bmaj / 3600.  # degrees\n    minor = row.bmin / 3600.  # degrees\n    theta = np.deg2rad(row.pa)\n    flux_peak = row['flux_peak']\n    flux_int = row['flux_int']\n    snr = row['snr']\n    noise = row['local_rms']\n\n    variables = [\n        theta_B,\n        theta_b,\n        major,\n        minor,\n        flux_peak,\n        flux_int,\n        snr,\n        noise\n    ]\n\n    # return 0 if the source is unrealistic. Should be rare\n    # given that these sources are also filtered out before hand.\n    if 0.0 in variables:\n        logger.debug(variables)\n        return 0., 0., 0., 0., 0., 0., 0.\n\n    try:\n\n        rho_sq1 = ((major * minor / (4. * theta_B * theta_b)) *\n                   (1. + (theta_B / major)**2)**alpha_maj1 *\n                   (1. + (theta_b / minor)**2)**alpha_min1 *\n                   snr**2)\n        rho_sq2 = ((major * minor / (4. * theta_B * theta_b)) *\n                   (1. + (theta_B / major)**2)**alpha_maj2 *\n                   (1. + (theta_b / minor)**2)**alpha_min2 *\n                   snr**2)\n        rho_sq3 = ((major * minor / (4. * theta_B * theta_b)) *\n                   (1. + (theta_B / major)**2)**alpha_maj3 *\n                   (1. + (theta_b / minor)**2)**alpha_min3 *\n                   snr**2)\n\n        rho1 = np.sqrt(rho_sq1)\n        rho2 = np.sqrt(rho_sq2)\n        rho3 = np.sqrt(rho_sq3)\n\n        # here we change the TraP code slightly and base it\n        # purely on Condon 97 and not the NVSS paper.\n        denom1 = np.sqrt(4. * np.log(2.)) * rho1\n        denom2 = np.sqrt(4. * np.log(2.)) * rho2\n\n        # these are the 'xo' and 'y0' errors from Condon\n        error_par_major = major / denom1\n        error_par_minor = minor / denom2\n\n        # ra and dec errors\n        errorra = np.sqrt((error_par_major * np.sin(theta))**2 +\n                          (error_par_minor * np.cos(theta))**2)\n        errordec = np.sqrt((error_par_major * np.cos(theta))**2 +\n                           (error_par_minor * np.sin(theta))**2)\n\n        errormajor = np.sqrt(2) * major / rho1\n        errorminor = np.sqrt(2) * minor / rho2\n\n        if major &gt; minor:\n            errortheta = 2.0 * (major * minor / (major**2 - minor**2)) / rho2\n        else:\n            errortheta = np.pi\n        if errortheta &gt; np.pi:\n            errortheta = np.pi\n\n        # correction to flux peak not currently used\n        # but might be in the future.\n        # Do not remove!\n        # flux_peak += -noise**2 / flux_peak + clean_bias\n\n        errorpeaksq = ((frac_flux_cal_error * flux_peak)**2 +\n                       clean_bias_error**2 +\n                       2. * flux_peak**2 / rho_sq3)\n\n        errorpeak = np.sqrt(errorpeaksq)\n\n        help1 = (errormajor / major)**2\n        help2 = (errorminor / minor)**2\n        help3 = theta_B * theta_b / (major * minor)\n        help4 = np.sqrt(errorpeaksq / flux_peak**2 + help3 * (help1 + help2))\n        errorflux = np.abs(flux_int) * help4\n\n        # need to return flux_peak if used.\n        return errorpeak, errorflux, errormajor, errorminor, errortheta, errorra, errordec\n\n    except Exception as e:\n        logger.debug(\n            \"Error in the calculation of Condon errors for a source\",\n            exc_info=True)\n        return 0., 0., 0., 0., 0., 0., 0.\n</code></pre>"},{"location":"reference/image/utils/#vast_pipeline.image.utils.calc_error_radius","title":"<code>calc_error_radius(ra, ra_err, dec, dec_err)</code>","text":"<p>Using the fitted errors from selavy, this function estimates the largest on sky angular size of the uncertainty. The four different combinations of the errors are analysed and the maximum is returned. Logic is taken from the TraP, where this is also used. Function has been vectorised for pandas. All inputs are in degrees.</p> <p>Parameters:</p> Name Type Description Default <code>ra</code> <code>float</code> <p>The right ascension of the coordinate (degrees).</p> required <code>ra_err</code> <code>float</code> <p>The error associated with the ra value (degrees).</p> required <code>dec</code> <code>float</code> <p>The declination of the coordinate (degrees).</p> required <code>dec_err</code> <code>float</code> <p>The error associated with the declination value (degrees).</p> required <p>Returns:</p> Type Description <code>float</code> <p>The calculated error radius (degrees).</p> Source code in <code>vast_pipeline/image/utils.py</code> <pre><code>def calc_error_radius(ra, ra_err, dec, dec_err) -&gt; float:\n    \"\"\"\n    Using the fitted errors from selavy, this function\n    estimates the largest on sky angular size of the\n    uncertainty. The four different combinations of the\n    errors are analysed and the maximum is returned.\n    Logic is taken from the TraP, where this is also\n    used. Function has been vectorised for pandas. All inputs are in\n    degrees.\n\n    Args:\n        ra (float): The right ascension of the coordinate (degrees).\n        ra_err (float): The error associated with the ra value (degrees).\n        dec (float): The declination of the coordinate (degrees).\n        dec_err (float): The error associated with the declination\n            value (degrees).\n\n    Returns:\n        The calculated error radius (degrees).\n    \"\"\"\n    ra_1 = np.deg2rad(ra)\n    dec_1 = np.deg2rad(dec)\n\n    ra_offsets = [\n        (ra + ra_err),\n        (ra + ra_err),\n        (ra - ra_err),\n        (ra - ra_err)\n    ]\n\n    dec_offsets = [\n        (dec + dec_err),\n        (dec - dec_err),\n        (dec + dec_err),\n        (dec - dec_err)\n    ]\n\n    seps = [\n        np.rad2deg(on_sky_sep(\n            ra_1,\n            np.deg2rad(i),\n            dec_1,\n            np.deg2rad(j)\n        )) for i, j in zip(ra_offsets, dec_offsets)\n    ]\n\n    seps = np.column_stack(seps)\n\n    return np.amax(seps, 1)\n</code></pre>"},{"location":"reference/image/utils/#vast_pipeline.image.utils.on_sky_sep","title":"<code>on_sky_sep(ra_1, ra_2, dec_1, dec_2)</code>","text":"<p>Simple on sky distance between two RA and Dec coordinates. Needed for fast calculation on dataframes as astropy is slow. All units are radians.</p> <p>Parameters:</p> Name Type Description Default <code>ra_1</code> <code>float</code> <p>The right ascension of coodinate 1 (radians).</p> required <code>ra_2</code> <code>float</code> <p>The right ascension of coodinate 2 (radians).</p> required <code>dec_1</code> <code>float</code> <p>The declination of coodinate 1 (radians).</p> required <code>dec_2</code> <code>float</code> <p>The declination of coodinate 2 (radians).</p> required <p>Returns:</p> Type Description <code>float</code> <p>The on-sky separation distance between the two coodinates (radians).</p> Source code in <code>vast_pipeline/image/utils.py</code> <pre><code>def on_sky_sep(ra_1, ra_2, dec_1, dec_2) -&gt; float:\n    \"\"\"\n    Simple on sky distance between two RA and Dec coordinates.\n    Needed for fast calculation on dataframes as astropy is\n    slow. All units are radians.\n\n    Args:\n        ra_1 (float): The right ascension of coodinate 1 (radians).\n        ra_2 (float): The right ascension of coodinate 2 (radians).\n        dec_1 (float): The declination of coodinate 1 (radians).\n        dec_2 (float): The declination of coodinate 2 (radians).\n\n    Returns:\n        The on-sky separation distance between the two coodinates (radians).\n    \"\"\"\n    separation = (\n        np.sin(dec_1) * np.sin(dec_2) +\n        np.cos(dec_1) * np.cos(dec_2) * np.cos(ra_1 - ra_2)\n    )\n    # fix errors on separation values over 1\n    separation[separation &gt; 1.] = 1.\n\n    return np.arccos(separation)\n</code></pre>"},{"location":"reference/image/utils/#vast_pipeline.image.utils.open_fits","title":"<code>open_fits(fits_path, memmap=True)</code>","text":"<p>This function opens both compressed and uncompressed fits files.</p> <p>Parameters:</p> Name Type Description Default <code>fits_path</code> <code>Union[str, Path]</code> <p>Path to the fits file</p> required <code>memmap</code> <code>Optional[bool]</code> <p>Open the fits file with mmap.</p> <code>True</code> <p>Returns:</p> Type Description <p>HDUList loaded from the fits file</p> Source code in <code>vast_pipeline/image/utils.py</code> <pre><code>def open_fits(fits_path: Union[str, Path], memmap: Optional[bool] = True):\n    \"\"\"\n    This function opens both compressed and uncompressed fits files.\n\n    Args:\n        fits_path: Path to the fits file\n        memmap: Open the fits file with mmap.\n\n    Returns:\n        HDUList loaded from the fits file\n    \"\"\"\n\n    if isinstance(fits_path, Path):\n        fits_path = str(fits_path)\n\n    hdul = fits.open(fits_path, memmap=memmap)\n\n    # This is a messy way to check, but I can't think of a better one\n    if len(hdul) == 1:\n        return hdul\n    elif type(hdul[1]) == fits.hdu.compressed.CompImageHDU:\n        return fits.HDUList(hdul[1:])\n    else:\n        return hdul\n</code></pre>"},{"location":"reference/management/helpers/","title":"helpers.py","text":"<p>Helper functions for the commands.</p>"},{"location":"reference/management/helpers/#vast_pipeline.management.helpers.get_p_run_name","title":"<code>get_p_run_name(name, return_folder=False)</code>","text":"<p>Determines the name of the pipeline run. Can also return the output folder if selected.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The user entered name of the pipeline run.</p> required <code>return_folder</code> <code>bool</code> <p>When <code>True</code> the pipeline directory is also returned.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The name of the pipeline run (always returned).</p> <code>str</code> <p>The run directory (if <code>return_folder</code> is set to <code>True</code>).</p> Source code in <code>vast_pipeline/management/helpers.py</code> <pre><code>def get_p_run_name(name: str, return_folder: bool = False) -&gt; Tuple[str, str]:\n    \"\"\"\n    Determines the name of the pipeline run. Can also return the output folder\n    if selected.\n\n    Args:\n        name: The user entered name of the pipeline run.\n        return_folder: When `True` the pipeline directory is also returned.\n\n    Returns:\n        The name of the pipeline run (always returned).\n        The run directory (if `return_folder` is set to `True`).\n    \"\"\"\n    if '/' in name:\n        folder = os.path.realpath(name)\n        run_name = os.path.basename(folder)\n        return (run_name, folder) if return_folder else run_name\n\n    folder = os.path.join(os.path.realpath(sett.PIPELINE_WORKING_DIR), name)\n\n    return (name, folder) if return_folder else name\n</code></pre>"},{"location":"reference/management/commands/clearpiperun/","title":"clearpiperun.py","text":""},{"location":"reference/management/commands/clearpiperun/#vast_pipeline.management.commands.clearpiperun.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>BaseCommand</code></p> <p>This script is used to clean the data for pipeline run(s). Use --help for usage.</p> Source code in <code>vast_pipeline/management/commands/clearpiperun.py</code> <pre><code>class Command(BaseCommand):\n    \"\"\"\n    This script is used to clean the data for pipeline run(s).\n    Use --help for usage.\n    \"\"\"\n\n    help = (\n        'Delete a pipeline run and all related images, sources, etc.'\n        ' Will not delete objects if they are also related to another '\n        'pipeline run.'\n    )\n\n    def add_arguments(self, parser: ArgumentParser) -&gt; None:\n        \"\"\"\n        Enables arguments for the command.\n\n        Args:\n            parser (ArgumentParser): The parser object of the command.\n\n        Returns:\n            None\n        \"\"\"\n        # positional arguments (required)\n        parser.add_argument(\n            'piperuns',\n            nargs='+',\n            type=str,\n            default=None,\n            help=(\n                'Name or path of pipeline run(s) to delete. Pass \"clearall\" to'\n                ' delete all the runs.'\n            )\n        )\n        # keyword arguments (optional)\n        parser.add_argument(\n            '--keep-parquet',\n            required=False,\n            default=False,\n            action='store_true',\n            help=(\n                'Flag to keep the pipeline run(s) parquet files. '\n                'Will also apply to arrow files if present.'\n            )\n        )\n        parser.add_argument(\n            '--remove-all',\n            required=False,\n            default=False,\n            action='store_true',\n            help='Flag to remove all the content of the pipeline run(s) folder.'\n        )\n\n    def handle(self, *args, **options) -&gt; None:\n        \"\"\"\n        Handle function of the command.\n\n        Args:\n            *args: Variable length argument list.\n            **options: Variable length options.\n\n        Returns:\n            None\n        \"\"\"\n        timer = StopWatch()\n\n        if options['verbosity'] &gt; 1:\n            # set root logger to use the DEBUG level\n            root_logger = logging.getLogger('')\n            root_logger.setLevel(logging.DEBUG)\n            # set the traceback on\n            options['traceback'] = True\n\n        if options['keep_parquet'] and options['remove_all']:\n            raise CommandError(\n                '\"--keep-parquets\" flag is incompatible with \"--remove-all\" flag'\n            )\n\n        piperuns = options['piperuns']\n        flag_all_runs = True if 'clearall' in piperuns else False\n\n        if flag_all_runs:\n            logger.info('Clearing all pipeline run in the database')\n            piperuns = list(Run.objects.values_list('name', flat=True))\n\n        for piperun in piperuns:\n            p_run_name = get_p_run_name(piperun)\n            try:\n                p_run = Run.objects.get(name=p_run_name)\n            except Run.DoesNotExist:\n                raise CommandError(f'Pipeline run {p_run_name} does not exist')\n\n            logger.info(\"Deleting pipeline run '%s' from database\", p_run_name)\n            with transaction.atomic():\n                p_run.status = 'DEL'\n                p_run.save()\n\n            timer.reset()\n            delete_pipeline_run_raw_sql(p_run)\n            t = timer.reset()\n            logger.info(\"Time to delete run from database: %.2f sec\", t)\n\n            # remove forced measurements in db if presents\n            forced_parquets = remove_forced_meas(p_run.path)\n            t = timer.reset()\n            logger.info(\"Time to delete forced measurements: %.2f sec\", t)\n\n            # Delete parquet or folder eventually\n            if not options['keep_parquet'] and not options['remove_all']:\n                logger.info('Deleting pipeline \"%s\" parquets', p_run_name)\n                parquets = (\n                    glob(os.path.join(p_run.path, '*.parquet'))\n                    + glob(os.path.join(p_run.path, '*.arrow'))\n                )\n                for parquet in parquets:\n                    try:\n                        os.remove(parquet)\n                    except OSError as e:\n                        self.stdout.write(self.style.WARNING(\n                            f'Parquet file \"{os.path.basename(parquet)}\" not existent'\n                        ))\n                        pass\n            t = timer.reset()\n            logger.info(\"Time to delete parquet files: %.2f sec\", t)\n\n            if options['remove_all']:\n                logger.info('Deleting pipeline folder')\n                try:\n                    shutil.rmtree(p_run.path)\n                except Exception as e:\n                    self.stdout.write(self.style.WARNING(\n                        f'Issues in removing run folder: {e}'\n                    ))\n                    pass\n</code></pre>"},{"location":"reference/management/commands/clearpiperun/#vast_pipeline.management.commands.clearpiperun.Command.add_arguments","title":"<code>add_arguments(parser)</code>","text":"<p>Enables arguments for the command.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>The parser object of the command.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/clearpiperun.py</code> <pre><code>def add_arguments(self, parser: ArgumentParser) -&gt; None:\n    \"\"\"\n    Enables arguments for the command.\n\n    Args:\n        parser (ArgumentParser): The parser object of the command.\n\n    Returns:\n        None\n    \"\"\"\n    # positional arguments (required)\n    parser.add_argument(\n        'piperuns',\n        nargs='+',\n        type=str,\n        default=None,\n        help=(\n            'Name or path of pipeline run(s) to delete. Pass \"clearall\" to'\n            ' delete all the runs.'\n        )\n    )\n    # keyword arguments (optional)\n    parser.add_argument(\n        '--keep-parquet',\n        required=False,\n        default=False,\n        action='store_true',\n        help=(\n            'Flag to keep the pipeline run(s) parquet files. '\n            'Will also apply to arrow files if present.'\n        )\n    )\n    parser.add_argument(\n        '--remove-all',\n        required=False,\n        default=False,\n        action='store_true',\n        help='Flag to remove all the content of the pipeline run(s) folder.'\n    )\n</code></pre>"},{"location":"reference/management/commands/clearpiperun/#vast_pipeline.management.commands.clearpiperun.Command.handle","title":"<code>handle(*args, **options)</code>","text":"<p>Handle function of the command.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**options</code> <p>Variable length options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/clearpiperun.py</code> <pre><code>def handle(self, *args, **options) -&gt; None:\n    \"\"\"\n    Handle function of the command.\n\n    Args:\n        *args: Variable length argument list.\n        **options: Variable length options.\n\n    Returns:\n        None\n    \"\"\"\n    timer = StopWatch()\n\n    if options['verbosity'] &gt; 1:\n        # set root logger to use the DEBUG level\n        root_logger = logging.getLogger('')\n        root_logger.setLevel(logging.DEBUG)\n        # set the traceback on\n        options['traceback'] = True\n\n    if options['keep_parquet'] and options['remove_all']:\n        raise CommandError(\n            '\"--keep-parquets\" flag is incompatible with \"--remove-all\" flag'\n        )\n\n    piperuns = options['piperuns']\n    flag_all_runs = True if 'clearall' in piperuns else False\n\n    if flag_all_runs:\n        logger.info('Clearing all pipeline run in the database')\n        piperuns = list(Run.objects.values_list('name', flat=True))\n\n    for piperun in piperuns:\n        p_run_name = get_p_run_name(piperun)\n        try:\n            p_run = Run.objects.get(name=p_run_name)\n        except Run.DoesNotExist:\n            raise CommandError(f'Pipeline run {p_run_name} does not exist')\n\n        logger.info(\"Deleting pipeline run '%s' from database\", p_run_name)\n        with transaction.atomic():\n            p_run.status = 'DEL'\n            p_run.save()\n\n        timer.reset()\n        delete_pipeline_run_raw_sql(p_run)\n        t = timer.reset()\n        logger.info(\"Time to delete run from database: %.2f sec\", t)\n\n        # remove forced measurements in db if presents\n        forced_parquets = remove_forced_meas(p_run.path)\n        t = timer.reset()\n        logger.info(\"Time to delete forced measurements: %.2f sec\", t)\n\n        # Delete parquet or folder eventually\n        if not options['keep_parquet'] and not options['remove_all']:\n            logger.info('Deleting pipeline \"%s\" parquets', p_run_name)\n            parquets = (\n                glob(os.path.join(p_run.path, '*.parquet'))\n                + glob(os.path.join(p_run.path, '*.arrow'))\n            )\n            for parquet in parquets:\n                try:\n                    os.remove(parquet)\n                except OSError as e:\n                    self.stdout.write(self.style.WARNING(\n                        f'Parquet file \"{os.path.basename(parquet)}\" not existent'\n                    ))\n                    pass\n        t = timer.reset()\n        logger.info(\"Time to delete parquet files: %.2f sec\", t)\n\n        if options['remove_all']:\n            logger.info('Deleting pipeline folder')\n            try:\n                shutil.rmtree(p_run.path)\n            except Exception as e:\n                self.stdout.write(self.style.WARNING(\n                    f'Issues in removing run folder: {e}'\n                ))\n                pass\n</code></pre>"},{"location":"reference/management/commands/createmeasarrow/","title":"createmeasarrow.py","text":"<p>This module defines the command for creating an arrow output file for a previously completed pipeline run.</p>"},{"location":"reference/management/commands/createmeasarrow/#vast_pipeline.management.commands.createmeasarrow.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>BaseCommand</code></p> <p>This command creates measurements and measurement_pairs arrow files for a completed pipeline run.</p> Source code in <code>vast_pipeline/management/commands/createmeasarrow.py</code> <pre><code>class Command(BaseCommand):\n    \"\"\"\n    This command creates measurements and measurement_pairs arrow files for a\n    completed pipeline run.\n    \"\"\"\n    help = (\n        'Create `measurements.arrow` and `measurement_pairs.arrow` files for a'\n        ' completed pipeline run.'\n    )\n\n    def add_arguments(self, parser: ArgumentParser) -&gt; None:\n        \"\"\"\n        Enables arguments for the command.\n\n        Args:\n            parser (ArgumentParser): The parser object of the command.\n\n        Returns:\n            None\n        \"\"\"\n        # positional arguments\n        parser.add_argument(\n            'piperun',\n            type=str,\n            help='Path or name of the pipeline run.'\n        )\n\n        parser.add_argument(\n            '--overwrite',\n            action='store_true',\n            required=False,\n            default=False,\n            help=\"Overwrite previous 'measurements.arrow' file.\",\n        )\n\n    def handle(self, *args, **options) -&gt; None:\n        \"\"\"\n        Handle function of the command.\n\n        Args:\n            *args: Variable length argument list.\n            **options: Variable length options.\n\n        Returns:\n            None\n        \"\"\"\n        piperun = options['piperun']\n\n        p_run_name, run_folder = get_p_run_name(\n            piperun,\n            return_folder=True\n        )\n\n        # configure logging\n        root_logger = logging.getLogger('')\n        f_handler = logging.FileHandler(\n            os.path.join(run_folder, timeStamped('gen_arrow_log.txt')),\n            mode='w'\n        )\n        f_handler.setFormatter(root_logger.handlers[0].formatter)\n        root_logger.addHandler(f_handler)\n\n        if options['verbosity'] &gt; 1:\n            # set root logger to use the DEBUG level\n            root_logger.setLevel(logging.DEBUG)\n            # set the traceback on\n            options['traceback'] = True\n\n        try:\n            p_run: Run = Run.objects.get(name=p_run_name)\n        except Run.DoesNotExist:\n            raise CommandError(f'Pipeline run {p_run_name} does not exist')\n\n        if p_run.status != 'END':\n            raise CommandError(f'Pipeline run {p_run_name} has not completed.')\n\n        measurements_arrow = os.path.join(run_folder, 'measurements.arrow')\n        measurement_pairs_arrow = os.path.join(\n            run_folder, 'measurement_pairs.arrow'\n        )\n\n        if os.path.isfile(measurements_arrow):\n            if options['overwrite']:\n                logger.info(\"Removing previous 'measurements.arrow' file.\")\n                os.remove(measurements_arrow)\n            else:\n                logger.error(\n                    f'Measurements arrow file already exists for {p_run_name}'\n                    ' and `--overwrite` has not been selected.'\n                )\n                raise CommandError(\n                    f'Measurements arrow file already exists for {p_run_name}'\n                    ' and `--overwrite` has not been selected.'\n                )\n\n        if os.path.isfile(measurement_pairs_arrow):\n            if options['overwrite']:\n                logger.info(\n                    \"Removing previous 'measurement_pairs.arrow' file.\"\n                )\n                os.remove(measurement_pairs_arrow)\n            else:\n                logger.error(\n                    'Measurement pairs arrow file already exists for'\n                    f' {p_run_name} and `--overwrite` has not been selected.'\n                )\n                raise CommandError(\n                    'Measurement pairs arrow file already exists for'\n                    f' {p_run_name} and `--overwrite` has not been selected.'\n                )\n\n        logger.info(\"Creating measurements arrow file for '%s'.\", p_run_name)\n\n        create_measurements_arrow_file(p_run)\n\n        if p_run.get_config(validate_inputs=False, prev=True)[\"variability\"][\"pair_metrics\"]:\n            logger.info(\n                \"Creating measurement pairs arrow file for '%s'.\", p_run_name\n            )\n\n            create_measurement_pairs_arrow_file(p_run)\n\n        logger.info(\n            \"Arrow files created successfully for '%s'!\", p_run_name\n        )\n</code></pre>"},{"location":"reference/management/commands/createmeasarrow/#vast_pipeline.management.commands.createmeasarrow.Command.add_arguments","title":"<code>add_arguments(parser)</code>","text":"<p>Enables arguments for the command.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>The parser object of the command.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/createmeasarrow.py</code> <pre><code>def add_arguments(self, parser: ArgumentParser) -&gt; None:\n    \"\"\"\n    Enables arguments for the command.\n\n    Args:\n        parser (ArgumentParser): The parser object of the command.\n\n    Returns:\n        None\n    \"\"\"\n    # positional arguments\n    parser.add_argument(\n        'piperun',\n        type=str,\n        help='Path or name of the pipeline run.'\n    )\n\n    parser.add_argument(\n        '--overwrite',\n        action='store_true',\n        required=False,\n        default=False,\n        help=\"Overwrite previous 'measurements.arrow' file.\",\n    )\n</code></pre>"},{"location":"reference/management/commands/createmeasarrow/#vast_pipeline.management.commands.createmeasarrow.Command.handle","title":"<code>handle(*args, **options)</code>","text":"<p>Handle function of the command.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**options</code> <p>Variable length options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/createmeasarrow.py</code> <pre><code>def handle(self, *args, **options) -&gt; None:\n    \"\"\"\n    Handle function of the command.\n\n    Args:\n        *args: Variable length argument list.\n        **options: Variable length options.\n\n    Returns:\n        None\n    \"\"\"\n    piperun = options['piperun']\n\n    p_run_name, run_folder = get_p_run_name(\n        piperun,\n        return_folder=True\n    )\n\n    # configure logging\n    root_logger = logging.getLogger('')\n    f_handler = logging.FileHandler(\n        os.path.join(run_folder, timeStamped('gen_arrow_log.txt')),\n        mode='w'\n    )\n    f_handler.setFormatter(root_logger.handlers[0].formatter)\n    root_logger.addHandler(f_handler)\n\n    if options['verbosity'] &gt; 1:\n        # set root logger to use the DEBUG level\n        root_logger.setLevel(logging.DEBUG)\n        # set the traceback on\n        options['traceback'] = True\n\n    try:\n        p_run: Run = Run.objects.get(name=p_run_name)\n    except Run.DoesNotExist:\n        raise CommandError(f'Pipeline run {p_run_name} does not exist')\n\n    if p_run.status != 'END':\n        raise CommandError(f'Pipeline run {p_run_name} has not completed.')\n\n    measurements_arrow = os.path.join(run_folder, 'measurements.arrow')\n    measurement_pairs_arrow = os.path.join(\n        run_folder, 'measurement_pairs.arrow'\n    )\n\n    if os.path.isfile(measurements_arrow):\n        if options['overwrite']:\n            logger.info(\"Removing previous 'measurements.arrow' file.\")\n            os.remove(measurements_arrow)\n        else:\n            logger.error(\n                f'Measurements arrow file already exists for {p_run_name}'\n                ' and `--overwrite` has not been selected.'\n            )\n            raise CommandError(\n                f'Measurements arrow file already exists for {p_run_name}'\n                ' and `--overwrite` has not been selected.'\n            )\n\n    if os.path.isfile(measurement_pairs_arrow):\n        if options['overwrite']:\n            logger.info(\n                \"Removing previous 'measurement_pairs.arrow' file.\"\n            )\n            os.remove(measurement_pairs_arrow)\n        else:\n            logger.error(\n                'Measurement pairs arrow file already exists for'\n                f' {p_run_name} and `--overwrite` has not been selected.'\n            )\n            raise CommandError(\n                'Measurement pairs arrow file already exists for'\n                f' {p_run_name} and `--overwrite` has not been selected.'\n            )\n\n    logger.info(\"Creating measurements arrow file for '%s'.\", p_run_name)\n\n    create_measurements_arrow_file(p_run)\n\n    if p_run.get_config(validate_inputs=False, prev=True)[\"variability\"][\"pair_metrics\"]:\n        logger.info(\n            \"Creating measurement pairs arrow file for '%s'.\", p_run_name\n        )\n\n        create_measurement_pairs_arrow_file(p_run)\n\n    logger.info(\n        \"Arrow files created successfully for '%s'!\", p_run_name\n    )\n</code></pre>"},{"location":"reference/management/commands/debugrun/","title":"debugrun.py","text":"<p>This module defines the command for debugging a pipeline run, which prints out statistics and logging.</p>"},{"location":"reference/management/commands/debugrun/#vast_pipeline.management.commands.debugrun.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>BaseCommand</code></p> <p>This script is used to debug data on specific pipeline run(s) or all. Use --help for usage.</p> Source code in <code>vast_pipeline/management/commands/debugrun.py</code> <pre><code>class Command(BaseCommand):\n    \"\"\"\n    This script is used to debug data on specific pipeline run(s) or all.\n    Use --help for usage.\n    \"\"\"\n\n    help = (\n        'Print out total metrics such as nr of measurements for runs'\n    )\n\n    def add_arguments(self, parser: ArgumentParser) -&gt; None:\n        \"\"\"\n        Enables arguments for the command.\n\n        Args:\n            parser (ArgumentParser): The parser object of the command.\n\n        Returns:\n            None\n        \"\"\"\n        # positional arguments (required)\n        parser.add_argument(\n            'piperuns',\n            nargs='+',\n            type=str,\n            help=(\n                'Name or path of pipeline run(s) to debug.Pass \"all\" to'\n                ' print summary data of all the runs.'\n            )\n        )\n\n    def handle(self, *args, **options) -&gt; None:\n        \"\"\"\n        Handle function of the command.\n\n        Args:\n            *args: Variable length argument list.\n            **options: Variable length options.\n\n        Returns:\n            None\n        \"\"\"\n        piperuns = options['piperuns']\n        flag_all_runs = True if 'all' in piperuns else False\n        if flag_all_runs:\n            piperuns = list(Run.objects.values_list('name', flat=True))\n\n        print(' '.join(40 * ['*']))\n        for piperun in piperuns:\n            p_run_name = get_p_run_name(piperun)\n            try:\n                p_run = Run.objects.get(name=p_run_name)\n            except Run.DoesNotExist:\n                raise CommandError(f'Pipeline run {p_run_name} does not exist')\n\n            print(\n                f'Printing summary data of pipeline run \"{p_run.name}\"'\n            )\n            images = list(p_run.image_set.values_list('name', flat=True))\n            print(f'Nr of images: {len(images)}', )\n            print(\n                'Nr of measurements:',\n                Measurement.objects.filter(image__name__in=images).count()\n            )\n            print(\n                'Nr of forced measurements:',\n                (\n                    Measurement.objects.filter(\n                        image__name__in=images,\n                        forced=True\n                    )\n                    .count()\n                )\n            )\n            sources = (\n                Source.objects.filter(run__name=p_run.name)\n                .values_list('id', flat=True)\n            )\n            print('Nr of sources:',len(sources))\n            print(\n                'Nr of association:',\n                Association.objects.filter(source_id__in=sources).count()\n                )\n            print(' '.join(40 * ['*']))\n</code></pre>"},{"location":"reference/management/commands/debugrun/#vast_pipeline.management.commands.debugrun.Command.add_arguments","title":"<code>add_arguments(parser)</code>","text":"<p>Enables arguments for the command.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>The parser object of the command.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/debugrun.py</code> <pre><code>def add_arguments(self, parser: ArgumentParser) -&gt; None:\n    \"\"\"\n    Enables arguments for the command.\n\n    Args:\n        parser (ArgumentParser): The parser object of the command.\n\n    Returns:\n        None\n    \"\"\"\n    # positional arguments (required)\n    parser.add_argument(\n        'piperuns',\n        nargs='+',\n        type=str,\n        help=(\n            'Name or path of pipeline run(s) to debug.Pass \"all\" to'\n            ' print summary data of all the runs.'\n        )\n    )\n</code></pre>"},{"location":"reference/management/commands/debugrun/#vast_pipeline.management.commands.debugrun.Command.handle","title":"<code>handle(*args, **options)</code>","text":"<p>Handle function of the command.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**options</code> <p>Variable length options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/debugrun.py</code> <pre><code>def handle(self, *args, **options) -&gt; None:\n    \"\"\"\n    Handle function of the command.\n\n    Args:\n        *args: Variable length argument list.\n        **options: Variable length options.\n\n    Returns:\n        None\n    \"\"\"\n    piperuns = options['piperuns']\n    flag_all_runs = True if 'all' in piperuns else False\n    if flag_all_runs:\n        piperuns = list(Run.objects.values_list('name', flat=True))\n\n    print(' '.join(40 * ['*']))\n    for piperun in piperuns:\n        p_run_name = get_p_run_name(piperun)\n        try:\n            p_run = Run.objects.get(name=p_run_name)\n        except Run.DoesNotExist:\n            raise CommandError(f'Pipeline run {p_run_name} does not exist')\n\n        print(\n            f'Printing summary data of pipeline run \"{p_run.name}\"'\n        )\n        images = list(p_run.image_set.values_list('name', flat=True))\n        print(f'Nr of images: {len(images)}', )\n        print(\n            'Nr of measurements:',\n            Measurement.objects.filter(image__name__in=images).count()\n        )\n        print(\n            'Nr of forced measurements:',\n            (\n                Measurement.objects.filter(\n                    image__name__in=images,\n                    forced=True\n                )\n                .count()\n            )\n        )\n        sources = (\n            Source.objects.filter(run__name=p_run.name)\n            .values_list('id', flat=True)\n        )\n        print('Nr of sources:',len(sources))\n        print(\n            'Nr of association:',\n            Association.objects.filter(source_id__in=sources).count()\n            )\n        print(' '.join(40 * ['*']))\n</code></pre>"},{"location":"reference/management/commands/ingestimages/","title":"ingestimages.py","text":""},{"location":"reference/management/commands/ingestimages/#vast_pipeline.management.commands.ingestimages.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>BaseCommand</code></p> <p>This script runs the first part of the pipeline only. It ingests a set of images into the database along with their measurements.</p> Source code in <code>vast_pipeline/management/commands/ingestimages.py</code> <pre><code>class Command(BaseCommand):\n    \"\"\"\n    This script runs the first part of the pipeline only. It ingests a set of\n    images into the database along with their measurements.\n    \"\"\"\n    help = (\n        'Ingest/add a set of images to the database'\n    )\n\n    def add_arguments(self, parser: ArgumentParser) -&gt; None:\n        \"\"\"\n        Enables arguments for the command.\n\n        Args:\n            parser (ArgumentParser): The parser object of the command.\n\n        Returns:\n            None\n        \"\"\"\n        parser.add_argument(\n            'image_ingest_config',\n            nargs=1,\n            type=str,\n            help=('Image ingestion configuration filename/path.')\n        )\n\n    def handle(self, *args, **options) -&gt; None:\n        \"\"\"\n        Handle function of the command.\n\n        Args:\n            *args: Variable length argument list.\n            **options: Variable length options.\n\n        Returns:\n            None\n        \"\"\"\n        # configure logging\n        if options['verbosity'] &gt; 1:\n            # set root logger to use the DEBUG level\n            root_logger = logging.getLogger('')\n            root_logger.setLevel(logging.DEBUG)\n            # set the traceback on\n            options['traceback'] = True\n\n        # Create image ingestion configuration object from input file\n        image_config = ImageIngestConfig.from_file(\n            options['image_ingest_config'][0], validate=False\n        )\n\n        # Validate the config\n        try:\n            image_config.validate()\n        except PipelineConfigError as e:\n            raise CommandError(e)\n\n        # Create a dummy Pipeline instance using the given image ingestion configuration options\n        d = _DummyPipeline(image_config)\n\n        # Read, measure and upload the images listed in the image ingestion config\n        make_upload_images(d.img_paths,image_config.image_opts())\n</code></pre>"},{"location":"reference/management/commands/ingestimages/#vast_pipeline.management.commands.ingestimages.Command.add_arguments","title":"<code>add_arguments(parser)</code>","text":"<p>Enables arguments for the command.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>The parser object of the command.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/ingestimages.py</code> <pre><code>def add_arguments(self, parser: ArgumentParser) -&gt; None:\n    \"\"\"\n    Enables arguments for the command.\n\n    Args:\n        parser (ArgumentParser): The parser object of the command.\n\n    Returns:\n        None\n    \"\"\"\n    parser.add_argument(\n        'image_ingest_config',\n        nargs=1,\n        type=str,\n        help=('Image ingestion configuration filename/path.')\n    )\n</code></pre>"},{"location":"reference/management/commands/ingestimages/#vast_pipeline.management.commands.ingestimages.Command.handle","title":"<code>handle(*args, **options)</code>","text":"<p>Handle function of the command.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**options</code> <p>Variable length options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/ingestimages.py</code> <pre><code>def handle(self, *args, **options) -&gt; None:\n    \"\"\"\n    Handle function of the command.\n\n    Args:\n        *args: Variable length argument list.\n        **options: Variable length options.\n\n    Returns:\n        None\n    \"\"\"\n    # configure logging\n    if options['verbosity'] &gt; 1:\n        # set root logger to use the DEBUG level\n        root_logger = logging.getLogger('')\n        root_logger.setLevel(logging.DEBUG)\n        # set the traceback on\n        options['traceback'] = True\n\n    # Create image ingestion configuration object from input file\n    image_config = ImageIngestConfig.from_file(\n        options['image_ingest_config'][0], validate=False\n    )\n\n    # Validate the config\n    try:\n        image_config.validate()\n    except PipelineConfigError as e:\n        raise CommandError(e)\n\n    # Create a dummy Pipeline instance using the given image ingestion configuration options\n    d = _DummyPipeline(image_config)\n\n    # Read, measure and upload the images listed in the image ingestion config\n    make_upload_images(d.img_paths,image_config.image_opts())\n</code></pre>"},{"location":"reference/management/commands/initingest/","title":"initingest.py","text":""},{"location":"reference/management/commands/initingest/#vast_pipeline.management.commands.initingest.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>BaseCommand</code></p> <p>This script creates a template image configuration file for use with image ingestion</p> Source code in <code>vast_pipeline/management/commands/initingest.py</code> <pre><code>class Command(BaseCommand):\n    \"\"\"\n    This script creates a template image configuration file for use with image ingestion\n    \"\"\"\n    help = (\n        'Create a template image ingestion configuration file'\n    )\n\n    def add_arguments(self, parser: ArgumentParser) -&gt; None:\n        \"\"\"\n        Enables arguments for the command.\n\n        Args:\n            parser (ArgumentParser): The parser object of the command.\n\n        Returns:\n            None\n        \"\"\"\n        parser.add_argument(\n            'config_file_name',\n            nargs=1,\n            type=str,\n            help=('Filename to write template ingest configuration to.')\n        )\n\n    def handle(self, *args, **options) -&gt; None:\n        \"\"\"\n        Handle function of the command.\n\n        Args:\n            *args: Variable length argument list.\n            **options: Variable length options.\n\n        Returns:\n            None\n        \"\"\"\n        # configure logging\n        if options['verbosity'] &gt; 1:\n            # set root logger to use the DEBUG level\n            root_logger = logging.getLogger('')\n            root_logger.setLevel(logging.DEBUG)\n            # set the traceback on\n            options['traceback'] = True\n\n        template_str = make_config_template(\n            ImageIngestConfig.TEMPLATE_PATH,\n            **settings.PIPE_RUN_CONFIG_DEFAULTS\n            )\n\n        fname = options['config_file_name'][0]\n\n        # Enforce .yml extension\n        if not fname.endswith('.yaml') and not fname.endswith('.yml'):\n            fname = fname + '.yml'\n\n        print(\"Writing template to: \", fname)\n        with open(fname, 'w') as f:\n            f.write(template_str+\"\\n\")\n</code></pre>"},{"location":"reference/management/commands/initingest/#vast_pipeline.management.commands.initingest.Command.add_arguments","title":"<code>add_arguments(parser)</code>","text":"<p>Enables arguments for the command.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>The parser object of the command.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/initingest.py</code> <pre><code>def add_arguments(self, parser: ArgumentParser) -&gt; None:\n    \"\"\"\n    Enables arguments for the command.\n\n    Args:\n        parser (ArgumentParser): The parser object of the command.\n\n    Returns:\n        None\n    \"\"\"\n    parser.add_argument(\n        'config_file_name',\n        nargs=1,\n        type=str,\n        help=('Filename to write template ingest configuration to.')\n    )\n</code></pre>"},{"location":"reference/management/commands/initingest/#vast_pipeline.management.commands.initingest.Command.handle","title":"<code>handle(*args, **options)</code>","text":"<p>Handle function of the command.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**options</code> <p>Variable length options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/initingest.py</code> <pre><code>def handle(self, *args, **options) -&gt; None:\n    \"\"\"\n    Handle function of the command.\n\n    Args:\n        *args: Variable length argument list.\n        **options: Variable length options.\n\n    Returns:\n        None\n    \"\"\"\n    # configure logging\n    if options['verbosity'] &gt; 1:\n        # set root logger to use the DEBUG level\n        root_logger = logging.getLogger('')\n        root_logger.setLevel(logging.DEBUG)\n        # set the traceback on\n        options['traceback'] = True\n\n    template_str = make_config_template(\n        ImageIngestConfig.TEMPLATE_PATH,\n        **settings.PIPE_RUN_CONFIG_DEFAULTS\n        )\n\n    fname = options['config_file_name'][0]\n\n    # Enforce .yml extension\n    if not fname.endswith('.yaml') and not fname.endswith('.yml'):\n        fname = fname + '.yml'\n\n    print(\"Writing template to: \", fname)\n    with open(fname, 'w') as f:\n        f.write(template_str+\"\\n\")\n</code></pre>"},{"location":"reference/management/commands/initpiperun/","title":"initpiperun.py","text":"<p>Initialises a pipeline run and creates the relevant directories.</p> <p>Usage: ./manage.py initpiperun pipeline_run_name</p>"},{"location":"reference/management/commands/initpiperun/#vast_pipeline.management.commands.initpiperun.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>BaseCommand</code></p> <p>This script initialise the Pipeline Run folder and related config for the pipeline.</p> Source code in <code>vast_pipeline/management/commands/initpiperun.py</code> <pre><code>class Command(BaseCommand):\n    \"\"\"\n    This script initialise the Pipeline Run folder and related config\n    for the pipeline.\n    \"\"\"\n    help = (\n        'Create the pipeline run folder structure to run a pipeline '\n        'instance'\n    )\n\n    def add_arguments(self, parser) -&gt; None:\n        \"\"\"\n        Enables arguments for the command.\n\n        Args:\n            parser (ArgumentParser): The parser object of the command.\n\n        Returns:\n            None\n        \"\"\"\n        # positional arguments\n        parser.add_argument(\n            'runname',\n            type=str,\n            help='Name of the pipeline run.'\n        )\n\n    def handle(self, *args, **options) -&gt; None:\n        \"\"\"\n        Handle function of the command.\n\n        Args:\n            *args: Variable length argument list.\n            **options: Variable length options.\n\n        Returns:\n            None\n        \"\"\"\n        # configure logging\n        if options['verbosity'] &gt; 1:\n            # set root logger to use the DEBUG level\n            root_logger = logging.getLogger('')\n            root_logger.setLevel(logging.DEBUG)\n            # set the traceback on\n            options['traceback'] = True\n\n        try:\n            _ = initialise_run(options['runname'])\n        except Exception as e:\n            raise CommandError(e)\n\n        logger.info((\n            'pipeline run initialisation successful! Please modify the '\n            '\"config.yaml\"'\n        ))\n</code></pre>"},{"location":"reference/management/commands/initpiperun/#vast_pipeline.management.commands.initpiperun.Command.add_arguments","title":"<code>add_arguments(parser)</code>","text":"<p>Enables arguments for the command.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>The parser object of the command.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/initpiperun.py</code> <pre><code>def add_arguments(self, parser) -&gt; None:\n    \"\"\"\n    Enables arguments for the command.\n\n    Args:\n        parser (ArgumentParser): The parser object of the command.\n\n    Returns:\n        None\n    \"\"\"\n    # positional arguments\n    parser.add_argument(\n        'runname',\n        type=str,\n        help='Name of the pipeline run.'\n    )\n</code></pre>"},{"location":"reference/management/commands/initpiperun/#vast_pipeline.management.commands.initpiperun.Command.handle","title":"<code>handle(*args, **options)</code>","text":"<p>Handle function of the command.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**options</code> <p>Variable length options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/initpiperun.py</code> <pre><code>def handle(self, *args, **options) -&gt; None:\n    \"\"\"\n    Handle function of the command.\n\n    Args:\n        *args: Variable length argument list.\n        **options: Variable length options.\n\n    Returns:\n        None\n    \"\"\"\n    # configure logging\n    if options['verbosity'] &gt; 1:\n        # set root logger to use the DEBUG level\n        root_logger = logging.getLogger('')\n        root_logger.setLevel(logging.DEBUG)\n        # set the traceback on\n        options['traceback'] = True\n\n    try:\n        _ = initialise_run(options['runname'])\n    except Exception as e:\n        raise CommandError(e)\n\n    logger.info((\n        'pipeline run initialisation successful! Please modify the '\n        '\"config.yaml\"'\n    ))\n</code></pre>"},{"location":"reference/management/commands/initpiperun/#vast_pipeline.management.commands.initpiperun.initialise_run","title":"<code>initialise_run(run_name, run_description=None, user=None, config=None)</code>","text":"<p>Initialise a pipeline run.</p> <p>Parameters:</p> Name Type Description Default <code>run_name</code> <code>str</code> <p>A unique name for the run.</p> required <code>run_description</code> <code>Optional[str]</code> <p>Description for the run, only used if initialised with the web UI. Defaults to None.</p> <code>None</code> <code>user</code> <code>Optional[User]</code> <p>User that created the run, only used if initialised with the web UI. Defaults to None.</p> <code>None</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of configuration values to pass to the run config template, only used if initialised with the web UI. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>PipelineInitError</code> <p><code>run_name</code> was not unique.</p> <code>PipelineInitError</code> <p>A directory named <code>run_name</code> already exists.</p> <p>Returns:</p> Type Description <code>Run</code> <p>The initialised pipeline Run Django model object.</p> Source code in <code>vast_pipeline/management/commands/initpiperun.py</code> <pre><code>def initialise_run(\n    run_name: str,\n    run_description: Optional[str] = None,\n    user: Optional[User] = None,\n    config: Optional[Dict[str, Any]] = None,\n) -&gt; Run:\n    \"\"\"Initialise a pipeline run.\n\n    Args:\n        run_name (str): A unique name for the run.\n        run_description (Optional[str], optional): Description for the run, only used if\n            initialised with the web UI. Defaults to None.\n        user (Optional[User], optional): User that created the run, only used if\n            initialised with the web UI. Defaults to None.\n        config (Optional[Dict[str, Any]], optional): Dictionary of configuration values\n            to pass to the run config template, only used if initialised with the web UI.\n            Defaults to None.\n\n    Raises:\n        PipelineInitError: `run_name` was not unique.\n        PipelineInitError: A directory named `run_name` already exists.\n\n    Returns:\n        The initialised pipeline Run Django model object.\n    \"\"\"\n    # check for duplicated run name\n    p_run = Run.objects.filter(name__exact=run_name)\n    if p_run:\n        msg = 'Pipeline run name already used. Change name'\n        raise PipelineInitError(msg)\n\n    # create the pipeline run folder\n    run_path = os.path.join(sett.PIPELINE_WORKING_DIR, run_name)\n\n    if os.path.exists(run_path):\n        msg = 'pipeline run path already present!'\n        raise PipelineInitError(msg)\n    else:\n        logger.info('creating pipeline run folder')\n        os.mkdir(run_path)\n\n    # copy default config into the pipeline run folder\n    logger.info('copying default config in pipeline run folder')\n    template_kwargs = config if config else sett.PIPE_RUN_CONFIG_DEFAULTS\n    template_str = make_config_template(\n        PipelineConfig.TEMPLATE_PATH, run_path=run_path, **template_kwargs\n    )\n    with open(os.path.join(run_path, 'config.yaml'), 'w') as fp:\n        fp.write(template_str)\n\n    # create entry in db\n    p_run, _ = get_create_p_run(run_name, run_path, run_description, user)\n\n    return p_run\n</code></pre>"},{"location":"reference/management/commands/restorepiperun/","title":"restorepiperun.py","text":""},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>BaseCommand</code></p> <p>This command is used to restore a pipeline run to the previous verion after add mode has been used. Use --help for usage.</p> Source code in <code>vast_pipeline/management/commands/restorepiperun.py</code> <pre><code>class Command(BaseCommand):\n    \"\"\"\n    This command is used to restore a pipeline run to the previous verion after\n    add mode has been used. Use --help for usage.\n    \"\"\"\n\n    help = (\n        'Restore a pipeline run to the previous person after image add mode'\n        ' has been used.'\n    )\n\n    def add_arguments(self, parser: ArgumentParser) -&gt; None:\n        \"\"\"\n        Enables arguments for the command.\n\n        Args:\n            parser (ArgumentParser): The parser object of the command.\n\n        Returns:\n            None\n        \"\"\"\n        # positional arguments (required)\n        parser.add_argument(\n            'piperun',\n            type=str,\n            default=None,\n            help='Name or path of pipeline run(s) to restore.'\n        )\n        # keyword arguments (optional)\n        parser.add_argument(\n            '--no-confirm',\n            required=False,\n            default=False,\n            action='store_true',\n            help=(\n                'Flag to skip the confirmation stage and proceed to restore'\n                ' the pipeline run.'\n            )\n        )\n\n    def handle(self, *args, **options) -&gt; None:\n        \"\"\"\n        Handle function of the command.\n\n        Args:\n            *args: Variable length argument list.\n            **options: Variable length options.\n\n        Returns:\n            None\n        \"\"\"\n        piperun = options['piperun']\n\n        p_run_name, run_folder = get_p_run_name(piperun, return_folder=True)\n\n        # configure logging\n        root_logger = logging.getLogger('')\n        f_handler = logging.FileHandler(\n            os.path.join(run_folder, timeStamped('restore_log.txt')),\n            mode='w'\n        )\n        f_handler.setFormatter(root_logger.handlers[0].formatter)\n        root_logger.addHandler(f_handler)\n\n        if options['verbosity'] &gt; 1:\n            # set root logger to use the DEBUG level\n            root_logger.setLevel(logging.DEBUG)\n            # set the traceback on\n            options['traceback'] = True\n\n        try:\n            p_run = Run.objects.get(name=p_run_name)\n        except Run.DoesNotExist:\n            raise CommandError(f'Pipeline run {p_run_name} does not exist')\n\n        if p_run.status not in ['END', 'ERR']:\n            raise CommandError(\n                f\"Run {p_run_name} does not have an 'END' or 'ERR' status.\"\n                \" Unable to run restore.\"\n            )\n\n        path = p_run.path\n        pipeline = Pipeline(\n            name=p_run_name,\n            config_path=os.path.join(path, 'config.yaml')\n        )\n        try:\n            # update pipeline run status to restoring\n            prev_status = p_run.status\n            pipeline.set_status(p_run, 'RES')\n\n            prev_config_file = os.path.join(p_run.path, 'config.yaml.bak')\n\n            if os.path.isfile(prev_config_file):\n                shutil.copy(\n                    prev_config_file,\n                    prev_config_file.replace('.yaml.bak', '.bak.yaml')\n                )\n                prev_config_file = prev_config_file.replace(\n                    '.yaml.bak', '.bak.yaml'\n                )\n                prev_config = PipelineConfig.from_file(prev_config_file)\n                os.remove(prev_config_file)\n            else:\n                raise CommandError(\n                    'Previous config file does not exist.'\n                    ' Cannot restore pipeline run.'\n                )\n\n            bak_files = {}\n            for i in [\n                'associations', 'bands', 'images', 'measurement_pairs',\n                'relations', 'skyregions', 'sources', 'config'\n            ]:\n                if i == 'config':\n                    f_name = os.path.join(p_run.path, f'{i}.yaml.bak')\n                else:\n                    f_name = os.path.join(p_run.path, f'{i}.parquet.bak')\n\n                if os.path.isfile(f_name):\n                    bak_files[i] = f_name\n                elif (\n                    i != \"measurement_pairs\"\n                    or pipeline.config[\"variability\"][\"pair_metrics\"]\n                ):\n                    raise CommandError(\n                        f'File {f_name} does not exist.'\n                        ' Cannot restore pipeline run.'\n                    )\n\n            logger_msg = \"Will restore the run to the following config:\\n\"\n            logger.info(logger_msg + prev_config._yaml.as_yaml())\n\n            user_continue = True if options['no_confirm'] else yesno(\"Would you like to restore the run\")\n\n            if user_continue:\n                restore_pipe(p_run, bak_files, prev_config)\n                pipeline.set_status(p_run, 'END')\n                logger.info('Restore complete.')\n            else:\n                pipeline.set_status(p_run, prev_status)\n                logger.info('No actions performed.')\n\n        except Exception as e:\n            logger.error('Restoring failed!')\n            logger.error(e)\n            pipeline.set_status(p_run, prev_status)\n</code></pre>"},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.Command.add_arguments","title":"<code>add_arguments(parser)</code>","text":"<p>Enables arguments for the command.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>The parser object of the command.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/restorepiperun.py</code> <pre><code>def add_arguments(self, parser: ArgumentParser) -&gt; None:\n    \"\"\"\n    Enables arguments for the command.\n\n    Args:\n        parser (ArgumentParser): The parser object of the command.\n\n    Returns:\n        None\n    \"\"\"\n    # positional arguments (required)\n    parser.add_argument(\n        'piperun',\n        type=str,\n        default=None,\n        help='Name or path of pipeline run(s) to restore.'\n    )\n    # keyword arguments (optional)\n    parser.add_argument(\n        '--no-confirm',\n        required=False,\n        default=False,\n        action='store_true',\n        help=(\n            'Flag to skip the confirmation stage and proceed to restore'\n            ' the pipeline run.'\n        )\n    )\n</code></pre>"},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.Command.handle","title":"<code>handle(*args, **options)</code>","text":"<p>Handle function of the command.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**options</code> <p>Variable length options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/restorepiperun.py</code> <pre><code>def handle(self, *args, **options) -&gt; None:\n    \"\"\"\n    Handle function of the command.\n\n    Args:\n        *args: Variable length argument list.\n        **options: Variable length options.\n\n    Returns:\n        None\n    \"\"\"\n    piperun = options['piperun']\n\n    p_run_name, run_folder = get_p_run_name(piperun, return_folder=True)\n\n    # configure logging\n    root_logger = logging.getLogger('')\n    f_handler = logging.FileHandler(\n        os.path.join(run_folder, timeStamped('restore_log.txt')),\n        mode='w'\n    )\n    f_handler.setFormatter(root_logger.handlers[0].formatter)\n    root_logger.addHandler(f_handler)\n\n    if options['verbosity'] &gt; 1:\n        # set root logger to use the DEBUG level\n        root_logger.setLevel(logging.DEBUG)\n        # set the traceback on\n        options['traceback'] = True\n\n    try:\n        p_run = Run.objects.get(name=p_run_name)\n    except Run.DoesNotExist:\n        raise CommandError(f'Pipeline run {p_run_name} does not exist')\n\n    if p_run.status not in ['END', 'ERR']:\n        raise CommandError(\n            f\"Run {p_run_name} does not have an 'END' or 'ERR' status.\"\n            \" Unable to run restore.\"\n        )\n\n    path = p_run.path\n    pipeline = Pipeline(\n        name=p_run_name,\n        config_path=os.path.join(path, 'config.yaml')\n    )\n    try:\n        # update pipeline run status to restoring\n        prev_status = p_run.status\n        pipeline.set_status(p_run, 'RES')\n\n        prev_config_file = os.path.join(p_run.path, 'config.yaml.bak')\n\n        if os.path.isfile(prev_config_file):\n            shutil.copy(\n                prev_config_file,\n                prev_config_file.replace('.yaml.bak', '.bak.yaml')\n            )\n            prev_config_file = prev_config_file.replace(\n                '.yaml.bak', '.bak.yaml'\n            )\n            prev_config = PipelineConfig.from_file(prev_config_file)\n            os.remove(prev_config_file)\n        else:\n            raise CommandError(\n                'Previous config file does not exist.'\n                ' Cannot restore pipeline run.'\n            )\n\n        bak_files = {}\n        for i in [\n            'associations', 'bands', 'images', 'measurement_pairs',\n            'relations', 'skyregions', 'sources', 'config'\n        ]:\n            if i == 'config':\n                f_name = os.path.join(p_run.path, f'{i}.yaml.bak')\n            else:\n                f_name = os.path.join(p_run.path, f'{i}.parquet.bak')\n\n            if os.path.isfile(f_name):\n                bak_files[i] = f_name\n            elif (\n                i != \"measurement_pairs\"\n                or pipeline.config[\"variability\"][\"pair_metrics\"]\n            ):\n                raise CommandError(\n                    f'File {f_name} does not exist.'\n                    ' Cannot restore pipeline run.'\n                )\n\n        logger_msg = \"Will restore the run to the following config:\\n\"\n        logger.info(logger_msg + prev_config._yaml.as_yaml())\n\n        user_continue = True if options['no_confirm'] else yesno(\"Would you like to restore the run\")\n\n        if user_continue:\n            restore_pipe(p_run, bak_files, prev_config)\n            pipeline.set_status(p_run, 'END')\n            logger.info('Restore complete.')\n        else:\n            pipeline.set_status(p_run, prev_status)\n            logger.info('No actions performed.')\n\n    except Exception as e:\n        logger.error('Restoring failed!')\n        logger.error(e)\n        pipeline.set_status(p_run, prev_status)\n</code></pre>"},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.restore_pipe","title":"<code>restore_pipe(p_run, bak_files, prev_config)</code>","text":"<p>Restores the pipeline to the backup files version.</p> <p>Parameters:</p> Name Type Description Default <code>p_run</code> <code>Run</code> <p>The run model object.</p> required <code>bak_files</code> <code>Dict[str, str]</code> <p>Dictionary containing the paths to the .bak files.</p> required <code>prev_config</code> <code>PipelineConfig</code> <p>Back up run configuration.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/restorepiperun.py</code> <pre><code>def restore_pipe(p_run: Run, bak_files: Dict[str, str], prev_config: PipelineConfig) -&gt; None:\n    \"\"\"\n    Restores the pipeline to the backup files version.\n\n    Args:\n        p_run (Run):\n            The run model object.\n        bak_files (Dict[str, str]):\n            Dictionary containing the paths to the .bak files.\n        prev_config (PipelineConfig):\n            Back up run configuration.\n\n    Returns:\n        None\n    \"\"\"\n    # check images match\n    img_f_list = prev_config[\"inputs\"][\"image\"]\n    if isinstance(img_f_list, dict):\n        img_f_list = [\n            item for sublist in img_f_list.values() for item in sublist\n        ]\n    img_f_list = [os.path.basename(i) for i in img_f_list]\n\n    prev_images = pd.read_parquet(\n        bak_files['images'], columns=['id', 'name', 'measurements_path']\n    )\n\n    if sorted(prev_images['name'].tolist()) != sorted(img_f_list):\n        raise CommandError(\n            'Images in previous config file does not'\n            ' match those found in the previous images.parquet.bak.'\n            ' Cannot restore pipeline run.'\n        )\n\n    # check forced measurements\n    monitor = prev_config[\"source_monitoring\"][\"monitor\"]\n    if monitor:\n        forced_parquets = glob(os.path.join(\n            p_run.path, 'forced_*.parquet.bak'\n        ))\n\n        if not forced_parquets:\n            raise CommandError(\n                'source_monitoring.monitor is \\'True\\' in the previous configuration but'\n                ' no .bak forced parquet files have been found.'\n                ' Cannot restore pipeline run.'\n            )\n        else:\n            # load old associations\n            bak_meas_id = pd.read_parquet(\n                bak_files['associations'],\n                columns=['meas_id']\n            )['meas_id'].unique()\n\n            # load backup forced measurements\n            forced_meas = pd.concat(\n                [pd.read_parquet(i, columns=['id']) for i in forced_parquets]\n            )\n\n            # load image meas\n            meas = pd.concat(\n                [pd.read_parquet(\n                    i, columns=['id']\n                ) for i in prev_images['measurements_path']]\n            )\n\n            # Get forced ids from the associations\n            forced_meas_id = bak_meas_id[\n                np.isin(bak_meas_id, meas['id'].to_numpy(), invert=True)\n            ]\n\n            if not np.array_equal(\n                np.sort(forced_meas_id),\n                np.sort(forced_meas['id'].to_numpy())\n            ):\n                raise CommandError(\n                    'The forced measurements .bak files do not match the'\n                    ' previous run.'\n                    ' Cannot restore pipeline run.'\n                )\n\n    logger.info(\"Restoring '%s' from backup parquet files.\", p_run.name)\n\n    # Delete any new sources\n    bak_sources = pd.read_parquet(bak_files['sources'])\n\n    sources_to_delete = (\n        Source.objects\n        .filter(run=p_run)\n        .exclude(id__in=bak_sources.index.to_numpy())\n    )\n\n    if sources_to_delete.exists():\n        with transaction.atomic():\n            n_del, detail_del = sources_to_delete.delete()\n            logger.info(\n                ('Deleting new sources and associated objects to restore run'\n                 ' Total objects deleted: %i'),\n                n_del,\n            )\n            logger.debug('(type, #deleted): %s', detail_del)\n\n    # Delete newly created relations of sources that still exist after deleting\n    # the new sources\n    bak_relations = pd.read_parquet(bak_files['relations'])\n    db_relations = pd.DataFrame(\n        list(RelatedSource.objects.filter(from_source_id__run=p_run).values())\n    )\n\n    diff = pd.merge(\n        db_relations,\n        bak_relations,\n        on=['from_source_id', 'to_source_id'],\n        how='left',\n        indicator='exist'\n    )\n\n    relations_to_drop = diff[diff['exist'] == 'left_only']['id'].to_numpy()\n    relations_to_drop = RelatedSource.objects.filter(id__in=relations_to_drop)\n\n    with transaction.atomic():\n        n_del, detail_del = relations_to_drop.delete()\n        logger.info(\n            ('Deleting left over relations after dropping new sources'\n             ' Total objects deleted: %i'),\n            n_del,\n        )\n        logger.debug('(type, #deleted): %s', detail_del)\n\n    if monitor:\n        current_forced_parquets = glob(os.path.join(\n            p_run.path, 'forced_*.parquet'\n        ))\n\n        current_forced_meas = pd.concat(\n            [pd.read_parquet(\n                i, columns=['id']\n            ) for i in current_forced_parquets]\n        )\n\n        ids_to_delete = current_forced_meas.loc[\n            ~current_forced_meas['id'].isin(forced_meas['id'].to_numpy()),\n            'id'\n        ]\n\n        meas_to_delete = Measurement.objects.filter(id__in=ids_to_delete)\n\n        del ids_to_delete\n\n        if meas_to_delete.exists():\n            with transaction.atomic():\n                n_del, detail_del = meas_to_delete.delete()\n                logger.info(\n                    ('Deleting forced measurement and associated'\n                     ' objects to restore run. Total objects deleted: %i'),\n                    n_del,\n                )\n                logger.debug('(type, #deleted): %s', detail_del)\n\n    # restore source metrics\n    logger.info(f'Restoring metrics for {bak_sources.shape[0]} sources.')\n    bak_sources = update_sources(bak_sources)\n\n    # remove images from run\n    images_to_remove = (\n        Image.objects\n        .filter(run=p_run)\n        .exclude(id__in=prev_images['id'].to_numpy())\n    )\n    logger.info(f'Removing {len(images_to_remove)} images from the run.')\n    if images_to_remove.exists():\n        with transaction.atomic():\n            p_run.image_set.remove(*images_to_remove)\n\n    # load old associations to remove all new assoc\n    bak_assoc = pd.read_parquet(\n        bak_files['associations'],\n        columns=['source_id', 'meas_id']\n    )\n\n    # get unique source and meas id values in the previous run\n    bak_source_ids = bak_assoc['source_id'].unique()\n    bak_meas_ids = bak_assoc['meas_id'].unique()\n\n    # create query to only obtain associations that are not part of the\n    # previous run\n    association_criteria_1 = Q(source_id__in=bak_source_ids)\n    association_criteria_2 = ~Q(meas_id__in=bak_meas_ids)\n    associations_to_delete = Association.objects.filter(\n        association_criteria_1 and association_criteria_2\n    )\n\n    if associations_to_delete.exists():\n        with transaction.atomic():\n            n_del, detail_del = associations_to_delete.delete()\n            logger.info(\n                ('Deleting associations to restore run.'\n                 ' Total objects deleted: %i'),\n                n_del,\n            )\n            logger.debug('(type, #deleted): %s', detail_del)\n\n    logger.info('Restoring run metrics.')\n    p_run.n_images = prev_images.shape[0]\n    p_run.n_sources = bak_sources.shape[0]\n    if monitor:\n        p_run.n_selavy_measurements = meas.shape[0]\n        p_run.n_forced_measurements = forced_meas.shape[0]\n    else:\n        p_run.n_selavy_measurements = bak_meas_ids.shape[0]\n\n    with transaction.atomic():\n        p_run.save()\n\n    # switch files and delete backups\n    logger.info('Restoring parquet files and removing .bak files.')\n    for i in bak_files:\n        bak_file = bak_files[i]\n        if i == 'config':\n            actual_file = bak_file.replace('.yaml.bak', '_prev.yaml')\n        else:\n            actual_file = bak_file.replace('.bak', '')\n        shutil.copy(bak_file, actual_file)\n        os.remove(bak_file)\n\n    if monitor:\n        for i in current_forced_parquets:\n            os.remove(i)\n\n        for i in forced_parquets:\n            new_file = i.replace('.bak', '')\n            shutil.copy(i, new_file)\n            os.remove(i)\n</code></pre>"},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.yesno","title":"<code>yesno(question)</code>","text":"<p>Simple Yes/No Function.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to show to the user for a y/n response.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if user enters 'y', False if 'n'.</p> Source code in <code>vast_pipeline/management/commands/restorepiperun.py</code> <pre><code>def yesno(question: str) -&gt; bool:\n    \"\"\"\n    Simple Yes/No Function.\n\n    Args:\n        question (str):\n            The question to show to the user for a y/n response.\n\n    Returns:\n        True if user enters 'y', False if 'n'.\n    \"\"\"\n    prompt = f'{question} ? (y/n): '\n    ans = input(prompt).strip().lower()\n    if ans not in ['y', 'n']:\n        print(f'{ans} is invalid, please try again...')\n        return yesno(question)\n    if ans == 'y':\n        return True\n    return False\n</code></pre>"},{"location":"reference/management/commands/runpipeline/","title":"runpipeline.py","text":"<p>The main command to launch the processing of a pipeline run.</p> <p>Usage: ./manage.py runpipeline pipeline_run_name</p>"},{"location":"reference/management/commands/runpipeline/#vast_pipeline.management.commands.runpipeline.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>BaseCommand</code></p> <p>This script is used to process images with the ASKAP transient pipeline. Use --help for usage, and refer README.</p> Source code in <code>vast_pipeline/management/commands/runpipeline.py</code> <pre><code>class Command(BaseCommand):\n    \"\"\"\n    This script is used to process images with the ASKAP transient pipeline.\n    Use --help for usage, and refer README.\n    \"\"\"\n    help = 'Process the pipeline for a list of images and Selavy catalogs'\n\n    def add_arguments(self, parser: ArgumentParser) -&gt; None:\n        \"\"\"\n        Enables arguments for the command.\n\n        Args:\n            parser (ArgumentParser): The parser object of the command.\n\n        Returns:\n            None\n        \"\"\"\n        # positional arguments\n        parser.add_argument(\n            'piperun',\n            type=str,\n            help='Path or name of the pipeline run.'\n        )\n\n        parser.add_argument(\n            '--full-rerun',\n            required=False,\n            default=False,\n            action='store_true',\n            help=(\n                'Flag to signify that a full re-run is requested.'\n                ' Old data is completely removed and replaced.')\n        )\n\n    def handle(self, *args, **options) -&gt; None:\n        \"\"\"\n        Handle function of the command.\n\n        Args:\n            *args: Variable length argument list.\n            **options: Variable length options.\n\n        Returns:\n            None\n        \"\"\"\n        p_run_name, run_folder = get_p_run_name(\n            options['piperun'],\n            return_folder=True\n        )\n        # configure logging\n        root_logger = logging.getLogger('')\n        f_handler = logging.FileHandler(\n            os.path.join(run_folder, timeStamped('log.txt')),\n            mode='w'\n        )\n        f_handler.setFormatter(root_logger.handlers[0].formatter)\n        root_logger.addHandler(f_handler)\n\n        if options['verbosity'] &gt; 1:\n            # set root logger to use the DEBUG level\n            root_logger.setLevel(logging.DEBUG)\n            # set the traceback on\n            options['traceback'] = True\n\n        # p_run_name = p_run_path\n        # remove ending / if present\n        if p_run_name[-1] == '/':\n            p_run_name = p_run_name[:-1]\n        # grab only the name from the path\n        p_run_name = p_run_name.split(os.path.sep)[-1]\n\n        debug_flag = True if options['verbosity'] &gt; 1 else False\n\n        _ = run_pipe(\n            p_run_name,\n            path_name=run_folder,\n            debug=debug_flag,\n            full_rerun=options[\"full_rerun\"],\n        )\n\n        self.stdout.write(self.style.SUCCESS('Finished'))\n</code></pre>"},{"location":"reference/management/commands/runpipeline/#vast_pipeline.management.commands.runpipeline.Command.add_arguments","title":"<code>add_arguments(parser)</code>","text":"<p>Enables arguments for the command.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>The parser object of the command.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/runpipeline.py</code> <pre><code>def add_arguments(self, parser: ArgumentParser) -&gt; None:\n    \"\"\"\n    Enables arguments for the command.\n\n    Args:\n        parser (ArgumentParser): The parser object of the command.\n\n    Returns:\n        None\n    \"\"\"\n    # positional arguments\n    parser.add_argument(\n        'piperun',\n        type=str,\n        help='Path or name of the pipeline run.'\n    )\n\n    parser.add_argument(\n        '--full-rerun',\n        required=False,\n        default=False,\n        action='store_true',\n        help=(\n            'Flag to signify that a full re-run is requested.'\n            ' Old data is completely removed and replaced.')\n    )\n</code></pre>"},{"location":"reference/management/commands/runpipeline/#vast_pipeline.management.commands.runpipeline.Command.handle","title":"<code>handle(*args, **options)</code>","text":"<p>Handle function of the command.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**options</code> <p>Variable length options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/management/commands/runpipeline.py</code> <pre><code>def handle(self, *args, **options) -&gt; None:\n    \"\"\"\n    Handle function of the command.\n\n    Args:\n        *args: Variable length argument list.\n        **options: Variable length options.\n\n    Returns:\n        None\n    \"\"\"\n    p_run_name, run_folder = get_p_run_name(\n        options['piperun'],\n        return_folder=True\n    )\n    # configure logging\n    root_logger = logging.getLogger('')\n    f_handler = logging.FileHandler(\n        os.path.join(run_folder, timeStamped('log.txt')),\n        mode='w'\n    )\n    f_handler.setFormatter(root_logger.handlers[0].formatter)\n    root_logger.addHandler(f_handler)\n\n    if options['verbosity'] &gt; 1:\n        # set root logger to use the DEBUG level\n        root_logger.setLevel(logging.DEBUG)\n        # set the traceback on\n        options['traceback'] = True\n\n    # p_run_name = p_run_path\n    # remove ending / if present\n    if p_run_name[-1] == '/':\n        p_run_name = p_run_name[:-1]\n    # grab only the name from the path\n    p_run_name = p_run_name.split(os.path.sep)[-1]\n\n    debug_flag = True if options['verbosity'] &gt; 1 else False\n\n    _ = run_pipe(\n        p_run_name,\n        path_name=run_folder,\n        debug=debug_flag,\n        full_rerun=options[\"full_rerun\"],\n    )\n\n    self.stdout.write(self.style.SUCCESS('Finished'))\n</code></pre>"},{"location":"reference/management/commands/runpipeline/#vast_pipeline.management.commands.runpipeline.run_pipe","title":"<code>run_pipe(name, path_name=None, run_dj_obj=None, cli=True, debug=False, user=None, full_rerun=False, prev_ui_status='END')</code>","text":"<p>Main function to run the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the pipeline run (p_run.name).</p> required <code>path_name</code> <code>Optional[str]</code> <p>The path of the directory of the pipeline run (p_run.path), defaults to None.</p> <code>None</code> <code>run_dj_obj</code> <code>Optional[Run]</code> <p>The Run object of the pipeline run, defaults to None.</p> <code>None</code> <code>cli</code> <code>bool</code> <p>Flag to signify whether the pipeline run has been run via the UI (False), or the command line (True). Defaults to True.</p> <code>True</code> <code>debug</code> <code>bool</code> <p>Flag to signify whether to enable debug verbosity to the logging output. Defaults to False.</p> <code>False</code> <code>user</code> <code>Optional[User]</code> <p>The User of the request if made through the UI. Defaults to None.</p> <code>None</code> <code>full_rerun</code> <code>bool</code> <p>If the run already exists, a complete rerun will be performed which will remove and replace all the previous results.</p> <code>False</code> <code>prev_ui_status</code> <code>str</code> <p>The previous status through the UI. Defaults to 'END'.</p> <code>'END'</code> <p>Returns:</p> Type Description <code>bool</code> <p>Boolean equal to <code>True</code> on a successful completion.</p> <p>Raises:</p> Type Description <code>CommandError</code> <p>Raised if error occurs during processing.</p> <code>PipelineConfigError</code> <p>Raised if an error is found in the pipeline config.</p> Source code in <code>vast_pipeline/management/commands/runpipeline.py</code> <pre><code>def run_pipe(\n    name: str, path_name: Optional[str] = None,\n    run_dj_obj: Optional[Run] = None, cli: bool = True,\n    debug: bool = False, user: Optional[User] = None, full_rerun: bool = False,\n    prev_ui_status: str = 'END'\n) -&gt; bool:\n    '''\n    Main function to run the pipeline.\n\n    Args:\n        name:\n            The name of the pipeline run (p_run.name).\n        path_name:\n            The path of the directory of the pipeline run (p_run.path),\n            defaults to None.\n        run_dj_obj:\n            The Run object of the pipeline run, defaults to None.\n        cli:\n            Flag to signify whether the pipeline run has been run via the UI\n            (False), or the command line (True). Defaults to True.\n        debug:\n            Flag to signify whether to enable debug verbosity to the logging\n            output. Defaults to False.\n        user:\n            The User of the request if made through the UI. Defaults to None.\n        full_rerun:\n            If the run already exists, a complete rerun will be performed which\n            will remove and replace all the previous results.\n        prev_ui_status:\n            The previous status through the UI. Defaults to 'END'.\n\n    Returns:\n        Boolean equal to `True` on a successful completion.\n\n    Raises:\n        CommandError: Raised if error occurs during processing.\n        PipelineConfigError: Raised if an error is found in the pipeline\n            config.\n    '''\n    path = run_dj_obj.path if run_dj_obj else path_name\n    # set up logging for running pipeline from UI\n    if not cli:\n        # set up the logger for the UI job\n        root_logger = logging.getLogger('')\n        if debug:\n            root_logger.setLevel(logging.DEBUG)\n        f_handler = logging.FileHandler(\n            os.path.join(path, timeStamped('log.txt')),\n            mode='w'\n        )\n        f_handler.setFormatter(root_logger.handlers[0].formatter)\n        root_logger.addHandler(f_handler)\n\n    pipeline = Pipeline(\n        name=run_dj_obj.name if run_dj_obj else name,\n        config_path=os.path.join(path, 'config.yaml'),\n        validate_config=False,  # delay validation\n    )\n\n    # Create the pipeline run in DB\n    p_run, flag_exist = get_create_p_run(\n        pipeline.name,\n        pipeline.config[\"run\"][\"path\"],\n    )\n\n    # backup the last successful outputs.\n    # if the run is being run again and the last status is END then the\n    # user is highly likely to be attempting to add images. Making the backups\n    # now from a guaranteed successful run is safer in case of problems\n    # with the config file below that causes an error.\n    if flag_exist:\n        if cli and p_run.status == 'END':\n            backup_parquets(p_run.path)\n        elif not cli and prev_ui_status == 'END':\n            backup_parquets(p_run.path)\n\n    # validate run configuration\n    try:\n        pipeline.config.validate(user=user)\n    except PipelineConfigError as e:\n        if debug:\n            traceback.print_exc()\n        logger.exception('Config error:\\n%s', e)\n        msg = f'Config error:\\n{e}'\n        # If the run is already created (e.g. through UI) then set status to\n        # error\n        pipeline.set_status(p_run, 'ERR')\n        raise CommandError(msg) if cli else PipelineConfigError(msg)\n\n    # clean up pipeline images and forced measurements for re-runs\n    # Scenarios:\n    # A. Complete Re-run: If the job is marked as successful then backup\n    # old parquets and proceed to remove parquets along with forced\n    # extractions from the database.\n    # B. Additional Run on successful run: Backup parquets, remove current\n    # parquets and proceed.\n    # C. Additional Run on errored run: Do not backup parquets, just delete\n    # current.\n    # D. Running on initialised run that errored and is still the init run.\n\n    # Flag on the pipeline object on whether the addition mode is on or off.\n    pipeline.add_mode = False\n    pipeline.previous_parquets = {}\n    prev_config_exists = False\n\n    try:\n        if not flag_exist:\n            # check for and remove any present .parquet (and .arrow) files\n            parquets = (\n                glob.glob(os.path.join(p_run.path, \"*.parquet\"))\n                # TODO Remove arrow when arrow files are no longer needed.\n                + glob.glob(os.path.join(p_run.path, \"*.arrow\"))\n                + glob.glob(os.path.join(p_run.path, \"*.bak\"))\n            )\n            for parquet in parquets:\n                os.remove(parquet)\n\n            # copy across config file at the start\n            logger.debug(\"Copying temp config file.\")\n            create_temp_config_file(p_run.path)\n\n        else:\n            # Check if the status is already running or queued. Exit if this is\n            # the case.\n            if p_run.status in ['RUN', 'RES']:\n                logger.error(\n                    \"The pipeline run requested to process already has a \"\n                    \"running or restoring status! Performing no actions. \"\n                    \"Exiting.\"\n                )\n                return True\n\n            # copy across config file at the start\n            logger.debug(\"Copying temp config file.\")\n            create_temp_config_file(p_run.path)\n\n            # Check if there is a previous run config and back up if so\n            if os.path.isfile(\n                os.path.join(p_run.path, 'config_prev.yaml')\n            ):\n                prev_config_exists = True\n                shutil.copy(\n                    os.path.join(p_run.path, 'config_prev.yaml'),\n                    os.path.join(p_run.path, 'config.yaml.bak')\n                )\n            logger.debug(f'config_prev.yaml exists: {prev_config_exists}')\n\n            # Check for an error status and whether any previous config file\n            # exists - if it doesn't exist it means the run has failed during\n            # the first run. In this case we want to clear anything that has\n            # gone on before so to do that `complete-rerun` mode is activated.\n            if not prev_config_exists:\n                if cli and p_run.status == \"ERR\":\n                    full_rerun = True\n                elif not cli and prev_ui_status == \"ERR\":\n                    full_rerun = True\n            logger.debug(f'Full re-run: {full_rerun}')\n\n            # Check if the run has only been initialised, if so we don't want\n            # to do any previous run checks or cleaning.\n            if p_run.status == 'INI':\n                initial_run = True\n            # check if coming from UI\n            elif cli is False and prev_ui_status == 'INI':\n                initial_run = True\n            else:\n                initial_run = False\n\n            if initial_run is False:\n                parquets = (\n                    glob.glob(os.path.join(p_run.path, \"*.parquet\"))\n                    # TODO Remove arrow when arrow files are no longer needed.\n                    + glob.glob(os.path.join(p_run.path, \"*.arrow\"))\n                )\n\n                if full_rerun:\n                    logger.info(\n                        'Cleaning up pipeline run before re-process data'\n                    )\n                    p_run.image_set.clear()\n\n                    logger.info(\n                        'Cleaning up forced measurements before re-process data'\n                    )\n                    remove_forced_meas(p_run.path)\n\n                    for parquet in parquets:\n                        os.remove(parquet)\n\n                    # remove bak files\n                    bak_files = glob.glob(os.path.join(p_run.path, \"*.bak\"))\n                    if bak_files:\n                        for bf in bak_files:\n                            os.remove(bf)\n\n                    # remove previous config if it exists\n                    if prev_config_exists:\n                        os.remove(os.path.join(p_run.path, 'config_prev.yaml'))\n\n                    # reset epoch_based flag\n                    with transaction.atomic():\n                        p_run.epoch_based = False\n                        p_run.save()\n                else:\n                    # Before parquets are started to be copied and backed up, a\n                    # check is run to see if anything has actually changed in\n                    # the config\n                    config_diff = pipeline.config.check_prev_config_diff()\n                    if config_diff:\n                        logger.info(\n                            \"The config file has either not changed since the\"\n                            \" previous run or other settings have changed such\"\n                            \" that a new or complete re-run should be performed\"\n                            \" instead. Performing no actions. Exiting.\"\n                        )\n                        os.remove(os.path.join(p_run.path, 'config_temp.yaml'))\n                        pipeline.set_status(p_run, 'END')\n\n                        return True\n\n                    if pipeline.config.epoch_based != p_run.epoch_based:\n                        logger.info(\n                            \"The 'epoch based' setting has changed since the\"\n                            \" previous run. A complete re-run is required if\"\n                            \" changing to epoch based mode or vice versa.\"\n                        )\n                        os.remove(os.path.join(p_run.path, 'config_temp.yaml'))\n                        pipeline.set_status(p_run, 'END')\n                        return True\n\n                    pipeline.add_mode = True\n\n                    for i in [\n                        'images', 'associations', 'sources', 'relations',\n                        'measurement_pairs'\n                    ]:\n                        pipeline.previous_parquets[i] = os.path.join(\n                            p_run.path, f'{i}.parquet.bak')\n    except Exception as e:\n        logger.error('Unexpected error occurred in pre-run steps!')\n        pipeline.set_status(p_run, 'ERR')\n        logger.exception('Processing error:\\n%s', e)\n        raise CommandError(f'Processing error:\\n{e}')\n\n    if pipeline.config[\"run\"][\"suppress_astropy_warnings\"]:\n        warnings.simplefilter(\"ignore\", category=AstropyWarning)\n\n    logger.info(\"VAST Pipeline version: %s\", pipeline_version)\n    logger.info(\n        \"Source finder: %s\",\n        pipeline.config[\"measurements\"][\"source_finder\"]\n    )\n    logger.info(\"Using pipeline run '%s'\", pipeline.name)\n    logger.info(\n        \"Source monitoring: %s\",\n        pipeline.config[\"source_monitoring\"][\"monitor\"]\n    )\n\n    # log the list of input data files for posterity\n    inputs = pipeline.config[\"inputs\"]\n    input_image_list = [\n        image\n        for image_list in inputs[\"image\"].values()\n        for image in image_list\n    ]\n    input_selavy_list = [\n        selavy\n        for selavy_list in inputs[\"selavy\"].values()\n        for selavy in selavy_list\n    ]\n    input_noise_list = [\n        noise\n        for noise_list in inputs[\"noise\"].values()\n        for noise in noise_list\n    ]\n    if \"background\" in inputs.keys():\n        input_background_list = [\n            background\n            for background_list in inputs[\"background\"].values()\n            for background in background_list\n        ]\n    else:\n        input_background_list = [\"N/A\", ] * len(input_image_list)\n    for image, selavy, noise, background in zip(\n        input_image_list, input_selavy_list, input_noise_list, input_background_list\n    ):\n        logger.info(\n            \"Matched inputs - image: %s, selavy: %s, noise: %s, background: %s\",\n            image,\n            selavy,\n            noise,\n            background,\n        )\n\n    stopwatch = StopWatch()\n\n    # run the pipeline operations\n    try:\n        # check if max runs number is reached\n        pipeline.check_current_runs()\n        # run the pipeline\n        pipeline.set_status(p_run, 'RUN')\n        pipeline.process_pipeline(p_run)\n        # Create arrow file after success if selected.\n        if pipeline.config[\"measurements\"][\"write_arrow_files\"]:\n            create_measurements_arrow_file(p_run)\n            if pipeline.config[\"variability\"][\"pair_metrics\"]:\n                create_measurement_pairs_arrow_file(p_run)\n    except Exception as e:\n        # set the pipeline status as error\n        pipeline.set_status(p_run, 'ERR')\n        logger.exception('Processing error:\\n%s', e)\n        raise CommandError(f'Processing error:\\n{e}')\n\n    # copy across config file now that it is successful\n    logger.debug(\"Copying and cleaning temp config file.\")\n    shutil.copyfile(\n        os.path.join(p_run.path, 'config_temp.yaml'),\n        os.path.join(p_run.path, 'config_prev.yaml'))\n    os.remove(os.path.join(p_run.path, 'config_temp.yaml'))\n\n    # set the pipeline status as completed\n    pipeline.set_status(p_run, 'END')\n\n    logger.info(\n        'Total pipeline processing time %.2f sec',\n        stopwatch.reset()\n    )\n\n    return True\n</code></pre>"},{"location":"reference/pipeline/association/","title":"association.py","text":"<p>This module contains all the functions required to perform source association.</p>"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.advanced_association","title":"<code>advanced_association(method, sources_df, skyc1_srcs, skyc1, skyc2_srcs, skyc2, dr_limit, bw_max, id_incr_par_assoc=0)</code>","text":"<p>The loop for advanced source association that uses the astropy 'search_around_sky' function (i.e. all matching sources are found). The BMAJ of the image * the user supplied beamwidth limit is the base distance for association. This is followed by calculating the 'de Ruiter' radius.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The advanced association method 'advanced' or 'deruiter'.</p> required <code>sources_df</code> <code>DataFrame</code> <p>The dataframe containing all current measurements along with their association source and relations.</p> required <code>skyc1_srcs</code> <code>DataFrame</code> <p>The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources.</p> required <code>skyc1</code> <code>SkyCoord</code> <p>A SkyCoord object with the weighted average sky positions from skyc1_srcs.</p> required <code>skyc2_srcs</code> <code>DataFrame</code> <p>The same structure as sources_df containing the measurements to be associated.</p> required <code>skyc2</code> <code>SkyCoord</code> <p>A SkyCoord object with the sky positions from skyc2_srcs.</p> required <code>dr_limit</code> <code>float</code> <p>The de Ruiter radius limit to use (applies to de ruiter only).</p> required <code>bw_max</code> <code>float</code> <p>The beamwidth limit to use (applies to de ruiter only).</p> required <code>id_incr_par_assoc</code> <code>int</code> <p>An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The output <code>sources_df</code> containing all input measurements along with the association and relation information.</p> <code>DataFrame</code> <p>The output <code>skyc1_srcs</code> with updated with new sources from the association.</p> Source code in <code>vast_pipeline/pipeline/association.py</code> <pre><code>def advanced_association(\n        method: str,\n        sources_df: pd.DataFrame,\n        skyc1_srcs: pd.DataFrame,\n        skyc1: SkyCoord,\n        skyc2_srcs: pd.DataFrame,\n        skyc2: SkyCoord,\n        dr_limit: float,\n        bw_max: float,\n        id_incr_par_assoc: int = 0\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    '''\n    The loop for advanced source association that uses the astropy\n    'search_around_sky' function (i.e. all matching sources are\n    found). The BMAJ of the image * the user supplied beamwidth\n    limit is the base distance for association. This is followed\n    by calculating the 'de Ruiter' radius.\n\n    Args:\n        method:\n            The advanced association method 'advanced' or 'deruiter'.\n        sources_df:\n            The dataframe containing all current measurements along with their\n            association source and relations.\n        skyc1_srcs:\n            The same structure as sources_df but only has one entry per\n            'source' along with a weighted average sky position of the current\n            assoociated sources.\n        skyc1:\n            A SkyCoord object with the weighted average sky positions from\n            skyc1_srcs.\n        skyc2_srcs:\n            The same structure as sources_df containing the measurements to be\n            associated.\n        skyc2:\n            A SkyCoord object with the sky positions from skyc2_srcs.\n        dr_limit:\n            The de Ruiter radius limit to use (applies to de ruiter only).\n        bw_max:\n            The beamwidth limit to use (applies to de ruiter only).\n        id_incr_par_assoc:\n            An increment value to be applied to source numbering when adding\n            new sources to the associations (applies when parallel and add\n            image are being used). Defaults to 0.\n\n    Returns:\n        The output `sources_df` containing all input measurements along with the\n            association and relation information.\n        The output `skyc1_srcs` with updated with new sources from the\n            association.\n    '''\n    # read the needed sources fields\n    # Step 1: get matches within semimajor axis of image.\n    idx_skyc1, idx_skyc2, d2d, d3d = skyc2.search_around_sky(\n        skyc1, bw_max\n    )\n\n    # Step 2: merge the candidates so the de ruiter can be calculated\n    temp_skyc1_srcs = (\n        skyc1_srcs.loc[idx_skyc1]\n        .reset_index()\n        .rename(columns={'index': 'index_old'})\n    )\n    temp_skyc2_srcs = (\n        skyc2_srcs.loc[idx_skyc2]\n        .reset_index()\n        .rename(columns={'index': 'index_old'})\n    )\n\n    temp_skyc2_srcs['d2d'] = d2d.arcsec\n    temp_srcs = temp_skyc1_srcs.merge(\n        temp_skyc2_srcs,\n        left_index=True,\n        right_index=True,\n        suffixes=('_skyc1', '_skyc2')\n    )\n\n    # drop the double d2d column and keep the d2d_skyc2 as assigned above\n    temp_srcs = (\n        temp_srcs\n        .drop(['d2d_skyc1', 'dr_skyc1', 'dr_skyc2'], axis=1)\n        .rename(columns={'d2d_skyc2': 'd2d'})\n    )\n\n    del temp_skyc1_srcs, temp_skyc2_srcs\n\n    # Step 3: Apply the beamwidth limit\n    temp_srcs = temp_srcs[d2d &lt;= bw_max].copy()\n\n    # Step 4: Calculate and perform De Ruiter radius cut\n    if method == 'deruiter':\n        temp_srcs['dr'] = calc_de_ruiter(temp_srcs)\n        temp_srcs = temp_srcs[temp_srcs['dr'] &lt;= dr_limit]\n    else:\n        temp_srcs['dr'] = 0.\n\n    # Now have the 'good' matches\n    # Step 5: Check for one-to-many, many-to-one and many-to-many\n    # associations. First the many-to-many\n    temp_srcs = many_to_many_advanced(temp_srcs, method)\n\n    # Next one-to-many\n    # Get the sources which are doubled\n    temp_srcs, sources_df = one_to_many_advanced(\n        temp_srcs, sources_df, method, id_incr_par_assoc\n    )\n\n    # Finally many-to-one associations, the opposite of above but we\n    # don't have to create new ids for these so it's much simpler in fact\n    # we don't need to do anything but lets get the number for debugging.\n    temp_srcs = many_to_one_advanced(temp_srcs)\n\n    # Now everything in place to append\n    # First the skyc2 sources with a match.\n    # This is created from the temp_srcs df.\n    # This will take care of the extra skyc2 sources needed.\n    skyc2_srcs_toappend = skyc2_srcs.loc[\n        temp_srcs['index_old_skyc2'].values\n    ].reset_index(drop=True)\n    skyc2_srcs_toappend['source'] = temp_srcs['source_skyc1'].values\n    skyc2_srcs_toappend['related'] = temp_srcs['related_skyc1'].values\n    skyc2_srcs_toappend['d2d'] = temp_srcs['d2d'].values\n    skyc2_srcs_toappend['dr'] = temp_srcs['dr'].values\n\n    # and get the skyc2 sources with no match\n    logger.info(\n        'Updating sources catalogue with new sources...'\n    )\n    new_sources = skyc2_srcs.loc[\n        skyc2_srcs.index.difference(\n            temp_srcs['index_old_skyc2'].values\n        )\n    ].reset_index(drop=True)\n    # update the src numbers for those sources in skyc2 with no match\n    # using the max current src as the start and incrementing by one\n    start_elem = sources_df['source'].values.max() + 1 + id_incr_par_assoc\n    new_sources['source'] = np.arange(\n        start_elem,\n        start_elem + new_sources.shape[0],\n        dtype=int\n    )\n    skyc2_srcs_toappend = pd.concat(\n        [skyc2_srcs_toappend, new_sources],\n        ignore_index=True\n    )\n\n    # and skyc2 is now ready to be concatenated with source_df\n    sources_df = pd.concat(\n        [sources_df, skyc2_srcs_toappend],\n        ignore_index=True\n    ).reset_index(drop=True)\n\n    # update skyc1 and df for next association iteration\n    # calculate average angles for skyc1\n    skyc1_srcs = pd.concat(\n        [skyc1_srcs, new_sources],\n        ignore_index=True\n    ).reset_index(drop=True)\n\n    # also need to append any related sources that created a new\n    # source, we can use the skyc2_srcs_toappend to get these\n    skyc1_srcs = pd.concat(\n        [\n            skyc1_srcs,\n            skyc2_srcs_toappend.loc[\n                ~skyc2_srcs_toappend.source.isin(skyc1_srcs.source)\n            ]\n        ]\n    )\n\n    return sources_df, skyc1_srcs\n</code></pre>"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.association","title":"<code>association(images_df, limit, dr_limit, bw_limit, duplicate_limit, config, add_mode, previous_parquets, done_images_df, id_incr_par_assoc=0, parallel=False)</code>","text":"<p>The main association function that does the common tasks between basic and advanced modes.</p> <p>Parameters:</p> Name Type Description Default <code>images_df</code> <code>DataFrame</code> <p>The input images to be associated.</p> required <code>limit</code> <code>Angle</code> <p>The association limit to use (applies to basic and advanced only).</p> required <code>dr_limit</code> <code>float</code> <p>The de Ruiter radius limit to use (applies to de ruiter only).</p> required <code>bw_limit</code> <code>float</code> <p>The beamwidth limit to use (applies to de ruiter only).</p> required <code>duplicate_limit</code> <code>Angle</code> <p>The limit of separation for which a measurement is considered to be a duplicate (epoch based association).</p> required <code>config</code> <code>PipelineConfig</code> <p>The pipeline configuration object.</p> required <code>add_mode</code> <code>bool</code> <p>Whether the pipeline is currently being run in add image mode.</p> required <code>previous_parquets</code> <code>Dict[str, str]</code> <p>Dictionary containing the paths of the previous successful run parquet files (used in add image mode).</p> required <code>done_images_df</code> <code>DataFrame</code> <p>Datafraame containing the images of the previous successful run (used in add image mode).</p> required <code>id_incr_par_assoc</code> <code>int</code> <p>An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0.</p> <code>0</code> <code>parallel</code> <code>bool</code> <p>Whether parallel association is being used.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The output sources_df containing all input measurements along with the association and relation information.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Raised if association method is not valid.</p> Source code in <code>vast_pipeline/pipeline/association.py</code> <pre><code>def association(\n    images_df: pd.DataFrame,\n    limit: Angle,\n    dr_limit: float,\n    bw_limit: float,\n    duplicate_limit: Angle,\n    config: PipelineConfig,\n    add_mode: bool,\n    previous_parquets: Dict[str, str],\n    done_images_df: pd.DataFrame,\n    id_incr_par_assoc: int = 0,\n    parallel: bool = False\n) -&gt; pd.DataFrame:\n    '''\n    The main association function that does the common tasks between basic\n    and advanced modes.\n\n    Args:\n        images_df:\n            The input images to be associated.\n        limit:\n            The association limit to use (applies to basic and advanced only).\n        dr_limit:\n            The de Ruiter radius limit to use (applies to de ruiter only).\n        bw_limit:\n            The beamwidth limit to use (applies to de ruiter only).\n        duplicate_limit:\n            The limit of separation for which a measurement is considered to\n            be a duplicate (epoch based association).\n        config:\n            The pipeline configuration object.\n        add_mode:\n            Whether the pipeline is currently being run in add image mode.\n        previous_parquets:\n            Dictionary containing the paths of the previous successful run\n            parquet files (used in add image mode).\n        done_images_df:\n            Datafraame containing the images of the previous successful run\n            (used in add image mode).\n        id_incr_par_assoc:\n            An increment value to be applied to source numbering when adding\n            new sources to the associations (applies when parallel and add\n            image are being used). Defaults to 0.\n        parallel:\n            Whether parallel association is being used.\n\n    Returns:\n        The output sources_df containing all input measurements along with the\n            association and relation information.\n\n    Raises:\n        Exception: Raised if association method is not valid.\n    '''\n    timer = StopWatch()\n\n    if parallel:\n        # Skip empty groups that seems to sometimes happen with the\n        # dask groupby\n        if len(images_df) == 0:\n            return images_df\n\n        images_df = (\n            images_df.sort_values(by='image_datetime')\n            .drop('image_datetime', axis=1)\n        )\n\n    if 'skyreg_group' in images_df.columns:\n        skyreg_group = images_df['skyreg_group'].iloc[0]\n        skyreg_tag = \" (sky region group %s)\" % skyreg_group\n    else:\n        skyreg_group = -1\n        skyreg_tag = \"\"\n\n    method = config[\"source_association\"][\"method\"]\n\n    logger.info('Starting association%s.', skyreg_tag)\n    logger.info('Association mode selected: %s.', method)\n\n    unique_epochs = np.sort(images_df['epoch'].unique())\n\n    if add_mode:\n        # Here the skyc1_srcs and sources_df are recreated and the done images\n        # are filtered out.\n        image_mask = images_df['image_name'].isin(done_images_df['name'])\n        images_df_done = images_df[image_mask].copy()\n        sources_df, skyc1_srcs = reconstruct_associtaion_dfs(\n            images_df_done,\n            previous_parquets,\n        )\n        images_df = images_df.loc[~image_mask]\n        if images_df.empty:\n            logger.info(\n                'No new images found, stopping association%s.', skyreg_tag\n            )\n            sources_df['interim_ew'] = (\n                sources_df['ra_source'].values * sources_df['weight_ew'].values\n            )\n            sources_df['interim_ns'] = (\n                sources_df['dec_source'].values * sources_df['weight_ns'].values\n            )\n            return (\n                sources_df\n                .drop(['ra', 'dec'], axis=1)\n                .rename(columns={'ra_source': 'ra', 'dec_source': 'dec'})\n            )\n        logger.info(\n            f'Found {images_df.shape[0]} images to add to the run{skyreg_tag}.')\n        # re-get the unique epochs\n        unique_epochs = np.sort(images_df['epoch'].unique())\n        start_epoch = 0\n\n    else:\n        # Do full set up for a new run.\n        first_images = (\n            images_df\n            .loc[images_df['epoch'] == unique_epochs[0], 'image_dj']\n            .to_list()\n        )\n\n        # initialise sky source dataframe\n        skyc1_srcs = prep_skysrc_df(\n            first_images,\n            config[\"measurements\"][\"flux_fractional_error\"],\n            duplicate_limit,\n            ini_df=True\n        )\n        skyc1_srcs['epoch'] = unique_epochs[0]\n        # create base catalogue\n        # initialise the sources dataframe using first image as base\n        sources_df = skyc1_srcs.copy()\n        start_epoch = 1\n\n    if unique_epochs.shape[0] == 1 and not add_mode:\n        # This means only one image is present - or one group of images (epoch\n        # mode) - so the same approach as above in add mode where there are no\n        # images to be added, the interim needs to be calculated and skyc1_srcs\n        # can just be returned as sources_df. ra_source and dec_source can just\n        # be dropped as the ra and dec are already the average values.\n        logger.warning(\n            'No images to associate with!%s.', skyreg_tag\n        )\n        logger.info(\n            'Returning base sources only%s.', skyreg_tag\n        )\n        # reorder the columns to match Dask expectations (parallel)\n        skyc1_srcs = skyc1_srcs[[\n            'id', 'uncertainty_ew', 'weight_ew', 'uncertainty_ns', 'weight_ns',\n            'flux_int', 'flux_int_err', 'flux_int_isl_ratio', 'flux_peak',\n            'flux_peak_err', 'flux_peak_isl_ratio', 'forced', 'compactness',\n            'has_siblings', 'snr', 'image', 'datetime', 'source', 'ra', 'dec',\n            'ra_source', 'dec_source', 'd2d', 'dr', 'related', 'epoch',\n        ]]\n        skyc1_srcs['interim_ew'] = (\n            skyc1_srcs['ra'].values * skyc1_srcs['weight_ew'].values\n        )\n        skyc1_srcs['interim_ns'] = (\n            skyc1_srcs['dec'].values * skyc1_srcs['weight_ns'].values\n        )\n\n        return skyc1_srcs.drop(['ra_source', 'dec_source'], axis=1)\n\n    skyc1 = SkyCoord(\n        skyc1_srcs['ra'].values,\n        skyc1_srcs['dec'].values,\n        unit=(u.deg, u.deg)\n    )\n\n    for it, epoch in enumerate(unique_epochs[start_epoch:]):\n        logger.info('Association iteration: #%i%s', it + 1, skyreg_tag)\n        # load skyc2 source measurements and create SkyCoord\n        images = (\n            images_df.loc[images_df['epoch'] == epoch, 'image_dj'].to_list()\n        )\n        max_beam_maj = (\n            images_df.loc[images_df['epoch'] == epoch, 'image_dj']\n            .apply(lambda x: x.beam_bmaj)\n            .max()\n        )\n        skyc2_srcs = prep_skysrc_df(\n            images,\n            config[\"measurements\"][\"flux_fractional_error\"],\n            duplicate_limit\n        )\n\n        skyc2_srcs['epoch'] = epoch\n        skyc2 = SkyCoord(\n            skyc2_srcs['ra'].values,\n            skyc2_srcs['dec'].values,\n            unit=(u.deg, u.deg)\n        )\n\n        if method == 'basic':\n            sources_df, skyc1_srcs = basic_association(\n                sources_df,\n                skyc1_srcs,\n                skyc1,\n                skyc2_srcs,\n                skyc2,\n                limit,\n                id_incr_par_assoc\n            )\n\n        elif method in ['advanced', 'deruiter']:\n            if method == 'deruiter':\n                bw_max = Angle(\n                    bw_limit * (max_beam_maj * 3600. / 2.) * u.arcsec\n                )\n            else:\n                bw_max = limit\n            sources_df, skyc1_srcs = advanced_association(\n                method,\n                sources_df,\n                skyc1_srcs,\n                skyc1,\n                skyc2_srcs,\n                skyc2,\n                dr_limit,\n                bw_max,\n                id_incr_par_assoc\n            )\n        else:\n            raise Exception('association method not implemented!')\n\n        logger.info(\n            'Calculating weighted average RA and Dec for sources%s...',\n            skyreg_tag\n        )\n\n        # account for RA wrapping\n        ra_wrap_mask = sources_df.ra &lt;= 0.1\n        sources_df['ra_wrap'] = sources_df.ra.values\n        sources_df.loc[\n            ra_wrap_mask, 'ra_wrap'\n        ] = sources_df[ra_wrap_mask].ra.values + 360.\n\n        sources_df['interim_ew'] = (\n            sources_df['ra_wrap'].values * sources_df['weight_ew'].values\n        )\n        sources_df['interim_ns'] = (\n            sources_df['dec'].values * sources_df['weight_ns'].values\n        )\n\n        sources_df = sources_df.drop(['ra_wrap'], axis=1)\n\n        tmp_srcs_df = (\n            sources_df.loc[\n                (sources_df['source'] != -1) &amp; (sources_df['forced'] == False),\n                [\n                    'ra', 'dec', 'uncertainty_ew', 'uncertainty_ns',\n                    'source', 'interim_ew', 'interim_ns', 'weight_ew',\n                    'weight_ns'\n                ]\n            ]\n            .groupby('source')\n        )\n\n        stats = StopWatch()\n\n        wm_ra = tmp_srcs_df['interim_ew'].sum() / tmp_srcs_df['weight_ew'].sum()\n        wm_uncertainty_ew = 1. / np.sqrt(tmp_srcs_df['weight_ew'].sum())\n\n        wm_dec = tmp_srcs_df['interim_ns'].sum() / tmp_srcs_df['weight_ns'].sum()\n        wm_uncertainty_ns = 1. / np.sqrt(tmp_srcs_df['weight_ns'].sum())\n\n        weighted_df = (\n            pd.concat(\n                [wm_ra, wm_uncertainty_ew, wm_dec, wm_uncertainty_ns],\n                axis=1,\n                sort=False\n            )\n            .reset_index()\n            .rename(\n                columns={\n                    0: 'ra',\n                    'weight_ew': 'uncertainty_ew',\n                    1: 'dec',\n                    'weight_ns': 'uncertainty_ns'\n            })\n        )\n\n        # correct the RA wrapping\n        ra_wrap_mask = weighted_df.ra &gt;= 360.\n        weighted_df.loc[\n            ra_wrap_mask, 'ra'\n        ] = weighted_df[ra_wrap_mask].ra.values - 360.\n\n        logger.debug('Groupby concat time %f', stats.reset())\n\n        logger.info(\n            'Finalising base sources catalogue ready for next iteration%s...',\n            skyreg_tag\n        )\n\n        # merge the weighted ra and dec and replace the values\n        skyc1_srcs = skyc1_srcs.merge(\n            weighted_df,\n            on='source',\n            how='left',\n            suffixes=('', '_skyc2')\n        )\n        del tmp_srcs_df, weighted_df\n        skyc1_srcs['ra'] = skyc1_srcs['ra_skyc2']\n        skyc1_srcs['dec'] = skyc1_srcs['dec_skyc2']\n        skyc1_srcs['uncertainty_ew'] = skyc1_srcs['uncertainty_ew_skyc2']\n        skyc1_srcs['uncertainty_ns'] = skyc1_srcs['uncertainty_ns_skyc2']\n        skyc1_srcs = skyc1_srcs.drop(\n            [\n                'ra_skyc2',\n                'dec_skyc2',\n                'uncertainty_ew_skyc2',\n                'uncertainty_ns_skyc2'\n            ], axis=1\n        )\n\n        # generate new sky coord ready for next iteration\n        skyc1 = SkyCoord(\n            skyc1_srcs['ra'].values,\n            skyc1_srcs['dec'].values,\n            unit=(u.deg, u.deg)\n        )\n\n        # and update relations in skyc1\n        skyc1_srcs = skyc1_srcs.drop('related', axis=1)\n        relations_unique = pd.DataFrame(\n            sources_df[sources_df['related'].notna()]\n            .explode('related')\n            .groupby('source')['related']\n            .apply(lambda x: x.unique().tolist())\n        )\n\n        skyc1_srcs = skyc1_srcs.merge(\n            relations_unique, how='left', left_on='source', right_index=True\n        )\n\n        logger.info(\n            'Association iteration #%i complete%s.', it + 1, skyreg_tag\n        )\n\n    # End of iteration over images, ra and dec columns are actually the\n    # average over each iteration so remove ave ra and ave dec used for\n    # calculation and use ra_source and dec_source columns\n    sources_df = (\n        sources_df.drop(['ra', 'dec'], axis=1)\n        .rename(columns={'ra_source': 'ra', 'dec_source': 'dec'})\n    )\n\n    del skyc1_srcs, skyc2_srcs\n\n    logger.info(\n        'Total association time: %.2f seconds%s.',\n        timer.reset_init(),\n        skyreg_tag\n    )\n\n    return sources_df\n</code></pre>"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.basic_association","title":"<code>basic_association(sources_df, skyc1_srcs, skyc1, skyc2_srcs, skyc2, limit, id_incr_par_assoc=0)</code>","text":"<p>The loop for basic source association that uses the astropy 'match_to_catalog_sky' function (i.e. only the nearest match between the catalogs). A direct on sky separation is used to define the association.</p> <p>Parameters:</p> Name Type Description Default <code>sources_df</code> <code>DataFrame</code> <p>The dataframe containing all current measurements along with their association source and relations.</p> required <code>skyc1_srcs</code> <code>DataFrame</code> <p>The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources.</p> required <code>skyc1</code> <code>SkyCoord</code> <p>A SkyCoord object with the weighted average sky positions from skyc1_srcs.</p> required <code>skyc2_srcs</code> <code>DataFrame</code> <p>The same structure as sources_df containing the measurements to be associated.</p> required <code>skyc2</code> <code>SkyCoord</code> <p>A SkyCoord object with the sky positions from skyc2_srcs.</p> required <code>limit</code> <code>Angle</code> <p>The association limit to use (applies to basic and advanced only).</p> required <code>id_incr_par_assoc</code> <code>int</code> <p>An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The output <code>sources_df</code> containing all input measurements along with the association and relation information.</p> <code>DataFrame</code> <p>The output <code>skyc1_srcs</code> with updated with new sources from the association.</p> Source code in <code>vast_pipeline/pipeline/association.py</code> <pre><code>def basic_association(\n        sources_df: pd.DataFrame,\n        skyc1_srcs: pd.DataFrame,\n        skyc1: SkyCoord,\n        skyc2_srcs: pd.DataFrame,\n        skyc2: SkyCoord,\n        limit: Angle,\n        id_incr_par_assoc: int = 0\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    '''\n    The loop for basic source association that uses the astropy\n    'match_to_catalog_sky' function (i.e. only the nearest match between\n    the catalogs). A direct on sky separation is used to define the association.\n\n    Args:\n        sources_df:\n            The dataframe containing all current measurements along with their\n            association source and relations.\n        skyc1_srcs:\n            The same structure as sources_df but only has one entry per\n            'source' along with a weighted average sky position of the current\n            assoociated sources.\n        skyc1:\n            A SkyCoord object with the weighted average sky positions from\n            skyc1_srcs.\n        skyc2_srcs:\n            The same structure as sources_df containing the measurements to be\n            associated.\n        skyc2:\n            A SkyCoord object with the sky positions from skyc2_srcs.\n        limit:\n            The association limit to use (applies to basic and advanced only).\n        id_incr_par_assoc:\n            An increment value to be applied to source numbering when adding\n            new sources to the associations (applies when parallel and add\n            image are being used). Defaults to 0.\n\n    Returns:\n        The output `sources_df` containing all input measurements along with the\n            association and relation information.\n        The output `skyc1_srcs` with updated with new sources from the\n            association.\n    '''\n    # match the new sources to the base\n    # idx gives the index of the closest match in the base for skyc2\n    idx, d2d, d3d = skyc2.match_to_catalog_sky(skyc1)\n    # acceptable selection\n    sel = d2d &lt;= limit\n\n    # The good matches can be assinged the src id from base\n    skyc2_srcs.loc[sel, 'source'] = skyc1_srcs.loc[idx[sel], 'source'].values\n    # Need the d2d to make analysing doubles easier.\n    skyc2_srcs.loc[sel, 'd2d'] = d2d[sel].arcsec\n\n    # must check for double matches in the acceptable matches just made\n    # this would mean that multiple sources in skyc2 have been matched\n    # to the same base source we want to keep closest match and move\n    # the other match(es) back to having a -1 src id\n    skyc2_srcs, sources_df = one_to_many_basic(\n        skyc2_srcs, sources_df, id_incr_par_assoc)\n\n    logger.info('Updating sources catalogue with new sources...')\n    # update the src numbers for those sources in skyc2 with no match\n    # using the max current src as the start and incrementing by one\n    start_elem = sources_df['source'].values.max() + 1 + id_incr_par_assoc\n    nan_sel = (skyc2_srcs['source'] == -1).values\n    skyc2_srcs.loc[nan_sel, 'source'] = (\n        np.arange(\n            start_elem,\n            start_elem + skyc2_srcs.loc[nan_sel].shape[0],\n            dtype=int\n        )\n    )\n\n    # and skyc2 is now ready to be concatenated with the new sources\n    sources_df = pd.concat(\n        [sources_df, skyc2_srcs],\n        ignore_index=True\n    ).reset_index(drop=True)\n\n    # and update skyc1 with the sources that were created from the one\n    # to many relations and any new sources.\n    skyc1_srcs = pd.concat(\n        [\n            skyc1_srcs,\n            skyc2_srcs[\n                ~skyc2_srcs['source'].isin(skyc1_srcs['source'])\n            ]\n        ],\n        ignore_index=True\n    ).reset_index(drop=True)\n\n    return sources_df, skyc1_srcs\n</code></pre>"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.calc_de_ruiter","title":"<code>calc_de_ruiter(df)</code>","text":"<p>Calculates the unitless 'de Ruiter' radius of the association. Works on the 'temp_df' dataframe of the advanced association, where the two sources associated with each other have been merged into one row.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The 'temp_df' from advanced association. It must contain the columns <code>ra_skyc1</code>, 'ra_skyc2', 'uncertainty_ew_skyc1', 'uncertainty_ew_skyc2', 'dec_skyc1', 'dec_skyc2', 'uncertainty_ns_skyc1' and 'uncertainty_ns_skyc2'.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array containing the de Ruiter radius for all rows in the df.</p> Source code in <code>vast_pipeline/pipeline/association.py</code> <pre><code>def calc_de_ruiter(df: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"\n    Calculates the unitless 'de Ruiter' radius of the\n    association. Works on the 'temp_df' dataframe of the\n    advanced association, where the two sources associated\n    with each other have been merged into one row.\n\n    Args:\n        df:\n            The 'temp_df' from advanced association. It must\n            contain the columns `ra_skyc1`, 'ra_skyc2', 'uncertainty_ew_skyc1',\n            'uncertainty_ew_skyc2', 'dec_skyc1', 'dec_skyc2',\n            'uncertainty_ns_skyc1' and 'uncertainty_ns_skyc2'.\n\n    Returns:\n        Array containing the de Ruiter radius for all rows in the df.\n    \"\"\"\n    ra_1 = df['ra_skyc1'].values\n    ra_2 = df['ra_skyc2'].values\n\n    # avoid wrapping issues\n    ra_1[ra_1 &gt; 270.] -= 180.\n    ra_2[ra_2 &gt; 270.] -= 180.\n    ra_1[ra_1 &lt; 90.] += 180.\n    ra_2[ra_2 &lt; 90.] += 180.\n\n    ra_1 = np.deg2rad(ra_1)\n    ra_2 = np.deg2rad(ra_2)\n\n    ra_1_err = np.deg2rad(df['uncertainty_ew_skyc1'].values)\n    ra_2_err = np.deg2rad(df['uncertainty_ew_skyc2'].values)\n\n    dec_1 = np.deg2rad(df['dec_skyc1'].values)\n    dec_2 = np.deg2rad(df['dec_skyc2'].values)\n\n    dec_1_err = np.deg2rad(df['uncertainty_ns_skyc1'].values)\n    dec_2_err = np.deg2rad(df['uncertainty_ns_skyc2'].values)\n\n    dr1 = (ra_1 - ra_2) * (ra_1 - ra_2)\n    dr1_1 = np.cos((dec_1 + dec_2) / 2.)\n    dr1 *= dr1_1 * dr1_1\n    dr1 /= ra_1_err * ra_1_err + ra_2_err * ra_2_err\n\n    dr2 = (dec_1 - dec_2) * (dec_1 - dec_2)\n    dr2 /= dec_1_err * dec_1_err + dec_2_err * dec_2_err\n\n    dr = np.sqrt(dr1 + dr2)\n\n    return dr\n</code></pre>"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.many_to_many_advanced","title":"<code>many_to_many_advanced(temp_srcs, method)</code>","text":"<p>Finds and processes the many-to-many associations in the advanced association. We do not want to build many-to-many associations as this will make the database get very large (see TraP documentation). The skyc2 sources which are listed more than once are found, and of these, those which have a skyc1 source association which is also listed twice in the associations are selected. The closest (by limit or de Ruiter radius, depending on the method) is kept where as the other associations are dropped.</p> <p>This follows the same logic used by the TraP (see TraP documentation).</p> <p>Parameters:</p> Name Type Description Default <code>temp_srcs</code> <code>DataFrame</code> <p>The temporary associtation dataframe used through the advanced association process.</p> required <code>method</code> <code>str</code> <p>Can be either 'advanced' or 'deruiter' to represent the advanced association method being used.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Updated temp_srcs with the many_to_many relations dropped.</p> Source code in <code>vast_pipeline/pipeline/association.py</code> <pre><code>def many_to_many_advanced(temp_srcs: pd.DataFrame, method: str) -&gt; pd.DataFrame:\n    '''\n    Finds and processes the many-to-many associations in the advanced\n    association. We do not want to build many-to-many associations as\n    this will make the database get very large (see TraP documentation).\n    The skyc2 sources which are listed more than once are found, and of\n    these, those which have a skyc1 source association which is also\n    listed twice in the associations are selected. The closest (by\n    limit or de Ruiter radius, depending on the method) is kept where\n    as the other associations are dropped.\n\n    This follows the same logic used by the TraP (see TraP documentation).\n\n    Args:\n        temp_srcs:\n            The temporary associtation dataframe used through the advanced\n            association process.\n        method:\n            Can be either 'advanced' or 'deruiter' to represent the advanced\n            association method being used.\n\n    Returns:\n        Updated temp_srcs with the many_to_many relations dropped.\n    '''\n    # Select those where the extracted source is listed more than once\n    # (e.g. index_old_skyc2 duplicated values) and of these get those that\n    # have a source id that is listed more than once (e.g. source_skyc1\n    # duplicated values) in the temps_srcs df\n    m_to_m = temp_srcs[(\n        temp_srcs['index_old_skyc2'].duplicated(keep=False) &amp;\n        temp_srcs['source_skyc1'].duplicated(keep=False)\n    )].copy()\n\n    if m_to_m.empty:\n        logger.debug('No many-to-many assocations.')\n        return temp_srcs\n\n    logger.debug(\n        'Detected #%i many-to-many assocations, cleaning...',\n        m_to_m.shape[0]\n    )\n\n    dist_col = 'd2d' if method == 'advanced' else 'dr'\n    min_col = 'min_' + dist_col\n\n    # get the minimum de ruiter value for each extracted source\n    m_to_m[min_col] = (\n        m_to_m.groupby('index_old_skyc2')[dist_col]\n        .transform('min')\n    )\n    # get the ids of those crossmatches that are larger than the minimum\n    m_to_m_to_drop = m_to_m[m_to_m[dist_col] != m_to_m[min_col]].index.values\n    # and drop these from the temp_srcs\n    temp_srcs = temp_srcs.drop(m_to_m_to_drop)\n\n    return temp_srcs\n</code></pre>"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.many_to_one_advanced","title":"<code>many_to_one_advanced(temp_srcs)</code>","text":"<p>Finds and processes the many-to-one associations in the advanced association. In this case in the related column of the 'many' sources we need to append the ids of all the other 'many' (expect for itself).</p> <p>Parameters:</p> Name Type Description Default <code>temp_srcs</code> <code>DataFrame</code> <p>The temporary association dataframe used through the advanced association process.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Updated temp_srcs with all many_to_one relation information added.</p> Source code in <code>vast_pipeline/pipeline/association.py</code> <pre><code>def many_to_one_advanced(temp_srcs: pd.DataFrame) -&gt; pd.DataFrame:\n    '''\n    Finds and processes the many-to-one associations in the advanced\n    association. In this case in the related column of the 'many' sources\n    we need to append the ids of all the other 'many' (expect for itself).\n\n    Args:\n        temp_srcs:\n            The temporary association dataframe used through the advanced\n            association process.\n\n    Returns:\n        Updated temp_srcs with all many_to_one relation information added.\n    '''\n    # use only these columns for easy debugging of the dataframe\n    cols = [\n        'index_old_skyc1', 'id_skyc1', 'source_skyc1',\n        'related_skyc1', 'index_old_skyc2', 'id_skyc2', 'source_skyc2',\n        'd2d', 'dr'\n    ]\n\n    # select those sources which have been matched to the same measurement\n    # in the sky catalogue 2.\n    duplicated_skyc2 = temp_srcs.loc[\n        temp_srcs['index_old_skyc2'].duplicated(keep=False),\n        cols\n    ]\n\n    # duplicated_skyc2\n    # +-----+-------------------+------------+----------------+\n    # |     |   index_old_skyc1 |   id_skyc1 |   source_skyc1 |\n    # |-----+-------------------+------------+----------------+\n    # | 447 |               477 |        478 |            478 |\n    # | 448 |               478 |        479 |            479 |\n    # | 477 |               507 |        508 |            508 |\n    # | 478 |               508 |        509 |            509 |\n    # | 695 |               738 |        739 |            739 |\n    # +-----+-------------------+------------+----------------+\n    # +-----------------+-------------------+------------+----------------+\n    # | related_skyc1   |   index_old_skyc2 |   id_skyc2 |   source_skyc2 |\n    # +-----------------+-------------------+------------+----------------+\n    # |                 |               305 |       5847 |             -1 |\n    # |                 |               305 |       5847 |             -1 |\n    # |                 |               648 |       6190 |             -1 |\n    # |                 |               648 |       6190 |             -1 |\n    # |                 |               561 |       6103 |             -1 |\n    # +-----------------+-------------------+------------+----------------+\n    # -------------+------+\n    #          d2d |   dr |\n    # -------------+------|\n    #      8.63598 |    0 |\n    #      8.63598 |    0 |\n    #      6.5777  |    0 |\n    #      6.5777  |    0 |\n    #      7.76527 |    0 |\n    # -------------+------+\n\n    # if there are none no action is required.\n    if duplicated_skyc2.empty:\n        logger.debug('No many-to-one associations.')\n        return temp_srcs\n\n    logger.debug(\n        'Detected #%i many-to-one associations',\n        duplicated_skyc2.shape[0]\n    )\n\n    # The new relations become that for each 'many' source we need to append\n    # the ids of the other 'many' sources that have been associationed with the\n    # 'one'. Below for each 'one' group we gather all the ids of the many\n    # sources.\n    new_relations = pd.DataFrame(\n        duplicated_skyc2\n        .groupby('index_old_skyc2')\n        .apply(lambda grp: grp['source_skyc1'].tolist())\n    ).rename(columns={0: 'new_relations'})\n\n    # new_relations\n    # +-------------------+-----------------+\n    # |   index_old_skyc2 | new_relations   |\n    # |-------------------+-----------------|\n    # |               305 | [478, 479]      |\n    # |               561 | [739, 740]      |\n    # |               648 | [508, 509]      |\n    # |               764 | [841, 842]      |\n    # |               816 | [1213, 1215]    |\n    # +-------------------+-----------------+\n\n    # these new relations are then added to the duplciated dataframe so\n    # they can easily be used by the next function.\n    duplicated_skyc2 = duplicated_skyc2.merge(\n        new_relations,\n        left_on='index_old_skyc2',\n        right_index=True,\n        how='left'\n    )\n\n    # Remove the 'self' relations. The 'x['source_skyc1']' is an integer so it\n    # is placed within a list notation, [], to be able to be easily subtracted\n    # from the new_relations.\n    duplicated_skyc2['new_relations'] = (\n        duplicated_skyc2.apply(\n            lambda x: list(set(x['new_relations']) - set([x['source_skyc1']])),\n            axis=1\n        )\n    )\n\n    # Use the 'add_new_many_to_one_relations' method to add tthe new relatitons\n    # to the actual `related_skyc1' column.\n    duplicated_skyc2['related_skyc1'] = (\n        duplicated_skyc2.apply(\n            add_new_many_to_one_relations,\n            axis=1\n        )\n    )\n\n    # Transfer the new relations from the duplicated df to the temp_srcs. The\n    # index is explicitly declared to avoid any mixups.\n    temp_srcs.loc[\n        duplicated_skyc2.index.values, 'related_skyc1'\n    ] = duplicated_skyc2.loc[\n        duplicated_skyc2.index.values, 'related_skyc1'\n    ].values\n\n    return temp_srcs\n</code></pre>"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.one_to_many_advanced","title":"<code>one_to_many_advanced(temp_srcs, sources_df, method, id_incr_par_assoc=0)</code>","text":"<p>Finds and processes the one-to-many associations in the advanced association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked.</p> <p>This is needed to be separate from the basic version as the data products between the two are different.</p> <p>Parameters:</p> Name Type Description Default <code>temp_srcs</code> <code>DataFrame</code> <p>The temporary associtation dataframe used through the advanced association process.</p> required <code>sources_df</code> <code>DataFrame</code> <p>The sources_df produced by each step of association holding the current 'sources'.</p> required <code>method</code> <code>str</code> <p>Can be either 'advanced' or 'deruiter' to represent the advanced association method being used.</p> required <code>id_incr_par_assoc</code> <code>int</code> <p>An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association</p> <code>0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Updated <code>temp_srcs</code> dataframe with all the one_to_many relation information added.</p> <code>DataFrame</code> <p>Updated <code>sources_df</code> dataframe with all the one_to_many relation information added.</p> Source code in <code>vast_pipeline/pipeline/association.py</code> <pre><code>def one_to_many_advanced(\n    temp_srcs: pd.DataFrame,\n    sources_df: pd.DataFrame,\n    method: str,\n    id_incr_par_assoc: int = 0\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    '''\n    Finds and processes the one-to-many associations in the advanced\n    association. For each one-to-many association, the nearest\n    associated source is assigned the original source id, where as\n    the others are given new ids. The original source in skyc1 then\n    is copied to the sources_df to provide the extra association for\n    that source, i.e. it is forked.\n\n    This is needed to be separate from the basic version\n    as the data products between the two are different.\n\n    Args:\n        temp_srcs:\n            The temporary associtation dataframe used through the advanced\n            association process.\n        sources_df:\n            The sources_df produced by each step of association holding\n            the current 'sources'.\n        method:\n            Can be either 'advanced' or 'deruiter' to represent the advanced\n            association method being used.\n        id_incr_par_assoc:\n            An increment value to add to new source ids when creating them.\n            Mainly useful for add mode with parallel association\n\n    Returns:\n        Updated `temp_srcs` dataframe with all the one_to_many relation\n            information added.\n        Updated `sources_df` dataframe with all the one_to_many relation\n            information added.\n    '''\n    # use only these columns for easy debugging of the dataframe\n    cols = [\n        'index_old_skyc1', 'id_skyc1', 'source_skyc1',\n        'related_skyc1', 'index_old_skyc2', 'id_skyc2', 'source_skyc2',\n        'd2d', 'dr'\n    ]\n    duplicated_skyc1 = temp_srcs.loc[\n        temp_srcs['source_skyc1'].duplicated(keep=False), cols\n    ].copy()\n\n    # duplicated_skyc1\n    # +-----+-------------------+------------+----------------+\n    # |     |   index_old_skyc1 |   id_skyc1 |   source_skyc1 |\n    # |-----+-------------------+------------+----------------+\n    # | 117 |               121 |        122 |            122 |\n    # | 118 |               121 |        122 |            122 |\n    # | 238 |               253 |        254 |            254 |\n    # | 239 |               253 |        254 |            254 |\n    # | 246 |               261 |        262 |            262 |\n    # +-----+-------------------+------------+----------------+\n    # -----------------+-------------------+------------+----------------+\n    #  related_skyc1   |   index_old_skyc2 |   id_skyc2 |   source_skyc2 |\n    # -----------------+-------------------+------------+----------------+\n    #                  |               526 |       6068 |             -1 |\n    #                  |               528 |       6070 |             -1 |\n    #                  |               264 |       5806 |             -1 |\n    #                  |               265 |       5807 |             -1 |\n    #                  |               327 |       5869 |             -1 |\n    # -----------------+-------------------+------------+----------------+\n    # -------------+------+\n    #          d2d |   dr |\n    # -------------+------|\n    #      3.07478 |    0 |\n    #      6.41973 |    0 |\n    #      2.04422 |    0 |\n    #      6.16881 |    0 |\n    #      3.20439 |    0 |\n    # -------------+------+\n\n    # If no relations then no action is required\n    if duplicated_skyc1.empty:\n        logger.debug('No one-to-many associations.')\n        return temp_srcs, sources_df\n\n    logger.debug(\n        'Detected #%i one-to-many assocations, cleaning...',\n        duplicated_skyc1.shape[0]\n    )\n\n    # Get the column to check for the minimum depending on the method\n    # set the column names needed for filtering the 'to-many'\n    # associations depending on the method (advanced or deruiter)\n    dist_col = 'd2d' if method == 'advanced' else 'dr'\n\n    # go through the doubles and\n    # 1. Keep the closest d2d or de ruiter as the primary id\n    # 2. Increment a new source id for others\n    # 3. Add a copy of the previously matched\n    # source into sources.\n    # multi_srcs = duplicated_skyc1['source_skyc1'].unique()\n\n    # Get the duplicated, sort by the distance column\n    duplicated_skyc1 = duplicated_skyc1.sort_values(\n        by=['source_skyc1', dist_col]\n    )\n\n    # Get those that need to be given a new ID number (i.e. not the min dist_col)\n    idx_to_change = duplicated_skyc1.index.values[\n        duplicated_skyc1.duplicated('source_skyc1')\n    ]\n\n    # Create a new `new_source_id` column to store the 'correct' IDs\n    duplicated_skyc1['new_source_id'] = duplicated_skyc1['source_skyc1']\n\n    # +-----------------+\n    # |   new_source_id |\n    # +-----------------|\n    # |             122 |\n    # |             122 |\n    # |             254 |\n    # |             254 |\n    # |             262 |\n    # +-----------------+\n\n    # Define the range of new source ids\n    start_new_src_id = sources_df['source'].values.max() + 1 + id_incr_par_assoc\n\n    # Create an arange to use to change the ones that need to be changed.\n    new_source_ids = np.arange(\n        start_new_src_id,\n        start_new_src_id + idx_to_change.shape[0],\n        dtype=int\n    )\n\n    # Assign the new IDs to those that need to be changed.\n    duplicated_skyc1.loc[idx_to_change, 'new_source_id'] = new_source_ids\n\n    # We also need to clear the relations for these 'new' sources\n    # otherwise it will inherit rogue relations from the original relation\n    duplicated_skyc1.loc[idx_to_change, 'related_skyc1'] = None\n\n    # Now we need to sort out the related, essentially here the 'original'\n    # and 'non original' need to be treated differently.\n    # The original source need all the assoicated new ids appended to the\n    # related column.\n    # The not_original ones need just the original ID appended.\n    not_original = duplicated_skyc1.loc[\n        idx_to_change\n    ].copy()\n\n    original = duplicated_skyc1.drop_duplicates(\n        'source_skyc1'\n    ).copy()\n\n    # This gathers all the new ids that need to be appended\n    # to the original related column.\n    new_original_related = pd.DataFrame(\n        not_original[\n            ['source_skyc1', 'new_source_id']\n        ].groupby('source_skyc1').apply(\n            lambda grp: grp['new_source_id'].tolist()\n        )\n    )\n\n    #new_original_related\n    # +----------------+--------+\n    # |   source_skyc1 | 0      |\n    # |----------------+--------|\n    # |            122 | [5542] |\n    # |            254 | [5543] |\n    # |            262 | [5544] |\n    # |            405 | [5545] |\n    # |            656 | [5546] |\n    # +----------------+--------+\n\n    # Append the relations in each case, using the above 'new_original_related'\n    # for the original ones.\n    # The not original only require the appending of the original index.\n    original['related_skyc1'] = (\n        original[['related_skyc1', 'source_skyc1']]\n        .apply(\n            add_new_one_to_many_relations,\n            args=(True, new_original_related),\n            axis=1\n        )\n    )\n\n    # what the column looks like after the above\n    # +-----------------+\n    # | related_skyc1   |\n    # +-----------------+\n    # | [5542]          |\n    # | [5543]          |\n    # | [5544]          |\n    # | [5545]          |\n    # | [5546]          |\n    # +-----------------+\n\n    not_original.loc[:, 'related_skyc1'] = not_original.apply(\n        add_new_one_to_many_relations,\n        args=(True,),\n        axis=1\n    )\n\n    # Merge them back together\n    duplicated_skyc1 = pd.concat([original, not_original])\n\n    del original, not_original\n\n    # Apply the updates to the actual temp_srcs.\n    temp_srcs.loc[idx_to_change, 'source_skyc1'] = new_source_ids\n    temp_srcs.loc[\n        duplicated_skyc1.index.values,\n        'related_skyc1'\n    ] = duplicated_skyc1.loc[\n        duplicated_skyc1.index.values,\n        'related_skyc1'\n    ].values\n\n    # Finally we need to create copies of the previous sources in the\n    # sources_df to complete the new sources.\n\n    # To do this we get only the non-original sources\n    duplicated_skyc1 = duplicated_skyc1.loc[\n        duplicated_skyc1.duplicated('source_skyc1')\n    ]\n\n    # Get all the indexes required for each original\n    # `source_skyc1` value\n    source_df_index_to_copy = pd.DataFrame(\n        duplicated_skyc1.groupby(\n            'source_skyc1'\n        ).apply(\n            lambda grp: sources_df[\n                sources_df['source'] == grp.name\n            ].index.values.tolist()\n        )\n    )\n\n    # source_df_index_to_copy\n    # +----------------+-------+\n    # |   source_skyc1 | 0     |\n    # |----------------+-------|\n    # |            122 | [121] |\n    # |            254 | [253] |\n    # |            262 | [261] |\n    # |            405 | [404] |\n    # |            656 | [655] |\n    # +----------------+-------+\n\n    # merge these so it's easy to explode and copy the index values.\n    duplicated_skyc1 = (\n        duplicated_skyc1.loc[:,['source_skyc1', 'new_source_id']]\n        .merge(\n            source_df_index_to_copy,\n            left_on='source_skyc1',\n            right_index=True,\n            how='left'\n        )\n        .rename(columns={0: 'source_index'})\n        .explode('source_index')\n    )\n\n    # duplicated_skyc1\n    # +-----+----------------+-----------------+----------------+\n    # |     |   source_skyc1 |   new_source_id |   source_index |\n    # |-----+----------------+-----------------+----------------|\n    # | 118 |            122 |            5542 |            121 |\n    # | 239 |            254 |            5543 |            253 |\n    # | 247 |            262 |            5544 |            261 |\n    # | 380 |            405 |            5545 |            404 |\n    # | 615 |            656 |            5546 |            655 |\n    # +-----+----------------+-----------------+----------------+\n\n    # Get the sources\n    sources_to_copy = sources_df.loc[\n        duplicated_skyc1['source_index'].values\n    ]\n\n    # Apply the new_source_id\n    sources_to_copy['source'] = duplicated_skyc1['new_source_id'].values\n\n    # Reset the related column to avoid rogue relations\n    sources_to_copy['related'] = None\n\n    # and finally concatenate.\n    sources_df = pd.concat([sources_df, sources_to_copy], ignore_index=True)\n\n    return temp_srcs, sources_df\n</code></pre>"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.one_to_many_basic","title":"<code>one_to_many_basic(skyc2_srcs, sources_df, id_incr_par_assoc=0)</code>","text":"<p>Finds and processes the one-to-many associations in the basic association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked.</p> <p>This is needed to be separate from the advanced version as the data products between the two are different.</p> <p>Parameters:</p> Name Type Description Default <code>skyc2_srcs</code> <code>DataFrame</code> <p>The sky catalogue 2 sources (i.e. the sources being associated to the base) used during basic association.</p> required <code>sources_df</code> <code>DataFrame</code> <p>The sources_df produced by each step of association holding the current 'sources'.</p> required <code>id_incr_par_assoc</code> <code>int</code> <p>An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association</p> <code>0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Updated 'skyc2_srcs' with all the one_to_many relation information added.</p> <code>DataFrame</code> <p>Updated 'sources_df' with all the one_to_many relation information added.</p> Source code in <code>vast_pipeline/pipeline/association.py</code> <pre><code>def one_to_many_basic(\n    skyc2_srcs: pd.DataFrame,\n    sources_df: pd.DataFrame,\n    id_incr_par_assoc: int = 0\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Finds and processes the one-to-many associations in the basic\n    association. For each one-to-many association, the nearest\n    associated source is assigned the original source id, where as\n    the others are given new ids. The original source in skyc1 then\n    is copied to the sources_df to provide the extra association for\n    that source, i.e. it is forked.\n\n    This is needed to be separate from the advanced version\n    as the data products between the two are different.\n\n    Args:\n        skyc2_srcs: The sky catalogue 2 sources (i.e. the sources being\n            associated to the base) used during basic association.\n        sources_df: The sources_df produced by each step of\n            association holding the current 'sources'.\n        id_incr_par_assoc: An increment value to add to new source ids\n            when creating them. Mainly useful for add mode with parallel\n            association\n\n    Returns:\n        Updated 'skyc2_srcs' with all the one_to_many relation information\n            added.\n        Updated 'sources_df' with all the one_to_many relation information\n            added.\n    \"\"\"\n    # select duplicated in 'source' field in skyc2_srcs, excluding -1\n    duplicated_skyc2 = skyc2_srcs.loc[\n        (skyc2_srcs['source'] != -1) &amp;\n        skyc2_srcs['source'].duplicated(keep=False),\n        ['source', 'related', 'd2d']\n    ]\n\n    # duplicated_skyc2\n    # +-----+----------+-----------+---------+\n    # |     |   source | related   |     d2d |\n    # |-----+----------+-----------+---------|\n    # | 264 |      254 |           | 2.04422 |\n    # | 265 |      254 |           | 6.16881 |\n    # | 327 |      262 |           | 3.20439 |\n    # | 328 |      262 |           | 3.84425 |\n    # | 526 |      122 |           | 3.07478 |\n    # +-----+----------+-----------+---------+\n\n    if duplicated_skyc2.empty:\n        logger.debug('No one-to-many associations.')\n        return skyc2_srcs, sources_df\n\n    logger.info(\n        'Detected #%i double matches, cleaning...',\n        duplicated_skyc2.shape[0]\n    )\n\n    # now we have the src values which are doubled.\n    # make the nearest match have the \"original\" src id\n    # give the other matched source a new src id\n    # and make sure to copy the other previously\n    # matched sources.\n    # Get the duplicated, sort by the distance column\n    duplicated_skyc2 = duplicated_skyc2.sort_values(by=['source', 'd2d'])\n\n    # Get those that need to be given a new ID number (i.e. not the min dist_col)\n    idx_to_change = duplicated_skyc2.index.values[\n        duplicated_skyc2.duplicated('source')\n    ]\n\n    # Create a new `new_source_id` column to store the 'correct' IDs\n    duplicated_skyc2['new_source_id'] = duplicated_skyc2['source']\n\n    # Define the range of new source ids\n    start_new_src_id = sources_df['source'].values.max() + 1 + id_incr_par_assoc\n\n    new_source_ids = np.arange(\n        start_new_src_id,\n        start_new_src_id + idx_to_change.shape[0],\n        dtype=int\n    )\n\n    # Assign the new IDs\n    duplicated_skyc2.loc[idx_to_change, 'new_source_id'] = new_source_ids\n\n    # duplicated_skyc2\n    # +-----+----------+-----------+---------+-----------------+\n    # |     |   source | related   |     d2d |   new_source_id |\n    # |-----+----------+-----------+---------+-----------------|\n    # | 526 |      122 |           | 3.07478 |             122 |\n    # | 528 |      122 |           | 6.41973 |            5542 |\n    # | 264 |      254 |           | 2.04422 |             254 |\n    # | 265 |      254 |           | 6.16881 |            5543 |\n    # | 327 |      262 |           | 3.20439 |             262 |\n    # +-----+----------+-----------+---------+-----------------+\n\n    # Now we need to sort out the related, essentially here the 'original'\n    # and 'non original' need to be treated differently.\n    # The original source need all the assoicated new ids appended to the\n    # related column.\n    # The not_original ones need just the original ID appended.\n    # copy() is used here to avoid chained indexing (set with copy warnings)\n    not_original = duplicated_skyc2.loc[\n        idx_to_change\n    ].copy()\n\n    original = duplicated_skyc2.drop_duplicates(\n        'source'\n    ).copy()\n\n    new_original_related = pd.DataFrame(\n        not_original[\n            ['source', 'new_source_id']\n        ].groupby('source').apply(\n            lambda grp: grp['new_source_id'].tolist()\n        )\n    )\n\n    # new_original_related\n    # +----------+--------+\n    # |   source | 0      |\n    # |----------+--------|\n    # |      122 | [5542] |\n    # |      254 | [5543] |\n    # |      262 | [5544] |\n    # |      405 | [5545] |\n    # |      656 | [5546] |\n    # +----------+--------+\n\n    # Append the relations in each case, using the above 'new_original_related'\n    # for the original ones.\n    # The not original only require the appending of the original index.\n    original['related'] = (\n        original[['related', 'source']]\n        .apply(\n            add_new_one_to_many_relations,\n            args=(False, new_original_related),\n            axis=1\n        )\n    )\n\n    not_original['related'] = not_original.apply(\n        add_new_one_to_many_relations,\n        args=(False,),\n        axis=1\n    )\n\n    duplicated_skyc2 = pd.concat([original, not_original])\n\n    # duplicated_skyc2\n    # +-----+----------+-----------+---------+-----------------+\n    # |     |   source | related   |     d2d |   new_source_id |\n    # |-----+----------+-----------+---------+-----------------|\n    # | 526 |      122 | [5542]    | 3.07478 |             122 |\n    # | 264 |      254 | [5543]    | 2.04422 |             254 |\n    # | 327 |      262 | [5544]    | 3.20439 |             262 |\n    # | 848 |      405 | [5545]    | 5.52865 |             405 |\n    # | 695 |      656 | [5546]    | 4.69094 |             656 |\n    # +-----+----------+-----------+---------+-----------------+\n\n    del original, not_original\n\n    # Apply the updates to the actual temp_srcs.\n    skyc2_srcs.loc[idx_to_change, 'source'] = new_source_ids\n    skyc2_srcs.loc[\n        duplicated_skyc2.index.values,\n        'related'\n    ] = duplicated_skyc2.loc[\n        duplicated_skyc2.index.values,\n        'related'\n    ].values\n\n    # Finally we need to copy copies of the previous sources in the\n    # sources_df to complete the new sources.\n\n    # To do this we get only the non-original sources\n    duplicated_skyc2 = duplicated_skyc2.loc[\n        duplicated_skyc2.duplicated('source')\n    ]\n\n    # Get all the indexes required for each original\n    # `source_skyc1` value\n    source_df_index_to_copy = pd.DataFrame(\n        duplicated_skyc2.groupby(\n            'source'\n        ).apply(\n            lambda grp: sources_df[\n                sources_df['source'] == grp.name\n            ].index.values.tolist()\n        )\n    )\n\n    # source_df_index_to_copy\n    # +----------+-------+\n    # |   source | 0     |\n    # |----------+-------|\n    # |      122 | [121] |\n    # |      254 | [253] |\n    # |      262 | [261] |\n    # |      405 | [404] |\n    # |      656 | [655] |\n    # +----------+-------+\n\n    # merge these so it's easy to explode and copy the index values.\n    duplicated_skyc2 = (\n        duplicated_skyc2[['source', 'new_source_id']]\n        .merge(\n            source_df_index_to_copy,\n            left_on='source',\n            right_index=True,\n            how='left'\n        )\n        .rename(columns={0: 'source_index'})\n        .explode('source_index')\n    )\n\n    # Get the sources - all columns from the sources_df table\n    sources_to_copy = sources_df.loc[\n        duplicated_skyc2['source_index'].values\n    ]\n\n    # Apply the new_source_id\n    sources_to_copy['source'] = duplicated_skyc2['new_source_id'].values\n\n    # Reset the related column to avoid rogue relations\n    sources_to_copy['related'] = None\n\n    # and finally concatenate.\n    sources_df = pd.concat(\n        [sources_df, sources_to_copy],\n        ignore_index=True\n    )\n\n    return skyc2_srcs, sources_df\n</code></pre>"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.parallel_association","title":"<code>parallel_association(images_df, limit, dr_limit, bw_limit, duplicate_limit, config, n_skyregion_groups, add_mode, previous_parquets, done_images_df, done_source_ids)</code>","text":"<p>Launches association on different sky region groups in parallel using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>images_df</code> <code>DataFrame</code> <p>Holds the images that are being processed. Also contains what sky region group the image belongs to.</p> required <code>limit</code> <code>Angle</code> <p>The association radius limit.</p> required <code>dr_limit</code> <code>float</code> <p>The de Ruiter radius limit.</p> required <code>bw_limit</code> <code>float</code> <p>The beamwidth limit.</p> required <code>duplicate_limit</code> <code>Angle</code> <p>The duplicate radius detection limit.</p> required <code>config</code> <code>module</code> <p>The pipeline config settings.</p> required <code>n_skyregion_groups</code> <code>int</code> <p>The number of sky region groups.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The combined association results of the parallel association with corrected source ids.</p> Source code in <code>vast_pipeline/pipeline/association.py</code> <pre><code>def parallel_association(\n    images_df: pd.DataFrame,\n    limit: Angle,\n    dr_limit: float,\n    bw_limit: float,\n    duplicate_limit: Angle,\n    config: PipelineConfig,\n    n_skyregion_groups: int,\n    add_mode: bool,\n    previous_parquets: Dict[str, str],\n    done_images_df: pd.DataFrame,\n    done_source_ids: List[int]\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Launches association on different sky region groups in parallel using Dask.\n\n    Args:\n        images_df: Holds the images that are being processed.\n            Also contains what sky region group the image belongs to.\n        limit: The association radius limit.\n        dr_limit: The de Ruiter radius limit.\n        bw_limit: The beamwidth limit.\n        duplicate_limit: The duplicate radius detection limit.\n        config (module): The pipeline config settings.\n        n_skyregion_groups: The number of sky region groups.\n\n    Returns:\n        The combined association results of the parallel\n            association with corrected source ids.\n    \"\"\"\n    logger.info(\n        \"Running parallel association for %i sky region groups.\",\n        n_skyregion_groups\n    )\n\n    timer = StopWatch()\n\n    meta = {\n        'id': 'i',\n        'uncertainty_ew': 'f',\n        'weight_ew': 'f',\n        'uncertainty_ns': 'f',\n        'weight_ns': 'f',\n        'flux_int': 'f',\n        'flux_int_err': 'f',\n        'flux_int_isl_ratio': 'f',\n        'flux_peak': 'f',\n        'flux_peak_err': 'f',\n        'flux_peak_isl_ratio': 'f',\n        'forced': '?',\n        'compactness': 'f',\n        'has_siblings': '?',\n        'snr': 'f',\n        'image': 'U',\n        'datetime': 'datetime64[ns]',\n        'source': 'i',\n        'ra': 'f',\n        'dec': 'f',\n        'd2d': 'f',\n        'dr': 'f',\n        'related': 'O',\n        'epoch': 'i',\n        'interim_ew': 'f',\n        'interim_ns': 'f',\n    }\n\n    # Add an increment to any new source values when using add_mode to avoid\n    # getting duplicates in the result laater\n    id_incr_par_assoc = max(done_source_ids) if add_mode else 0\n    n_workers, n_partitions = calculate_workers_and_partitions(\n        images_df,\n        n_cpu=config['processing']['num_workers'],\n        max_partition_mb=config['processing']['max_partition_mb']\n        )\n    logger.debug(f\"Running association with {n_workers} CPUs\")\n    # pass each skyreg_group through the normal association process.\n    results = (\n        dd.from_pandas(images_df.set_index('skyreg_group'), npartitions=n_partitions)\n        .groupby('skyreg_group')\n        .apply(\n            association,\n            limit=limit,\n            dr_limit=dr_limit,\n            bw_limit=bw_limit,\n            duplicate_limit=duplicate_limit,\n            config=config,\n            add_mode=add_mode,\n            previous_parquets=previous_parquets,\n            done_images_df=done_images_df,\n            id_incr_par_assoc=id_incr_par_assoc,\n            parallel=True,\n            meta=meta\n        ).compute(n_workers=n_workers, scheduler='processes')\n    )\n\n    # results are the normal dataframe of results with the columns:\n    # 'id', 'uncertainty_ew', 'weight_ew', 'uncertainty_ns', 'weight_ns',\n    # 'flux_int', 'flux_int_err', 'flux_peak', 'flux_peak_err', 'forced',\n    # 'compactness', 'has_siblings', 'snr', 'image', 'datetime', 'source',\n    # 'ra', 'dec', 'd2d', 'dr', 'related', 'epoch', 'interim_ew' and\n    # 'interim_ns'.\n\n    # The index however is now a multi index with the skyregion group and\n    # a general result index. Hence the general result index is repeated for\n    # each skyreg_group along with the source_ids. This needs to be collapsed\n    # and the source id's corrected.\n\n    # Index example:\n    #                        id\n    # skyreg_group\n    # --------------------------\n    # 2            0      15640\n    #              1      15641\n    #              2      15642\n    #              3      15643\n    #              4      15644\n    # ...                   ...\n    # 1            46975  53992\n    #              46976  54062\n    #              46977  54150\n    #              46978  54161\n    #              46979  54164\n\n    # Get the indexes (skyreg_groups) to loop over for source id correction\n    indexes = results.index.levels[0].values\n\n    if add_mode:\n        # Need to correct all skyreg_groups.\n        # First get the starting id for new sources.\n        new_id = max(done_source_ids) + 1\n        for i in indexes:\n            corr_df, new_id = _correct_parallel_source_ids_add_mode(\n                results.loc[i, ['source', 'related']],\n                done_source_ids,\n                new_id\n            )\n            results.loc[\n                (i, slice(None)), ['source', 'related']\n            ] = corr_df.values\n    else:\n        # The first index acts as the base, so the others are looped over and\n        # corrected.\n        for i, val in enumerate(indexes):\n            # skip first one, makes the enumerate easier to deal with\n            if i == 0:\n                continue\n            # Get the maximum source ID from the previous group.\n            max_id = results.loc[indexes[i - 1]].source.max()\n            # Run through the correction function, only the 'source' and\n            # 'related'\n            # columns are passed and returned (corrected).\n            corr_df = _correct_parallel_source_ids(\n                results.loc[val, ['source', 'related']],\n                max_id\n            )\n            # replace the values in the results with the corrected source and\n            # related values\n            results.loc[\n                (val, slice(None)), ['source', 'related']\n            ] = corr_df.values\n\n            del corr_df\n\n    # reset the indeex of the final corrected and collapsed result\n    results = results.reset_index(drop=True)\n\n    logger.info(\n        'Total parallel association time: %.2f seconds', timer.reset_init()\n    )\n\n    return results\n</code></pre>"},{"location":"reference/pipeline/config/","title":"config.py","text":""},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.ImageIngestConfig","title":"<code>ImageIngestConfig</code>","text":"<p>               Bases: <code>PipelineConfig</code></p> <p>Image ingest configuration derived from PipelineConfig.</p> <p>Attributes:</p> Name Type Description <code>SCHEMA</code> <p>class attribute containing the YAML schema for the image ingest config.</p> <code>TEMPLATE_PATH</code> <code>str</code> <p>class attribute containing the path to the default Jinja2 image ingest config template file.</p> <p>Raises:</p> Type Description <code>PipelineConfigError</code> <p>the input YAML config violates the schema.</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>class ImageIngestConfig(PipelineConfig):\n    \"\"\"Image ingest configuration derived from PipelineConfig.\n\n    Attributes:\n        SCHEMA: class attribute containing the YAML schema for the image ingest config.\n        TEMPLATE_PATH: class attribute containing the path to the default Jinja2 image ingest\n            config template file.\n\n    Raises:\n        PipelineConfigError: the input YAML config violates the schema.\n    \"\"\"\n    # path to default image ingest config template\n    TEMPLATE_PATH: str = os.path.join(\n        settings.BASE_DIR, \"vast_pipeline\", \"ingest_config_template.yaml.j2\"\n    )\n    _VALID_ASSOC_METHODS = None\n    SCHEMA = yaml.Map(\n        {\n            \"inputs\": yaml.Map(PipelineConfig._SCHEMA_INPUTS),\n            \"measurements\": yaml.Map(\n                {\n                    \"condon_errors\": yaml.Bool(),\n                    \"selavy_local_rms_fill_value\": yaml.Float(),\n                    \"ra_uncertainty\": yaml.Float(),\n                    \"dec_uncertainty\": yaml.Float(),\n                }\n            ),\n        }\n    )\n</code></pre>"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.PipelineConfig","title":"<code>PipelineConfig</code>","text":"<p>Pipeline run configuration.</p> <p>Attributes:</p> Name Type Description <code>SCHEMA</code> <p>class attribute containing the YAML schema for the run config.</p> <code>TEMPLATE_PATH</code> <code>str</code> <p>class attribute containing the path to the default Jinja2 run config template file.</p> <code>epoch_based</code> <code>bool</code> <p>boolean indicating if the original run config inputs were provided with user-defined epochs.</p> <p>Raises:</p> Type Description <code>PipelineConfigError</code> <p>the input YAML config violates the schema.</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>class PipelineConfig:\n    \"\"\"Pipeline run configuration.\n\n    Attributes:\n        SCHEMA: class attribute containing the YAML schema for the run config.\n        TEMPLATE_PATH: class attribute containing the path to the default Jinja2 run\n            config template file.\n        epoch_based: boolean indicating if the original run config inputs were provided\n            with user-defined epochs.\n\n    Raises:\n        PipelineConfigError: the input YAML config violates the schema.\n    \"\"\"\n\n    # key: config input type, value: boolean indicating if it is required\n    _REQUIRED_INPUT_TYPES: Dict[str, bool] = {\n        \"image\": True,\n        \"selavy\": True,\n        \"noise\": True,\n        \"background\": False,\n    }\n    # Inputs may be optional. All inputs will be either a unique list or a mapping (epoch\n    # mode and/or glob expressions). These possibilities cannot be validated at once, so\n    # it will accept Any and then revalidate later.\n    _SCHEMA_INPUTS = {\n        (k if v else yaml.Optional(k)): yaml.MapPattern(yaml.Str(), yaml.Any())\n        | yaml.UniqueSeq(yaml.Str())\n        for k, v in _REQUIRED_INPUT_TYPES.items()\n    }\n    _SCHEMA_GLOB_INPUTS = {\"glob\": yaml.Str() | yaml.Seq(yaml.Str())}\n    _VALID_ASSOC_METHODS: List[str] = [\"basic\", \"advanced\", \"deruiter\"]\n    SCHEMA = yaml.Map(\n        {\n            \"run\": yaml.Map(\n                {\n                    \"path\": yaml.Str(),\n                    \"suppress_astropy_warnings\": yaml.Bool(),\n                }\n            ),\n            \"inputs\": yaml.Map(_SCHEMA_INPUTS),\n            \"source_monitoring\": yaml.Map(\n                {\n                    \"monitor\": yaml.Bool(),\n                    \"min_sigma\": yaml.Float(),\n                    \"edge_buffer_scale\": yaml.Float(),\n                    \"cluster_threshold\": yaml.Float(),\n                    \"allow_nan\": yaml.Bool(),\n                }\n            ),\n            \"source_association\": yaml.Map(\n                {\n                    \"method\": yaml.Enum(_VALID_ASSOC_METHODS),\n                    \"radius\": yaml.Float(),\n                    \"deruiter_radius\": yaml.Float(),\n                    \"deruiter_beamwidth_limit\": yaml.Float(),\n                    \"parallel\": yaml.Bool(),\n                    \"epoch_duplicate_radius\": yaml.Float(),\n                }\n            ),\n            \"new_sources\": yaml.Map(\n                {\n                    \"min_sigma\": yaml.Float(),\n                }\n            ),\n            \"measurements\": yaml.Map(\n                {\n                    \"source_finder\": yaml.Enum([\"selavy\"]),\n                    \"flux_fractional_error\": yaml.Float(),\n                    \"condon_errors\": yaml.Bool(),\n                    \"selavy_local_rms_fill_value\": yaml.Float(),\n                    \"write_arrow_files\": yaml.Bool(),\n                    \"ra_uncertainty\": yaml.Float(),\n                    \"dec_uncertainty\": yaml.Float(),\n                }\n            ),\n            \"variability\": yaml.Map(\n                {\n                    \"pair_metrics\": yaml.Bool(),\n                    \"source_aggregate_pair_metrics_min_abs_vs\": yaml.Float(),\n                }\n            ),\n            yaml.Optional(\"processing\"): yaml.Map(\n                {\n                    yaml.Optional(\n                        \"num_workers\",\n                        default=settings.PIPE_RUN_CONFIG_DEFAULTS['num_workers']):\n                        yaml.NullNone() | yaml.Int() | yaml.Str(),\n                    yaml.Optional(\n                        \"num_workers_io\",\n                        default=settings.PIPE_RUN_CONFIG_DEFAULTS['num_workers_io']):\n                        yaml.NullNone() | yaml.Int() | yaml.Str(),\n                    yaml.Optional(\n                        \"max_partition_mb\",\n                        default=settings.PIPE_RUN_CONFIG_DEFAULTS['max_partition_mb']):\n                        yaml.Int()\n                }\n            )\n        }\n    )\n    # path to default run config template\n    TEMPLATE_PATH: str = os.path.join(\n        settings.BASE_DIR, \"vast_pipeline\", \"config_template.yaml.j2\"\n    )\n\n    def __init__(self, config_yaml: yaml.YAML, validate_inputs: bool = True):\n        \"\"\"Initialises PipelineConfig with parsed (but not necessarily validated) YAML.\n\n        Args:\n            config_yaml (yaml.YAML): Input YAML, usually the output of `strictyaml.load`.\n            validate_inputs (bool, optional): Validate the config input files. Ensures\n                that the inputs match (e.g. each image has a catalogue), and that each\n                path exists. Set to False to skip these checks. Defaults to True.\n\n        Raises:\n            PipelineConfigError: The input YAML config violates the schema.\n        \"\"\"\n        self._yaml: yaml.YAML = config_yaml\n        # The epoch_based parameter below is for if the user has entered just lists we\n        # don't have access to the dates until the Image instances are created. So we\n        # flag this as true so that we can reorder the epochs once the date information\n        # is available. It is also recorded in the database such that there is a record\n        # of the fact that the run was processed in an epoch based mode.\n        self.epoch_based: bool\n\n        # Determine if epoch-based association should be used based on input files.\n        # If inputs have been parsed to dicts, then the user has defined their own epochs.\n        # If inputs have been parsed to lists, we must convert to dicts and auto-fill\n        # the epochs.\n\n        # ensure the inputs are valid in case .from_file(..., validate=False) was used\n        if not validate_inputs:\n            return\n\n        try:\n            self._validate_inputs()\n        except yaml.YAMLValidationError as e:\n            raise PipelineConfigError(e)\n\n        # detect simple list inputs and convert them to epoch-mode inputs\n        yaml_inputs = self._yaml[\"inputs\"]\n        inputs = yaml_inputs.data\n        for input_file_type in self._REQUIRED_INPUT_TYPES:\n            # skip missing optional input types, e.g. background\n            if (\n                not self._REQUIRED_INPUT_TYPES[input_file_type]\n                and input_file_type not in self[\"inputs\"]\n            ):\n                continue\n\n            input_files = inputs[input_file_type]\n\n            # resolve glob expressions if present\n            if isinstance(input_files, dict):\n                # must be either a glob expression, list of glob expressions, or epoch-mode\n                if \"glob\" in input_files:\n                    # resolve the glob expressions\n                    self.epoch_based = False\n                    file_list = self._resolve_glob_expressions(\n                        yaml_inputs[input_file_type]\n                    )\n                    inputs[input_file_type] = self._create_input_epochs(file_list)\n                else:\n                    # epoch-mode with either a list of files or glob expressions\n                    self.epoch_based = True\n                    for epoch in input_files:\n                        if \"glob\" in input_files[epoch]:\n                            # resolve the glob expressions\n                            file_list = self._resolve_glob_expressions(\n                                yaml_inputs[input_file_type][epoch]\n                            )\n                            inputs[input_file_type][epoch] = file_list\n            else:\n                # Epoch-based association not requested and no globs present. Replace\n                # input lists with dicts where each input file has it's own epoch.\n                self.epoch_based = False\n                inputs[input_file_type] = self._create_input_epochs(\n                    input_files\n                )\n        self._yaml[\"inputs\"] = inputs\n\n    def __getitem__(self, name: str):\n        \"\"\"Retrieves the requested YAML chunk as a native Python object.\"\"\"\n        return self._yaml[name].data\n\n    @staticmethod\n    def _create_input_epochs(input_files: List[str]) -&gt; Dict[str, List[str]]:\n        \"\"\"Convert a list of input files into a dict where each list element is placed\n        into its own list of length 1 and mapped to by a unique key, a string that is a\n        0-padded integer. For example, [\"A\", \"B\", \"C\", ..., \"Z\"] would be converted to\n        {\n            \"01\": [\"A\"],\n            \"02\": [\"B\"],\n            \"03\": [\"C\"],\n            ...\n            \"26\": [\"Z\"],\n        }\n        The keys are 0-padded to ensure the strings are sortable regardless of the\n        length of `input_files`.\n        This conversion is required for run configs that are not defined in \"epoch mode\"\n        as after config validation, the pipeline assumes that there will be defined\n        epochs.\n\n        Args:\n            input_files: the list of input file paths.\n\n        Returns:\n            The input file paths mapped to by unique epoch keys.\n        \"\"\"\n        pad_width = len(str(len(input_files)))\n        input_files_dict = {\n            f\"{i + 1:0{pad_width}}\": [val] for i, val in enumerate(input_files)\n        }\n        return input_files_dict\n\n    @classmethod\n    def from_file(\n        cls,\n        yaml_path: str,\n        label: str = \"run config\",\n        validate: bool = True,\n        validate_inputs: bool = True,\n        add_defaults: bool = True,\n    ) -&gt; \"PipelineConfig\":\n        \"\"\"Create a PipelineConfig object from a run configuration YAML file.\n\n        Args:\n            yaml_path: Path to the run config YAML file.\n            label: A label for the config object that will be used in error messages.\n                Default is \"run config\".\n            validate: Perform config schema validation immediately after loading\n                the config file. If set to False, the full schema validation\n                will not be performed until PipelineConfig.validate() is\n                explicitly called. The inputs are always validated regardless.\n                Defaults to True.\n            validate_inputs: Validate the config input files. Ensures that the inputs\n                match (e.g. each image has a catalogue), and that each path exists. Set\n                to False to skip these checks. Defaults to True.\n            add_defaults: Add missing configuration parameters using configured\n                defaults. The defaults are read from the Django settings file.\n                Defaults to True.\n\n        Raises:\n            PipelineConfigError: The run config YAML file fails schema validation.\n\n        \"\"\"\n        schema = cls.SCHEMA if validate else yaml.Any()\n        with open(yaml_path) as fh:\n            config_str = fh.read()\n        try:\n            config_yaml = yaml.load(config_str, schema=schema, label=label)\n        except yaml.YAMLValidationError as e:\n            raise PipelineConfigError(e)\n\n        if add_defaults:\n            # make a template config based on defaults\n            config_defaults_str = make_config_template(\n                cls.TEMPLATE_PATH,\n                **settings.PIPE_RUN_CONFIG_DEFAULTS,\n            )\n            config_defaults_dict: Dict[str, Any] = yaml.load(config_defaults_str).data\n\n            # merge configs\n            config_dict = dict_merge(config_defaults_dict, config_yaml.data)\n            config_yaml = yaml.as_document(config_dict, schema=schema, label=label)\n        return cls(config_yaml, validate_inputs=validate_inputs)\n\n    @staticmethod\n    def _resolve_glob_expressions(input_files: yaml.YAML) -&gt; List[str]:\n        \"\"\"Resolve glob expressions in a YAML chunk, returning a list of sorted file\n        paths.\n\n        Args:\n            input_files (yaml.YAML): A validated YAML chunk of input files that is a\n                mapping of \"glob\" to either a single glob expression or a sequence of\n                glob expressions. e.g.\n                ---\n                glob: /foo/*.fits\n                ---\n                or\n                ---\n                glob:\n                - /foo/A/*.fits\n                - /foo/B/*.fits\n                ---\n\n        Returns:\n            The resolved file paths in lexicographical order.\n        \"\"\"\n        file_list: List[str] = []\n        if input_files[\"glob\"].is_sequence():\n            for glob_expr in input_files[\"glob\"]:\n                file_list.extend(sorted(list(glob(glob_expr.data))))\n        else:\n            file_list.extend(sorted(list(glob(input_files[\"glob\"].data))))\n        return file_list\n\n    def _validate_inputs(self):\n        \"\"\"Validate the input files. Each input type (i.e. image, selavy, noise,\n        background) may be given as one of the following:\n            1. A list of files.\n            2. A glob expression.\n            3. A list of glob expressions.\n            4. A mapping of epochs to any of the above.\n        Each input type is validated individually. Extra input validation steps, e.g. to\n        ensure each input type has the same number of files, are performed in\n        `validate()`.\n\n        Raises:\n            PipelineConfigError: The run config inputs fail schema validation.\n        \"\"\"\n        try:\n            # first pass validation\n            self._yaml[\"inputs\"].revalidate(yaml.Map(self._SCHEMA_INPUTS))\n\n            for input_type in self._yaml[\"inputs\"]:\n                input_yaml = self._yaml[\"inputs\"][input_type]\n                if input_yaml.is_mapping():\n                    # inputs are either epoch-mode, glob expressions, or both\n                    if \"glob\" in input_yaml:\n                        # validate globs\n                        input_yaml.revalidate(yaml.Map(self._SCHEMA_GLOB_INPUTS))\n                    else:\n                        # validate epoch mode which may also contain glob expressions\n                        input_yaml.revalidate(\n                            yaml.MapPattern(\n                                yaml.Str(),\n                                yaml.UniqueSeq(yaml.Str())\n                                | yaml.Map(self._SCHEMA_GLOB_INPUTS),\n                            )\n                        )\n        except yaml.YAMLValidationError as e:\n            raise PipelineConfigError(e)\n\n    def validate(self, user: User = None):\n        \"\"\"Perform extra validation steps not covered by the default schema validation.\n        The following checks are performed in order. If a check fails, an exception is\n        raised and no further checks are performed.\n\n        1. All input files have the same number of epochs and the same number of files\n            per epoch.\n        2. The number of input files does not exceed the configured pipeline maximum.\n            This is only enforced if a regular user (not staff/admin) created the run.\n        3. There are at least two input images.\n        4. Background input images are required if source monitoring is turned on.\n        5. All input files exist.\n\n        Args:\n            user: Optional. The User of the request if made through the UI. Defaults to\n                None.\n\n        Raises:\n            PipelineConfigError: a validation check failed.\n        \"\"\"\n        # run standard base schema validation\n        try:\n            self._yaml.revalidate(self.SCHEMA)\n        except yaml.YAMLValidationError as e:\n            raise PipelineConfigError(e)\n\n        inputs = self[\"inputs\"]\n\n        # epochs defined for images only, used as the reference list of epochs\n        epochs_image = inputs[\"image\"].keys()\n        # map input type to a set of epochs\n        epochs_by_input_type = {\n            input_type: set(inputs[input_type].keys())\n            for input_type in inputs.keys()\n        }\n        # map input type to total number of files from all epochs\n        n_files_by_input_type: Dict[str, int] = {}\n        epoch_n_files: Dict[str, Dict[str, int]] = {}\n        n_files = 0\n        for input_type, epochs_set in epochs_by_input_type.items():\n            epoch_n_files[input_type] = {}\n            n_files_by_input_type[input_type] = 0\n            for epoch in epochs_set:\n                n = len(inputs[input_type][epoch])\n                n_files_by_input_type[input_type] += n\n                epoch_n_files[input_type][epoch] = n\n                n_files += n\n\n        # Note by this point the input files have been converted to a mapping regardless\n        # of the user's input format.\n        # Ensure all input file types have the same epochs.\n        try:\n            schema = yaml.Map({epoch: yaml.Seq(yaml.Str()) for epoch in epochs_image})\n            for input_type in inputs.keys():\n                # Generate a new YAML object on-the-fly per input to avoid saving\n                # a validation schema per file in the PipelineConfig object\n                # (These can consume a lot of RAM for long lists of input files).\n                yaml.load(self._yaml[\"inputs\"][input_type].as_yaml(), schema=schema)\n        except yaml.YAMLValidationError:\n            # number of epochs could be different or the name of the epochs may not match\n            # find out which by counting the number of unique epochs per input type\n            n_epochs_per_input_type = [\n                len(epochs_set) for epochs_set in epochs_by_input_type.values()\n            ]\n            if len(set(n_epochs_per_input_type)) &gt; 1:\n                if self.epoch_based:\n                    error_msg = \"The number of epochs must match for all input types.\\n\"\n                else:\n                    error_msg = \"The number of files must match for all input types.\\n\"\n            else:\n                error_msg = \"The name of the epochs must match for all input types.\\n\"\n            counts_str = \"\"\n            if self.epoch_based:\n                for input_type in epoch_n_files.keys():\n                    n = len(epoch_n_files[input_type])\n                    counts_str += (\n                        f\"{input_type} has {n} epoch{'s' if n &gt; 1 else ''}:\"\n                        f\" {', '.join(epoch_n_files[input_type].keys())}\\n\"\n                    )\n            else:\n                for input_type, n in n_files_by_input_type.items():\n                    counts_str += f\"{input_type} has {n} file{'s' if n &gt; 1 else ''}\\n\"\n\n            counts_str = counts_str[:-1]\n            raise PipelineConfigError(error_msg + counts_str)\n\n        # Ensure all input file type epochs have the same number of files per epoch.\n        # This could be combined with the number of epochs validation above, but we want\n        # to give specific feedback to the user on failure.\n        try:\n            schema = yaml.Map(\n                {epoch: yaml.FixedSeq([yaml.Str()] * epoch_n_files[\"image\"][epoch])\n                for epoch in epochs_image})\n            for input_type in inputs.keys():\n                yaml.load(self._yaml[\"inputs\"][input_type].as_yaml(), schema=schema)\n        except yaml.YAMLValidationError:\n            # map input type to a mapping of epoch to file count\n            file_counts_str = \"\"\n            for input_type in inputs.keys():\n                file_counts_str += f\"{input_type}:\\n\"\n                for epoch in sorted(inputs[input_type].keys()):\n                    file_counts_str += (\n                        f\"  {epoch}: {len(inputs[input_type][epoch])}\\n\"\n                    )\n            file_counts_str = file_counts_str[:-1]\n            raise PipelineConfigError(\n                \"The number of files per epoch does not match between input types.\\n\"\n                + file_counts_str\n            )\n\n        # ensure the number of input files is less than the user limit\n        if user and n_files &gt; settings.MAX_PIPERUN_IMAGES:\n            if user.is_staff:\n                logger.warning(\n                    \"Maximum number of images\"\n                    f\" ({settings.MAX_PIPERUN_IMAGES}) rule bypassed with\"\n                    \" admin status.\"\n                )\n            else:\n                raise PipelineConfigError(\n                    f\"The number of images entered ({n_files})\"\n                    \" exceeds the maximum number of images currently\"\n                    f\" allowed ({settings.MAX_PIPERUN_IMAGES}). Please ask\"\n                    \" an administrator for advice on processing your run.\"\n                )\n\n        # ensure at least two inputs are provided\n        check = [n_files_by_input_type[input_type] &lt; 2 for input_type in inputs.keys()]\n        if any(check):\n            raise PipelineConfigError(\n                \"Number of image files must to be larger than 1\"\n            )\n\n        # ensure background files are provided if source monitoring is requested\n        try:\n            monitor = self[\"source_monitoring\"][\"monitor\"]\n        except KeyError:\n            monitor = False\n\n        if monitor:\n            inputs_schema = yaml.Map(\n                {\n                    k: yaml.UniqueSeq(yaml.Str())\n                    | yaml.MapPattern(yaml.Str(), yaml.UniqueSeq(yaml.Str()))\n                    for k in self._REQUIRED_INPUT_TYPES\n                }\n            )\n            try:\n                self._yaml[\"inputs\"].revalidate(inputs_schema)\n            except yaml.YAMLValidationError:\n                raise PipelineConfigError(\n                    \"Background files must be provided if source monitoring is enabled.\"\n                )\n\n        # ensure the input files all exist\n        for input_type in inputs.keys():\n            for epoch, file_list in inputs[input_type].items():\n                for file in file_list:\n                    if not os.path.exists(file):\n                        raise PipelineConfigError(f\"{file} does not exist.\")\n\n        # ensure num_workers and num_workers_io are\n        # either None (from null in config yaml) or an integer\n        for param_name in ('num_workers', 'num_workers_io'):\n            param_value = self['processing'][param_name]\n            if (param_value is not None) and (type(param_value) is not int):\n                raise PipelineConfigError(f\"{param_name} can only be an integer or 'null'\")\n\n    def check_prev_config_diff(self) -&gt; bool:\n        \"\"\"\n        Checks if the previous config file differs from the current config file. Used in\n        add mode. Only returns true if the images are different and the other general\n        settings are the same (the requirement for add mode). Otherwise False is returned.\n\n        Returns:\n            `True` if images are different but general settings are the same,\n                otherwise `False` is returned.\n        \"\"\"\n        prev_config = PipelineConfig.from_file(\n            os.path.join(self[\"run\"][\"path\"], \"config_prev.yaml\"),\n            label=\"previous run config\",\n        )\n        if self._yaml == prev_config._yaml:\n            return True\n\n        # are the input image files different?\n        images_changed = self[\"inputs\"][\"image\"] != prev_config[\"inputs\"][\"image\"]\n\n        # are all the non-input file configs the same?\n        config_dict = self._yaml.data\n        prev_config_dict = prev_config._yaml.data\n        _ = config_dict.pop(\"inputs\")\n        _ = prev_config_dict.pop(\"inputs\")\n        settings_check = config_dict == prev_config_dict\n\n        if images_changed and settings_check:\n            return False\n        return True\n\n    def image_opts(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the config options required for image ingestion only.\n        Namely:\n            - selavy_local_rms_fill_value\n            - condon_errors\n            - ra_uncertainty\n            - dec_uncertainty\n\n        Returns:\n            The relevant key value pairs\n        \"\"\"\n        keys = [\n            \"selavy_local_rms_fill_value\",\n            \"condon_errors\",\n            \"ra_uncertainty\",\n            \"dec_uncertainty\"\n        ]\n        return {key: self[\"measurements\"][key] for key in keys}\n</code></pre>"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.PipelineConfig.__getitem__","title":"<code>__getitem__(name)</code>","text":"<p>Retrieves the requested YAML chunk as a native Python object.</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>def __getitem__(self, name: str):\n    \"\"\"Retrieves the requested YAML chunk as a native Python object.\"\"\"\n    return self._yaml[name].data\n</code></pre>"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.PipelineConfig.__init__","title":"<code>__init__(config_yaml, validate_inputs=True)</code>","text":"<p>Initialises PipelineConfig with parsed (but not necessarily validated) YAML.</p> <p>Parameters:</p> Name Type Description Default <code>config_yaml</code> <code>YAML</code> <p>Input YAML, usually the output of <code>strictyaml.load</code>.</p> required <code>validate_inputs</code> <code>bool</code> <p>Validate the config input files. Ensures that the inputs match (e.g. each image has a catalogue), and that each path exists. Set to False to skip these checks. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>PipelineConfigError</code> <p>The input YAML config violates the schema.</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>def __init__(self, config_yaml: yaml.YAML, validate_inputs: bool = True):\n    \"\"\"Initialises PipelineConfig with parsed (but not necessarily validated) YAML.\n\n    Args:\n        config_yaml (yaml.YAML): Input YAML, usually the output of `strictyaml.load`.\n        validate_inputs (bool, optional): Validate the config input files. Ensures\n            that the inputs match (e.g. each image has a catalogue), and that each\n            path exists. Set to False to skip these checks. Defaults to True.\n\n    Raises:\n        PipelineConfigError: The input YAML config violates the schema.\n    \"\"\"\n    self._yaml: yaml.YAML = config_yaml\n    # The epoch_based parameter below is for if the user has entered just lists we\n    # don't have access to the dates until the Image instances are created. So we\n    # flag this as true so that we can reorder the epochs once the date information\n    # is available. It is also recorded in the database such that there is a record\n    # of the fact that the run was processed in an epoch based mode.\n    self.epoch_based: bool\n\n    # Determine if epoch-based association should be used based on input files.\n    # If inputs have been parsed to dicts, then the user has defined their own epochs.\n    # If inputs have been parsed to lists, we must convert to dicts and auto-fill\n    # the epochs.\n\n    # ensure the inputs are valid in case .from_file(..., validate=False) was used\n    if not validate_inputs:\n        return\n\n    try:\n        self._validate_inputs()\n    except yaml.YAMLValidationError as e:\n        raise PipelineConfigError(e)\n\n    # detect simple list inputs and convert them to epoch-mode inputs\n    yaml_inputs = self._yaml[\"inputs\"]\n    inputs = yaml_inputs.data\n    for input_file_type in self._REQUIRED_INPUT_TYPES:\n        # skip missing optional input types, e.g. background\n        if (\n            not self._REQUIRED_INPUT_TYPES[input_file_type]\n            and input_file_type not in self[\"inputs\"]\n        ):\n            continue\n\n        input_files = inputs[input_file_type]\n\n        # resolve glob expressions if present\n        if isinstance(input_files, dict):\n            # must be either a glob expression, list of glob expressions, or epoch-mode\n            if \"glob\" in input_files:\n                # resolve the glob expressions\n                self.epoch_based = False\n                file_list = self._resolve_glob_expressions(\n                    yaml_inputs[input_file_type]\n                )\n                inputs[input_file_type] = self._create_input_epochs(file_list)\n            else:\n                # epoch-mode with either a list of files or glob expressions\n                self.epoch_based = True\n                for epoch in input_files:\n                    if \"glob\" in input_files[epoch]:\n                        # resolve the glob expressions\n                        file_list = self._resolve_glob_expressions(\n                            yaml_inputs[input_file_type][epoch]\n                        )\n                        inputs[input_file_type][epoch] = file_list\n        else:\n            # Epoch-based association not requested and no globs present. Replace\n            # input lists with dicts where each input file has it's own epoch.\n            self.epoch_based = False\n            inputs[input_file_type] = self._create_input_epochs(\n                input_files\n            )\n    self._yaml[\"inputs\"] = inputs\n</code></pre>"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.PipelineConfig.check_prev_config_diff","title":"<code>check_prev_config_diff()</code>","text":"<p>Checks if the previous config file differs from the current config file. Used in add mode. Only returns true if the images are different and the other general settings are the same (the requirement for add mode). Otherwise False is returned.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if images are different but general settings are the same, otherwise <code>False</code> is returned.</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>def check_prev_config_diff(self) -&gt; bool:\n    \"\"\"\n    Checks if the previous config file differs from the current config file. Used in\n    add mode. Only returns true if the images are different and the other general\n    settings are the same (the requirement for add mode). Otherwise False is returned.\n\n    Returns:\n        `True` if images are different but general settings are the same,\n            otherwise `False` is returned.\n    \"\"\"\n    prev_config = PipelineConfig.from_file(\n        os.path.join(self[\"run\"][\"path\"], \"config_prev.yaml\"),\n        label=\"previous run config\",\n    )\n    if self._yaml == prev_config._yaml:\n        return True\n\n    # are the input image files different?\n    images_changed = self[\"inputs\"][\"image\"] != prev_config[\"inputs\"][\"image\"]\n\n    # are all the non-input file configs the same?\n    config_dict = self._yaml.data\n    prev_config_dict = prev_config._yaml.data\n    _ = config_dict.pop(\"inputs\")\n    _ = prev_config_dict.pop(\"inputs\")\n    settings_check = config_dict == prev_config_dict\n\n    if images_changed and settings_check:\n        return False\n    return True\n</code></pre>"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.PipelineConfig.from_file","title":"<code>from_file(yaml_path, label='run config', validate=True, validate_inputs=True, add_defaults=True)</code>  <code>classmethod</code>","text":"<p>Create a PipelineConfig object from a run configuration YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>str</code> <p>Path to the run config YAML file.</p> required <code>label</code> <code>str</code> <p>A label for the config object that will be used in error messages. Default is \"run config\".</p> <code>'run config'</code> <code>validate</code> <code>bool</code> <p>Perform config schema validation immediately after loading the config file. If set to False, the full schema validation will not be performed until PipelineConfig.validate() is explicitly called. The inputs are always validated regardless. Defaults to True.</p> <code>True</code> <code>validate_inputs</code> <code>bool</code> <p>Validate the config input files. Ensures that the inputs match (e.g. each image has a catalogue), and that each path exists. Set to False to skip these checks. Defaults to True.</p> <code>True</code> <code>add_defaults</code> <code>bool</code> <p>Add missing configuration parameters using configured defaults. The defaults are read from the Django settings file. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>PipelineConfigError</code> <p>The run config YAML file fails schema validation.</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>@classmethod\ndef from_file(\n    cls,\n    yaml_path: str,\n    label: str = \"run config\",\n    validate: bool = True,\n    validate_inputs: bool = True,\n    add_defaults: bool = True,\n) -&gt; \"PipelineConfig\":\n    \"\"\"Create a PipelineConfig object from a run configuration YAML file.\n\n    Args:\n        yaml_path: Path to the run config YAML file.\n        label: A label for the config object that will be used in error messages.\n            Default is \"run config\".\n        validate: Perform config schema validation immediately after loading\n            the config file. If set to False, the full schema validation\n            will not be performed until PipelineConfig.validate() is\n            explicitly called. The inputs are always validated regardless.\n            Defaults to True.\n        validate_inputs: Validate the config input files. Ensures that the inputs\n            match (e.g. each image has a catalogue), and that each path exists. Set\n            to False to skip these checks. Defaults to True.\n        add_defaults: Add missing configuration parameters using configured\n            defaults. The defaults are read from the Django settings file.\n            Defaults to True.\n\n    Raises:\n        PipelineConfigError: The run config YAML file fails schema validation.\n\n    \"\"\"\n    schema = cls.SCHEMA if validate else yaml.Any()\n    with open(yaml_path) as fh:\n        config_str = fh.read()\n    try:\n        config_yaml = yaml.load(config_str, schema=schema, label=label)\n    except yaml.YAMLValidationError as e:\n        raise PipelineConfigError(e)\n\n    if add_defaults:\n        # make a template config based on defaults\n        config_defaults_str = make_config_template(\n            cls.TEMPLATE_PATH,\n            **settings.PIPE_RUN_CONFIG_DEFAULTS,\n        )\n        config_defaults_dict: Dict[str, Any] = yaml.load(config_defaults_str).data\n\n        # merge configs\n        config_dict = dict_merge(config_defaults_dict, config_yaml.data)\n        config_yaml = yaml.as_document(config_dict, schema=schema, label=label)\n    return cls(config_yaml, validate_inputs=validate_inputs)\n</code></pre>"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.PipelineConfig.image_opts","title":"<code>image_opts()</code>","text":"<p>Get the config options required for image ingestion only. Namely:     - selavy_local_rms_fill_value     - condon_errors     - ra_uncertainty     - dec_uncertainty</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The relevant key value pairs</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>def image_opts(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get the config options required for image ingestion only.\n    Namely:\n        - selavy_local_rms_fill_value\n        - condon_errors\n        - ra_uncertainty\n        - dec_uncertainty\n\n    Returns:\n        The relevant key value pairs\n    \"\"\"\n    keys = [\n        \"selavy_local_rms_fill_value\",\n        \"condon_errors\",\n        \"ra_uncertainty\",\n        \"dec_uncertainty\"\n    ]\n    return {key: self[\"measurements\"][key] for key in keys}\n</code></pre>"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.PipelineConfig.validate","title":"<code>validate(user=None)</code>","text":"<p>Perform extra validation steps not covered by the default schema validation. The following checks are performed in order. If a check fails, an exception is raised and no further checks are performed.</p> <ol> <li>All input files have the same number of epochs and the same number of files     per epoch.</li> <li>The number of input files does not exceed the configured pipeline maximum.     This is only enforced if a regular user (not staff/admin) created the run.</li> <li>There are at least two input images.</li> <li>Background input images are required if source monitoring is turned on.</li> <li>All input files exist.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>User</code> <p>Optional. The User of the request if made through the UI. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>PipelineConfigError</code> <p>a validation check failed.</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>def validate(self, user: User = None):\n    \"\"\"Perform extra validation steps not covered by the default schema validation.\n    The following checks are performed in order. If a check fails, an exception is\n    raised and no further checks are performed.\n\n    1. All input files have the same number of epochs and the same number of files\n        per epoch.\n    2. The number of input files does not exceed the configured pipeline maximum.\n        This is only enforced if a regular user (not staff/admin) created the run.\n    3. There are at least two input images.\n    4. Background input images are required if source monitoring is turned on.\n    5. All input files exist.\n\n    Args:\n        user: Optional. The User of the request if made through the UI. Defaults to\n            None.\n\n    Raises:\n        PipelineConfigError: a validation check failed.\n    \"\"\"\n    # run standard base schema validation\n    try:\n        self._yaml.revalidate(self.SCHEMA)\n    except yaml.YAMLValidationError as e:\n        raise PipelineConfigError(e)\n\n    inputs = self[\"inputs\"]\n\n    # epochs defined for images only, used as the reference list of epochs\n    epochs_image = inputs[\"image\"].keys()\n    # map input type to a set of epochs\n    epochs_by_input_type = {\n        input_type: set(inputs[input_type].keys())\n        for input_type in inputs.keys()\n    }\n    # map input type to total number of files from all epochs\n    n_files_by_input_type: Dict[str, int] = {}\n    epoch_n_files: Dict[str, Dict[str, int]] = {}\n    n_files = 0\n    for input_type, epochs_set in epochs_by_input_type.items():\n        epoch_n_files[input_type] = {}\n        n_files_by_input_type[input_type] = 0\n        for epoch in epochs_set:\n            n = len(inputs[input_type][epoch])\n            n_files_by_input_type[input_type] += n\n            epoch_n_files[input_type][epoch] = n\n            n_files += n\n\n    # Note by this point the input files have been converted to a mapping regardless\n    # of the user's input format.\n    # Ensure all input file types have the same epochs.\n    try:\n        schema = yaml.Map({epoch: yaml.Seq(yaml.Str()) for epoch in epochs_image})\n        for input_type in inputs.keys():\n            # Generate a new YAML object on-the-fly per input to avoid saving\n            # a validation schema per file in the PipelineConfig object\n            # (These can consume a lot of RAM for long lists of input files).\n            yaml.load(self._yaml[\"inputs\"][input_type].as_yaml(), schema=schema)\n    except yaml.YAMLValidationError:\n        # number of epochs could be different or the name of the epochs may not match\n        # find out which by counting the number of unique epochs per input type\n        n_epochs_per_input_type = [\n            len(epochs_set) for epochs_set in epochs_by_input_type.values()\n        ]\n        if len(set(n_epochs_per_input_type)) &gt; 1:\n            if self.epoch_based:\n                error_msg = \"The number of epochs must match for all input types.\\n\"\n            else:\n                error_msg = \"The number of files must match for all input types.\\n\"\n        else:\n            error_msg = \"The name of the epochs must match for all input types.\\n\"\n        counts_str = \"\"\n        if self.epoch_based:\n            for input_type in epoch_n_files.keys():\n                n = len(epoch_n_files[input_type])\n                counts_str += (\n                    f\"{input_type} has {n} epoch{'s' if n &gt; 1 else ''}:\"\n                    f\" {', '.join(epoch_n_files[input_type].keys())}\\n\"\n                )\n        else:\n            for input_type, n in n_files_by_input_type.items():\n                counts_str += f\"{input_type} has {n} file{'s' if n &gt; 1 else ''}\\n\"\n\n        counts_str = counts_str[:-1]\n        raise PipelineConfigError(error_msg + counts_str)\n\n    # Ensure all input file type epochs have the same number of files per epoch.\n    # This could be combined with the number of epochs validation above, but we want\n    # to give specific feedback to the user on failure.\n    try:\n        schema = yaml.Map(\n            {epoch: yaml.FixedSeq([yaml.Str()] * epoch_n_files[\"image\"][epoch])\n            for epoch in epochs_image})\n        for input_type in inputs.keys():\n            yaml.load(self._yaml[\"inputs\"][input_type].as_yaml(), schema=schema)\n    except yaml.YAMLValidationError:\n        # map input type to a mapping of epoch to file count\n        file_counts_str = \"\"\n        for input_type in inputs.keys():\n            file_counts_str += f\"{input_type}:\\n\"\n            for epoch in sorted(inputs[input_type].keys()):\n                file_counts_str += (\n                    f\"  {epoch}: {len(inputs[input_type][epoch])}\\n\"\n                )\n        file_counts_str = file_counts_str[:-1]\n        raise PipelineConfigError(\n            \"The number of files per epoch does not match between input types.\\n\"\n            + file_counts_str\n        )\n\n    # ensure the number of input files is less than the user limit\n    if user and n_files &gt; settings.MAX_PIPERUN_IMAGES:\n        if user.is_staff:\n            logger.warning(\n                \"Maximum number of images\"\n                f\" ({settings.MAX_PIPERUN_IMAGES}) rule bypassed with\"\n                \" admin status.\"\n            )\n        else:\n            raise PipelineConfigError(\n                f\"The number of images entered ({n_files})\"\n                \" exceeds the maximum number of images currently\"\n                f\" allowed ({settings.MAX_PIPERUN_IMAGES}). Please ask\"\n                \" an administrator for advice on processing your run.\"\n            )\n\n    # ensure at least two inputs are provided\n    check = [n_files_by_input_type[input_type] &lt; 2 for input_type in inputs.keys()]\n    if any(check):\n        raise PipelineConfigError(\n            \"Number of image files must to be larger than 1\"\n        )\n\n    # ensure background files are provided if source monitoring is requested\n    try:\n        monitor = self[\"source_monitoring\"][\"monitor\"]\n    except KeyError:\n        monitor = False\n\n    if monitor:\n        inputs_schema = yaml.Map(\n            {\n                k: yaml.UniqueSeq(yaml.Str())\n                | yaml.MapPattern(yaml.Str(), yaml.UniqueSeq(yaml.Str()))\n                for k in self._REQUIRED_INPUT_TYPES\n            }\n        )\n        try:\n            self._yaml[\"inputs\"].revalidate(inputs_schema)\n        except yaml.YAMLValidationError:\n            raise PipelineConfigError(\n                \"Background files must be provided if source monitoring is enabled.\"\n            )\n\n    # ensure the input files all exist\n    for input_type in inputs.keys():\n        for epoch, file_list in inputs[input_type].items():\n            for file in file_list:\n                if not os.path.exists(file):\n                    raise PipelineConfigError(f\"{file} does not exist.\")\n\n    # ensure num_workers and num_workers_io are\n    # either None (from null in config yaml) or an integer\n    for param_name in ('num_workers', 'num_workers_io'):\n        param_value = self['processing'][param_name]\n        if (param_value is not None) and (type(param_value) is not int):\n            raise PipelineConfigError(f\"{param_name} can only be an integer or 'null'\")\n</code></pre>"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.make_config_template","title":"<code>make_config_template(template_path, **kwargs)</code>","text":"<p>Generate the contents of a run configuration file from a Jinja2 template.</p> <p>Parameters:</p> Name Type Description Default <code>template_path</code> <code>str</code> <p>Path to a Jinja2 template.</p> required <code>**kwargs</code> <p>keyword arguments passed to the template renderer to fill in template variables.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Filled in template string.</p> Source code in <code>vast_pipeline/pipeline/config.py</code> <pre><code>def make_config_template(template_path: str, **kwargs) -&gt; str:\n    \"\"\"Generate the contents of a run configuration file from a Jinja2 template.\n\n    Args:\n        template_path: Path to a Jinja2 template.\n        **kwargs: keyword arguments passed to the template renderer to fill in template\n            variables.\n\n    Returns:\n        Filled in template string.\n    \"\"\"\n    with open(template_path, \"r\") as fp:\n        template_str = fp.read()\n    env = Environment(trim_blocks=True, lstrip_blocks=True)\n    template = env.from_string(template_str)\n    return template.render(**kwargs)\n</code></pre>"},{"location":"reference/pipeline/errors/","title":"errors.py","text":"<p>Defines errors for the pipeline to return.</p>"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.MaxPipelineRunsError","title":"<code>MaxPipelineRunsError</code>","text":"<p>               Bases: <code>PipelineError</code></p> <p>Error for reporting the number of concurrent jobs is maxed out.</p> Source code in <code>vast_pipeline/pipeline/errors.py</code> <pre><code>class MaxPipelineRunsError(PipelineError):\n    \"\"\"\n    Error for reporting the number of concurrent jobs is maxed out.\n    \"\"\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Returns the string representation.\n\n        Returns:\n            The string representation of the error.\n        \"\"\"\n        return 'Max pipeline concurrent runs reached!'\n</code></pre>"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.MaxPipelineRunsError.__str__","title":"<code>__str__()</code>","text":"<p>Returns the string representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the error.</p> Source code in <code>vast_pipeline/pipeline/errors.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Returns the string representation.\n\n    Returns:\n        The string representation of the error.\n    \"\"\"\n    return 'Max pipeline concurrent runs reached!'\n</code></pre>"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineConfigError","title":"<code>PipelineConfigError</code>","text":"<p>               Bases: <code>PipelineError</code></p> <p>Error for issue in the pipeline configuration</p> Source code in <code>vast_pipeline/pipeline/errors.py</code> <pre><code>class PipelineConfigError(PipelineError):\n    \"\"\"\n    Error for issue in the pipeline configuration\n    \"\"\"\n    def __init__(self, msg: Optional[str] = None):\n        \"\"\"\n        Initialises the config error.\n\n        Args:\n            msg: The error message returned by the pipeline.\n        \"\"\"\n        super(PipelineConfigError, self).__init__(msg)\n</code></pre>"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineConfigError.__init__","title":"<code>__init__(msg=None)</code>","text":"<p>Initialises the config error.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Optional[str]</code> <p>The error message returned by the pipeline.</p> <code>None</code> Source code in <code>vast_pipeline/pipeline/errors.py</code> <pre><code>def __init__(self, msg: Optional[str] = None):\n    \"\"\"\n    Initialises the config error.\n\n    Args:\n        msg: The error message returned by the pipeline.\n    \"\"\"\n    super(PipelineConfigError, self).__init__(msg)\n</code></pre>"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineError","title":"<code>PipelineError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Generic pipeline error</p> <p>Attributes:</p> Name Type Description <code>msg</code> <code>str</code> <p>The full error string to return.</p> Source code in <code>vast_pipeline/pipeline/errors.py</code> <pre><code>class PipelineError(Exception):\n    \"\"\"\n    Generic pipeline error\n\n    Attributes:\n        msg (str): The full error string to return.\n    \"\"\"\n\n    def __init__(self, msg: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Initialises the error.\n\n        Args:\n            msg: The error message returned by the pipeline.\n        \"\"\"\n        self.msg = (\n            'Pipeline error: {0}.'.format(msg) if msg else\n            'Undefined Pipeline error.'\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Returns the string representation.\n\n        Returns:\n            The string representation of the error.\n        \"\"\"\n        return self.msg\n</code></pre>"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineError.__init__","title":"<code>__init__(msg=None)</code>","text":"<p>Initialises the error.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Optional[str]</code> <p>The error message returned by the pipeline.</p> <code>None</code> Source code in <code>vast_pipeline/pipeline/errors.py</code> <pre><code>def __init__(self, msg: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Initialises the error.\n\n    Args:\n        msg: The error message returned by the pipeline.\n    \"\"\"\n    self.msg = (\n        'Pipeline error: {0}.'.format(msg) if msg else\n        'Undefined Pipeline error.'\n    )\n</code></pre>"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineError.__str__","title":"<code>__str__()</code>","text":"<p>Returns the string representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the error.</p> Source code in <code>vast_pipeline/pipeline/errors.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Returns the string representation.\n\n    Returns:\n        The string representation of the error.\n    \"\"\"\n    return self.msg\n</code></pre>"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineInitError","title":"<code>PipelineInitError</code>","text":"<p>               Bases: <code>PipelineError</code></p> <p>Error for issue in the pipeline initialisation</p> Source code in <code>vast_pipeline/pipeline/errors.py</code> <pre><code>class PipelineInitError(PipelineError):\n    \"\"\"\n    Error for issue in the pipeline initialisation\n    \"\"\"\n    def __init__(self, msg: Optional[str] = None):\n        \"\"\"\n        Initialises the init error.\n\n        Args:\n            msg: The error message returned by the pipeline.\n        \"\"\"\n        super(PipelineInitError, self).__init__(msg)\n</code></pre>"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineInitError.__init__","title":"<code>__init__(msg=None)</code>","text":"<p>Initialises the init error.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Optional[str]</code> <p>The error message returned by the pipeline.</p> <code>None</code> Source code in <code>vast_pipeline/pipeline/errors.py</code> <pre><code>def __init__(self, msg: Optional[str] = None):\n    \"\"\"\n    Initialises the init error.\n\n    Args:\n        msg: The error message returned by the pipeline.\n    \"\"\"\n    super(PipelineInitError, self).__init__(msg)\n</code></pre>"},{"location":"reference/pipeline/finalise/","title":"finalise.py","text":""},{"location":"reference/pipeline/finalise/#vast_pipeline.pipeline.finalise.calculate_measurement_pair_aggregate_metrics","title":"<code>calculate_measurement_pair_aggregate_metrics(measurement_pairs_df, min_vs, flux_type='peak')</code>","text":"<p>Calculate the aggregate maximum measurement pair variability metrics to be stored in <code>Source</code> objects. Only measurement pairs with abs(Vs metric) &gt;= <code>min_vs</code> are considered. The measurement pairs are filtered on abs(Vs metric) &gt;= <code>min_vs</code>, grouped by the source ID column <code>source</code>, then the row index of the maximum abs(m) metric is found. The absolute Vs and m metric values from this row are returned for each source.</p> <p>Parameters:</p> Name Type Description Default <code>measurement_pairs_df</code> <code>DataFrame</code> <p>The measurement pairs and their variability metrics. Must at least contain the columns: source, vs_{flux_type}, m_{flux_type}.</p> required <code>min_vs</code> <code>float</code> <p>The minimum value of the Vs metric (i.e. column <code>vs_{flux_type}</code>) the measurement pair must have to be included in the aggregate metric determination.</p> required <code>flux_type</code> <code>str</code> <p>The flux type on which to perform the aggregation, either \"peak\" or \"int\". Default is \"peak\".</p> <code>'peak'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Measurement pair aggregate metrics indexed by the source ID, <code>source</code>. The metric columns are named: <code>vs_abs_significant_max_{flux_type}</code> and <code>m_abs_significant_max_{flux_type}</code>.</p> Source code in <code>vast_pipeline/pipeline/finalise.py</code> <pre><code>def calculate_measurement_pair_aggregate_metrics(\n    measurement_pairs_df: pd.DataFrame,\n    min_vs: float,\n    flux_type: str = \"peak\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the aggregate maximum measurement pair variability metrics\n    to be stored in `Source` objects. Only measurement pairs with\n    abs(Vs metric) &gt;= `min_vs` are considered.\n    The measurement pairs are filtered on abs(Vs metric) &gt;= `min_vs`,\n    grouped by the source ID column `source`, then the row index of the\n    maximum abs(m) metric is found. The absolute Vs and m metric values from\n    this row are returned for each source.\n\n    Args:\n        measurement_pairs_df:\n            The measurement pairs and their variability metrics. Must at least\n            contain the columns: source, vs_{flux_type}, m_{flux_type}.\n        min_vs:\n            The minimum value of the Vs metric (i.e. column `vs_{flux_type}`)\n            the measurement pair must have to be included in the aggregate\n            metric determination.\n        flux_type:\n            The flux type on which to perform the aggregation, either \"peak\"\n            or \"int\". Default is \"peak\".\n\n    Returns:\n        Measurement pair aggregate metrics indexed by the source ID, `source`.\n            The metric columns are named: `vs_abs_significant_max_{flux_type}`\n            and `m_abs_significant_max_{flux_type}`.\n    \"\"\"\n    check_df = measurement_pairs_df.query(f\"abs(vs_{flux_type}) &gt;= @min_vs\")\n\n    # This check is performed due to a bug that was occuring after updating the\n    # pandas dependancy (1.4) when performing the tests. The bug was that the\n    # grouby and agg stage below was being performed on an empty series in the\n    # basic association test and causing a failure. Hence this only performs\n    # the groupby if the original query dataframe is not empty.\n    if check_df.empty:\n        pair_agg_metrics = pd.DataFrame(\n            columns=[f\"vs_{flux_type}\", f\"m_{flux_type}\", \"source\"]\n        )\n    else:\n        pair_agg_metrics = measurement_pairs_df.iloc[\n            check_df\n            .groupby(\"source\")\n            .agg(m_abs_max_idx=(f\"m_{flux_type}\", lambda x: x.abs().idxmax()),)\n            # cast row indices to int and select them\n            .astype(np.int32)[\"m_abs_max_idx\"]\n            .reset_index(drop=True)  # keep only the row indices\n        ][[f\"vs_{flux_type}\", f\"m_{flux_type}\", \"source\"]]\n\n    pair_agg_metrics = pair_agg_metrics.abs().rename(columns={\n        f\"vs_{flux_type}\": f\"vs_abs_significant_max_{flux_type}\",\n        f\"m_{flux_type}\": f\"m_abs_significant_max_{flux_type}\",\n    }).set_index('source')\n    return pair_agg_metrics\n</code></pre>"},{"location":"reference/pipeline/finalise/#vast_pipeline.pipeline.finalise.final_operations","title":"<code>final_operations(sources_df, p_run, new_sources_df, calculate_pairs, source_aggregate_pair_metrics_min_abs_vs, add_mode, done_source_ids, previous_parquets, n_cpu=0, max_partition_mb=15)</code>","text":"<p>Performs the final operations of the pipeline: - Calculates the statistics for the final sources. - Uploads sources and writes parquet. - Uploads related sources and writes parquet. - Uploads associations and writes parquet.</p> <p>Parameters:</p> Name Type Description Default <code>sources_df</code> <code>DataFrame</code> <p>The main sources_df dataframe produced from the pipeline. Contains all measurements and the association information. The <code>id</code> column is the Measurement object primary key that has already been saved to the database.</p> required <code>p_run</code> <code>Run</code> <p>The pipeline Run object of which the sources are associated with.</p> required <code>new_sources_df</code> <code>DataFrame</code> <p>The new sources dataframe, only contains the 'new_source_high_sigma' column (source_id is the index).</p> required <code>calculate_pairs</code> <code>bool</code> <p>Whether to calculate the measurement pairs and their 2-epoch metrics, Vs and m.</p> required <code>source_aggregate_pair_metrics_min_abs_vs</code> <code>float</code> <p>Only measurement pairs where the Vs metric exceeds this value are selected for the aggregate pair metrics that are stored in <code>Source</code> objects.</p> required <code>add_mode</code> <code>bool</code> <p>Whether the pipeline is running in add mode.</p> required <code>done_source_ids</code> <code>List[int]</code> <p>A list containing the source ids that have already been uploaded in the previous run in add mode.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of sources contained in the pipeline run (used in the next steps of main.py).</p> <code>int</code> <p>The number of new sources contained in the pipeline run (used in the next steps of main.py).</p> Source code in <code>vast_pipeline/pipeline/finalise.py</code> <pre><code>def final_operations(\n    sources_df: pd.DataFrame,\n    p_run: Run,\n    new_sources_df: pd.DataFrame,\n    calculate_pairs: bool,\n    source_aggregate_pair_metrics_min_abs_vs: float,\n    add_mode: bool,\n    done_source_ids: List[int],\n    previous_parquets: Dict[str, str],\n    n_cpu: int = 0,\n    max_partition_mb: int = 15\n) -&gt; Tuple[int, int]:\n    \"\"\"\n    Performs the final operations of the pipeline:\n    - Calculates the statistics for the final sources.\n    - Uploads sources and writes parquet.\n    - Uploads related sources and writes parquet.\n    - Uploads associations and writes parquet.\n\n    Args:\n        sources_df:\n            The main sources_df dataframe produced from the pipeline.\n            Contains all measurements and the association information.\n            The `id` column is the Measurement object primary key that has\n            already been saved to the database.\n        p_run:\n            The pipeline Run object of which the sources are associated with.\n        new_sources_df:\n            The new sources dataframe, only contains the\n            'new_source_high_sigma' column (source_id is the index).\n        calculate_pairs:\n            Whether to calculate the measurement pairs and their 2-epoch\n            metrics, Vs and m.\n        source_aggregate_pair_metrics_min_abs_vs:\n            Only measurement pairs where the Vs metric exceeds this value\n            are selected for the aggregate pair metrics that are stored in\n            `Source` objects.\n        add_mode:\n            Whether the pipeline is running in add mode.\n        done_source_ids:\n            A list containing the source ids that have already been uploaded\n            in the previous run in add mode.\n\n    Returns:\n        The number of sources contained in the pipeline run (used in the next\n            steps of main.py).\n        The number of new sources contained in the pipeline run (used in the\n            next steps of main.py).\n    \"\"\"\n    timer = StopWatch()\n\n    # calculate source fields\n    logger.info(\n        'Calculating statistics for %i sources...',\n        sources_df.source.unique().shape[0]\n    )\n    log_total_memory_usage()\n\n    srcs_df = parallel_groupby(sources_df,\n                               n_cpu=n_cpu,\n                               max_partition_mb=max_partition_mb)\n\n    mem_usage = get_df_memory_usage(srcs_df)\n    logger.info('Groupby-apply time: %.2f seconds', timer.reset())\n    logger.debug(f\"Initial srcs_df memory: {mem_usage}MB\")\n    log_total_memory_usage()\n\n    # add new sources\n    srcs_df[\"new\"] = srcs_df.index.isin(new_sources_df.index)\n    srcs_df = pd.merge(\n        srcs_df,\n        new_sources_df[\"new_high_sigma\"],\n        left_on=\"source\",\n        right_index=True,\n        how=\"left\",\n    )\n    srcs_df[\"new_high_sigma\"] = srcs_df[\"new_high_sigma\"].fillna(0.0)\n\n    mem_usage = get_df_memory_usage(srcs_df)\n    logger.debug(f\"srcs_df memory after adding new sources: {mem_usage}MB\")\n    log_total_memory_usage()\n\n    # calculate nearest neighbour\n    srcs_skycoord = SkyCoord(\n        srcs_df['wavg_ra'].values,\n        srcs_df['wavg_dec'].values,\n        unit=(u.deg, u.deg)\n    )\n    idx, d2d, _ = srcs_skycoord.match_to_catalog_sky(\n        srcs_skycoord,\n        nthneighbor=2\n    )\n\n    # add the separation distance in degrees\n    srcs_df['n_neighbour_dist'] = d2d.deg\n\n    mem_usage = get_df_memory_usage(srcs_df)\n    logger.debug(f\"srcs_df memory after nearest-neighbour: {mem_usage}MB\")\n    log_total_memory_usage()\n\n    # create measurement pairs, aka 2-epoch metrics\n    if calculate_pairs:\n        timer.reset()\n        measurement_pairs_df = calculate_measurement_pair_metrics(\n            sources_df,\n            n_cpu=n_cpu,\n            max_partition_mb=max_partition_mb)\n        logger.info(\n            'Measurement pair metrics time: %.2f seconds',\n            timer.reset())\n        mem_usage = get_df_memory_usage(measurement_pairs_df)\n        logger.debug(f\"measurment_pairs_df memory: {mem_usage}MB\")\n        log_total_memory_usage()\n\n        # calculate measurement pair metric aggregates for sources by finding\n        # the row indices of the aggregate max of the abs(m) metric for each\n        # flux type.\n        pair_agg_metrics = pd.merge(\n            calculate_measurement_pair_aggregate_metrics(\n                measurement_pairs_df,\n                source_aggregate_pair_metrics_min_abs_vs,\n                flux_type=\"peak\",\n            ),\n            calculate_measurement_pair_aggregate_metrics(\n                measurement_pairs_df,\n                source_aggregate_pair_metrics_min_abs_vs,\n                flux_type=\"int\",\n            ),\n            how=\"outer\",\n            left_index=True,\n            right_index=True,\n        )\n\n        # join with sources and replace agg metrics NaNs with 0 as the\n        # DataTables API JSON serialization doesn't like them\n        srcs_df = srcs_df.join(pair_agg_metrics).fillna(value={\n            \"vs_abs_significant_max_peak\": 0.0,\n            \"m_abs_significant_max_peak\": 0.0,\n            \"vs_abs_significant_max_int\": 0.0,\n            \"m_abs_significant_max_int\": 0.0,\n        })\n        logger.info(\n            \"Measurement pair aggregate metrics time: %.2f seconds\",\n            timer.reset())\n        mem_usage = get_df_memory_usage(srcs_df)\n        logger.debug(f\"srcs_df memory after calculate_pairs: {mem_usage}MB\")\n        log_total_memory_usage()\n    else:\n        logger.info(\n            \"Skipping measurement pair metric calculation as specified in \"\n            \"the run configuration.\"\n        )\n\n    # upload sources to DB, column 'id' with DB id is contained in return\n    if add_mode:\n        # if add mode is being used some sources need to updated where as some\n        # need to be newly uploaded.\n        # upload new ones first (new id's are fetched)\n        src_done_mask = srcs_df.index.isin(done_source_ids)\n        srcs_df_upload = srcs_df.loc[~src_done_mask].copy()\n\n        mem_usage = get_df_memory_usage(srcs_df_upload)\n        logger.debug(f\"srcs_df_upload initial memory: {mem_usage}MB\")\n        log_total_memory_usage()\n\n        srcs_df_upload = make_upload_sources(srcs_df_upload, p_run, add_mode)\n\n        mem_usage = get_df_memory_usage(srcs_df_upload)\n        logger.debug(f\"srcs_df_upload memory after upload: {mem_usage}MB\")\n        log_total_memory_usage()\n\n        # And now update\n        srcs_df_update = srcs_df.loc[src_done_mask].copy()\n        logger.info(\n            f\"Updating {srcs_df_update.shape[0]} sources with new metrics.\")\n        mem_usage = get_df_memory_usage(srcs_df_update)\n        logger.debug(f\"srcs_df_update memory: {mem_usage}MB\")\n        log_total_memory_usage()\n\n        srcs_df = update_sources(srcs_df_update, batch_size=1000)\n        mem_usage = get_df_memory_usage(srcs_df_update)\n        logger.debug(f\"srcs_df_update memory: {mem_usage}MB\")\n        log_total_memory_usage()\n        # Add back together\n        if not srcs_df_upload.empty:\n            srcs_df = pd.concat([srcs_df, srcs_df_upload])\n    else:\n        srcs_df = make_upload_sources(srcs_df, p_run, add_mode)\n\n    mem_usage = get_df_memory_usage(srcs_df)\n    logger.debug(f\"srcs_df memory after uploading sources: {mem_usage}MB\")\n    log_total_memory_usage()\n\n    # gather the related df, upload to db and save to parquet file\n    # the df will look like\n    #\n    #         from_source_id  to_source_id\n    # source\n    # 714     60              14396\n    # 1211    94              12961\n    #\n    # the index ('source') has the initial id generated by the pipeline to\n    # identify unique sources, the 'from_source_id' column has the django\n    # model id (in db), the 'to_source_id' has the pipeline index\n\n    related_df = (\n        srcs_df.loc[srcs_df[\"related_list\"] != -1, [\"id\", \"related_list\"]]\n        .explode(\"related_list\")\n        .rename(columns={\"id\": \"from_source_id\",\n                         \"related_list\": \"to_source_id\"\n                         })\n    )\n\n    # for the column 'from_source_id', replace relation source ids with db id\n    related_df[\"to_source_id\"] = related_df[\"to_source_id\"].map(\n        srcs_df[\"id\"].to_dict())\n    # drop relationships with the same source\n    related_df = related_df[related_df[\"from_source_id\"]\n                            != related_df[\"to_source_id\"]]\n\n    # write symmetrical relations to parquet\n    related_df.to_parquet(\n        os.path.join(p_run.path, 'relations.parquet'),\n        index=False\n    )\n\n    # upload the relations to DB\n    # check for add_mode first\n    if add_mode:\n        # Load old relations so the already uploaded ones can be removed\n        old_relations = (\n            pd.read_parquet(previous_parquets['relations'])\n        )\n\n        related_df = (\n            pd.concat([related_df, old_relations], ignore_index=True)\n            .drop_duplicates(keep=False)\n        )\n        logger.debug(f'Add mode: #{related_df.shape[0]} relations to upload.')\n\n    make_upload_related_sources(related_df)\n\n    del related_df\n\n    # write sources to parquet file\n    srcs_df = srcs_df.drop([\"related_list\", \"img_list\"], axis=1)\n    (\n        # set the index to db ids, dropping the source idx\n        srcs_df.set_index('id')\n        .to_parquet(os.path.join(p_run.path, 'sources.parquet'))\n    )\n\n    # update measurements with sources to get associations\n    sources_df = (\n        sources_df.drop('related', axis=1)\n        .merge(srcs_df.rename(columns={'id': 'source_id'}), on='source')\n    )\n\n    mem_usage = get_df_memory_usage(sources_df)\n    logger.debug(f\"sources_df memory after srcs_df merge: {mem_usage}MB\")\n    log_total_memory_usage()\n\n    if add_mode:\n        # Load old associations so the already uploaded ones can be removed\n        old_assoications = (\n            pd.read_parquet(previous_parquets['associations'])\n            .rename(columns={'meas_id': 'id'})\n        )\n        sources_df_upload = pd.concat(\n            [sources_df, old_assoications],\n            ignore_index=True\n        )\n        sources_df_upload = sources_df_upload.drop_duplicates(\n            ['source_id', 'id', 'd2d', 'dr'], keep=False\n        )\n        logger.debug(\n            f'Add mode: #{sources_df_upload.shape[0]} associations to upload.')\n    else:\n        sources_df_upload = sources_df\n\n    # upload associations into DB\n    make_upload_associations(sources_df_upload)\n\n    # write associations to parquet file\n    sources_df[['source_id', 'id', 'd2d', 'dr']]. \\\n        rename(columns={'id': 'meas_id'}). \\\n            to_parquet(os.path.join(p_run.path, 'associations.parquet'))\n\n    if calculate_pairs:\n        # get the Source object primary keys for the measurement pairs\n        measurement_pairs_df = measurement_pairs_df.join(\n            srcs_df.id.rename(\"source_id\"), on=\"source\"\n        )\n\n        # optimize measurement pair DataFrame and save to parquet file\n        measurement_pairs_df = optimize_ints(\n            optimize_floats(\n                measurement_pairs_df.drop(columns=[\"source\"]).rename(\n                    columns={\"id_a\": \"meas_id_a\", \"id_b\": \"meas_id_b\"}\n                )\n            )\n        )\n        measurement_pairs_df.to_parquet(\n            os.path.join(p_run.path, \"measurement_pairs.parquet\"), index=False\n        )\n\n    logger.info(\n        \"Total final operations time: %.2f seconds\",\n        timer.reset_init())\n\n    nr_sources = srcs_df[\"id\"].count()\n    nr_new_sources = srcs_df['new'].sum()\n\n    # calculate and return total number of extracted sources\n    return (nr_sources, nr_new_sources)\n</code></pre>"},{"location":"reference/pipeline/forced_extraction/","title":"forced_extraction.py","text":""},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.extract_from_image","title":"<code>extract_from_image(df, image, background, noise, edge_buffer, cluster_threshold, allow_nan, **kwargs)</code>","text":"<p>Extract the flux, its erros and chi squared data from the image files (image FIT, background and noise files) and return a dictionary with the dataframe and image name</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak]</p> required <code>image</code> <code>str</code> <p>a string with the path of the image FIT file</p> required <code>background</code> <code>str</code> <p>a string with the path of the image background file</p> required <code>noise</code> <code>str</code> <p>a string with the path of the image noise file</p> required <code>edge_buffer</code> <code>float</code> <p>flag to pass to ForcedPhot.measure method</p> required <code>cluster_threshold</code> <code>float</code> <p>flag to pass to ForcedPhot.measure method</p> required <code>allow_nan</code> <code>bool</code> <p>flag to pass to ForcedPhot.measure method</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary with input dataframe with added columns (flux_int, flux_int_err, chi_squared_fit) and image name.</p> Source code in <code>vast_pipeline/pipeline/forced_extraction.py</code> <pre><code>def extract_from_image(\n    df: pd.DataFrame,\n    image: str,\n    background: str,\n    noise: str,\n    edge_buffer: float,\n    cluster_threshold: float,\n    allow_nan: bool,\n    **kwargs,\n) -&gt; Dict:\n    \"\"\"\n    Extract the flux, its erros and chi squared data from the image\n    files (image FIT, background and noise files) and return a dictionary\n    with the dataframe and image name\n\n    Args:\n        df:\n            input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec,\n            image_name, flux_peak]\n        image:\n            a string with the path of the image FIT file\n        background:\n            a string with the path of the image background file\n        noise:\n            a string with the path of the image noise file\n        edge_buffer:\n            flag to pass to ForcedPhot.measure method\n        cluster_threshold:\n            flag to pass to ForcedPhot.measure method\n        allow_nan:\n            flag to pass to ForcedPhot.measure method\n\n    Returns:\n        Dictionary with input dataframe with added columns (flux_int,\n            flux_int_err, chi_squared_fit) and image name.\n    \"\"\"\n    timer = StopWatch()\n\n    # create the skycoord obj to pass to the forced extraction\n    # see usage https://github.com/dlakaplan/forced_phot\n    P_islands = SkyCoord(\n        df['wavg_ra'].values,\n        df['wavg_dec'].values,\n        unit=(u.deg, u.deg)\n    )\n    # load the image, background and noisemaps into memory\n    # a dedicated function may seem unneccesary, but will be useful if we\n    # split the load to a separate thread.\n    forcedphot_input = _forcedphot_preload(image,\n                                           background,\n                                           noise,\n                                           memmap=False\n                                           )\n    FP = ForcedPhot(*forcedphot_input)\n\n    flux, flux_err, chisq, DOF, cluster_id = FP.measure(\n        P_islands,\n        cluster_threshold=cluster_threshold,\n        allow_nan=allow_nan,\n        edge_buffer=edge_buffer\n    )\n    df['flux_int'] = flux * 1.e3\n    df['flux_int_err'] = flux_err * 1.e3\n    df['chi_squared_fit'] = chisq\n\n    logger.debug(f\"Time to measure FP for {image}: {timer.reset()}s\")\n\n    return {'df': df, 'image': df['image_name'].iloc[0]}\n</code></pre>"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.finalise_forced_dfs","title":"<code>finalise_forced_dfs(df, prefix, max_id, beam_bmaj, beam_bmin, beam_bpa, id, datetime, image)</code>","text":"<p>Compute populate leftover columns for the dataframe with forced photometry data given the input parameters</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak, flux_int, flux_int_err, chi_squared_fit]</p> required <code>prefix</code> <code>str</code> <p>string to use to generate the 'island_id' column</p> required <code>max_id</code> <code>int</code> <p>integer to use to generate the 'island_id' column</p> required <code>beam_bmaj</code> <code>float</code> <p>image beam major axis</p> required <code>beam_bmin</code> <code>float</code> <p>image beam minor axis</p> required <code>beam_bpa</code> <code>float</code> <p>image beam position angle</p> required <code>id</code> <code>int</code> <p>image id in database</p> required <code>datetime</code> <code>datetime</code> <p>timestamp of the image file (from header)</p> required <code>image</code> <code>str</code> <p>string with the image name</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Input dataframe with added columns island_id, component_id, name, bmaj, bmin, pa, image_id, time.</p> Source code in <code>vast_pipeline/pipeline/forced_extraction.py</code> <pre><code>def finalise_forced_dfs(\n    df: pd.DataFrame, prefix: str, max_id: int, beam_bmaj: float,\n    beam_bmin: float, beam_bpa: float, id: int, datetime: datetime.datetime,\n    image: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute populate leftover columns for the dataframe with forced\n    photometry data given the input parameters\n\n    Args:\n        df:\n            input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec,\n            image_name, flux_peak, flux_int, flux_int_err, chi_squared_fit]\n        prefix:\n            string to use to generate the 'island_id' column\n        max_id:\n            integer to use to generate the 'island_id' column\n        beam_bmaj:\n            image beam major axis\n        beam_bmin:\n            image beam minor axis\n        beam_bpa:\n            image beam position angle\n        id:\n            image id in database\n        datetime:\n            timestamp of the image file (from header)\n        image:\n            string with the image name\n\n    Returns:\n        Input dataframe with added columns island_id, component_id,\n            name, bmaj, bmin, pa, image_id, time.\n    \"\"\"\n    # make up the measurements name from the image island_id and component_id\n    df['island_id'] = np.char.add(\n        prefix,\n        np.arange(max_id, max_id + df.shape[0]).astype(str)\n    )\n    df['component_id'] = df['island_id'].str.replace(\n        'island', 'component'\n    ) + 'a'\n    df['name'] = df['component_id']\n    # assign all the other columns\n    # convert fluxes to mJy\n    # store source bmaj and bmin in arcsec\n    df['bmaj'] = beam_bmaj * 3600.\n    df['bmin'] = beam_bmin * 3600.\n    df['pa'] = beam_bpa\n    # add image id and time\n    df['image_id'] = id\n    df['time'] = datetime\n\n    return df\n</code></pre>"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.forced_extraction","title":"<code>forced_extraction(sources_df, cfg_err_ra, cfg_err_dec, p_run, extr_df, min_sigma, edge_buffer, cluster_threshold, allow_nan, add_mode, done_images_df, done_source_ids, n_cpu=5)</code>","text":"<p>Check and extract expected measurements, and associated them with the related source(s).</p> <p>Parameters:</p> Name Type Description Default <code>sources_df</code> <code>DataFrame</code> <p>Dataframe containing all the extracted measurements and associations (product from association step).</p> required <code>cfg_err_ra</code> <code>float</code> <p>The minimum RA error from the config file (in degrees).</p> required <code>cfg_err_dec</code> <code>float</code> <p>The minimum declination error from the config file (in degrees).</p> required <code>p_run</code> <code>Run</code> <p>The pipeline run object.</p> required <code>extr_df</code> <code>DataFrame</code> <p>The dataframe containing the information on what sources are missing from which images (output from get_src_skyregion_merged_df in main.py).</p> required <code>min_sigma</code> <code>float</code> <p>Minimum sigma value to drop forced extracted measurements.</p> required <code>edge_buffer</code> <code>float</code> <p>Flag to pass to ForcedPhot.measure method.</p> required <code>cluster_threshold</code> <code>float</code> <p>Flag to pass to ForcedPhot.measure method.</p> required <code>allow_nan</code> <code>bool</code> <p>Flag to pass to ForcedPhot.measure method.</p> required <code>add_mode</code> <code>bool</code> <p>True when the pipeline is running in add image mode.</p> required <code>done_images_df</code> <code>DataFrame</code> <p>Dataframe containing the images that thave already been processed in a previous run (used in add image mode).</p> required <code>done_source_ids</code> <code>List[int]</code> <p>List of the source ids that were already present in the previous run (used in add image mode).</p> required <code>n_cpu</code> <code>int</code> <p>The desired number of workers for Dask.</p> <code>5</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The <code>sources_df</code> with the extracted sources added.</p> <code>int</code> <p>The total number of forced measurements present in the run.</p> Source code in <code>vast_pipeline/pipeline/forced_extraction.py</code> <pre><code>def forced_extraction(\n    sources_df: pd.DataFrame, cfg_err_ra: float, cfg_err_dec: float,\n    p_run: Run, extr_df: pd.DataFrame, min_sigma: float, edge_buffer: float,\n    cluster_threshold: float, allow_nan: bool, add_mode: bool,\n    done_images_df: pd.DataFrame, done_source_ids: List[int],\n    n_cpu: int = 5\n) -&gt; Tuple[pd.DataFrame, int]:\n    \"\"\"\n    Check and extract expected measurements, and associated them with the\n    related source(s).\n\n    Args:\n        sources_df:\n            Dataframe containing all the extracted measurements and\n            associations (product from association step).\n        cfg_err_ra:\n            The minimum RA error from the config file (in degrees).\n        cfg_err_dec:\n            The minimum declination error from the config file (in degrees).\n        p_run:\n            The pipeline run object.\n        extr_df:\n            The dataframe containing the information on what sources are\n            missing from which images (output from\n            get_src_skyregion_merged_df in main.py).\n        min_sigma:\n            Minimum sigma value to drop forced extracted measurements.\n        edge_buffer:\n            Flag to pass to ForcedPhot.measure method.\n        cluster_threshold:\n            Flag to pass to ForcedPhot.measure method.\n        allow_nan:\n            Flag to pass to ForcedPhot.measure method.\n        add_mode:\n            True when the pipeline is running in add image mode.\n        done_images_df:\n            Dataframe containing the images that thave already been processed\n            in a previous run (used in add image mode).\n        done_source_ids:\n            List of the source ids that were already present in the previous\n            run (used in add image mode).\n        n_cpu:\n            The desired number of workers for Dask.\n\n    Returns:\n        The `sources_df` with the extracted sources added.\n        The total number of forced measurements present in the run.\n    \"\"\"\n    logger.info(\n        'Starting force extraction step.'\n    )\n\n    timer = StopWatch()\n\n    # get all the skyregions and related images\n    cols = [\n        'id', 'name', 'measurements_path', 'path', 'noise_path',\n        'beam_bmaj', 'beam_bmin', 'beam_bpa', 'background_path',\n        'rms_min', 'datetime', 'skyreg__centre_ra',\n        'skyreg__centre_dec', 'skyreg__xtr_radius'\n    ]\n\n    images_df = pd.DataFrame(list(\n        Image.objects.filter(\n            run=p_run\n        ).select_related('skyreg').order_by('datetime').values(*tuple(cols))\n    )).set_index('name')\n# | name                          |   id | measurements_path   | path         | noise_path   |\n# |:------------------------------|-----:|:--------------------|:-------------|:-------------|\n# | VAST_2118-06A.EPOCH01.I.fits  |    1 | path/to/file        | path/to/file | path/to/file |\n# | VAST_2118-06A.EPOCH03x.I.fits |    3 | path/to/file        | path/to/file | path/to/file |\n# | VAST_2118-06A.EPOCH02.I.fits  |    2 | path/to/file        | path/to/file | path/to/file |\n\n# | name                          |   beam_bmaj |   beam_bmin |   beam_bpa | background_path   |\n# |:------------------------------|------------:|------------:|-----------:|:------------------|\n# | VAST_2118-06A.EPOCH01.I.fits  |  0.00589921 |  0.00326088 |   -70.4032 | path/to/file      |\n# | VAST_2118-06A.EPOCH03x.I.fits |  0.00470991 |  0.00300502 |   -83.1128 | path/to/file      |\n# | VAST_2118-06A.EPOCH02.I.fits  |  0.00351331 |  0.00308565 |    77.2395 | path/to/file      |\n\n# | name                          |   rms_min | datetime                         |   skyreg__centre_ra |   skyreg__centre_dec |   skyreg__xtr_radius |\n# |:------------------------------|----------:|:---------------------------------|--------------------:|---------------------:|---------------------:|\n# | VAST_2118-06A.EPOCH01.I.fits  |  0.173946 | 2019-08-27 18:12:16.700000+00:00 |             319.652 |              -6.2989 |               6.7401 |\n# | VAST_2118-06A.EPOCH03x.I.fits |  0.165395 | 2019-10-29 10:01:20.500000+00:00 |             319.652 |              -6.2989 |               6.7401 |\n# | VAST_2118-06A.EPOCH02.I.fits  |  0.16323  | 2019-10-30 08:31:20.200000+00:00 |             319.652 |              -6.2989 |               6.7401 |\n\n    # Explode out the img_diff column.\n    extr_df = extr_df.explode('img_diff').reset_index()\n    total_to_extract = extr_df.shape[0]\n\n    if add_mode:\n        # If we are adding images to the run we assume that monitoring was\n        # also performed before (enforced by the pre-run checks) so now we\n        # only want to force extract in three situations:\n        # 1. Any force extraction in a new image.\n        # 2. The forced extraction is attached to a new source from the new\n        # images.\n        # 3. A new relation has been created and they need the forced\n        # measuremnts filled in (actually covered by 2.)\n\n        extr_df = pd.concat(\n            [\n                extr_df[~extr_df['img_diff'].isin(done_images_df['name'])],\n                extr_df[\n                    (~extr_df['source'].isin(done_source_ids))\n                    &amp; (extr_df['img_diff'].isin(done_images_df.name))\n                ]\n            ]\n        ).sort_index()\n\n        logger.info(\n            f\"{extr_df.shape[0]} new measurements to force extract\"\n            f\" (from {total_to_extract} total)\"\n        )\n\n    # Don't care about n_partitions in this step\n    n_workers, _ = calculate_workers_and_partitions(None, n_cpu)\n\n    timer.reset()\n    extr_df = parallel_extraction(\n        extr_df, images_df, sources_df[['source', 'image', 'flux_peak']],\n        min_sigma, edge_buffer, cluster_threshold, allow_nan, add_mode,\n        p_run.path, n_workers=n_workers\n    )\n    logger.info(\n        'Force extraction step time: %.2f seconds', timer.reset()\n    )\n\n    # make measurement names unique for db constraint\n    extr_df['name'] = extr_df['name'] + f'_f_run{p_run.id:03d}'\n\n    # select sensible flux values and set the columns with fix values\n    values = {\n        'flux_int': 0,\n        'flux_int_err': 0\n    }\n    extr_df = extr_df.fillna(value=values)\n\n    extr_df = extr_df[\n        (extr_df['flux_int'] != 0)\n        &amp; (extr_df['flux_int_err'] != 0)\n        &amp; (extr_df['chi_squared_fit'] != np.inf)\n        &amp; (extr_df['chi_squared_fit'] != np.nan)\n    ]\n\n    default_pos_err = settings.POS_DEFAULT_MIN_ERROR / 3600.\n    extr_df['ra_err'] = default_pos_err\n    extr_df['dec_err'] = default_pos_err\n    extr_df['err_bmaj'] = 0.\n    extr_df['err_bmin'] = 0.\n    extr_df['err_pa'] = 0.\n    extr_df['ew_sys_err'] = cfg_err_ra\n    extr_df['ns_sys_err'] = cfg_err_dec\n    extr_df['error_radius'] = 0.\n\n    extr_df['uncertainty_ew'] = np.hypot(\n        cfg_err_ra,\n        default_pos_err\n    )\n    extr_df['weight_ew'] = 1. / extr_df['uncertainty_ew'].values**2\n    extr_df['uncertainty_ns'] = np.hypot(\n        cfg_err_dec,\n        default_pos_err\n    )\n    extr_df['weight_ns'] = 1. / extr_df['uncertainty_ns'].values**2\n\n    extr_df['flux_peak'] = extr_df['flux_int']\n    extr_df['flux_peak_err'] = extr_df['flux_int_err']\n    extr_df['local_rms'] = extr_df['flux_int_err']\n    extr_df['snr'] = (\n        extr_df['flux_peak'].values\n        / extr_df['local_rms'].values\n    )\n    extr_df['spectral_index'] = 0.\n    extr_df['dr'] = 0.\n    extr_df['d2d'] = 0.\n    extr_df['forced'] = True\n    extr_df['compactness'] = 1.\n    extr_df['psf_bmaj'] = extr_df['bmaj']\n    extr_df['psf_bmin'] = extr_df['bmin']\n    extr_df['psf_pa'] = extr_df['pa']\n    extr_df['flag_c4'] = False\n    extr_df['spectral_index_from_TT'] = False\n    extr_df['has_siblings'] = False\n    extr_df['flux_int_isl_ratio'] = 1.0\n    extr_df['flux_peak_isl_ratio'] = 1.0\n\n    col_order = read_schema(\n        images_df.iloc[0]['measurements_path']\n    ).names\n    col_order.remove('id')\n\n    remaining = list(set(extr_df.columns) - set(col_order))\n\n    extr_df = extr_df[col_order + remaining]\n\n    # upload the measurements, a column 'id' is returned with the DB id\n    extr_df = make_upload_measurements(extr_df)\n\n    extr_df = extr_df.rename(columns={'source_tmp_id': 'source'})\n\n    # write forced measurements to specific parquet\n    logger.info(\n        'Saving forced measurements to specific parquet file...'\n    )\n    write_forced_parquet(extr_df, p_run.path, add_mode)\n\n    # Required to rename this column for the image add mode.\n    extr_df = extr_df.rename(columns={'time': 'datetime'})\n\n    # append new meas into main df and proceed with source groupby etc\n    sources_df = pd.concat(\n        [\n            sources_df,\n            extr_df.loc[:, extr_df.columns.isin(sources_df.columns)]\n        ],\n        ignore_index=True\n    )\n\n    # get the number of forced extractions for the run\n    forced_parquets = glob(\n        os.path.join(p_run.path, \"forced_measurements*.parquet\"))\n    if forced_parquets:\n        n_forced = (\n            dd.read_parquet(forced_parquets, columns=['id'])\n            .count()\n            .compute(num_workers=n_workers, scheduler='processes')\n            .values[0]\n        )\n    else:\n        n_forced = 0\n\n    logger.info(\n        'Total forced extraction time: %.2f seconds', timer.reset_init()\n    )\n    return sources_df, n_forced\n</code></pre>"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.get_data_from_parquet","title":"<code>get_data_from_parquet(file_and_image_id, p_run_path, add_mode=False)</code>","text":"<p>Get the prefix, max id and image id from the measurements parquets</p> <p>Parameters:</p> Name Type Description Default <code>file_and_image_id</code> <code>Tuple[str, int]</code> <p>a tuple containing the path of the measurements parquet file and the image ID.</p> required <code>p_run_path</code> <code>str</code> <p>Pipeline run path to get forced parquet in case of add mode.</p> required <code>add_mode</code> <code>bool</code> <p>Whether image add mode is being used where the forced parquet needs to be used instead.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary with prefix string, an interger max_id and a string with the id of the image.</p> Source code in <code>vast_pipeline/pipeline/forced_extraction.py</code> <pre><code>def get_data_from_parquet(\n    file_and_image_id: Tuple[str, int], p_run_path: str, add_mode: bool = False\n) -&gt; Dict:\n    '''\n    Get the prefix, max id and image id from the measurements parquets\n\n    Args:\n        file_and_image_id:\n            a tuple containing the path of the measurements parquet file and\n            the image ID.\n        p_run_path:\n            Pipeline run path to get forced parquet in case of add mode.\n        add_mode:\n            Whether image add mode is being used where the forced parquet\n            needs to be used instead.\n\n    Returns:\n        Dictionary with prefix string, an interger max_id and a string with the\n            id of the image.\n    '''\n    file, image_id = file_and_image_id\n    if add_mode:\n        image_name = file.split(\"/\")[-2]\n        forced_parquet = os.path.join(\n            p_run_path,\n            f\"forced_measurements_{image_name}.parquet\"\n        )\n        if os.path.isfile(forced_parquet):\n            file = forced_parquet\n    # get max component id from parquet file\n    df = pd.read_parquet(file, columns=['island_id', 'image_id'])\n    if len(df) &gt; 0:\n        prefix = df['island_id'].iloc[0].rsplit('_', maxsplit=1)[0] + '_'\n        max_id = (\n            df['island_id'].str.rsplit('_', n=1)\n            .str.get(-1)\n            .astype(int)\n            .values.max() + 1\n        )\n    else:\n        prefix = \"island_\"\n        max_id = 1\n    return {'prefix': prefix, 'max_id': max_id, 'id': image_id}\n</code></pre>"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.parallel_extraction","title":"<code>parallel_extraction(df, df_images, df_sources, min_sigma, edge_buffer, cluster_threshold, allow_nan, add_mode, p_run_path, n_workers=5)</code>","text":"<p>Parallelize forced extraction with Dask</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with columns 'wavg_ra', 'wavg_dec', 'img_diff', 'detection'</p> required <code>df_images</code> <code>DataFrame</code> <p>dataframe with the images data and columns 'id', 'measurements_path', 'path', 'noise_path', 'beam_bmaj', 'beam_bmin', 'beam_bpa', 'background_path', 'rms_min', 'datetime', 'skyreg__centre_ra', 'skyreg__centre_dec', 'skyreg__xtr_radius' and 'name' as the index.</p> required <code>df_sources</code> <code>DataFrame</code> <p>dataframe derived from the measurement data with columns 'source', 'image', 'flux_peak'.</p> required <code>min_sigma</code> <code>float</code> <p>minimum sigma value to drop forced extracted measurements.</p> required <code>edge_buffer</code> <code>float</code> <p>flag to pass to ForcedPhot.measure method.</p> required <code>cluster_threshold</code> <code>float</code> <p>flag to pass to ForcedPhot.measure method.</p> required <code>allow_nan</code> <code>bool</code> <p>flag to pass to ForcedPhot.measure method.</p> required <code>add_mode</code> <code>bool</code> <p>True when the pipeline is running in add image mode.</p> required <code>p_run_path</code> <code>str</code> <p>The system path of the pipeline run output.</p> required <code>n_workers</code> <code>int</code> <p>The desired number of workers for Dask</p> <code>5</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with forced extracted measurements data, columns are 'source_tmp_id', 'ra', 'dec', 'image', 'flux_peak', 'island_id', 'component_id', 'name', 'flux_int', 'flux_int_err'</p> Source code in <code>vast_pipeline/pipeline/forced_extraction.py</code> <pre><code>def parallel_extraction(\n    df: pd.DataFrame, df_images: pd.DataFrame, df_sources: pd.DataFrame,\n    min_sigma: float, edge_buffer: float, cluster_threshold: float,\n    allow_nan: bool, add_mode: bool, p_run_path: str, n_workers: int = 5\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Parallelize forced extraction with Dask\n\n    Args:\n        df:\n            dataframe with columns 'wavg_ra', 'wavg_dec', 'img_diff',\n            'detection'\n        df_images:\n            dataframe with the images data and columns 'id',\n            'measurements_path', 'path', 'noise_path', 'beam_bmaj',\n            'beam_bmin', 'beam_bpa', 'background_path', 'rms_min', 'datetime',\n            'skyreg__centre_ra', 'skyreg__centre_dec', 'skyreg__xtr_radius'\n            and 'name' as the index.\n        df_sources:\n            dataframe derived from the measurement data with columns 'source',\n            'image', 'flux_peak'.\n        min_sigma:\n            minimum sigma value to drop forced extracted measurements.\n        edge_buffer:\n            flag to pass to ForcedPhot.measure method.\n        cluster_threshold:\n            flag to pass to ForcedPhot.measure method.\n        allow_nan:\n            flag to pass to ForcedPhot.measure method.\n        add_mode:\n            True when the pipeline is running in add image mode.\n        p_run_path:\n            The system path of the pipeline run output.\n        n_workers:\n            The desired number of workers for Dask\n\n    Returns:\n        Dataframe with forced extracted measurements data, columns are\n            'source_tmp_id', 'ra', 'dec', 'image', 'flux_peak', 'island_id',\n            'component_id', 'name', 'flux_int', 'flux_int_err'\n    \"\"\"\n    # explode the lists in 'img_diff' column (this will make a copy of the df)\n    out = (\n        df.rename(columns={'img_diff': 'image', 'source': 'source_tmp_id'})\n        # merge the rms_min column from df_images\n        .merge(\n            df_images[['rms_min']],\n            left_on='image',\n            right_on='name',\n            how='left'\n        )\n        .rename(columns={'rms_min': 'image_rms_min'})\n        # merge the measurements columns 'source', 'image', 'flux_peak'\n        .merge(\n            df_sources,\n            left_on=['source_tmp_id', 'detection'],\n            right_on=['source', 'image'],\n            how='left'\n        )\n        .drop(columns=['image_y', 'source'])\n        .rename(columns={'image_x': 'image'})\n    )\n\n    # drop the source for which we would have no hope of detecting\n    predrop_shape = out.shape[0]\n    out['max_snr'] = out['flux_peak'].values / out['image_rms_min'].values\n    out = out[out['max_snr'] &gt; min_sigma].reset_index(drop=True)\n    logger.debug(\"Min forced sigma dropped %i sources\",\n                 predrop_shape - out.shape[0]\n                 )\n\n    # drop some columns that are no longer needed and the df should look like\n    # out\n    # |   | source_tmp_id | wavg_ra | wavg_dec | image_name       | flux_peak |\n    # |--:|--------------:|--------:|---------:|:-----------------|----------:|\n    # | 0 |            81 | 317.607 | -8.66952 | VAST_2118-06A... |    11.555 |\n    # | 1 |           894 | 323.803 | -2.6899  | VAST_2118-06A... |     2.178 |\n    # | 2 |          1076 | 316.147 | -3.11408 | VAST_2118-06A... |     6.815 |\n    # | 3 |          1353 | 322.094 | -4.44977 | VAST_2118-06A... |     1.879 |\n    # | 4 |          1387 | 321.734 | -6.82934 | VAST_2118-06A... |     1.61  |\n\n    out = (\n        out.drop(['max_snr', 'image_rms_min', 'detection'], axis=1)\n        .rename(columns={'image': 'image_name'})\n    )\n\n    # get the unique images to extract from\n    unique_images_to_extract = out['image_name'].unique().tolist()\n\n    # create a list of dictionaries with image file paths and dataframes\n    # with data related to each images\n    def image_data_func(image_name: str) -&gt; Dict[str, Any]:\n        # `out` refers to the `out` declared in nearest enclosing scope\n        nonlocal out\n        return {\n            'image_id': df_images.at[image_name, 'id'],\n            'image': df_images.at[image_name, 'path'],\n            'background': df_images.at[image_name, 'background_path'],\n            'noise': df_images.at[image_name, 'noise_path'],\n            'df': out[out['image_name'] == image_name]\n        }\n    list_to_map = list(map(image_data_func, unique_images_to_extract))\n    # create a list of all the measurements parquet files to extract data from,\n    # such as prefix and max_id\n    list_meas_parquets = list(map(\n        lambda image_name: (\n            df_images.at[image_name, 'measurements_path'],\n            df_images.at[image_name, 'id'],\n        ),\n        unique_images_to_extract\n    ))\n    del out, unique_images_to_extract, image_data_func\n\n    # get a map of the columns that have a fixed value\n    mapping = (\n        db.from_sequence(\n            list_meas_parquets,\n            npartitions=len(list_meas_parquets)\n        )\n        .map(get_data_from_parquet, p_run_path, add_mode)\n        .compute(num_workers=n_workers, scheduler=\"processes\")\n    )\n    mapping = pd.DataFrame(mapping)\n    # remove not used columns from images_df and merge into mapping\n    col_to_drop = list(filter(\n        lambda x: ('path' in x) or ('skyreg' in x),\n        df_images.columns.values.tolist()\n    ))\n    mapping = (\n        mapping.merge(\n            df_images.drop(col_to_drop, axis=1).reset_index(),\n            on='id',\n            how='left'\n        )\n        .drop('rms_min', axis=1)\n        .set_index('name')\n    )\n    del col_to_drop\n\n    bags = db.from_sequence(list_to_map, npartitions=len(list_to_map))\n    forced_dfs = (\n        bags.map(lambda x: extract_from_image(\n            edge_buffer=edge_buffer,\n            cluster_threshold=cluster_threshold,\n            allow_nan=allow_nan,\n            **x\n        ))\n        .compute(num_workers=n_workers, scheduler='processes')\n    )\n    del bags\n    # create intermediates dfs combining the mapping data and the forced\n    # extracted data from the images\n    intermediate_df = list(map(\n        lambda x: {**(mapping.loc[x['image'], :].to_dict()), **x},\n        forced_dfs\n    ))\n\n    # compute the rest of the columns\n    # NOTE: Avoid using dask bags to parallelise the mapping\n    # over DataFrames, since these tend to get very large in memory and\n    # dask bags make a copy of the output before collecting the results.\n    # There is also a minimal speed penalty for doing this step without\n    # parallelism.\n    intermediate_df = list(map(\n        lambda x: finalise_forced_dfs(**x),\n        intermediate_df\n        ))\n    df_out = (\n        pd.concat(intermediate_df, axis=0, sort=False)\n        .rename(\n            columns={\n                'wavg_ra': 'ra', 'wavg_dec': 'dec', 'image_name': 'image'\n            }\n        )\n    )\n\n    return df_out\n</code></pre>"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.remove_forced_meas","title":"<code>remove_forced_meas(run_path)</code>","text":"<p>Remove forced measurements from the database if forced parquet files are found.</p> <p>Parameters:</p> Name Type Description Default <code>run_path</code> <code>str</code> <p>The run path of the pipeline run.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/pipeline/forced_extraction.py</code> <pre><code>def remove_forced_meas(run_path: str) -&gt; None:\n    '''\n    Remove forced measurements from the database if forced parquet files\n    are found.\n\n    Args:\n        run_path:\n            The run path of the pipeline run.\n\n    Returns:\n        None\n    '''\n    path_glob = glob(\n        os.path.join(run_path, 'forced_measurements_*.parquet')\n    )\n    if path_glob:\n        ids = (\n            dd.read_parquet(path_glob, columns='id')\n            .values\n            .compute()\n            .tolist()\n        )\n        obj_to_delete = Measurement.objects.filter(id__in=ids)\n        del ids\n        if obj_to_delete.exists():\n            with transaction.atomic():\n                n_del, detail_del = obj_to_delete.delete()\n                logger.info(\n                    ('Deleting all previous forced measurement and association'\n                     ' objects for this run. Total objects deleted: %i'),\n                    n_del,\n                )\n                logger.debug('(type, #deleted): %s', detail_del)\n</code></pre>"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.write_forced_parquet","title":"<code>write_forced_parquet(df, run_path, add_mode=False)</code>","text":"<p>Write parquet files for forced measurements.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing all the extracted measurements.</p> required <code>run_path</code> <code>str</code> <p>The run path of the pipeline run.</p> required <code>add_mode</code> <code>bool</code> <p>True when the pipeline is running in add image mode.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/pipeline/forced_extraction.py</code> <pre><code>def write_forced_parquet(\n        df: pd.DataFrame, run_path: str, add_mode: bool = False) -&gt; None:\n    '''\n    Write parquet files for forced measurements.\n\n    Args:\n        df:\n            Dataframe containing all the extracted measurements.\n        run_path:\n            The run path of the pipeline run.\n        add_mode:\n            True when the pipeline is running in add image mode.\n\n    Returns:\n        None\n    '''\n    images = df['image'].unique().tolist()\n\n    def get_fname(n): return os.path.join(\n        run_path,\n        'forced_measurements_' + n.replace('.', '_') + '.parquet'\n    )\n    # Avoid saving the maping to a list since this copies the the entire\n    # DataFrame which can already be very large in memory at this point.\n    dfs = map(lambda x: (df[df['image'] == x], get_fname(x)), images)\n\n    # Write parquets\n    for this_df, fname in dfs:\n        write_group_to_parquet(this_df, fname, add_mode)\n    pass\n</code></pre>"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.write_group_to_parquet","title":"<code>write_group_to_parquet(df, fname, add_mode)</code>","text":"<p>Write a dataframe correpondent to a single group/image to a parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing all the extracted measurements.</p> required <code>fname</code> <code>str</code> <p>The file name of the output parquet.</p> required <code>add_mode</code> <code>bool</code> <p>True when the pipeline is running in add image mode.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/pipeline/forced_extraction.py</code> <pre><code>def write_group_to_parquet(\n        df: pd.DataFrame, fname: str, add_mode: bool) -&gt; None:\n    '''\n    Write a dataframe correpondent to a single group/image\n    to a parquet file.\n\n    Args:\n        df:\n            Dataframe containing all the extracted measurements.\n        fname:\n            The file name of the output parquet.\n        add_mode:\n            True when the pipeline is running in add image mode.\n\n    Returns:\n        None\n    '''\n    out_df = df.drop(['d2d', 'dr', 'source', 'image'], axis=1)\n    if os.path.isfile(fname) and add_mode:\n        exist_df = pd.read_parquet(fname)\n        out_df = pd.concat([exist_df, out_df])\n\n    out_df.to_parquet(fname, index=False)\n\n    pass\n</code></pre>"},{"location":"reference/pipeline/loading/","title":"loading.py","text":""},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.SQL_update","title":"<code>SQL_update(df, model, index=None, columns=None)</code>","text":"<p>Generate the SQL code required to update the database.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database.</p> required <code>model</code> <code>Model</code> <p>The model that is being updated.</p> required <code>index</code> <code>Optional[str]</code> <p>Header of the column to join on, determines which rows in the different tables match. If None, then use the primary key column.</p> <code>None</code> <code>columns</code> <code>Optional[List[str]]</code> <p>The column headers of the columns to be updated. If None, updates all columns except the index column.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The SQL command to update the database.</p> Source code in <code>vast_pipeline/pipeline/loading.py</code> <pre><code>def SQL_update(\n    df: pd.DataFrame, model: models.Model, index: Optional[str] = None,\n    columns: Optional[List[str]] = None\n) -&gt; str:\n    '''\n    Generate the SQL code required to update the database.\n\n    Args:\n        df:\n            DataFrame containing the new data to be uploaded to the database.\n            The columns to be updated need to have the same headers between\n            the df and the table in the database.\n        model:\n            The model that is being updated.\n        index:\n            Header of the column to join on, determines which rows in the\n            different tables match. If None, then use the primary key column.\n        columns:\n            The column headers of the columns to be updated. If None, updates\n            all columns except the index column.\n\n    Returns:\n        The SQL command to update the database.\n    '''\n    # set index and columns if None\n    if index is None:\n        index = model._meta.pk.name\n    if columns is None:\n        columns = df.columns.tolist()\n        columns.remove(index)\n\n    # get names\n    table = model._meta.db_table\n    new_columns = ', '.join('new_' + c for c in columns)\n    set_columns = ', '.join(c + '=new_' + c for c in columns)\n\n    # get index values and new values\n    column_headers = [index]\n    column_headers.extend(columns)\n    data_arr = df[column_headers].to_numpy()\n    values = []\n    for row in data_arr:\n        val_row = '(' + ', '.join(f'{val}' for val in row) + ')'\n        values.append(val_row)\n    values = ', '.join(values)\n\n    # update database\n    SQL_comm = f\"\"\"\n        UPDATE {table}\n        SET {set_columns}\n        FROM (VALUES {values})\n        AS new_values (index_col, {new_columns})\n        WHERE {index}=index_col;\n    \"\"\"\n\n    return SQL_comm\n</code></pre>"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.bulk_upload_model","title":"<code>bulk_upload_model(djmodel, generator, batch_size=10000, return_ids=False)</code>","text":"<p>Bulk upload a list of generator objects of django models to db.</p> <p>Parameters:</p> Name Type Description Default <code>djmodel</code> <code>Model</code> <p>The Django pipeline model to be uploaded.</p> required <code>generator</code> <code>Iterable[Generator[Model, None, None]]</code> <p>The generator objects of the model to upload.</p> required <code>batch_size</code> <code>int</code> <p>How many records to upload at once.</p> <code>10000</code> <code>return_ids</code> <code>bool</code> <p>When set to True, the database IDs of the uploaded objects are returned.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[int]</code> <p>None or a list of the database IDs of the uploaded objects.</p> Source code in <code>vast_pipeline/pipeline/loading.py</code> <pre><code>@transaction.atomic\ndef bulk_upload_model(\n    djmodel: models.Model,\n    generator: Iterable[Generator[models.Model, None, None]],\n    batch_size: int = 10_000,\n    return_ids: bool = False,\n) -&gt; List[int]:\n    '''\n    Bulk upload a list of generator objects of django models to db.\n\n    Args:\n        djmodel:\n            The Django pipeline model to be uploaded.\n        generator:\n            The generator objects of the model to upload.\n        batch_size:\n            How many records to upload at once.\n        return_ids:\n            When set to True, the database IDs of the uploaded objects are\n            returned.\n\n    Returns:\n        None or a list of the database IDs of the uploaded objects.\n\n    '''\n    reset_queries()\n\n    bulk_ids = []\n    while True:\n        items = list(islice(generator, batch_size))\n        if not items:\n            break\n        out_bulk = djmodel.objects.bulk_create(items)\n        logger.info('Bulk created #%i %s', len(out_bulk), djmodel.__name__)\n        # save the DB ids to return\n        if return_ids:\n            bulk_ids.extend(list(map(lambda i: i.id, out_bulk)))\n\n    if return_ids:\n        return bulk_ids\n</code></pre>"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_associations","title":"<code>make_upload_associations(associations_df)</code>","text":"<p>Uploads the associations from the supplied associations DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>associations_df</code> <code>DataFrame</code> <p>DataFrame containing the associations information from the pipeline.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>vast_pipeline/pipeline/loading.py</code> <pre><code>def make_upload_associations(associations_df: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Uploads the associations from the supplied associations DataFrame.\n\n    Args:\n        associations_df:\n            DataFrame containing the associations information from the\n            pipeline.\n\n    Returns:\n        None.\n    \"\"\"\n    logger.info('Upload associations...')\n\n    mem_usage = get_df_memory_usage(associations_df)\n    logger.debug(f\"associations_df memory usage: {mem_usage}MB\")\n    log_total_memory_usage()\n\n    assoc_chunk_size = 100000\n    for i in range(0, len(associations_df), assoc_chunk_size):\n        bulk_upload_model(\n            Association,\n            association_models_generator(\n                associations_df[i:i + assoc_chunk_size])\n        )\n</code></pre>"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_images","title":"<code>make_upload_images(paths, image_config)</code>","text":"<p>Carry the first part of the pipeline, by uploading all the images to the image table and populated band and skyregion objects.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>Dict[str, Dict[str, str]]</code> <p>Dictionary containing the image, noise and background paths of all the images in the pipeline run. The primary keys are <code>selavy</code>, 'noise' and 'background' with the secondary key being the image name.</p> required <code>image_config</code> <code>Dict</code> <p>Dictionary of configuration options for the image ingestion.</p> required <p>Returns:</p> Type Description <code>List[Image]</code> <p>A list of Image objects that have been uploaded.</p> <code>List[SkyRegion]</code> <p>A list of SkyRegion objects that have been uploaded.</p> <code>List[Band]</code> <p>A list of Band objects that have been uploaded.</p> Source code in <code>vast_pipeline/pipeline/loading.py</code> <pre><code>def make_upload_images(\n    paths: Dict[str, Dict[str, str]], image_config: Dict\n) -&gt; Tuple[List[Image], List[SkyRegion], List[Band]]:\n    '''\n    Carry the first part of the pipeline, by uploading all the images\n    to the image table and populated band and skyregion objects.\n\n    Args:\n        paths:\n            Dictionary containing the image, noise and background paths of all\n            the images in the pipeline run. The primary keys are `selavy`,\n            'noise' and 'background' with the secondary key being the image\n            name.\n        image_config:\n            Dictionary of configuration options for the image ingestion.\n\n    Returns:\n        A list of Image objects that have been uploaded.\n        A list of SkyRegion objects that have been uploaded.\n        A list of Band objects that have been uploaded.\n    '''\n    timer = StopWatch()\n    images = []\n    skyregions = []\n    bands = []\n\n    for path in paths['selavy']:\n        # STEP #1: Load image and measurements\n        image = SelavyImage(\n            path,\n            paths,\n            image_config\n        )\n        logger.info('Reading image %s ...', image.name)\n\n        # 1.1 get/create the frequency band\n        with transaction.atomic():\n            band = get_create_img_band(image)\n        if band not in bands:\n            bands.append(band)\n\n        # 1.2 create image and skyregion entry in DB\n        with transaction.atomic():\n            img, exists_f = get_create_img(band.id, image)\n            skyreg = img.skyreg\n\n            # add image and skyregion to respective lists\n            images.append(img)\n            if skyreg not in skyregions:\n                skyregions.append(skyreg)\n\n            if exists_f:\n                logger.info(\"Image %s already processed\", img.name)\n                continue\n\n            # 1.3 get the image measurements and save them in DB\n            measurements = image.read_selavy(img)\n            logger.info(\n                \"Processed measurements dataframe of shape: (%i, %i)\",\n                measurements.shape[0],\n                measurements.shape[1],\n            )\n\n            # upload measurements, a column with the db is added to the df\n            measurements = make_upload_measurements(measurements)\n\n            # save measurements to parquet file in pipeline run folder\n            base_folder = os.path.dirname(img.measurements_path)\n            if not os.path.exists(base_folder):\n                os.makedirs(base_folder)\n\n            measurements.to_parquet(img.measurements_path, index=False)\n            del measurements, image, band, img\n\n    logger.info(\n        'Total images upload/loading time: %.2f seconds',\n        timer.reset_init()\n    )\n\n    return images, skyregions, bands\n</code></pre>"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_measurements","title":"<code>make_upload_measurements(measurements_df)</code>","text":"<p>Uploads the measurements from the supplied measurements DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>measurements_df</code> <code>DataFrame</code> <p>DataFrame containing the measurements information from the pipeline.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Original DataFrame with the database ID attached to each row.</p> Source code in <code>vast_pipeline/pipeline/loading.py</code> <pre><code>def make_upload_measurements(measurements_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Uploads the measurements from the supplied measurements DataFrame.\n\n    Args:\n        measurements_df:\n            DataFrame containing the measurements information from the\n            pipeline.\n\n    Returns:\n        Original DataFrame with the database ID attached to each row.\n    \"\"\"\n\n    logger.info(\"Upload measurements...\")\n    mem_usage = get_df_memory_usage(measurements_df)\n    logger.debug(f\"measurements_df memory usage: {mem_usage}MB\")\n    log_total_memory_usage()\n\n    meas_dj_ids = bulk_upload_model(\n        Measurement,\n        measurement_models_generator(measurements_df),\n        return_ids=True\n    )\n\n    measurements_df['id'] = meas_dj_ids\n\n    return measurements_df\n</code></pre>"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_related_sources","title":"<code>make_upload_related_sources(related_df)</code>","text":"<p>Uploads the related sources from the supplied related sources DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>related_df</code> <code>DataFrame</code> <p>DataFrame containing the related sources information from the pipeline.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>vast_pipeline/pipeline/loading.py</code> <pre><code>def make_upload_related_sources(related_df: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Uploads the related sources from the supplied related sources DataFrame.\n\n    Args:\n        related_df:\n            DataFrame containing the related sources information from the\n            pipeline.\n\n    Returns:\n        None.\n    \"\"\"\n    logger.info('Populate \"related\" field of sources...')\n    mem_usage = get_df_memory_usage(related_df)\n    logger.debug(f\"related_df memory usage: {mem_usage}MB\")\n    log_total_memory_usage()\n    bulk_upload_model(RelatedSource, related_models_generator(related_df))\n</code></pre>"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_sources","title":"<code>make_upload_sources(sources_df, pipeline_run, add_mode=False)</code>","text":"<p>Delete previous sources for given pipeline run and bulk upload new found sources as well as related sources.</p> <p>Parameters:</p> Name Type Description Default <code>sources_df</code> <code>DataFrame</code> <p>Holds the measurements associated into sources. The output of of thE association step.</p> required <code>pipeline_run</code> <code>Run</code> <p>The pipeline Run object.</p> required <code>add_mode</code> <code>bool</code> <p>Whether the pipeline is running in add image mode.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The input dataframe with the 'id' column added.</p> Source code in <code>vast_pipeline/pipeline/loading.py</code> <pre><code>def make_upload_sources(\n    sources_df: pd.DataFrame, pipeline_run: Run, add_mode: bool = False\n) -&gt; pd.DataFrame:\n    '''\n    Delete previous sources for given pipeline run and bulk upload\n    new found sources as well as related sources.\n\n    Args:\n        sources_df:\n            Holds the measurements associated into sources. The output of of\n            thE association step.\n        pipeline_run:\n            The pipeline Run object.\n        add_mode:\n            Whether the pipeline is running in add image mode.\n\n    Returns:\n        The input dataframe with the 'id' column added.\n    '''\n\n    logger.debug(\"Uploading sources...\")\n    mem_usage = get_df_memory_usage(sources_df)\n    logger.debug(f\"sources_df memory usage: {mem_usage}MB\")\n    log_total_memory_usage()\n\n    # create sources in DB\n    with transaction.atomic():\n        if (add_mode is False and\n                Source.objects.filter(run=pipeline_run).exists()):\n            logger.info('Removing objects from previous pipeline run')\n            n_del, detail_del = (\n                Source.objects.filter(run=pipeline_run).delete()\n            )\n            logger.info(\n                ('Deleting all sources and related objects for this run. '\n                 'Total objects deleted: %i'),\n                n_del,\n            )\n            logger.debug('(type, #deleted): %s', detail_del)\n\n    src_dj_ids = bulk_upload_model(\n        Source,\n        source_models_generator(sources_df, pipeline_run=pipeline_run),\n        return_ids=True\n    )\n\n    sources_df['id'] = src_dj_ids\n\n    return sources_df\n</code></pre>"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.update_sources","title":"<code>update_sources(sources_df, batch_size=10000)</code>","text":"<p>Update database using SQL code. This function opens one connection to the database, and closes it after the update is done.</p> <p>Parameters:</p> Name Type Description Default <code>sources_df</code> <code>DataFrame</code> <p>DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database.</p> required <code>batch_size</code> <code>int</code> <p>The df rows are broken into chunks, each chunk is executed in a separate SQL command, batch_size determines the maximum size of the chunk.</p> <code>10000</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the new data to be uploaded to the database.</p> Source code in <code>vast_pipeline/pipeline/loading.py</code> <pre><code>def update_sources(\n    sources_df: pd.DataFrame, batch_size: int = 10_000\n) -&gt; pd.DataFrame:\n    '''\n    Update database using SQL code. This function opens one connection to the\n    database, and closes it after the update is done.\n\n    Args:\n        sources_df:\n            DataFrame containing the new data to be uploaded to the database.\n            The columns to be updated need to have the same headers between\n            the df and the table in the database.\n        batch_size:\n            The df rows are broken into chunks, each chunk is executed in a\n            separate SQL command, batch_size determines the maximum size of the\n            chunk.\n\n    Returns:\n        DataFrame containing the new data to be uploaded to the database.\n    '''\n    # Get all possible columns from the model\n    all_source_table_cols = [\n        fld.attname for fld in Source._meta.get_fields()\n        if getattr(fld, 'attname', None) is not None\n    ]\n\n    # Filter to those present in sources_df\n    columns = [\n        col for col in all_source_table_cols if col in sources_df.columns\n    ]\n\n    sources_df['id'] = sources_df.index.values\n\n    batches = np.ceil(len(sources_df) / batch_size)\n    dfs = np.array_split(sources_df, batches)\n    with connection.cursor() as cursor:\n        for df_batch in dfs:\n            SQL_comm = SQL_update(\n                df_batch, Source, index='id', columns=columns\n            )\n            cursor.execute(SQL_comm)\n\n    return sources_df\n</code></pre>"},{"location":"reference/pipeline/main/","title":"main.py","text":"<p>This module contains the main pipeline class used for processing a pipeline run.</p>"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline","title":"<code>Pipeline</code>","text":"<p>Instance of a pipeline. All the methods runs the pipeline opearations, such as association.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the pipeline run.</p> <code>config</code> <code>PipelineConfig</code> <p>The pipeline run configuration.</p> <code>img_paths</code> <code>Dict[str, Dict[str, str]]</code> <p>A dict mapping input image paths to their selavy/noise/background counterpart path. e.g. <code>img_paths[\"selavy\"][&lt;image_path&gt;]</code> contains the selavy catalogue file path for <code>&lt;image_path&gt;</code>.</p> <code>img_epochs</code> <code>Dict[str, str]</code> <p>A dict mapping input image names to their provided epoch.</p> <code>add_mode</code> <code>bool</code> <p>A boolean indicating if this run is adding new images to a previously executed pipeline run.</p> <code>previous_parquets</code> <code>Dict[str, str]</code> <p>A dict mapping that provides the paths to parquet files for previous executions of this pipeline run.</p> Source code in <code>vast_pipeline/pipeline/main.py</code> <pre><code>class Pipeline():\n    \"\"\"Instance of a pipeline. All the methods runs the pipeline opearations,\n    such as association.\n\n    Attributes:\n        name (str): The name of the pipeline run.\n        config (PipelineConfig): The pipeline run configuration.\n        img_paths: A dict mapping input image paths to their\n            selavy/noise/background counterpart path. e.g.\n            `img_paths[\"selavy\"][&lt;image_path&gt;]` contains the selavy catalogue\n            file path for `&lt;image_path&gt;`.\n        img_epochs: A dict mapping input image names to their provided epoch.\n        add_mode: A boolean indicating if this run is adding new images to a\n            previously executed pipeline run.\n        previous_parquets: A dict mapping that provides the paths to parquet\n            files for previous executions of this pipeline run.\n    \"\"\"\n\n    def __init__(self, name: str, config_path: str,\n                 validate_config: bool = True):\n        \"\"\"Initialise an instance of Pipeline with a name and configuration\n        file path.\n\n        Args:\n            name (str): The name of the pipeline run.\n            config_path (str): The path to a YAML run configuration file.\n            validate_config (bool, optional): Validate the run configuration\n                immediately. Defaults to True.\n        \"\"\"\n        self.name: str = name\n        self.config: PipelineConfig = PipelineConfig.from_file(\n            config_path, validate=validate_config\n        )\n        self.img_paths: Dict[str, Dict[str, str]] = {\n            'selavy': {},\n            'noise': {},\n            'background': {},\n        }\n        # maps input image paths to their selavy/noise/background\n        # counterpart path\n        # maps image names to their provided epoch\n        self.img_epochs: Dict[str, str] = {}\n        self.add_mode: bool = False\n        self.previous_parquets: Dict[str, str]\n\n    def match_images_to_data(self) -&gt; None:\n        \"\"\"\n        Loops through images and matches the selavy, noise and bkg images.\n        Assumes that user has enteted images and other data in the same order.\n\n        Returns:\n            None\n        \"\"\"\n        inputs = self.config[\"inputs\"]\n        for key in sorted(inputs[\"image\"].keys()):\n            for x, y in zip(\n                inputs[\"image\"][key],\n                inputs[\"selavy\"][key],\n            ):\n                self.img_paths[\"selavy\"][x] = y\n                self.img_epochs[os.path.basename(x)] = key\n            for x, y in zip(\n                inputs[\"image\"][key],\n                inputs[\"noise\"][key]\n            ):\n                self.img_paths[\"noise\"][x] = y\n            if \"background\" in self.config[\"inputs\"]:\n                for x, y in zip(\n                    inputs[\"image\"][key],\n                    inputs[\"background\"][key],\n                ):\n                    self.img_paths[\"background\"][x] = y\n\n    def process_pipeline(self, p_run: Run) -&gt; None:\n        \"\"\"\n        The function that performs the processing operations of the pipeline\n        run.\n\n        Args:\n            p_run: The pipeline run model object.\n\n        Returns:\n            None\n        \"\"\"\n        logger.info(f'Epoch based association: {self.config.epoch_based}')\n        if self.add_mode:\n            logger.info('Running in image add mode.')\n\n        # Update epoch based flag to not cause user confusion when running\n        # the pipeline (i.e. if it was only updated at the end). It is not\n        # updated if the pipeline is being run in add mode.\n        if self.config.epoch_based and not self.add_mode:\n            with transaction.atomic():\n                p_run.epoch_based = self.config.epoch_based\n                p_run.save()\n\n        # Match the image files to the respective selavy, noise and bkg files.\n        # Do this after validation is successful.\n        self.match_images_to_data()\n\n        # upload/retrieve image data\n        images, skyregions, bands = make_upload_images(\n            self.img_paths,\n            self.config.image_opts()\n        )\n\n        # associate the pipeline run with each image\n        with transaction.atomic():\n            for img in images:\n                add_run_to_img(p_run, img)\n\n        # write parquet files and retrieve skyregions as a dataframe\n        skyregs_df = write_parquets(\n            images, skyregions, bands, self.config[\"run\"][\"path\"])\n\n        # STEP #2: measurements association\n        # order images by time\n        images.sort(key=operator.attrgetter('datetime'))\n\n        # If the user has given lists we need to reorder the\n        # image epochs such that they are in date order.\n        if self.config.epoch_based is False:\n            self.img_epochs = {}\n            for i, img in enumerate(images):\n                self.img_epochs[img.name] = i + 1\n\n        image_epochs = [\n            self.img_epochs[img.name] for img in images\n        ]\n        limit = Angle(self.config[\"source_association\"][\"radius\"] * u.arcsec)\n        dr_limit = self.config[\"source_association\"][\"deruiter_radius\"]\n        bw_limit = self.config[\"source_association\"][\"deruiter_beamwidth_limit\"]\n        duplicate_limit = Angle(\n            self.config[\"source_association\"][\"epoch_duplicate_radius\"] * u.arcsec\n        )\n\n        # 2.1 Check if sky regions to be associated can be\n        # split into connected point groups\n        skyregion_groups = group_skyregions(\n            skyregs_df[['id', 'centre_ra', 'centre_dec', 'xtr_radius']]\n        )\n        n_skyregion_groups = skyregion_groups[\n            'skyreg_group'\n        ].unique().shape[0]\n\n        # Get already done images if in add mode\n        if self.add_mode:\n            done_images_df = pd.read_parquet(\n                self.previous_parquets['images'], columns=['id', 'name']\n            )\n            done_source_ids = pd.read_parquet(\n                self.previous_parquets['sources'],\n                columns=['wavg_ra']\n            ).index.tolist()\n        else:\n            done_images_df = None\n            done_source_ids = None\n\n        # 2.2 Associate with other measurements\n        if self.config[\"source_association\"][\"parallel\"] and n_skyregion_groups &gt; 1:\n            images_df = get_parallel_assoc_image_df(\n                images, skyregion_groups\n            )\n            images_df['epoch'] = image_epochs\n\n            sources_df = parallel_association(\n                images_df,\n                limit,\n                dr_limit,\n                bw_limit,\n                duplicate_limit,\n                self.config,\n                n_skyregion_groups,\n                self.add_mode,\n                self.previous_parquets,\n                done_images_df,\n                done_source_ids\n            )\n        else:\n            images_df = pd.DataFrame.from_dict(\n                {\n                    'image_dj': images,\n                    'epoch': image_epochs\n                }\n            )\n\n            images_df['skyreg_id'] = images_df['image_dj'].apply(\n                lambda x: x.skyreg_id\n            )\n\n            images_df['image_name'] = images_df['image_dj'].apply(\n                lambda x: x.name\n            )\n\n            sources_df = association(\n                images_df,\n                limit,\n                dr_limit,\n                bw_limit,\n                duplicate_limit,\n                self.config,\n                self.add_mode,\n                self.previous_parquets,\n                done_images_df\n            )\n\n        mem_usage = get_df_memory_usage(sources_df)\n        logger.debug(f\"Step 2: sources_df memory usage: {mem_usage}MB\")\n        log_total_memory_usage()\n\n        # Obtain the number of selavy measurements for the run\n        # n_selavy_measurements = sources_df.\n        nr_selavy_measurements = sources_df['id'].unique().shape[0]\n\n        # STEP #3: Merge sky regions and sources ready for\n        # steps 4 and 5 below.\n        missing_source_cols = [\n            'source', 'datetime', 'image', 'epoch',\n            'interim_ew', 'weight_ew', 'interim_ns', 'weight_ns'\n        ]\n        # need to make sure no forced measurments are being passed which\n        # could happen in add mode, otherwise the wrong detection image is\n        # assigned.\n        missing_sources_df = get_src_skyregion_merged_df(\n            sources_df.loc[sources_df['forced'] == False, missing_source_cols],\n            images_df,\n            skyregs_df,\n            n_cpu=self.config['processing']['num_workers'],\n            max_partition_mb=self.config['processing']['max_partition_mb']\n        )\n\n        # STEP #4 New source analysis\n        new_sources_df = new_sources(\n            sources_df,\n            missing_sources_df,\n            self.config[\"new_sources\"][\"min_sigma\"],\n            self.config[\"source_monitoring\"][\"edge_buffer_scale\"],\n            p_run,\n            n_cpu=self.config['processing']['num_workers_io'],\n            max_partition_mb=self.config['processing']['max_partition_mb']\n        )\n\n        # Drop column no longer required in missing_sources_df.\n        missing_sources_df = (\n            missing_sources_df.drop(['in_primary'], axis=1)\n        )\n\n        # STEP #5: Run forced extraction/photometry if asked\n        if self.config[\"source_monitoring\"][\"monitor\"]:\n            (\n                sources_df,\n                nr_forced_measurements\n            ) = forced_extraction(\n                sources_df,\n                self.config[\"measurements\"][\"ra_uncertainty\"] / 3600.,\n                self.config[\"measurements\"][\"dec_uncertainty\"] / 3600.,\n                p_run,\n                missing_sources_df,\n                self.config[\"source_monitoring\"][\"min_sigma\"],\n                self.config[\"source_monitoring\"][\"edge_buffer_scale\"],\n                self.config[\"source_monitoring\"][\"cluster_threshold\"],\n                self.config[\"source_monitoring\"][\"allow_nan\"],\n                self.add_mode,\n                done_images_df,\n                done_source_ids,\n                n_cpu=self.config['processing']['num_workers_io']\n            )\n            mem_usage = get_df_memory_usage(sources_df)\n            logger.debug(f\"Step 5: sources_df memory usage: {mem_usage}MB\")\n            log_total_memory_usage()\n\n        del missing_sources_df\n\n        log_total_memory_usage()\n\n        # STEP #6: finalise the df getting unique sources, calculating\n        # metrics and upload data to database\n        nr_sources, nr_new_sources = final_operations(\n            sources_df,\n            p_run,\n            new_sources_df,\n            self.config[\"variability\"][\"pair_metrics\"],\n            self.config[\"variability\"][\"source_aggregate_pair_metrics_min_abs_vs\"],\n            self.add_mode,\n            done_source_ids,\n            self.previous_parquets,\n            n_cpu=self.config['processing']['num_workers'],\n            max_partition_mb=self.config['processing']['max_partition_mb']\n        )\n\n        log_total_memory_usage()\n\n        # calculate number processed images\n        nr_img_processed = len(images)\n\n        # update pipeline run with the nr images and sources\n        with transaction.atomic():\n            p_run.n_images = nr_img_processed\n            p_run.n_sources = nr_sources\n            p_run.n_selavy_measurements = nr_selavy_measurements\n            p_run.n_forced_measurements = (\n                nr_forced_measurements if self.config[\"source_monitoring\"][\"monitor\"] else 0\n            )\n            p_run.n_new_sources = nr_new_sources\n            p_run.save()\n\n        pass\n\n    @staticmethod\n    def check_current_runs() -&gt; None:\n        \"\"\"\n        Checks the number of pipeline runs currently being processed.\n\n        Returns:\n            None\n\n        Raises:\n            MaxPipelineRunsError: Raised if the number of pipeline runs\n                currently being processed is larger than the allowed\n                maximum.\n        \"\"\"\n        if Run.objects.check_max_runs(settings.MAX_PIPELINE_RUNS):\n            raise MaxPipelineRunsError\n\n    @staticmethod\n    def set_status(pipe_run: Run, status: str = None) -&gt; None:\n        \"\"\"\n        Function to change the status of a pipeline run model object and save\n        to the database.\n\n        Args:\n            pipe_run: The pipeline run model object.\n            status: The status to set.\n\n        Returns:\n            None\n        \"\"\"\n        # TODO: This function gives no feedback if the status is not accepted?\n        choices = [x[0] for x in Run._meta.get_field('status').choices]\n        if status and status in choices and pipe_run.status != status:\n            with transaction.atomic():\n                pipe_run.status = status\n                pipe_run.save()\n</code></pre>"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.__init__","title":"<code>__init__(name, config_path, validate_config=True)</code>","text":"<p>Initialise an instance of Pipeline with a name and configuration file path.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the pipeline run.</p> required <code>config_path</code> <code>str</code> <p>The path to a YAML run configuration file.</p> required <code>validate_config</code> <code>bool</code> <p>Validate the run configuration immediately. Defaults to True.</p> <code>True</code> Source code in <code>vast_pipeline/pipeline/main.py</code> <pre><code>def __init__(self, name: str, config_path: str,\n             validate_config: bool = True):\n    \"\"\"Initialise an instance of Pipeline with a name and configuration\n    file path.\n\n    Args:\n        name (str): The name of the pipeline run.\n        config_path (str): The path to a YAML run configuration file.\n        validate_config (bool, optional): Validate the run configuration\n            immediately. Defaults to True.\n    \"\"\"\n    self.name: str = name\n    self.config: PipelineConfig = PipelineConfig.from_file(\n        config_path, validate=validate_config\n    )\n    self.img_paths: Dict[str, Dict[str, str]] = {\n        'selavy': {},\n        'noise': {},\n        'background': {},\n    }\n    # maps input image paths to their selavy/noise/background\n    # counterpart path\n    # maps image names to their provided epoch\n    self.img_epochs: Dict[str, str] = {}\n    self.add_mode: bool = False\n    self.previous_parquets: Dict[str, str]\n</code></pre>"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.check_current_runs","title":"<code>check_current_runs()</code>  <code>staticmethod</code>","text":"<p>Checks the number of pipeline runs currently being processed.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>MaxPipelineRunsError</code> <p>Raised if the number of pipeline runs currently being processed is larger than the allowed maximum.</p> Source code in <code>vast_pipeline/pipeline/main.py</code> <pre><code>@staticmethod\ndef check_current_runs() -&gt; None:\n    \"\"\"\n    Checks the number of pipeline runs currently being processed.\n\n    Returns:\n        None\n\n    Raises:\n        MaxPipelineRunsError: Raised if the number of pipeline runs\n            currently being processed is larger than the allowed\n            maximum.\n    \"\"\"\n    if Run.objects.check_max_runs(settings.MAX_PIPELINE_RUNS):\n        raise MaxPipelineRunsError\n</code></pre>"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.match_images_to_data","title":"<code>match_images_to_data()</code>","text":"<p>Loops through images and matches the selavy, noise and bkg images. Assumes that user has enteted images and other data in the same order.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/pipeline/main.py</code> <pre><code>def match_images_to_data(self) -&gt; None:\n    \"\"\"\n    Loops through images and matches the selavy, noise and bkg images.\n    Assumes that user has enteted images and other data in the same order.\n\n    Returns:\n        None\n    \"\"\"\n    inputs = self.config[\"inputs\"]\n    for key in sorted(inputs[\"image\"].keys()):\n        for x, y in zip(\n            inputs[\"image\"][key],\n            inputs[\"selavy\"][key],\n        ):\n            self.img_paths[\"selavy\"][x] = y\n            self.img_epochs[os.path.basename(x)] = key\n        for x, y in zip(\n            inputs[\"image\"][key],\n            inputs[\"noise\"][key]\n        ):\n            self.img_paths[\"noise\"][x] = y\n        if \"background\" in self.config[\"inputs\"]:\n            for x, y in zip(\n                inputs[\"image\"][key],\n                inputs[\"background\"][key],\n            ):\n                self.img_paths[\"background\"][x] = y\n</code></pre>"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.process_pipeline","title":"<code>process_pipeline(p_run)</code>","text":"<p>The function that performs the processing operations of the pipeline run.</p> <p>Parameters:</p> Name Type Description Default <code>p_run</code> <code>Run</code> <p>The pipeline run model object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/pipeline/main.py</code> <pre><code>def process_pipeline(self, p_run: Run) -&gt; None:\n    \"\"\"\n    The function that performs the processing operations of the pipeline\n    run.\n\n    Args:\n        p_run: The pipeline run model object.\n\n    Returns:\n        None\n    \"\"\"\n    logger.info(f'Epoch based association: {self.config.epoch_based}')\n    if self.add_mode:\n        logger.info('Running in image add mode.')\n\n    # Update epoch based flag to not cause user confusion when running\n    # the pipeline (i.e. if it was only updated at the end). It is not\n    # updated if the pipeline is being run in add mode.\n    if self.config.epoch_based and not self.add_mode:\n        with transaction.atomic():\n            p_run.epoch_based = self.config.epoch_based\n            p_run.save()\n\n    # Match the image files to the respective selavy, noise and bkg files.\n    # Do this after validation is successful.\n    self.match_images_to_data()\n\n    # upload/retrieve image data\n    images, skyregions, bands = make_upload_images(\n        self.img_paths,\n        self.config.image_opts()\n    )\n\n    # associate the pipeline run with each image\n    with transaction.atomic():\n        for img in images:\n            add_run_to_img(p_run, img)\n\n    # write parquet files and retrieve skyregions as a dataframe\n    skyregs_df = write_parquets(\n        images, skyregions, bands, self.config[\"run\"][\"path\"])\n\n    # STEP #2: measurements association\n    # order images by time\n    images.sort(key=operator.attrgetter('datetime'))\n\n    # If the user has given lists we need to reorder the\n    # image epochs such that they are in date order.\n    if self.config.epoch_based is False:\n        self.img_epochs = {}\n        for i, img in enumerate(images):\n            self.img_epochs[img.name] = i + 1\n\n    image_epochs = [\n        self.img_epochs[img.name] for img in images\n    ]\n    limit = Angle(self.config[\"source_association\"][\"radius\"] * u.arcsec)\n    dr_limit = self.config[\"source_association\"][\"deruiter_radius\"]\n    bw_limit = self.config[\"source_association\"][\"deruiter_beamwidth_limit\"]\n    duplicate_limit = Angle(\n        self.config[\"source_association\"][\"epoch_duplicate_radius\"] * u.arcsec\n    )\n\n    # 2.1 Check if sky regions to be associated can be\n    # split into connected point groups\n    skyregion_groups = group_skyregions(\n        skyregs_df[['id', 'centre_ra', 'centre_dec', 'xtr_radius']]\n    )\n    n_skyregion_groups = skyregion_groups[\n        'skyreg_group'\n    ].unique().shape[0]\n\n    # Get already done images if in add mode\n    if self.add_mode:\n        done_images_df = pd.read_parquet(\n            self.previous_parquets['images'], columns=['id', 'name']\n        )\n        done_source_ids = pd.read_parquet(\n            self.previous_parquets['sources'],\n            columns=['wavg_ra']\n        ).index.tolist()\n    else:\n        done_images_df = None\n        done_source_ids = None\n\n    # 2.2 Associate with other measurements\n    if self.config[\"source_association\"][\"parallel\"] and n_skyregion_groups &gt; 1:\n        images_df = get_parallel_assoc_image_df(\n            images, skyregion_groups\n        )\n        images_df['epoch'] = image_epochs\n\n        sources_df = parallel_association(\n            images_df,\n            limit,\n            dr_limit,\n            bw_limit,\n            duplicate_limit,\n            self.config,\n            n_skyregion_groups,\n            self.add_mode,\n            self.previous_parquets,\n            done_images_df,\n            done_source_ids\n        )\n    else:\n        images_df = pd.DataFrame.from_dict(\n            {\n                'image_dj': images,\n                'epoch': image_epochs\n            }\n        )\n\n        images_df['skyreg_id'] = images_df['image_dj'].apply(\n            lambda x: x.skyreg_id\n        )\n\n        images_df['image_name'] = images_df['image_dj'].apply(\n            lambda x: x.name\n        )\n\n        sources_df = association(\n            images_df,\n            limit,\n            dr_limit,\n            bw_limit,\n            duplicate_limit,\n            self.config,\n            self.add_mode,\n            self.previous_parquets,\n            done_images_df\n        )\n\n    mem_usage = get_df_memory_usage(sources_df)\n    logger.debug(f\"Step 2: sources_df memory usage: {mem_usage}MB\")\n    log_total_memory_usage()\n\n    # Obtain the number of selavy measurements for the run\n    # n_selavy_measurements = sources_df.\n    nr_selavy_measurements = sources_df['id'].unique().shape[0]\n\n    # STEP #3: Merge sky regions and sources ready for\n    # steps 4 and 5 below.\n    missing_source_cols = [\n        'source', 'datetime', 'image', 'epoch',\n        'interim_ew', 'weight_ew', 'interim_ns', 'weight_ns'\n    ]\n    # need to make sure no forced measurments are being passed which\n    # could happen in add mode, otherwise the wrong detection image is\n    # assigned.\n    missing_sources_df = get_src_skyregion_merged_df(\n        sources_df.loc[sources_df['forced'] == False, missing_source_cols],\n        images_df,\n        skyregs_df,\n        n_cpu=self.config['processing']['num_workers'],\n        max_partition_mb=self.config['processing']['max_partition_mb']\n    )\n\n    # STEP #4 New source analysis\n    new_sources_df = new_sources(\n        sources_df,\n        missing_sources_df,\n        self.config[\"new_sources\"][\"min_sigma\"],\n        self.config[\"source_monitoring\"][\"edge_buffer_scale\"],\n        p_run,\n        n_cpu=self.config['processing']['num_workers_io'],\n        max_partition_mb=self.config['processing']['max_partition_mb']\n    )\n\n    # Drop column no longer required in missing_sources_df.\n    missing_sources_df = (\n        missing_sources_df.drop(['in_primary'], axis=1)\n    )\n\n    # STEP #5: Run forced extraction/photometry if asked\n    if self.config[\"source_monitoring\"][\"monitor\"]:\n        (\n            sources_df,\n            nr_forced_measurements\n        ) = forced_extraction(\n            sources_df,\n            self.config[\"measurements\"][\"ra_uncertainty\"] / 3600.,\n            self.config[\"measurements\"][\"dec_uncertainty\"] / 3600.,\n            p_run,\n            missing_sources_df,\n            self.config[\"source_monitoring\"][\"min_sigma\"],\n            self.config[\"source_monitoring\"][\"edge_buffer_scale\"],\n            self.config[\"source_monitoring\"][\"cluster_threshold\"],\n            self.config[\"source_monitoring\"][\"allow_nan\"],\n            self.add_mode,\n            done_images_df,\n            done_source_ids,\n            n_cpu=self.config['processing']['num_workers_io']\n        )\n        mem_usage = get_df_memory_usage(sources_df)\n        logger.debug(f\"Step 5: sources_df memory usage: {mem_usage}MB\")\n        log_total_memory_usage()\n\n    del missing_sources_df\n\n    log_total_memory_usage()\n\n    # STEP #6: finalise the df getting unique sources, calculating\n    # metrics and upload data to database\n    nr_sources, nr_new_sources = final_operations(\n        sources_df,\n        p_run,\n        new_sources_df,\n        self.config[\"variability\"][\"pair_metrics\"],\n        self.config[\"variability\"][\"source_aggregate_pair_metrics_min_abs_vs\"],\n        self.add_mode,\n        done_source_ids,\n        self.previous_parquets,\n        n_cpu=self.config['processing']['num_workers'],\n        max_partition_mb=self.config['processing']['max_partition_mb']\n    )\n\n    log_total_memory_usage()\n\n    # calculate number processed images\n    nr_img_processed = len(images)\n\n    # update pipeline run with the nr images and sources\n    with transaction.atomic():\n        p_run.n_images = nr_img_processed\n        p_run.n_sources = nr_sources\n        p_run.n_selavy_measurements = nr_selavy_measurements\n        p_run.n_forced_measurements = (\n            nr_forced_measurements if self.config[\"source_monitoring\"][\"monitor\"] else 0\n        )\n        p_run.n_new_sources = nr_new_sources\n        p_run.save()\n\n    pass\n</code></pre>"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.set_status","title":"<code>set_status(pipe_run, status=None)</code>  <code>staticmethod</code>","text":"<p>Function to change the status of a pipeline run model object and save to the database.</p> <p>Parameters:</p> Name Type Description Default <code>pipe_run</code> <code>Run</code> <p>The pipeline run model object.</p> required <code>status</code> <code>str</code> <p>The status to set.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/pipeline/main.py</code> <pre><code>@staticmethod\ndef set_status(pipe_run: Run, status: str = None) -&gt; None:\n    \"\"\"\n    Function to change the status of a pipeline run model object and save\n    to the database.\n\n    Args:\n        pipe_run: The pipeline run model object.\n        status: The status to set.\n\n    Returns:\n        None\n    \"\"\"\n    # TODO: This function gives no feedback if the status is not accepted?\n    choices = [x[0] for x in Run._meta.get_field('status').choices]\n    if status and status in choices and pipe_run.status != status:\n        with transaction.atomic():\n            pipe_run.status = status\n            pipe_run.save()\n</code></pre>"},{"location":"reference/pipeline/model_generator/","title":"model_generator.py","text":""},{"location":"reference/pipeline/model_generator/#vast_pipeline.pipeline.model_generator.association_models_generator","title":"<code>association_models_generator(assoc_df)</code>","text":"<p>Creates a generator object containing yielded Association objects from an input pipeline association dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>assoc_df</code> <code>DataFrame</code> <p>The dataframe from the pipeline containing the associations between measurements and sources.</p> required <p>Returns:</p> Type Description <code>Iterable[Generator[Association, None, None]]</code> <p>An iterable generator object containing the yielded Association objects.</p> Source code in <code>vast_pipeline/pipeline/model_generator.py</code> <pre><code>def association_models_generator(\n    assoc_df: pd.DataFrame\n) -&gt; Iterable[Generator[Association, None, None]]:\n    \"\"\"\n    Creates a generator object containing yielded Association objects from\n    an input pipeline association dataframe.\n\n    Args:\n        assoc_df:\n            The dataframe from the pipeline containing the associations between\n            measurements and sources.\n\n    Returns:\n        An iterable generator object containing the yielded Association objects.\n    \"\"\"\n    logger.debug(f\"Building {len(assoc_df)} association generators\")\n    for row in assoc_df.itertuples():\n        yield Association(\n            meas_id=row.id,\n            source_id=row.source_id,\n            d2d=row.d2d,\n            dr=row.dr,\n        )\n    logger.debug(f\"Built {len(assoc_df)} association generators\")\n</code></pre>"},{"location":"reference/pipeline/model_generator/#vast_pipeline.pipeline.model_generator.measurement_models_generator","title":"<code>measurement_models_generator(meas_df)</code>","text":"<p>Creates a generator object containing yielded Measurement objects from an input pipeline measurement dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>meas_df</code> <code>DataFrame</code> <p>The dataframe from the pipeline containing the measurements of an image.</p> required <p>Returns:</p> Type Description <code>Iterable[Generator[Measurement, None, None]]</code> <p>An iterable generator object containing the yielded Measurement objects.</p> Source code in <code>vast_pipeline/pipeline/model_generator.py</code> <pre><code>def measurement_models_generator(\n    meas_df: pd.DataFrame\n) -&gt; Iterable[Generator[Measurement, None, None]]:\n    \"\"\"\n    Creates a generator object containing yielded Measurement objects from\n    an input pipeline measurement dataframe.\n\n    Args:\n        meas_df:\n            The dataframe from the pipeline containing the measurements of an\n            image.\n\n    Returns:\n        An iterable generator object containing the yielded Measurement\n            objects.\n    \"\"\"\n    for row in meas_df.itertuples():\n        one_m = Measurement()\n        for fld in one_m._meta.get_fields():\n            if getattr(fld, 'attname', None) and hasattr(row, fld.attname):\n                setattr(one_m, fld.attname, getattr(row, fld.attname))\n        yield one_m\n</code></pre>"},{"location":"reference/pipeline/model_generator/#vast_pipeline.pipeline.model_generator.related_models_generator","title":"<code>related_models_generator(related_df)</code>","text":"<p>Creates a generator object containing yielded Association objects from an input pipeline association dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>related_df</code> <code>DataFrame</code> <p>The dataframe from the pipeline containing the relations between sources.</p> required <p>Returns:</p> Type Description <code>Iterable[Generator[RelatedSource, None, None]]</code> <p>An iterable generator object containing the yielded Association objects.</p> Source code in <code>vast_pipeline/pipeline/model_generator.py</code> <pre><code>def related_models_generator(\n    related_df: pd.DataFrame\n) -&gt; Iterable[Generator[RelatedSource, None, None]]:\n    \"\"\"\n    Creates a generator object containing yielded Association objects from\n    an input pipeline association dataframe.\n\n    Args:\n        related_df:\n            The dataframe from the pipeline containing the relations between\n            sources.\n\n    Returns:\n        An iterable generator object containing the yielded Association objects.\n    \"\"\"\n    for row in related_df.itertuples(index=False):\n        yield RelatedSource(**row._asdict())\n</code></pre>"},{"location":"reference/pipeline/model_generator/#vast_pipeline.pipeline.model_generator.source_models_generator","title":"<code>source_models_generator(src_df, pipeline_run)</code>","text":"<p>Creates a generator object containing yielded Source objects from an input pipeline sources dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>src_df</code> <code>DataFrame</code> <p>The dataframe from the pipeline containing the measurements of an image.</p> required <code>pipeline_run</code> <code>Run</code> <p>The pipeline Run object of which the sources are associated with.</p> required <p>Returns:</p> Type Description <code>Iterable[Generator[Source, None, None]]</code> <p>An iterable generator object containing the yielded Source objects.</p> Source code in <code>vast_pipeline/pipeline/model_generator.py</code> <pre><code>def source_models_generator(\n    src_df: pd.DataFrame, pipeline_run: Run\n) -&gt; Iterable[Generator[Source, None, None]]:\n    \"\"\"\n    Creates a generator object containing yielded Source objects from\n    an input pipeline sources dataframe.\n\n    Args:\n        src_df:\n            The dataframe from the pipeline containing the measurements of\n            an image.\n        pipeline_run:\n            The pipeline Run object of which the sources are associated with.\n\n    Returns:\n        An iterable generator object containing the yielded Source objects.\n    \"\"\"\n    for row in src_df.itertuples():\n        # generate an IAU compliant source name, see\n        # https://cdsweb.u-strasbg.fr/Dic/iau-spec.html\n        name = (\n            f\"J{deg2hms(row.wavg_ra, precision=1, truncate=True)}\"\n            f\"{deg2dms(row.wavg_dec, precision=0, truncate=True)}\"\n        ).replace(\":\", \"\")\n        src = Source()\n        src.run_id = pipeline_run.id\n        src.name = name\n        for fld in src._meta.get_fields():\n            if getattr(fld, 'attname', None) and hasattr(row, fld.attname):\n                setattr(src, fld.attname, getattr(row, fld.attname))\n\n        yield src\n</code></pre>"},{"location":"reference/pipeline/new_sources/","title":"new_sources.py","text":""},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.check_primary_image","title":"<code>check_primary_image(row)</code>","text":"<p>Checks if the primary image is in the image list.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Row of the missing_sources_df, need the keys 'primary' and 'img_list'.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the primary image is in the image list.</p> Source code in <code>vast_pipeline/pipeline/new_sources.py</code> <pre><code>def check_primary_image(row: pd.Series) -&gt; bool:\n    \"\"\"\n    Checks if the primary image is in the image list.\n\n    Args:\n        row:\n            Row of the missing_sources_df, need the keys 'primary' and\n            'img_list'.\n\n    Returns:\n        True if the primary image is in the image list.\n    \"\"\"\n    return row['primary'] in row['img_list']\n</code></pre>"},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.gen_array_coords_from_wcs","title":"<code>gen_array_coords_from_wcs(coords, wcs)</code>","text":"<p>Converts SkyCoord coordinates to array coordinates given a wcs.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>SkyCoord</code> <p>The coordinates to convert.</p> required <code>wcs</code> <code>WCS</code> <p>The WCS to use for the conversion.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array containing the x and y array coordinates of the input sky coordinates, e.g.: np.array([[x1, x2, x3], [y1, y2, y3]])</p> Source code in <code>vast_pipeline/pipeline/new_sources.py</code> <pre><code>def gen_array_coords_from_wcs(coords: SkyCoord, wcs: WCS) -&gt; np.ndarray:\n    \"\"\"\n    Converts SkyCoord coordinates to array coordinates given a wcs.\n\n    Args:\n        coords:\n            The coordinates to convert.\n        wcs:\n            The WCS to use for the conversion.\n\n    Returns:\n        Array containing the x and y array coordinates of the input sky\n            coordinates, e.g.:\n            np.array([[x1, x2, x3], [y1, y2, y3]])\n    \"\"\"\n    array_coords = wcs.world_to_array_index(coords)\n    array_coords = np.array([\n        np.array(array_coords[0]),\n        np.array(array_coords[1]),\n    ])\n\n    return array_coords\n</code></pre>"},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.get_image_rms_measurements","title":"<code>get_image_rms_measurements(group, nbeam=3, edge_buffer=1.0)</code>","text":"<p>Take the coordinates provided from the group and measure the array cell value in the provided image.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>DataFrame</code> <p>The group of sources to measure in the image, requiring the columns: 'source', 'wavg_ra', 'wavg_dec' and 'img_diff_rms_path'.</p> required <code>nbeam</code> <code>int</code> <p>The number of half beamwidths (BMAJ) away from the edge of the image or a NaN value that is acceptable.</p> <code>3</code> <code>edge_buffer</code> <code>float</code> <p>Multiplicative factor applied to nbeam to act as a buffer.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The group dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail.</p> Source code in <code>vast_pipeline/pipeline/new_sources.py</code> <pre><code>def get_image_rms_measurements(\n    group: pd.DataFrame, nbeam: int = 3, edge_buffer: float = 1.0\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Take the coordinates provided from the group\n    and measure the array cell value in the provided image.\n\n    Args:\n        group:\n            The group of sources to measure in the image, requiring the\n            columns: 'source', 'wavg_ra', 'wavg_dec' and 'img_diff_rms_path'.\n        nbeam:\n            The number of half beamwidths (BMAJ) away from the edge of the\n            image or a NaN value that is acceptable.\n        edge_buffer:\n            Multiplicative factor applied to nbeam to act as a buffer.\n\n    Returns:\n        The group dataframe with the 'img_diff_true_rms' column added. The\n            column will contain 'NaN' entires for sources that fail.\n    \"\"\"\n    if len(group) == 0:\n        # input dataframe is empty, nothing to do\n        logger.debug(f\"No image RMS measurements to get, returning\")\n        return group\n    image = group.iloc[0]['img_diff_rms_path']\n\n    logger.debug(f\"{image} - num. meas. to get: {len(group)}\")\n    partition_mem = get_df_memory_usage(group)\n    logger.debug(f\"{image} - partition memory usage: {partition_mem}MB\")\n\n    get_rms_timer = StopWatch()\n\n    with open_fits(image) as hdul:\n        header = hdul[0].header\n        wcs = WCS(header, naxis=2)\n        data = hdul[0].data.squeeze()\n\n    logger.debug(f\"{image} - Time to load fits: {get_rms_timer.reset()}s\")\n\n    # Here we mimic the forced fits behaviour,\n    # sources within 3 half BMAJ widths of the image\n    # edges are ignored. The user buffer is also\n    # applied for consistency.\n    pixelscale = (\n        proj_plane_pixel_scales(wcs)[1] * u.deg\n    ).to(u.arcsec)\n\n    bmaj = header[\"BMAJ\"] * u.deg\n\n    npix = round(\n        (nbeam / 2. * bmaj.to('arcsec') /\n         pixelscale).value\n    )\n\n    npix = int(round(npix * edge_buffer))\n\n    coords = SkyCoord(\n        group.wavg_ra, group.wavg_dec, unit=(u.deg, u.deg)\n    )\n\n    array_coords = gen_array_coords_from_wcs(coords, wcs)\n\n    # check for pixel wrapping\n    x_valid = np.logical_or(\n        array_coords[0] &gt;= (data.shape[0] - npix),\n        array_coords[0] &lt; npix\n    )\n\n    y_valid = np.logical_or(\n        array_coords[1] &gt;= (data.shape[1] - npix),\n        array_coords[1] &lt; npix\n    )\n\n    valid = ~np.logical_or(\n        x_valid, y_valid\n    )\n\n    valid_indexes = group[valid].index.values\n\n    group = group.loc[valid_indexes]\n\n    if group.empty:\n        # early return if all sources failed range check\n        logger.debug(\n            'All sources out of range in new source rms measurement'\n            f' for image {image}.'\n        )\n        group['img_diff_true_rms'] = np.nan\n        return group\n\n    # Now we also need to check proximity to NaN values\n    # as forced fits may also drop these values\n    coords = SkyCoord(\n        group.wavg_ra, group.wavg_dec, unit=(u.deg, u.deg)\n    )\n\n    array_coords = gen_array_coords_from_wcs(coords, wcs)\n\n    acceptable_no_nan_dist = int(\n        round(bmaj.to('arcsec').value / 2. / pixelscale.value)\n    )\n\n    nan_valid = []\n\n    # Get slices of each source and check NaN is not included.\n    for i, j in zip(array_coords[0], array_coords[1]):\n        sl = tuple((\n            slice(i - acceptable_no_nan_dist, i + acceptable_no_nan_dist),\n            slice(j - acceptable_no_nan_dist, j + acceptable_no_nan_dist)\n        ))\n        if np.any(np.isnan(data[sl])):\n            nan_valid.append(False)\n        else:\n            nan_valid.append(True)\n\n    valid_indexes = group[nan_valid].index.values\n\n    if np.any(nan_valid):\n        # only run if there are actual values to measure\n        rms_values = data[\n            array_coords[0][nan_valid],\n            array_coords[1][nan_valid]\n        ]\n\n        # not matched ones will be NaN.\n        group.loc[\n            valid_indexes, 'img_diff_true_rms'\n        ] = rms_values.astype(np.float64) * 1.e3\n\n    else:\n        group['img_diff_true_rms'] = np.nan\n\n    return group\n</code></pre>"},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.new_sources","title":"<code>new_sources(sources_df, missing_sources_df, min_sigma, edge_buffer, p_run, n_cpu=5, max_partition_mb=15)</code>","text":"<p>Processes the new sources detected to check that they are valid new sources. This involves checking to see that the source should be seen at all in     the images where it is not detected. For valid new sources the snr value the source would have in non-detected images is also calculated.</p> <p>Parameters:</p> Name Type Description Default <code>sources_df</code> <code>DataFrame</code> <p>The sources found from the association step.</p> required <code>missing_sources_df</code> <code>DataFrame</code> <p>The dataframe containing the 'missing detections' for each source. See the source code comments for the layout of this dataframe.</p> required <code>min_sigma</code> <code>float</code> <p>The minimum sigma value acceptable when compared to the minimum rms of the respective image.</p> required <code>edge_buffer</code> <code>float</code> <p>Multiplicative factor to be passed to the 'get_image_rms_measurements' function.</p> required <code>p_run</code> <code>Run</code> <p>The pipeline run.</p> required <code>n_cpu</code> <code>int</code> <p>The desired number of workers for Dask</p> <code>5</code> <code>max_partition_mb</code> <code>int</code> <p>The desired maximum size (in MB) of the partitions for Dask.</p> <code>15</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Columns:     source - source id, int.     img_list - list of images, List.     wavg_ra - weighted average RA, float.     wavg_dec - weighted average Dec, float.     skyreg_img_list - list of sky regions of images in img_list,         List.     img_diff - The images missing from coverage, List.     primary - What should be the first image, str.     detection - The first detection image, str.     detection_time - Datetime of detection, datetime.datetime.     img_diff_time - Datetime of img_diff list, datetime.datetime.     img_diff_rms_min - Minimum rms of diff images, float.     img_diff_rms_median - Median rms of diff images, float.     img_diff_rms_path - rms path of diff images, str.     flux_peak - Flux peak of source (detection), float.     diff_sigma - SNR in differnce images (compared to minimum),         float.     img_diff_true_rms - The true rms value from the diff images,         float.     new_high_sigma - peak flux / true rms value, float.</p> Source code in <code>vast_pipeline/pipeline/new_sources.py</code> <pre><code>def new_sources(\n    sources_df: pd.DataFrame, missing_sources_df: pd.DataFrame,\n    min_sigma: float, edge_buffer: float, p_run: Run, n_cpu: int = 5,\n    max_partition_mb: int = 15\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Processes the new sources detected to check that they are valid new\n    sources. This involves checking to see that the source *should* be seen at\n    all in     the images where it is not detected. For valid new sources the\n    snr value the source would have in non-detected images is also calculated.\n\n    Args:\n        sources_df:\n            The sources found from the association step.\n        missing_sources_df:\n            The dataframe containing the 'missing detections' for each source.\n            See the source code comments for the layout of this dataframe.\n        min_sigma:\n            The minimum sigma value acceptable when compared to the minimum\n            rms of the respective image.\n        edge_buffer:\n            Multiplicative factor to be passed to the\n            'get_image_rms_measurements' function.\n        p_run:\n            The pipeline run.\n        n_cpu:\n            The desired number of workers for Dask\n        max_partition_mb:\n            The desired maximum size (in MB) of the partitions for Dask.\n\n    Returns:\n        The original input dataframe with the 'img_diff_true_rms' column\n            added. The column will contain 'NaN' entires for sources that fail.\n            Columns:\n                source - source id, int.\n                img_list - list of images, List.\n                wavg_ra - weighted average RA, float.\n                wavg_dec - weighted average Dec, float.\n                skyreg_img_list - list of sky regions of images in img_list,\n                    List.\n                img_diff - The images missing from coverage, List.\n                primary - What should be the first image, str.\n                detection - The first detection image, str.\n                detection_time - Datetime of detection, datetime.datetime.\n                img_diff_time - Datetime of img_diff list, datetime.datetime.\n                img_diff_rms_min - Minimum rms of diff images, float.\n                img_diff_rms_median - Median rms of diff images, float.\n                img_diff_rms_path - rms path of diff images, str.\n                flux_peak - Flux peak of source (detection), float.\n                diff_sigma - SNR in differnce images (compared to minimum),\n                    float.\n                img_diff_true_rms - The true rms value from the diff images,\n                    float.\n                new_high_sigma - peak flux / true rms value, float.\n    \"\"\"\n    # Missing sources df layout\n    # +----------+----------------------------------+-----------+------------+\n    # |   source | img_list                         |   wavg_ra |   wavg_dec |\n    # |----------+----------------------------------+-----------+------------+\n    # |      278 | ['VAST_0127-73A.EPOCH01.I.fits'] |  22.2929  |   -71.8717 |\n    # |      702 | ['VAST_0127-73A.EPOCH01.I.fits'] |  28.8125  |   -69.3547 |\n    # |      776 | ['VAST_0127-73A.EPOCH01.I.fits'] |  31.8223  |   -70.4674 |\n    # |      844 | ['VAST_0127-73A.EPOCH01.I.fits'] |  17.3152  |   -72.346  |\n    # |      934 | ['VAST_0127-73A.EPOCH01.I.fits'] |   9.75754 |   -72.9629 |\n    # +----------+----------------------------------+-----------+------------+\n    # ------------------------------------------------------------------+\n    #  skyreg_img_list                                                  |\n    # ------------------------------------------------------------------+\n    #  ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] |\n    #  ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] |\n    #  ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] |\n    #  ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] |\n    #  ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] |\n    # ------------------------------------------------------------------+\n    # ----------------------------------+\n    #  img_diff                         |\n    # ----------------------------------|\n    #  ['VAST_0127-73A.EPOCH08.I.fits'] |\n    #  ['VAST_0127-73A.EPOCH08.I.fits'] |\n    #  ['VAST_0127-73A.EPOCH08.I.fits'] |\n    #  ['VAST_0127-73A.EPOCH08.I.fits'] |\n    #  ['VAST_0127-73A.EPOCH08.I.fits'] |\n    # ----------------------------------+\n    timer = StopWatch()\n    debug_timer = StopWatch()\n\n    logger.info(\"Starting new source analysis.\")\n\n    cols = [\n        'id', 'name', 'noise_path', 'datetime',\n        'rms_median', 'rms_min', 'rms_max',\n    ]\n\n    images_df = pd.DataFrame(list(\n        Image.objects.filter(\n            run=p_run\n        ).values(*tuple(cols))\n    )).set_index('name')\n\n    # Get rid of sources that are not 'new', i.e. sources which the\n    # first sky region image is not in the image list\n    new_sources_df = missing_sources_df[\n        missing_sources_df['in_primary'] == False\n    ].drop(\n        columns=['in_primary']\n    )\n\n    # Check if the previous sources would have actually been seen\n    # i.e. are the previous images sensitive enough\n\n    # save the index before exploding\n    new_sources_df = new_sources_df.reset_index()\n\n    # Explode now to avoid two loops below\n    new_sources_df = new_sources_df.explode('img_diff')\n\n    # Merge the respective image information to the df\n    new_sources_df = new_sources_df.merge(\n        images_df[['datetime']],\n        left_on='detection',\n        right_on='name',\n        how='left'\n    ).rename(columns={'datetime': 'detection_time'})\n\n    new_sources_df = new_sources_df.merge(\n        images_df[[\n            'datetime', 'rms_min', 'rms_median',\n            'noise_path'\n        ]],\n        left_on='img_diff',\n        right_on='name',\n        how='left'\n    ).rename(columns={\n        'datetime': 'img_diff_time',\n        'rms_min': 'img_diff_rms_min',\n        'rms_median': 'img_diff_rms_median',\n        'noise_path': 'img_diff_rms_path'\n    })\n\n    logger.debug(f\"Time to reset &amp; merge image info into new_sources_df: \"\n                 f\"{debug_timer.reset()}s\"\n                 )\n\n    # Select only those images that come before the detection image\n    # in time.\n    new_sources_df = new_sources_df[\n        new_sources_df.img_diff_time &lt; new_sources_df.detection_time\n    ]\n\n    # merge the detection fluxes in\n    new_sources_df = pd.merge(\n        new_sources_df, sources_df[['source', 'image', 'flux_peak']],\n        left_on=['source', 'detection'], right_on=['source', 'image'],\n        how='left'\n    ).drop(columns=['image'])\n\n    logger.debug(f\"Time to merge detection fluxes into new_sources_df: \"\n                 f\"{debug_timer.reset()}s\"\n                 )\n\n    # calculate the sigma of the source if it was placed in the\n    # minimum rms region of the previous images\n    new_sources_df['diff_sigma'] = (\n        new_sources_df['flux_peak'].values\n        / new_sources_df['img_diff_rms_min'].values\n    )\n\n    # keep those that are above the user specified threshold\n    new_sources_df = new_sources_df.loc[\n        new_sources_df['diff_sigma'] &gt;= min_sigma\n    ]\n\n    logger.debug(f\"Time to do new_sources_df threshold calcs: \"\n                 f\"{debug_timer.reset()}s\"\n                 )\n\n    # Now have list of sources that should have been seen before given\n    # previous images minimum rms values.\n\n    # Current inaccurate sky regions may mean that the source\n    # was in a previous 'NaN' area of the image. This needs to be\n    # checked. Currently the check is done by filtering out of range\n    # pixels once the values have been obtained (below).\n    # This could be done using MOCpy however this is reasonably\n    # fast and the io of a MOC fits may take more time.\n\n    # So these sources will be flagged as new sources, but we can also\n    # make a guess of how signficant they are. For this the next step is\n    # to measure the true rms at the source location.\n\n    # measure the actual rms in the previous images at\n    # the source location.\n\n    # PR#713: This part of the code should be rewritten to reflect the new\n    # behaviour of parallel_get_rms_measurements. That function should be\n    # renamed to something like parallel_get_new_high_sigma and all of the\n    # subsequent code in this function moved into it.\n\n    logger.debug(\"Getting rms measurements...\")\n\n    new_sources_df = parallel_get_rms_measurements(\n        new_sources_df, edge_buffer=edge_buffer,\n        n_cpu=n_cpu, max_partition_mb=max_partition_mb\n    )\n    logger.debug(f\"Time to get rms measurements: {debug_timer.reset()}s\")\n\n    # this removes those that are out of range\n    new_sources_df['img_diff_true_rms'] = (\n        new_sources_df['img_diff_true_rms'].fillna(0.)\n    )\n    new_sources_df = new_sources_df[\n        new_sources_df['img_diff_true_rms'] != 0\n    ]\n\n    # calculate the true sigma\n    new_sources_df['true_sigma'] = (\n        new_sources_df['flux_peak'].values\n        / new_sources_df['img_diff_true_rms'].values\n    )\n\n    # We only care about the highest true sigma\n    # new_sources_df = new_sources_df.sort_values(\n    #    by=['source', 'true_sigma']\n    # )\n\n    # keep only the highest for each source, rename for the daatabase\n    new_sources_df = (\n        new_sources_df\n        # .drop_duplicates('source')\n        .set_index('source')\n        .rename(columns={'true_sigma': 'new_high_sigma'})\n    )\n\n    # moving forward only the new_high_sigma columns is needed, drop all\n    # others.\n    new_sources_df = new_sources_df[['new_high_sigma']]\n\n    logger.debug(f\"Time to to do final cleanup steps {debug_timer.reset()}s\")\n\n    logger.info(\n        'Total new source analysis time: %.2f seconds', timer.reset_init()\n    )\n\n    return new_sources_df\n</code></pre>"},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.parallel_get_rms_measurements","title":"<code>parallel_get_rms_measurements(df, edge_buffer=1.0, n_cpu=0, max_partition_mb=15)</code>","text":"<p>Wrapper function to use 'get_image_rms_measurements' in parallel with Dask. nbeam is not an option here as that parameter is fixed in forced extraction and so is made sure to be fixed here to. This may change in the future.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The group of sources to measure in the images.</p> required <code>edge_buffer</code> <code>float</code> <p>Multiplicative factor to be passed to the 'get_image_rms_measurements' function.</p> <code>1.0</code> <code>n_cpu</code> <code>int</code> <p>The desired number of workers for Dask</p> <code>0</code> <code>max_partition_mb</code> <code>int</code> <p>The desired maximum size (in MB) of the partitions for Dask.</p> <code>15</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail.</p> Source code in <code>vast_pipeline/pipeline/new_sources.py</code> <pre><code>def parallel_get_rms_measurements(\n    df: pd.DataFrame, edge_buffer: float = 1.0,\n    n_cpu: int = 0, max_partition_mb: int = 15\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Wrapper function to use 'get_image_rms_measurements'\n    in parallel with Dask. nbeam is not an option here as that parameter\n    is fixed in forced extraction and so is made sure to be fixed here to. This\n    may change in the future.\n\n    Args:\n        df:\n            The group of sources to measure in the images.\n        edge_buffer:\n            Multiplicative factor to be passed to the\n            'get_image_rms_measurements' function.\n        n_cpu:\n            The desired number of workers for Dask\n        max_partition_mb:\n            The desired maximum size (in MB) of the partitions for Dask.\n\n\n    Returns:\n        The original input dataframe with the 'img_diff_true_rms' column\n            added. The column will contain 'NaN' entires for sources that fail.\n    \"\"\"\n\n    out = df[[\n        'source', 'wavg_ra', 'wavg_dec',\n        'img_diff_rms_path'\n    ]]\n\n    col_dtype = {\n        'source': 'i',\n        'wavg_ra': 'f',\n        'wavg_dec': 'f',\n        'img_diff_rms_path': 'U',\n        'img_diff_true_rms': 'f',\n    }\n\n    n_workers, n_partitions = calculate_workers_and_partitions(\n        df,\n        n_cpu=n_cpu,\n        max_partition_mb=max_partition_mb\n    )\n    logger.debug(f\"Running get_image_rms_measurements with {n_workers} CPUs\")\n\n    out = (\n        dd.from_pandas(out, npartitions=n_partitions)\n        .groupby('img_diff_rms_path')\n        .apply(\n            get_image_rms_measurements,\n            edge_buffer=edge_buffer,\n            meta=col_dtype\n        ).compute(num_workers=n_workers, scheduler='processes')\n    )\n\n    # We don't need all of the RMS measurements, just the lowest. Keeping all\n    # of them results in huge memory usage when merging. However, there is an\n    # existing bug: https://github.com/askap-vast/vast-pipeline/issues/713\n    # that means that we actually want the _highest_ in order to reproduce the\n    # current behaviour. Fixing the bug is beyond the scope of this PR because\n    # it means rewriting tests and test data.\n\n    df_to_merge = (df.drop_duplicates('source')\n                   .drop(['img_diff_rms_path'], axis=1)\n                   )\n\n    out_to_merge = (out.sort_values(\n        by=['source', 'img_diff_true_rms'], ascending=False\n    )\n        .drop_duplicates('source')\n    )\n\n    df = df_to_merge.merge(\n        out_to_merge[['source', 'img_diff_true_rms']],\n        left_on='source', right_on='source',\n        how='left'\n    )\n\n    return df\n</code></pre>"},{"location":"reference/pipeline/pairs/","title":"pairs.py","text":""},{"location":"reference/pipeline/pairs/#vast_pipeline.pipeline.pairs.calculate_m_metric","title":"<code>calculate_m_metric(flux_a, flux_b)</code>","text":"<p>Calculate the m variability metric which is the modulation index between two fluxes. This is proportional to the fractional variability. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105.</p> <p>Parameters:</p> Name Type Description Default <code>flux_a</code> <code>float</code> <p>flux value \"A\".</p> required <code>flux_b</code> <code>float</code> <p>flux value \"B\".</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>the m metric for flux values \"A\" and \"B\".</p> Source code in <code>vast_pipeline/pipeline/pairs.py</code> <pre><code>def calculate_m_metric(flux_a: float, flux_b: float) -&gt; float:\n    \"\"\"Calculate the m variability metric which is the modulation index between two fluxes.\n    This is proportional to the fractional variability.\n    See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105.\n\n    Args:\n        flux_a (float): flux value \"A\".\n        flux_b (float): flux value \"B\".\n\n    Returns:\n        float: the m metric for flux values \"A\" and \"B\".\n    \"\"\"\n    return 2 * ((flux_a - flux_b) / (flux_a + flux_b))\n</code></pre>"},{"location":"reference/pipeline/pairs/#vast_pipeline.pipeline.pairs.calculate_measurement_pair_metrics","title":"<code>calculate_measurement_pair_metrics(df, n_cpu=0, max_partition_mb=15)</code>","text":"<p>Generate a DataFrame of measurement pairs and their 2-epoch variability metrics from a DataFrame of measurements. For more information on the variability metrics, see Section 5 of Mooley et al. (2016), DOI: 10.3847/0004-637X/818/2/105.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input measurements. Must contain columns: id, source, flux_int, flux_int_err, flux_peak, flux_peak_err, has_siblings.</p> required <code>n_cpu</code> <code>int</code> <p>The desired number of workers for Dask</p> <code>0</code> <code>max_partition_mb</code> <code>int</code> <p>The desired maximum size (in MB) of the partitions for Dask.</p> <code>15</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Measurement pairs and 2-epoch metrics. Will contain columns: source - the source ID id_a, id_b - the measurement IDs flux_int_a, flux_int_b - measurement integrated fluxes in mJy flux_int_err_a, flux_int_err_b - measurement integrated flux errors in mJy flux_peak_a, flux_peak_b - measurement peak fluxes in mJy/beam flux_peak_err_a, flux_peak_err_b - measurement peak flux errors in mJy/beam vs_peak, vs_int - variability t-statistic m_peak, m_int - variability modulation index</p> Source code in <code>vast_pipeline/pipeline/pairs.py</code> <pre><code>def calculate_measurement_pair_metrics(\n        df: pd.DataFrame, n_cpu: int = 0, max_partition_mb: int = 15) -&gt; pd.DataFrame:\n    \"\"\"Generate a DataFrame of measurement pairs and their 2-epoch variability metrics\n    from a DataFrame of measurements. For more information on the variability metrics, see\n    Section 5 of Mooley et al. (2016), DOI: 10.3847/0004-637X/818/2/105.\n\n    Args:\n        df (pd.DataFrame): Input measurements. Must contain columns: id, source, flux_int,\n            flux_int_err, flux_peak, flux_peak_err, has_siblings.\n        n_cpu:\n            The desired number of workers for Dask\n        max_partition_mb:\n            The desired maximum size (in MB) of the partitions for Dask.\n\n    Returns:\n        Measurement pairs and 2-epoch metrics. Will contain columns:\n            source - the source ID\n            id_a, id_b - the measurement IDs\n            flux_int_a, flux_int_b - measurement integrated fluxes in mJy\n            flux_int_err_a, flux_int_err_b - measurement integrated flux errors in mJy\n            flux_peak_a, flux_peak_b - measurement peak fluxes in mJy/beam\n            flux_peak_err_a, flux_peak_err_b - measurement peak flux errors in mJy/beam\n            vs_peak, vs_int - variability t-statistic\n            m_peak, m_int - variability modulation index\n    \"\"\"\n\n    n_workers, n_partitions = calculate_workers_and_partitions(\n        df,\n        n_cpu=n_cpu,\n        max_partition_mb=max_partition_mb\n    )\n    logger.debug(f\"Running association with {n_workers} CPUs\")\n\n    \"\"\"Create a DataFrame containing all measurement ID combinations per source.\n    Resultant DataFrame will have a MultiIndex([\"source\", RangeIndex]) where \"source\" is\n    the source ID and RangeIndex is an unnamed temporary ID for each measurement pair,\n    unique only together with source.\n    DataFrame will have columns [0, 1], each containing a measurement ID. e.g.\n                       0      1\n        source\n        1       0      1   9284\n                1      1  17597\n                2      1  26984\n                3   9284  17597\n                4   9284  26984\n        ...          ...    ...\n        11105   2  11845  19961\n        11124   0   3573  12929\n                1   3573  21994\n                2  12929  21994\n        11128   0   6216  23534\n    \"\"\"\n    measurement_combinations = (\n        dd.from_pandas(df, npartitions=n_partitions)\n        .groupby(\"source\")[\"id\"]\n        .apply(\n            lambda x: pd.DataFrame(list(combinations(x, 2))), meta={0: \"i\", 1: \"i\"},)\n        .compute(num_workers=n_workers, scheduler=\"processes\")\n    )\n\n    \"\"\"Drop the RangeIndex from the MultiIndex as it isn't required and rename the columns.\n    Example resultant DataFrame:\n               source   id_a   id_b\n        0           1      1   9284\n        1           1      1  17597\n        2           1      1  26984\n        3           1   9284  17597\n        4           1   9284  26984\n        ...       ...    ...    ...\n        33640   11105  11845  19961\n        33641   11124   3573  12929\n        33642   11124   3573  21994\n        33643   11124  12929  21994\n        33644   11128   6216  23534\n    Where source is the source ID, id_a and id_b are measurement IDs.\n    \"\"\"\n    measurement_combinations = measurement_combinations.reset_index(\n        level=1, drop=True\n    ).rename(columns={0: \"id_a\", 1: \"id_b\"}).astype(int).reset_index()\n\n    # Dask has a tendency to swap which order the measurement pairs are\n    # defined in, even if the dataframe is pre-sorted. We want the pairs to be\n    # in date order (a &lt; b) so the code below corrects any that are not.\n    measurement_combinations = measurement_combinations.join(\n        df[['source', 'id', 'datetime']].set_index(['source', 'id']),\n        on=['source', 'id_a'],\n    )\n\n    measurement_combinations = measurement_combinations.join(\n        df[['source', 'id', 'datetime']].set_index(['source', 'id']),\n        on=['source', 'id_b'], lsuffix='_a', rsuffix='_b'\n    )\n\n    to_correct_mask = (\n        measurement_combinations['datetime_a']\n        &gt; measurement_combinations['datetime_b']\n    )\n\n    if np.any(to_correct_mask):\n        logger.debug('Correcting measurement pairs order')\n        (\n            measurement_combinations.loc[to_correct_mask, 'id_a'],\n            measurement_combinations.loc[to_correct_mask, 'id_b']\n        ) = np.array([\n            measurement_combinations.loc[to_correct_mask, 'id_b'].values,\n            measurement_combinations.loc[to_correct_mask, 'id_a'].values\n        ])\n\n    measurement_combinations = measurement_combinations.drop(\n        ['datetime_a', 'datetime_b'], axis=1\n    )\n\n    # add the measurement fluxes and errors\n    association_fluxes = df.set_index([\"source\", \"id\"])[\n        [\"flux_int\", \"flux_int_err\", \"flux_peak\", \"flux_peak_err\", \"image\"]\n    ].rename(columns={\"image\": \"image_name\"})\n    measurement_combinations = measurement_combinations.join(\n        association_fluxes,\n        on=[\"source\", \"id_a\"],\n    ).join(\n        association_fluxes,\n        on=[\"source\", \"id_b\"],\n        lsuffix=\"_a\",\n        rsuffix=\"_b\",\n    )\n\n    # calculate 2-epoch metrics\n    measurement_combinations[\"vs_peak\"] = calculate_vs_metric(\n        measurement_combinations.flux_peak_a,\n        measurement_combinations.flux_peak_b,\n        measurement_combinations.flux_peak_err_a,\n        measurement_combinations.flux_peak_err_b,\n    )\n    measurement_combinations[\"vs_int\"] = calculate_vs_metric(\n        measurement_combinations.flux_int_a,\n        measurement_combinations.flux_int_b,\n        measurement_combinations.flux_int_err_a,\n        measurement_combinations.flux_int_err_b,\n    )\n    measurement_combinations[\"m_peak\"] = calculate_m_metric(\n        measurement_combinations.flux_peak_a,\n        measurement_combinations.flux_peak_b,\n    )\n    measurement_combinations[\"m_int\"] = calculate_m_metric(\n        measurement_combinations.flux_int_a,\n        measurement_combinations.flux_int_b,\n    )\n\n    return measurement_combinations\n</code></pre>"},{"location":"reference/pipeline/pairs/#vast_pipeline.pipeline.pairs.calculate_vs_metric","title":"<code>calculate_vs_metric(flux_a, flux_b, flux_err_a, flux_err_b)</code>","text":"<p>Calculate the Vs variability metric which is the t-statistic that the provided fluxes are variable. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105.</p> <p>Parameters:</p> Name Type Description Default <code>flux_a</code> <code>float</code> <p>flux value \"A\".</p> required <code>flux_b</code> <code>float</code> <p>flux value \"B\".</p> required <code>flux_err_a</code> <code>float</code> <p>error of <code>flux_a</code>.</p> required <code>flux_err_b</code> <code>float</code> <p>error of <code>flux_b</code>.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>the Vs metric for flux values \"A\" and \"B\".</p> Source code in <code>vast_pipeline/pipeline/pairs.py</code> <pre><code>def calculate_vs_metric(\n    flux_a: float, flux_b: float, flux_err_a: float, flux_err_b: float\n) -&gt; float:\n    \"\"\"Calculate the Vs variability metric which is the t-statistic that the provided\n    fluxes are variable. See Section 5 of Mooley et al. (2016) for details,\n    DOI: 10.3847/0004-637X/818/2/105.\n\n    Args:\n        flux_a (float): flux value \"A\".\n        flux_b (float): flux value \"B\".\n        flux_err_a (float): error of `flux_a`.\n        flux_err_b (float): error of `flux_b`.\n\n    Returns:\n        float: the Vs metric for flux values \"A\" and \"B\".\n    \"\"\"\n    return (flux_a - flux_b) / np.hypot(flux_err_a, flux_err_b)\n</code></pre>"},{"location":"reference/pipeline/utils/","title":"utils.py","text":"<p>This module contains utility functions that are used by the pipeline during the processing of a run.</p>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.add_new_many_to_one_relations","title":"<code>add_new_many_to_one_relations(row)</code>","text":"<p>This handles the relation information being created from the many_to_one function in advanced association. It is a lot simpler than the one_to_many case as it purely just adds the new relations to the relation column, taking into account if it is already a list of relations or not (i.e. no previous relations).</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>The relation information Series from the association dataframe. Only the columns ['related_skyc1', 'new_relations'] are required.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The new related field for the source in question, containing the appended ids.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def add_new_many_to_one_relations(row: pd.Series) -&gt; List[int]:\n    \"\"\"\n    This handles the relation information being created from the\n    many_to_one function in advanced association.\n    It is a lot simpler than the one_to_many case as it purely just adds\n    the new relations to the relation column, taking into account if it is\n    already a list of relations or not (i.e. no previous relations).\n\n    Args:\n        row:\n            The relation information Series from the association dataframe.\n            Only the columns ['related_skyc1', 'new_relations'] are required.\n\n    Returns:\n        The new related field for the source in question, containing the\n            appended ids.\n    \"\"\"\n    out = row['new_relations'].copy()\n\n    if isinstance(row['related_skyc1'], list):\n        out += row['related_skyc1'].copy()\n\n    return out\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.add_new_one_to_many_relations","title":"<code>add_new_one_to_many_relations(row, advanced=False, source_ids=None)</code>","text":"<p>This handles the relation information being created from the one_to_many functions in association.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>The relation information Series from the association dataframe. Only the columns ['related_skyc1', 'source_skyc1'] are required for advanced, these are instead called ['related', 'source'] for basic.</p> required <code>advanced</code> <code>bool</code> <p>Whether advanced association is being used which changes the names of the columns involved.</p> <code>False</code> <code>source_ids</code> <code>Optional[DataFrame]</code> <p>A dataframe that contains the other ids to append to related for each original source. +----------------+--------+ |   source_skyc1 | 0      | |----------------+--------| |            122 | [5542] | |            254 | [5543] | |            262 | [5544] | |            405 | [5545] | |            656 | [5546] | +----------------+--------+</p> <code>None</code> <p>Returns:</p> Type Description <code>List[int]</code> <p>The new related field for the source in question, containing the appended ids.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def add_new_one_to_many_relations(\n    row: pd.Series, advanced: bool = False,\n    source_ids: Optional[pd.DataFrame] = None\n) -&gt; List[int]:\n    \"\"\"\n    This handles the relation information being created from the\n    one_to_many functions in association.\n\n    Args:\n        row:\n            The relation information Series from the association dataframe.\n            Only the columns ['related_skyc1', 'source_skyc1'] are required\n            for advanced, these are instead called ['related', 'source']\n            for basic.\n        advanced:\n            Whether advanced association is being used which changes the names\n            of the columns involved.\n        source_ids:\n            A dataframe that contains the other ids to append to related for\n            each original source.\n            +----------------+--------+\n            |   source_skyc1 | 0      |\n            |----------------+--------|\n            |            122 | [5542] |\n            |            254 | [5543] |\n            |            262 | [5544] |\n            |            405 | [5545] |\n            |            656 | [5546] |\n            +----------------+--------+\n\n    Returns:\n        The new related field for the source in question, containing the\n            appended ids.\n    \"\"\"\n    if source_ids is None:\n        source_ids = pd.DataFrame()\n\n    related_col = 'related_skyc1' if advanced else 'related'\n    source_col = 'source_skyc1' if advanced else 'source'\n\n    # this is the not_original case where the original source id is appended.\n    if source_ids.empty:\n        if isinstance(row[related_col], list):\n            out = row[related_col]\n            out.append(row[source_col])\n        else:\n            out = [row[source_col], ]\n\n    else:  # the original case to append all the new ids.\n        source_ids = source_ids.loc[row[source_col]].iloc[0]\n        if isinstance(row[related_col], list):\n            out = row[related_col] + source_ids\n        else:\n            out = source_ids\n\n    return out\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.add_run_to_img","title":"<code>add_run_to_img(pipeline_run, img)</code>","text":"<p>Add a pipeline run to an Image (and corresponding SkyRegion) in the db</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_run</code> <code>Run</code> <p>Pipeline run object you want to add.</p> required <code>img</code> <code>Image</code> <p>Image object you want to add to.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def add_run_to_img(pipeline_run: Run, img: Image) -&gt; None:\n    \"\"\"\n    Add a pipeline run to an Image (and corresponding SkyRegion) in the db\n\n    Args:\n        pipeline_run:\n            Pipeline run object you want to add.\n        img:\n            Image object you want to add to.\n\n    Returns:\n        None\n    \"\"\"\n    skyreg = img.skyreg\n    # check and add the many to many if not existent\n    if not Image.objects.filter(id=img.id, run__id=pipeline_run.id).exists():\n        logger.info('Adding %s to image %s', pipeline_run, img.name)\n        img.run.add(pipeline_run)\n\n    if pipeline_run not in skyreg.run.all():\n        logger.info('Adding %s to sky region %s', pipeline_run, skyreg)\n        skyreg.run.add(pipeline_run)\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.backup_parquets","title":"<code>backup_parquets(p_run_path)</code>","text":"<p>Backups up all the existing parquet files in a pipeline run directory. Backups are named with a '.bak' suffix in the pipeline run directory.</p> <p>Parameters:</p> Name Type Description Default <code>p_run_path</code> <code>str</code> <p>The path of the pipeline run where the parquets are stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def backup_parquets(p_run_path: str) -&gt; None:\n    \"\"\"\n    Backups up all the existing parquet files in a pipeline run directory.\n    Backups are named with a '.bak' suffix in the pipeline run directory.\n\n    Args:\n        p_run_path:\n            The path of the pipeline run where the parquets are stored.\n\n    Returns:\n        None\n    \"\"\"\n    parquets = (\n        glob.glob(os.path.join(p_run_path, \"*.parquet\"))\n        # TODO Remove arrow when arrow files are no longer required.\n        + glob.glob(os.path.join(p_run_path, \"*.arrow\")))\n\n    for i in parquets:\n        backup_name = i + '.bak'\n        if os.path.isfile(backup_name):\n            logger.debug(f'Removing old backup file: {backup_name}.')\n            os.remove(backup_name)\n        shutil.copyfile(i, backup_name)\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.calc_ave_coord","title":"<code>calc_ave_coord(grp)</code>","text":"<p>Calculates the average coordinate of the grouped by sources dataframe for each unique group, along with defining the image and epoch list for each unique source (group).</p> <p>Parameters:</p> Name Type Description Default <code>grp</code> <code>DataFrame</code> <p>The current group dataframe (unique source) of the grouped by dataframe being acted upon.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A pandas series containing the average coordinate along with the image and epoch lists.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def calc_ave_coord(grp: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"\n    Calculates the average coordinate of the grouped by sources dataframe for\n    each unique group, along with defining the image and epoch list for each\n    unique source (group).\n\n    Args:\n        grp: The current group dataframe (unique source) of the grouped by\n            dataframe being acted upon.\n\n    Returns:\n        A pandas series containing the average coordinate along with the\n            image and epoch lists.\n    \"\"\"\n    d = {}\n    grp = grp.sort_values(by='datetime')\n    d['img_list'] = grp['image'].values.tolist()\n    d['epoch_list'] = grp['epoch'].values.tolist()\n    d['wavg_ra'] = grp['interim_ew'].sum() / grp['weight_ew'].sum()\n    d['wavg_dec'] = grp['interim_ns'].sum() / grp['weight_ns'].sum()\n\n    return pd.Series(d)\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.check_primary_image","title":"<code>check_primary_image(row)</code>","text":"<p>Checks whether the primary image of the ideal source dataframe is in the image list for the source.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Input dataframe row, with columns ['primary'] and ['img_list'].</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if primary in image list else False.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def check_primary_image(row: pd.Series) -&gt; bool:\n    \"\"\"\n    Checks whether the primary image of the ideal source\n    dataframe is in the image list for the source.\n\n    Args:\n        row:\n            Input dataframe row, with columns ['primary'] and ['img_list'].\n\n    Returns:\n        True if primary in image list else False.\n    \"\"\"\n    return row['primary'] in row['img_list']\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.create_measurement_pairs_arrow_file","title":"<code>create_measurement_pairs_arrow_file(p_run)</code>","text":"<p>Creates a measurement_pairs.arrow file using the parquet outputs of a pipeline run.</p> <p>Parameters:</p> Name Type Description Default <code>p_run</code> <code>Run</code> <p>Pipeline model instance.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def create_measurement_pairs_arrow_file(p_run: Run) -&gt; None:\n    \"\"\"\n    Creates a measurement_pairs.arrow file using the parquet outputs\n    of a pipeline run.\n\n    Args:\n        p_run:\n            Pipeline model instance.\n\n    Returns:\n        None\n    \"\"\"\n    logger.info('Creating measurement_pairs.arrow for run %s.', p_run.name)\n\n    measurement_pairs_df = pd.read_parquet(\n        os.path.join(\n            p_run.path,\n            'measurement_pairs.parquet'\n        )\n    )\n\n    logger.debug('Optimising dataframe.')\n    measurement_pairs_df = optimize_ints(optimize_floats(measurement_pairs_df))\n\n    logger.debug(\"Loading to pyarrow table.\")\n    measurement_pairs_df = pa.Table.from_pandas(measurement_pairs_df)\n\n    logger.debug(\"Exporting to arrow file.\")\n    outname = os.path.join(p_run.path, 'measurement_pairs.arrow')\n\n    local = pa.fs.LocalFileSystem()\n\n    with local.open_output_stream(outname) as file:\n        with pa.RecordBatchFileWriter(file, measurement_pairs_df.schema) as writer:\n            writer.write_table(measurement_pairs_df)\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.create_measurements_arrow_file","title":"<code>create_measurements_arrow_file(p_run)</code>","text":"<p>Creates a measurements.arrow file using the parquet outputs of a pipeline run.</p> <p>Parameters:</p> Name Type Description Default <code>p_run</code> <code>Run</code> <p>Pipeline model instance.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def create_measurements_arrow_file(p_run: Run) -&gt; None:\n    \"\"\"\n    Creates a measurements.arrow file using the parquet outputs\n    of a pipeline run.\n\n    Args:\n        p_run:\n            Pipeline model instance.\n\n    Returns:\n        None\n    \"\"\"\n    logger.info('Creating measurements.arrow for run %s.', p_run.name)\n\n    associations = pd.read_parquet(\n        os.path.join(\n            p_run.path,\n            'associations.parquet'\n        )\n    )\n    images = pd.read_parquet(\n        os.path.join(\n            p_run.path,\n            'images.parquet'\n        )\n    )\n\n    m_files = images['measurements_path'].tolist()\n\n    m_files += glob.glob(os.path.join(\n        p_run.path,\n        'forced*.parquet'\n    ))\n\n    logger.debug('Loading %i files...', len(m_files))\n    measurements = dd.read_parquet(m_files, engine='pyarrow').compute()\n\n    measurements = measurements.loc[\n        measurements['id'].isin(associations['meas_id'].values)\n    ]\n\n    measurements = (\n        associations.loc[:, ['meas_id', 'source_id']]\n        .set_index('meas_id')\n        .merge(\n            measurements,\n            left_index=True,\n            right_on='id'\n        )\n        .rename(columns={'source_id': 'source'})\n    )\n\n    # drop timezone from datetime for vaex compatibility\n    # TODO: Look to keep the timezone if/when vaex is compatible.\n    measurements['time'] = measurements['time'].dt.tz_localize(None)\n\n    logger.debug('Optimising dataframes.')\n    measurements = optimize_ints(optimize_floats(measurements))\n\n    logger.debug(\"Loading to pyarrow table.\")\n    measurements = pa.Table.from_pandas(measurements)\n\n    logger.debug(\"Exporting to arrow file.\")\n    outname = os.path.join(p_run.path, 'measurements.arrow')\n\n    local = pa.fs.LocalFileSystem()\n\n    with local.open_output_stream(outname) as file:\n        with pa.RecordBatchFileWriter(file, measurements.schema) as writer:\n            writer.write_table(measurements)\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.create_temp_config_file","title":"<code>create_temp_config_file(p_run_path)</code>","text":"<p>Creates the temp config file which is saved at the beginning of each run.</p> <p>It is to avoid issues created by users changing the config while the run is running.</p> <p>Parameters:</p> Name Type Description Default <code>p_run_path</code> <code>str</code> <p>The path of the pipeline run of the config to be copied.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def create_temp_config_file(p_run_path: str) -&gt; None:\n    \"\"\"\n    Creates the temp config file which is saved at the beginning of each run.\n\n    It is to avoid issues created by users changing the config while the run\n    is running.\n\n    Args:\n        p_run_path:\n            The path of the pipeline run of the config to be copied.\n\n    Returns:\n        None\n    \"\"\"\n    config_name = 'config.yaml'\n    temp_config_name = 'config_temp.yaml'\n\n    shutil.copyfile(\n        os.path.join(p_run_path, config_name),\n        os.path.join(p_run_path, temp_config_name)\n    )\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.cross_join","title":"<code>cross_join(left, right)</code>","text":"<p>A convenience function to merge two dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>left</code> <code>DataFrame</code> <p>The base pandas DataFrame to merge.</p> required <code>right</code> <code>DataFrame</code> <p>The pandas DataFrame to merge to the left.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The resultant merged DataFrame.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def cross_join(left: pd.DataFrame, right: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    A convenience function to merge two dataframes.\n\n    Args:\n        left: The base pandas DataFrame to merge.\n        right: The pandas DataFrame to merge to the left.\n\n    Returns:\n        The resultant merged DataFrame.\n    \"\"\"\n    return (\n        left.assign(key=1)\n        .merge(right.assign(key=1), on='key')\n        .drop('key', axis=1)\n    )\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_create_img","title":"<code>get_create_img(band_id, image)</code>","text":"<p>Function to fetch or create the Image and Sky Region objects for an image.</p> <p>Parameters:</p> Name Type Description Default <code>band_id</code> <code>int</code> <p>The integer database id value of the frequency band of the image.</p> required <code>image</code> <code>SelavyImage</code> <p>The image object.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>The resulting image django ORM object.</p> <code>bool</code> <p><code>True</code> the image already existed in the database, <code>False</code> if not.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def get_create_img(band_id: int, image: SelavyImage) -&gt; Tuple[Image, bool]:\n    \"\"\"\n    Function to fetch or create the Image and Sky Region objects for an image.\n\n    Args:\n        band_id: The integer database id value of the frequency band of the\n            image.\n        image: The image object.\n\n    Returns:\n        The resulting image django ORM object.\n        `True` the image already existed in the database, `False` if not.\n    \"\"\"\n    images = Image.objects.filter(name__exact=image.name)\n    exists = images.exists()\n    if exists:\n        img: Image = images.get()\n        # Add background path if not originally provided\n        if image.background_path and not img.background_path:\n            img.background_path = image.background_path\n            img.save()\n    else:\n        # at this stage, measurement parquet file is not created but\n        # assume location\n        img_folder_name = image.name.replace('.', '_')\n        measurements_path = os.path.join(\n            settings.PIPELINE_WORKING_DIR,\n            'images',\n            img_folder_name,\n            'measurements.parquet'\n        )\n        img = Image(\n            band_id=band_id,\n            measurements_path=measurements_path\n        )\n\n        # set the attributes and save the image,\n        # by selecting only valid (not hidden) attributes\n        # FYI attributs and/or method starting with _ are hidden\n        # and with __ can't be modified/called\n        for fld in img._meta.get_fields():\n            if getattr(fld, 'attname', None) and (\n                    getattr(image, fld.attname, None) is not None):\n                setattr(img, fld.attname, getattr(image, fld.attname))\n\n        img.rms_median, img.rms_min, img.rms_max = get_rms_noise_image_values(\n            img.noise_path)\n\n        # get create the sky region and associate with image\n        img.skyreg = get_create_skyreg(img)\n        img.save()\n\n    return (img, exists)\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_create_img_band","title":"<code>get_create_img_band(image)</code>","text":"<p>Return the existing Band row for the given FitsImage. An image is considered to belong to a band if its frequency is within some tolerance of the band's frequency. Returns a Band row or None if no matching band.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>FitsImage</code> <p>The image Django ORM object.</p> required <p>Returns:</p> Type Description <code>Band</code> <p>The band Django ORM object.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def get_create_img_band(image: FitsImage) -&gt; Band:\n    '''\n    Return the existing Band row for the given FitsImage.\n    An image is considered to belong to a band if its frequency is within some\n    tolerance of the band's frequency.\n    Returns a Band row or None if no matching band.\n\n    Args:\n        image: The image Django ORM object.\n\n    Returns:\n        The band Django ORM object.\n    '''\n    # For now we match bands using the central frequency.\n    # This assumes that every band has a unique frequency,\n    # which is true for the data we've used so far.\n    freq = int(image.freq_eff * 1.e-6)\n    freq_band = int(image.freq_bw * 1.e-6)\n    # TODO: refine the band query\n    for band in Band.objects.all():\n        diff = abs(freq - band.frequency) / float(band.frequency)\n        if diff &lt; 0.02:\n            return band\n\n    # no band has been found so create it\n    band = Band(name=str(freq), frequency=freq, bandwidth=freq_band)\n    logger.info('Adding new frequency band: %s', band)\n    band.save()\n\n    return band\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_create_p_run","title":"<code>get_create_p_run(name, path, description=None, user=None)</code>","text":"<p>Get or create a pipeline run in db, return the run django object and a flag True/False if has been created or already exists.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the pipeline run.</p> required <code>path</code> <code>str</code> <p>The system path to the pipeline run folder which contains the configuration file and where outputs will be saved.</p> required <code>description</code> <code>str</code> <p>An optional description of the pipeline run.</p> <code>None</code> <code>user</code> <code>User</code> <p>The Django user that launched the pipeline run.</p> <code>None</code> <p>Returns:</p> Type Description <code>Run</code> <p>The pipeline run object.</p> <code>bool</code> <p>Whether the pipeline run already existed ('True') or not ('False').</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def get_create_p_run(\n    name: str, path: str, description: str = None, user: User = None\n) -&gt; Tuple[Run, bool]:\n    '''\n    Get or create a pipeline run in db, return the run django object and\n    a flag True/False if has been created or already exists.\n\n    Args:\n        name: The name of the pipeline run.\n        path: The system path to the pipeline run folder which contains the\n            configuration file and where outputs will be saved.\n        description: An optional description of the pipeline run.\n        user: The Django user that launched the pipeline run.\n\n    Returns:\n        The pipeline run object.\n        Whether the pipeline run already existed ('True') or not ('False').\n    '''\n    p_run = Run.objects.filter(name__exact=name)\n    if p_run:\n        return p_run.get(), True\n\n    description = \"\" if description is None else description\n    p_run = Run(name=name, description=description, path=path)\n    if user:\n        p_run.user = user\n    p_run.save()\n\n    return p_run, False\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_create_skyreg","title":"<code>get_create_skyreg(image)</code>","text":"<p>This creates a Sky Region object in Django ORM given the related image object.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The image Django ORM object.</p> required <p>Returns:</p> Type Description <code>SkyRegion</code> <p>The sky region Django ORM object.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def get_create_skyreg(image: Image) -&gt; SkyRegion:\n    '''\n    This creates a Sky Region object in Django ORM given the related\n    image object.\n\n    Args:\n        image: The image Django ORM object.\n\n    Returns:\n        The sky region Django ORM object.\n    '''\n    # In the calculations below, it is assumed the image has square\n    # pixels (this pipeline has been designed for ASKAP images, so it\n    # should always be square). It will likely give wrong results if not\n    skyregions = SkyRegion.objects.filter(\n        centre_ra=image.ra,\n        centre_dec=image.dec,\n        xtr_radius=image.fov_bmin\n    )\n    if skyregions:\n        skyr = skyregions.get()\n        logger.info('Found sky region %s', skyr)\n    else:\n        x, y, z = eq_to_cart(image.ra, image.dec)\n        skyr = SkyRegion(\n            centre_ra=image.ra,\n            centre_dec=image.dec,\n            width_ra=image.physical_bmin,\n            width_dec=image.physical_bmaj,\n            xtr_radius=image.fov_bmin,\n            x=x,\n            y=y,\n            z=z,\n        )\n        skyr.save()\n        logger.info('Created sky region %s', skyr)\n\n    return skyr\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_df_memory_usage","title":"<code>get_df_memory_usage(df)</code>","text":"<p>This function calculates the memory usage of a pandas dataframe and logs it.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The pandas dataframe to calculate the memory usage of.</p> required <p>Returns:</p> Type Description <p>The pandas dataframe memory usage in MB</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def get_df_memory_usage(df):\n    \"\"\"\n    This function calculates the memory usage of a pandas dataframe and\n    logs it.\n\n    Args:\n        df: The pandas dataframe to calculate the memory usage of.\n\n    Returns:\n        The pandas dataframe memory usage in MB\n    \"\"\"\n    mem = df.memory_usage(deep=True).sum() / 1e6\n\n    return mem\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_eta_metric","title":"<code>get_eta_metric(row, df, peak=False)</code>","text":"<p>Calculates the eta variability metric of a source. Works on the grouped by dataframe using the fluxes of the associated measurements.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Dict[str, float]</code> <p>Dictionary containing statistics for the current source.</p> required <code>df</code> <code>DataFrame</code> <p>The grouped by sources dataframe of the measurements containing all the flux and flux error information,</p> required <code>peak</code> <code>bool</code> <p>Whether to use peak_flux for the calculation. If False then the integrated flux is used.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>The calculated eta value.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def get_eta_metric(\n    row: Dict[str, float], df: pd.DataFrame, peak: bool = False\n) -&gt; float:\n    '''\n    Calculates the eta variability metric of a source.\n    Works on the grouped by dataframe using the fluxes\n    of the associated measurements.\n\n    Args:\n        row: Dictionary containing statistics for the current source.\n        df: The grouped by sources dataframe of the measurements containing all\n            the flux and flux error information,\n        peak: Whether to use peak_flux for the calculation. If False then the\n            integrated flux is used.\n\n    Returns:\n        The calculated eta value.\n    '''\n    if row['n_meas'] == 1:\n        return 0.\n\n    suffix = 'peak' if peak else 'int'\n    weights = 1. / df[f'flux_{suffix}_err'].values**2\n    fluxes = df[f'flux_{suffix}'].values\n    eta = (row['n_meas'] / (row['n_meas'] - 1)) * (\n        (weights * fluxes**2).mean() - (\n            (weights * fluxes).mean()**2 / weights.mean()\n        )\n    )\n    return eta\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_image_list_diff","title":"<code>get_image_list_diff(row)</code>","text":"<p>Calculate the difference between the ideal coverage image list of a source and the actual observed image list. Also checks whether an epoch does in fact contain a detection but is not in the expected 'ideal' image for that epoch.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>The row from the sources dataframe that is being iterated over.</p> required <p>Returns:</p> Type Description <code>Union[List[str], int]</code> <p>A list of the images missing from the observed image list.</p> <code>Union[List[str], int]</code> <p>A '-1' integer value if there are no missing images.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def get_image_list_diff(row: pd.Series) -&gt; Union[List[str], int]:\n    \"\"\"\n    Calculate the difference between the ideal coverage image list of a source\n    and the actual observed image list. Also checks whether an epoch does in\n    fact contain a detection but is not in the expected 'ideal' image for that\n    epoch.\n\n    Args:\n        row: The row from the sources dataframe that is being iterated over.\n\n    Returns:\n        A list of the images missing from the observed image list.\n        A '-1' integer value if there are no missing images.\n    \"\"\"\n    out = list(\n        filter(lambda arg: arg not in row['img_list'], row['skyreg_img_list'])\n    )\n\n    # set empty list to -1\n    if not out:\n        return -1\n\n    # Check that an epoch has not already been seen (just not in the 'ideal'\n    # image)\n    out_epochs = [\n        row['skyreg_epoch'][pair[0]] for pair in enumerate(\n            row['skyreg_img_list']\n        ) if pair[1] in out\n    ]\n\n    out = [\n        out[pair[0]] for pair in enumerate(\n            out_epochs\n        ) if pair[1] not in row['epoch_list']\n    ]\n\n    if not out:\n        return -1\n\n    return out\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_names_and_epochs","title":"<code>get_names_and_epochs(grp)</code>","text":"<p>Convenience function to group together the image names, epochs and datetimes into one list object which is then returned as a pandas series. This is necessary for easier processing in the ideal coverage analysis.</p> <p>Parameters:</p> Name Type Description Default <code>grp</code> <code>DataFrame</code> <p>A group from the grouped by sources DataFrame.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Pandas series containing the list object that contains the lists of the image names, epochs and datetimes.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def get_names_and_epochs(grp: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"\n    Convenience function to group together the image names, epochs and\n    datetimes into one list object which is then returned as a pandas series.\n    This is necessary for easier processing in the ideal coverage analysis.\n\n    Args:\n        grp: A group from the grouped by sources DataFrame.\n\n    Returns:\n        Pandas series containing the list object that contains the lists of the\n            image names, epochs and datetimes.\n    \"\"\"\n    d = {}\n    d['skyreg_img_epoch_list'] = [[[x, ], y, z] for x, y, z in zip(\n        grp['name'].values.tolist(),\n        grp['epoch'].values.tolist(),\n        grp['datetime'].values.tolist()\n    )]\n\n    return pd.Series(d)\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_parallel_assoc_image_df","title":"<code>get_parallel_assoc_image_df(images, skyregion_groups)</code>","text":"<p>Merge the sky region groups with the images and skyreg_ids.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List[Image]</code> <p>A list of the Image objects.</p> required <code>skyregion_groups</code> <code>DataFrame</code> <p>The sky region group of each skyregion id. +----+----------------+ |    |   skyreg_group | |----+----------------| |  2 |              1 | |  3 |              1 | |  1 |              2 | +----+----------------+</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe containing the merged images and skyreg_id and skyreg_group (see source code for output format).</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def get_parallel_assoc_image_df(\n    images: List[Image], skyregion_groups: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Merge the sky region groups with the images and skyreg_ids.\n\n    Args:\n        images:\n            A list of the Image objects.\n        skyregion_groups:\n            The sky region group of each skyregion id.\n            +----+----------------+\n            |    |   skyreg_group |\n            |----+----------------|\n            |  2 |              1 |\n            |  3 |              1 |\n            |  1 |              2 |\n            +----+----------------+\n\n    Returns:\n        Dataframe containing the merged images and skyreg_id and skyreg_group\n            (see source code for output format).\n    \"\"\"\n    # Output format\n    # +----+-------------------------------+-------------+----------------+\n    # |    | image                         |   skyreg_id |   skyreg_group |\n    # |----+-------------------------------+-------------+----------------|\n    # |  0 | VAST_2118+00A.EPOCH01.I.fits  |           2 |              1 |\n    # |  1 | VAST_2118-06A.EPOCH01.I.fits  |           3 |              1 |\n    # |  2 | VAST_0127-73A.EPOCH01.I.fits  |           1 |              2 |\n    # |  3 | VAST_2118-06A.EPOCH03x.I.fits |           3 |              1 |\n    # |  4 | VAST_2118-06A.EPOCH02.I.fits  |           3 |              1 |\n    # |  5 | VAST_2118-06A.EPOCH05x.I.fits |           3 |              1 |\n    # |  6 | VAST_2118-06A.EPOCH06x.I.fits |           3 |              1 |\n    # |  7 | VAST_0127-73A.EPOCH08.I.fits  |           1 |              2 |\n    # +----+-------------------------------+-------------+----------------+\n    skyreg_ids = [i.skyreg_id for i in images]\n\n    images_df = pd.DataFrame({\n        'image_dj': images,\n        'skyreg_id': skyreg_ids,\n    })\n\n    images_df = images_df.merge(\n        skyregion_groups,\n        how='left',\n        left_on='skyreg_id',\n        right_index=True\n    )\n\n    images_df['image_name'] = images_df['image_dj'].apply(\n        lambda x: x.name\n    )\n\n    images_df['image_datetime'] = images_df['image_dj'].apply(\n        lambda x: x.datetime\n    )\n\n    return images_df\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_rms_noise_image_values","title":"<code>get_rms_noise_image_values(rms_path)</code>","text":"<p>Open the RMS noise FITS file and compute the median, max and min rms values to be added to the image model and then used in the calculations.</p> <p>Parameters:</p> Name Type Description Default <code>rms_path</code> <code>str</code> <p>The system path to the RMS FITS image.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The median value of the RMS image.</p> <code>float</code> <p>The minimum value of the RMS image.</p> <code>float</code> <p>The maximum value of the RMS image.</p> <p>Raises:</p> Type Description <code>IOError</code> <p>Raised when the RMS FITS file cannot be found.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def get_rms_noise_image_values(rms_path: str) -&gt; Tuple[float, float, float]:\n    '''\n    Open the RMS noise FITS file and compute the median, max and min\n    rms values to be added to the image model and then used in the\n    calculations.\n\n    Args:\n        rms_path: The system path to the RMS FITS image.\n\n    Returns:\n        The median value of the RMS image.\n        The minimum value of the RMS image.\n        The maximum value of the RMS image.\n\n    Raises:\n        IOError: Raised when the RMS FITS file cannot be found.\n    '''\n    logger.debug('Extracting Image RMS values from Noise file...')\n    med_val = min_val = max_val = 0.\n    try:\n        with open_fits(rms_path) as f:\n            data = f[0].data\n            data = data[np.isfinite(data) &amp; (data &gt; 0.)]\n            med_val = np.median(data) * 1e+3\n            min_val = np.min(data) * 1e+3\n            max_val = np.max(data) * 1e+3\n            del data\n    except Exception:\n        raise IOError(f'Could not read this RMS FITS file: {rms_path}')\n    logger.debug('Image RMS Min: %.3g Max: %.3g Median: %.3g', min_val, max_val, med_val)\n\n    return med_val, min_val, max_val\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_src_skyregion_merged_df","title":"<code>get_src_skyregion_merged_df(sources_df, images_df, skyreg_df, n_cpu=0, max_partition_mb=15)</code>","text":"<p>Analyses the current sources_df to determine what the 'ideal coverage' for each source should be. In other words, what images is the source missing in when it should have been seen.</p> <p>Parameters:</p> Name Type Description Default <code>sources_df</code> <code>DataFrame</code> <p>The output of the association  step containing the measurements associated into sources.</p> required <code>images_df</code> <code>DataFrame</code> <p>Contains the images of the pipeline run. I.e. all image objects for the run loaded into a dataframe.</p> required <code>skyreg_df</code> <code>DataFrame</code> <p>Contains the sky regions of the pipeline run. I.e. all sky region objects for the run loaded into a dataframe.</p> required <code>n_cpu</code> <code>int</code> <p>The desired number of workers for Dask</p> <code>0</code> <code>max_partition_mb</code> <code>int</code> <p>The desired maximum size (in MB) of the partitions for Dask.</p> <code>15</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing missing image information (see source code for dataframe format).</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def get_src_skyregion_merged_df(\n    sources_df: pd.DataFrame, images_df: pd.DataFrame, skyreg_df: pd.DataFrame,\n    n_cpu: int = 0, max_partition_mb: int = 15\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Analyses the current sources_df to determine what the 'ideal coverage'\n    for each source should be. In other words, what images is the source\n    missing in when it should have been seen.\n\n    Args:\n        sources_df:\n            The output of the association  step containing the\n            measurements associated into sources.\n        images_df:\n            Contains the images of the pipeline run. I.e. all image\n            objects for the run loaded into a dataframe.\n        skyreg_df:\n            Contains the sky regions of the pipeline run. I.e. all\n            sky region objects for the run loaded into a dataframe.\n        n_cpu:\n            The desired number of workers for Dask\n        max_partition_mb:\n            The desired maximum size (in MB) of the partitions for Dask.\n\n    Returns:\n        DataFrame containing missing image information (see source code for\n            dataframe format).\n    \"\"\"\n    # Output format:\n    # +----------+----------------------------------+-----------+------------+\n    # |   source | img_list                         |   wavg_ra |   wavg_dec |\n    # |----------+----------------------------------+-----------+------------+\n    # |      278 | ['VAST_0127-73A.EPOCH01.I.fits'] |  22.2929  |   -71.8717 |\n    # |      702 | ['VAST_0127-73A.EPOCH01.I.fits'] |  28.8125  |   -69.3547 |\n    # |      844 | ['VAST_0127-73A.EPOCH01.I.fits'] |  17.3152  |   -72.346  |\n    # |      934 | ['VAST_0127-73A.EPOCH01.I.fits'] |   9.75754 |   -72.9629 |\n    # |     1290 | ['VAST_0127-73A.EPOCH01.I.fits'] |  20.8455  |   -76.8269 |\n    # +----------+----------------------------------+-----------+------------+\n    # ------------------------------------------------------------------+\n    #  skyreg_img_list                                                  |\n    # ------------------------------------------------------------------+\n    #  ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] |\n    #  ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] |\n    #  ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] |\n    #  ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] |\n    #  ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] |\n    # ------------------------------------------------------------------+\n    # ----------------------------------+------------------------------+\n    #  img_diff                         | primary                      |\n    # ----------------------------------+------------------------------+\n    #  ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits |\n    #  ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits |\n    #  ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits |\n    #  ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits |\n    #  ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits |\n    # ----------------------------------+------------------------------+\n    # ------------------------------+--------------+\n    #  detection                    | in_primary   |\n    # ------------------------------+--------------|\n    #  VAST_0127-73A.EPOCH01.I.fits | True         |\n    #  VAST_0127-73A.EPOCH01.I.fits | True         |\n    #  VAST_0127-73A.EPOCH01.I.fits | True         |\n    #  VAST_0127-73A.EPOCH01.I.fits | True         |\n    #  VAST_0127-73A.EPOCH01.I.fits | True         |\n    # ------------------------------+--------------+\n    logger.info(\"Creating ideal source coverage df...\")\n\n    merged_timer = StopWatch()\n\n    skyreg_df = skyreg_df.drop(\n        ['x', 'y', 'z', 'width_ra', 'width_dec'], axis=1\n    )\n\n    images_df['name'] = images_df['image_dj'].apply(\n        lambda x: x.name\n    )\n    images_df['datetime'] = images_df['image_dj'].apply(\n        lambda x: x.datetime\n    )\n\n    skyreg_df = skyreg_df.join(\n        pd.DataFrame(\n            images_df.groupby('skyreg_id').apply(\n                get_names_and_epochs\n            )\n        ),\n        on='id'\n    )\n\n    sources_df = sources_df.sort_values(by='datetime')\n    # calculate some metrics on sources\n    # compute only some necessary metrics in the groupby\n    timer = StopWatch()\n    srcs_df = parallel_groupby_coord(sources_df,\n                                     n_cpu=n_cpu,\n                                     max_partition_mb=max_partition_mb)\n    logger.debug('Groupby-apply time: %.2f seconds', timer.reset())\n\n    del sources_df\n\n    # crossmatch sources with sky regions up to the max sky region radius\n    skyreg_coords = SkyCoord(\n        ra=skyreg_df.centre_ra, dec=skyreg_df.centre_dec, unit=\"deg\"\n    )\n    srcs_coords = SkyCoord(\n        ra=srcs_df.wavg_ra,\n        dec=srcs_df.wavg_dec,\n        unit=\"deg\")\n    skyreg_idx, srcs_idx, sep, _ = srcs_coords.search_around_sky(\n        skyreg_coords, skyreg_df.xtr_radius.max() * u.deg\n    )\n    skyreg_df = skyreg_df.drop(\n        columns=[\n            \"centre_ra\",\n            \"centre_dec\"]).set_index(\"id\")\n\n    # select rows where separation is less than sky region radius\n    # drop not more useful columns and groupby source id\n    # compute list of images\n    src_skyrg_df = (\n        pd.DataFrame(\n            {\n                \"source\": srcs_df.iloc[srcs_idx].index,\n                \"id\": skyreg_df.iloc[skyreg_idx].index,\n                \"sep\": sep.to(\"deg\").value,\n            }\n        )\n        .merge(skyreg_df, left_on=\"id\", right_index=True)\n        .query(\"sep &lt; xtr_radius\")\n        .drop(columns=[\"id\", \"xtr_radius\"])\n        .explode(\"skyreg_img_epoch_list\")\n    )\n\n    del skyreg_df\n\n    src_skyrg_df[\n        ['skyreg_img_list', 'skyreg_epoch', 'skyreg_datetime']\n    ] = pd.DataFrame(\n        src_skyrg_df['skyreg_img_epoch_list'].tolist(),\n        index=src_skyrg_df.index\n    )\n\n    src_skyrg_df = src_skyrg_df.drop('skyreg_img_epoch_list', axis=1)\n\n    src_skyrg_df = (\n        src_skyrg_df.sort_values(\n            ['source', 'sep']\n        )\n        .drop_duplicates(['source', 'skyreg_epoch'])\n        .sort_values(by='skyreg_datetime')\n        .drop(\n            ['sep', 'skyreg_datetime'],\n            axis=1\n        )\n    )\n    # annoyingly epoch needs to be not a list to drop duplicates\n    # but then we need to sum the epochs into a list\n    src_skyrg_df['skyreg_epoch'] = src_skyrg_df['skyreg_epoch'].apply(\n        lambda x: [x, ]\n    )\n\n    src_skyrg_df = (\n        src_skyrg_df.groupby('source')\n        .sum(numeric_only=False)  # sum because we need to preserve order\n    )\n\n    # merge into main df and compare the images\n    srcs_df = srcs_df.merge(\n        src_skyrg_df, left_index=True, right_index=True\n    )\n\n    del src_skyrg_df\n\n    srcs_df['img_diff'] = srcs_df[\n        ['img_list', 'skyreg_img_list', 'epoch_list', 'skyreg_epoch']\n    ].apply(\n        get_image_list_diff, axis=1\n    )\n\n    srcs_df = srcs_df.loc[\n        srcs_df['img_diff'] != -1\n    ]\n\n    srcs_df = srcs_df.drop(\n        ['epoch_list', 'skyreg_epoch'],\n        axis=1\n    )\n\n    srcs_df['primary'] = srcs_df[\n        'skyreg_img_list'\n    ].apply(lambda x: x[0])\n\n    srcs_df['detection'] = srcs_df[\n        'img_list'\n    ].apply(lambda x: x[0])\n\n    srcs_df['in_primary'] = srcs_df[\n        ['primary', 'img_list']\n    ].apply(\n        check_primary_image,\n        axis=1\n    )\n\n    srcs_df = srcs_df.drop(['img_list', 'skyreg_img_list', 'primary'], axis=1)\n\n    logger.info(\n        'Ideal source coverage time: %.2f seconds', merged_timer.reset()\n    )\n\n    return srcs_df\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_total_memory_usage","title":"<code>get_total_memory_usage()</code>","text":"<p>This function gets the current memory usage and returns a string.</p> <p>Returns:</p> Type Description <p>A float containing the current resource usage.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def get_total_memory_usage():\n    \"\"\"\n    This function gets the current memory usage and returns a string.\n\n    Returns:\n        A float containing the current resource usage.\n    \"\"\"\n    mem = psutil.virtual_memory()[3]  # resource usage in bytes\n    mem = mem / 1024**3  # resource usage in GB\n\n    return mem\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.group_skyregions","title":"<code>group_skyregions(df)</code>","text":"<p>Logic to group sky regions into overlapping groups. Returns a dataframe containing the sky region id as the index and a column containing a list of the sky region group number it belongs to.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe containing all the sky regions of the run. Only the 'id', 'centre_ra', 'centre_dec' and 'xtr_radius' columns are required. +------+-------------+--------------+--------------+ |   id |   centre_ra |   centre_dec |   xtr_radius | |------+-------------+--------------+--------------| |    2 |    319.652  |    0.0030765 |      6.72488 | |    3 |    319.652  |   -6.2989    |      6.7401  | |    1 |     21.8361 |  -73.121     |      7.24662 | +------+-------------+--------------+--------------+</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The sky region group of each skyregion id. +----+----------------+ |    |   skyreg_group | |----+----------------| |  2 |              1 | |  3 |              1 | |  1 |              2 | +----+----------------+</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def group_skyregions(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Logic to group sky regions into overlapping groups.\n    Returns a dataframe containing the sky region id as\n    the index and a column containing a list of the\n    sky region group number it belongs to.\n\n    Args:\n        df:\n            A dataframe containing all the sky regions of the run. Only the\n            'id', 'centre_ra', 'centre_dec' and 'xtr_radius' columns are\n            required.\n            +------+-------------+--------------+--------------+\n            |   id |   centre_ra |   centre_dec |   xtr_radius |\n            |------+-------------+--------------+--------------|\n            |    2 |    319.652  |    0.0030765 |      6.72488 |\n            |    3 |    319.652  |   -6.2989    |      6.7401  |\n            |    1 |     21.8361 |  -73.121     |      7.24662 |\n            +------+-------------+--------------+--------------+\n\n    Returns:\n        The sky region group of each skyregion id.\n            +----+----------------+\n            |    |   skyreg_group |\n            |----+----------------|\n            |  2 |              1 |\n            |  3 |              1 |\n            |  1 |              2 |\n            +----+----------------+\n    \"\"\"\n    sr_coords = SkyCoord(\n        df['centre_ra'],\n        df['centre_dec'],\n        unit=(u.deg, u.deg)\n    )\n\n    df = df.set_index('id')\n\n    results = df.apply(\n        _get_skyregion_relations,\n        args=(sr_coords, df.index),\n        axis=1\n    )\n\n    skyreg_groups: Dict[int, List[Any]] = {}\n\n    master_done = []  # keep track of all checked ids in master done\n\n    for skyreg_id, neighbours in results.iteritems():\n\n        if skyreg_id not in master_done:\n            local_done = []   # a local done list for the sky region group.\n            # add the current skyreg_id to both master and local done.\n            master_done.append(skyreg_id)\n            local_done.append(skyreg_id)\n            # Define the new group number based on the existing ones.\n            skyreg_group = len(skyreg_groups) + 1\n            # Add all the ones that we know are neighbours that were obtained\n            # from _get_skyregion_relations.\n            skyreg_groups[skyreg_group] = list(neighbours)\n\n            # Now the sky region group is extended out to include all those sky\n            # regions that overlap with the neighbours.\n            # Each neighbour is checked and added to the local done list.\n            # Checked means that for each neighbour, it's own neighbours are\n            # added to the current group if not in already.\n            # When the local done is equal to the skyreg group we know that\n            # we have exhausted all possible neighbours and that results in a\n            # sky region group.\n            while sorted(local_done) != sorted(skyreg_groups[skyreg_group]):\n                # Loop over each neighbour\n                for other_skyreg_id in skyreg_groups[skyreg_group]:\n                    # If we haven't checked this neighbour locally proceed.\n                    if other_skyreg_id not in local_done:\n                        # Add it to the local checked.\n                        local_done.append(other_skyreg_id)\n                        # Get the neighbours neighbour and add these.\n                        new_vals = results.loc[other_skyreg_id]\n                        for k in new_vals:\n                            if k not in skyreg_groups[skyreg_group]:\n                                skyreg_groups[skyreg_group].append(k)\n\n            # Reached the end of the group so append all to the master\n            # done list\n            for j in skyreg_groups[skyreg_group]:\n                master_done.append(j)\n        else:\n            # continue if already placed in group\n            continue\n\n    # flip the dictionary around\n    skyreg_group_ids = {}\n    for i in skyreg_groups:\n        for j in skyreg_groups[i]:\n            skyreg_group_ids[j] = i\n\n    skyreg_group_ids = pd.DataFrame.from_dict(\n        skyreg_group_ids, orient='index'\n    ).rename(columns={0: 'skyreg_group'})\n\n    return skyreg_group_ids\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.groupby_funcs","title":"<code>groupby_funcs(df)</code>","text":"<p>Performs calculations on the unique sources to get the lightcurve properties. Works on the grouped by source dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The current iteration dataframe of the grouped by sources dataframe.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Pandas series containing the calculated metrics of the source.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def groupby_funcs(df: pd.DataFrame) -&gt; pd.Series:\n    '''\n    Performs calculations on the unique sources to get the\n    lightcurve properties. Works on the grouped by source\n    dataframe.\n\n    Args:\n        df: The current iteration dataframe of the grouped by sources\n            dataframe.\n\n    Returns:\n        Pandas series containing the calculated metrics of the source.\n    '''\n    # calculated average ra, dec, fluxes and metrics\n    d = {}\n    d['img_list'] = df['image'].values.tolist()\n    d['n_meas_forced'] = df['forced'].sum()\n    d['n_meas'] = df['id'].count()\n    d['n_meas_sel'] = d['n_meas'] - d['n_meas_forced']\n    d['n_sibl'] = df['has_siblings'].sum()\n    if d['n_meas_forced'] &gt; 0:\n        non_forced_sel = ~df['forced']\n        d['wavg_ra'] = (\n            df.loc[non_forced_sel, 'interim_ew'].sum() /\n            df.loc[non_forced_sel, 'weight_ew'].sum()\n        )\n        d['wavg_dec'] = (\n            df.loc[non_forced_sel, 'interim_ns'].sum() /\n            df.loc[non_forced_sel, 'weight_ns'].sum()\n        )\n        d['avg_compactness'] = df.loc[\n            non_forced_sel, 'compactness'\n        ].mean()\n        d['min_snr'] = df.loc[\n            non_forced_sel, 'snr'\n        ].min()\n        d['max_snr'] = df.loc[\n            non_forced_sel, 'snr'\n        ].max()\n\n    else:\n        d['wavg_ra'] = df['interim_ew'].sum() / df['weight_ew'].sum()\n        d['wavg_dec'] = df['interim_ns'].sum() / df['weight_ns'].sum()\n        d['avg_compactness'] = df['compactness'].mean()\n        d['min_snr'] = df['snr'].min()\n        d['max_snr'] = df['snr'].max()\n\n    d['wavg_uncertainty_ew'] = 1. / np.sqrt(df['weight_ew'].sum())\n    d['wavg_uncertainty_ns'] = 1. / np.sqrt(df['weight_ns'].sum())\n    for col in ['avg_flux_int', 'avg_flux_peak']:\n        d[col] = df[col.split('_', 1)[1]].mean()\n    for col in ['max_flux_peak', 'max_flux_int']:\n        d[col] = df[col.split('_', 1)[1]].max()\n    for col in ['min_flux_peak', 'min_flux_int']:\n        d[col] = df[col.split('_', 1)[1]].min()\n    for col in ['min_flux_peak_isl_ratio', 'min_flux_int_isl_ratio']:\n        d[col] = df[col.split('_', 1)[1]].min()\n\n    for col in ['flux_int', 'flux_peak']:\n        d[f'{col}_sq'] = (df[col]**2).mean()\n    d['v_int'] = df['flux_int'].std() / df['flux_int'].mean()\n    d['v_peak'] = df['flux_peak'].std() / df['flux_peak'].mean()\n    d['eta_int'] = get_eta_metric(d, df)\n    d['eta_peak'] = get_eta_metric(d, df, peak=True)\n    # remove not used cols\n    for col in ['flux_int_sq', 'flux_peak_sq']:\n        d.pop(col)\n\n    # get unique related sources\n    list_uniq_related = list(set(\n        chain.from_iterable(\n            lst for lst in df['related'] if isinstance(lst, list)\n        )\n    ))\n    d['related_list'] = list_uniq_related if list_uniq_related else -1\n\n    return pd.Series(d).fillna(value={\"v_int\": 0.0, \"v_peak\": 0.0})\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.log_total_memory_usage","title":"<code>log_total_memory_usage()</code>","text":"<p>This function gets the current memory usage and logs it.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def log_total_memory_usage():\n    \"\"\"\n    This function gets the current memory usage and logs it.\n\n    Returns:\n        None\n    \"\"\"\n    mem = get_total_memory_usage()\n\n    logger.debug(f\"Current memory usage: {mem:.3f}GB\")\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.parallel_groupby","title":"<code>parallel_groupby(df, n_cpu=0, max_partition_mb=15)</code>","text":"<p>Performs the parallel source dataframe operations to calculate the source metrics using Dask and returns the resulting dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The sources dataframe produced by the previous pipeline stages.</p> required <code>n_cpu</code> <code>int</code> <p>The desired number of workers for Dask</p> <code>0</code> <code>max_partition_mb</code> <code>int</code> <p>The desired maximum size (in MB) of the partitions for Dask.</p> <code>15</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The source dataframe with the calculated metric columns.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def parallel_groupby(df: pd.DataFrame, n_cpu: int = 0, max_partition_mb: int = 15) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs the parallel source dataframe operations to calculate the source\n    metrics using Dask and returns the resulting dataframe.\n\n    Args:\n        df: The sources dataframe produced by the previous pipeline stages.\n        n_cpu: The desired number of workers for Dask\n        max_partition_mb: The desired maximum size (in MB) of the partitions for Dask.\n\n    Returns:\n        The source dataframe with the calculated metric columns.\n    \"\"\"\n    col_dtype = {\n        'img_list': 'O',\n        'n_meas_forced': 'i',\n        'n_meas': 'i',\n        'n_meas_sel': 'i',\n        'n_sibl': 'i',\n        'wavg_ra': 'f',\n        'wavg_dec': 'f',\n        'avg_compactness': 'f',\n        'min_snr': 'f',\n        'max_snr': 'f',\n        'wavg_uncertainty_ew': 'f',\n        'wavg_uncertainty_ns': 'f',\n        'avg_flux_int': 'f',\n        'avg_flux_peak': 'f',\n        'max_flux_peak': 'f',\n        'max_flux_int': 'f',\n        'min_flux_peak': 'f',\n        'min_flux_int': 'f',\n        'min_flux_peak_isl_ratio': 'f',\n        'min_flux_int_isl_ratio': 'f',\n        'v_int': 'f',\n        'v_peak': 'f',\n        'eta_int': 'f',\n        'eta_peak': 'f',\n        'related_list': 'O'\n    }\n    n_workers, n_partitions = calculate_workers_and_partitions(\n        df,\n        n_cpu=n_cpu,\n        max_partition_mb=max_partition_mb)\n    logger.debug(f\"Running association with {n_workers} CPUs\")\n    out = dd.from_pandas(df.set_index('source'), npartitions=n_partitions)\n    out = (\n        out.groupby('source')\n        .apply(\n            groupby_funcs,\n            meta=col_dtype\n        )\n        .compute(num_workers=n_workers, scheduler='processes')\n    )\n\n    out['n_rel'] = out['related_list'].apply(\n        lambda x: 0 if x == -1 else len(x))\n\n    return out\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.parallel_groupby_coord","title":"<code>parallel_groupby_coord(df, n_cpu=0, max_partition_mb=15)</code>","text":"<p>This function uses Dask to perform the average coordinate and unique image and epoch lists calculation. The result from the Dask compute is returned which is a dataframe containing the results for each source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The sources dataframe produced by the pipeline.</p> required <code>n_cpu</code> <code>int</code> <p>The desired number of workers for Dask</p> <code>0</code> <code>max_partition_mb</code> <code>int</code> <p>The desired maximum size (in MB) of the partitions for Dask.</p> <code>15</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The resulting average coordinate values and unique image and epoch lists for each unique source (group).</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def parallel_groupby_coord(df: pd.DataFrame, n_cpu: int = 0, max_partition_mb: int = 15) -&gt; pd.DataFrame:\n    \"\"\"\n    This function uses Dask to perform the average coordinate and unique image\n    and epoch lists calculation. The result from the Dask compute is returned\n    which is a dataframe containing the results for each source.\n\n    Args:\n        df: The sources dataframe produced by the pipeline.\n        n_cpu: The desired number of workers for Dask\n        max_partition_mb: The desired maximum size (in MB) of the partitions for Dask.\n\n    Returns:\n        The resulting average coordinate values and unique image and epoch\n            lists for each unique source (group).\n    \"\"\"\n    col_dtype = {\n        'img_list': 'O',\n        'epoch_list': 'O',\n        'wavg_ra': 'f',\n        'wavg_dec': 'f',\n    }\n    n_workers, n_partitions = calculate_workers_and_partitions(\n        df,\n        n_cpu=n_cpu,\n        max_partition_mb=max_partition_mb)\n    logger.debug(f\"Running association with {n_workers} CPUs\")\n\n    out = dd.from_pandas(df.set_index('source'), npartitions=n_partitions)\n    out = (\n        out.groupby('source')\n        .apply(calc_ave_coord, meta=col_dtype)\n        .compute(num_workers=n_workers, scheduler='processes')\n    )\n\n    return out\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.prep_skysrc_df","title":"<code>prep_skysrc_df(images, perc_error=0.0, duplicate_limit=None, ini_df=False)</code>","text":"<p>Initialise the source dataframe to use in association logic by reading the measurement parquet file and creating columns. When epoch based association is used it will also remove duplicate measurements from the list of sources.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List[Image]</code> <p>A list holding the Image objects of the images to load measurements for.</p> required <code>perc_error</code> <code>float</code> <p>A percentage flux error to apply to the flux errors of the measurements. Defaults to 0.</p> <code>0.0</code> <code>duplicate_limit</code> <code>Optional[Angle]</code> <p>The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used in the 'remove_duplicate_measurements' function (usual ASKAP pixel size).</p> <code>None</code> <code>ini_df</code> <code>bool</code> <p>Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The measurements of the image(s) with some extra values set ready for association and duplicates removed if necessary.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def prep_skysrc_df(\n    images: List[Image],\n    perc_error: float = 0.,\n    duplicate_limit: Optional[Angle] = None,\n    ini_df: bool = False\n) -&gt; pd.DataFrame:\n    '''\n    Initialise the source dataframe to use in association logic by\n    reading the measurement parquet file and creating columns. When epoch\n    based association is used it will also remove duplicate measurements from\n    the list of sources.\n\n    Args:\n        images:\n            A list holding the Image objects of the images to load measurements\n            for.\n        perc_error:\n            A percentage flux error to apply to the flux errors of the\n            measurements. Defaults to 0.\n        duplicate_limit:\n            The separation limit of when a source is considered a duplicate.\n            Defaults to None in which case 2.5 arcsec is used in the\n            'remove_duplicate_measurements' function (usual ASKAP pixel size).\n        ini_df:\n            Boolean to indicate whether these sources are part of the initial\n            source list creation for association. If 'True' the source ids are\n            reset ready for the first iteration. Defaults to 'False'.\n\n    Returns:\n        The measurements of the image(s) with some extra values set ready for\n            association and duplicates removed if necessary.\n    '''\n    cols = [\n        'id',\n        'ra',\n        'uncertainty_ew',\n        'weight_ew',\n        'dec',\n        'uncertainty_ns',\n        'weight_ns',\n        'flux_int',\n        'flux_int_err',\n        'flux_int_isl_ratio',\n        'flux_peak',\n        'flux_peak_err',\n        'flux_peak_isl_ratio',\n        'forced',\n        'compactness',\n        'has_siblings',\n        'snr'\n    ]\n\n    df = _load_measurements(images[0], cols, ini_df=ini_df)\n\n    if len(images) &gt; 1:\n        for img in images[1:]:\n            df = pd.concat(\n                [\n                    df,\n                    _load_measurements(\n                        img, cols, df.source.max(), ini_df=ini_df\n                    )\n                ],\n                ignore_index=True\n            )\n\n        df = remove_duplicate_measurements(\n            df, dup_lim=duplicate_limit, ini_df=ini_df\n        )\n\n    df = df.drop('dist_from_centre', axis=1)\n\n    if perc_error != 0.0:\n        logger.info('Correcting flux errors with config error setting...')\n        for col in ['flux_int', 'flux_peak']:\n            df[f'{col}_err'] = np.hypot(\n                df[f'{col}_err'].values, perc_error * df[col].values\n            )\n\n    return df\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.reconstruct_associtaion_dfs","title":"<code>reconstruct_associtaion_dfs(images_df_done, previous_parquet_paths)</code>","text":"<p>This function is used with add image mode and performs the necessary manipulations to reconstruct the sources_df and skyc1_srcs required by association.</p> <p>Parameters:</p> Name Type Description Default <code>images_df_done</code> <code>DataFrame</code> <p>The images_df output from the existing run (from the parquet).</p> required <code>previous_parquet_paths</code> <code>Dict[str, str]</code> <p>Dictionary that contains the paths for the previous run parquet files. Keys are 'images', 'associations', 'sources', 'relations' and 'measurement_pairs'.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The reconstructed <code>sources_df</code> dataframe.</p> <code>DataFrame</code> <p>The reconstructed <code>skyc1_srs</code> dataframes.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def reconstruct_associtaion_dfs(\n    images_df_done: pd.DataFrame,\n    previous_parquet_paths: Dict[str, str]\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    This function is used with add image mode and performs the necessary\n    manipulations to reconstruct the sources_df and skyc1_srcs required by\n    association.\n\n    Args:\n        images_df_done:\n            The images_df output from the existing run (from the parquet).\n        previous_parquet_paths:\n            Dictionary that contains the paths for the previous run parquet\n            files. Keys are 'images', 'associations', 'sources', 'relations'\n            and 'measurement_pairs'.\n\n    Returns:\n        The reconstructed `sources_df` dataframe.\n        The reconstructed `skyc1_srs` dataframes.\n    \"\"\"\n    prev_associations = pd.read_parquet(previous_parquet_paths['associations'])\n\n    logger.debug(images_df_done)\n    logger.debug(images_df_done['image_dj'])\n\n    # Get the parquet paths from the image objects\n    img_meas_paths = (\n        images_df_done['image_dj'].apply(lambda x: x.measurements_path)\n        .to_list()\n    )\n    logger.debug(img_meas_paths)\n\n    # Obtain the pipeline run path in order to fetch forced measurements.\n    run_path = previous_parquet_paths['sources'].replace(\n        'sources.parquet.bak', ''\n    )\n\n    # Get the forced measurement paths.\n    img_fmeas_paths = []\n\n    for i in images_df_done.image_name.values:\n        forced_parquet = os.path.join(\n            run_path, \"forced_measurements_{}.parquet\".format(\n                i.replace(\".\", \"_\")\n            )\n        )\n        if os.path.isfile(forced_parquet):\n            img_fmeas_paths.append(forced_parquet)\n\n    # Create union of paths.\n    img_meas_paths += img_fmeas_paths\n\n    # Define the columns that are required\n    cols = [\n        'id',\n        'ra',\n        'uncertainty_ew',\n        'weight_ew',\n        'dec',\n        'uncertainty_ns',\n        'weight_ns',\n        'flux_int',\n        'flux_int_err',\n        'flux_int_isl_ratio',\n        'flux_peak',\n        'flux_peak_err',\n        'flux_peak_isl_ratio',\n        'forced',\n        'compactness',\n        'has_siblings',\n        'snr',\n        'image_id',\n        'time',\n    ]\n\n    # Open all the parquets\n    logger.debug(\n        \"Opening all measurement parquet files to use in reconstruction...\"\n    )\n    measurements = pd.concat(\n        [pd.read_parquet(f, columns=cols) for f in img_meas_paths]\n    )\n\n    # Create mask to drop measurements for epoch mode (epoch based mode).\n    measurements_mask = measurements['id'].isin(\n        prev_associations['meas_id'])\n    measurements = measurements.loc[measurements_mask].set_index('id')\n\n    # Set the index on images_df for faster merging.\n    images_df_done['image_id'] = images_df_done['image_dj'].apply(\n        lambda x: x.id).values\n    images_df_done = images_df_done.set_index('image_id')\n\n    # Merge image information to measurements\n    measurements = (\n        measurements.merge(\n            images_df_done[['image_name', 'epoch']],\n            left_on='image_id', right_index=True\n        )\n        .rename(columns={'image_name': 'image'})\n    )\n\n    # Drop any associations that are not used in this sky region group.\n    associations_mask = prev_associations['meas_id'].isin(\n        measurements.index.values)\n\n    prev_associations = prev_associations.loc[associations_mask]\n\n    # Merge measurements into the associations to form the sources_df.\n    sources_df = (\n        prev_associations.merge(\n            measurements, left_on='meas_id', right_index=True\n        )\n        .rename(columns={\n            'source_id': 'source', 'time': 'datetime', 'meas_id': 'id',\n            'ra': 'ra_source', 'dec': 'dec_source',\n            'uncertainty_ew': 'uncertainty_ew_source',\n            'uncertainty_ns': 'uncertainty_ns_source',\n        })\n    )\n\n    # Load up the previous unique sources.\n    prev_sources = pd.read_parquet(\n        previous_parquet_paths['sources'], columns=[\n            'wavg_ra', 'wavg_dec',\n            'wavg_uncertainty_ew', 'wavg_uncertainty_ns',\n        ]\n    )\n\n    # Merge the wavg ra and dec to the sources_df - this is required to\n    # create the skyc1_srcs below (but MUST be converted back to the source\n    # ra and dec)\n    sources_df = (\n        sources_df.merge(\n            prev_sources, left_on='source', right_index=True)\n        .rename(columns={\n            'wavg_ra': 'ra', 'wavg_dec': 'dec',\n            'wavg_uncertainty_ew': 'uncertainty_ew',\n            'wavg_uncertainty_ns': 'uncertainty_ns',\n        })\n    )\n\n    # Load the previous relations\n    prev_relations = pd.read_parquet(previous_parquet_paths['relations'])\n\n    # Form relation lists to merge in.\n    prev_relations = pd.DataFrame(\n        prev_relations\n        .groupby('from_source_id')['to_source_id']\n        .apply(lambda x: x.values.tolist())\n    ).rename(columns={'to_source_id': 'related'})\n\n    # Append the relations to only the last instance of each source\n    # First get the ids of the sources\n    relation_ids = sources_df[\n        sources_df.source.isin(prev_relations.index.values)].drop_duplicates(\n            'source', keep='last'\n    ).index.values\n    # Make sure we attach the correct source id\n    source_ids = sources_df.loc[relation_ids].source.values\n    sources_df['related'] = np.nan\n    relations_to_update = prev_relations.loc[source_ids].to_numpy().copy()\n    relations_to_update = np.reshape(\n        relations_to_update, relations_to_update.shape[0])\n    sources_df.loc[relation_ids, 'related'] = relations_to_update\n\n    # Reorder so we don't mess up the dask metas.\n    sources_df = sources_df[[\n        'id', 'uncertainty_ew', 'weight_ew', 'uncertainty_ns', 'weight_ns',\n        'flux_int', 'flux_int_err', 'flux_int_isl_ratio', 'flux_peak',\n        'flux_peak_err', 'flux_peak_isl_ratio', 'forced', 'compactness',\n        'has_siblings', 'snr', 'image', 'datetime', 'source', 'ra', 'dec',\n        'ra_source', 'dec_source', 'd2d', 'dr', 'related', 'epoch',\n        'uncertainty_ew_source', 'uncertainty_ns_source'\n    ]]\n\n    # Create the unique skyc1_srcs dataframe.\n    skyc1_srcs = (\n        sources_df[~sources_df['forced']]\n        .sort_values(by='id')\n        .drop('related', axis=1)\n        .drop_duplicates('source')\n    ).copy(deep=True)\n\n    # Get relations into the skyc1_srcs (as we only keep the first instance\n    # which does not have the relation information)\n    skyc1_srcs = skyc1_srcs.merge(\n        prev_relations, how='left', left_on='source', right_index=True\n    )\n\n    # Need to break the pointer relationship between the related sources (\n    # deep=True copy does not truly copy mutable type objects)\n    relation_mask = skyc1_srcs.related.notna()\n    relation_vals = skyc1_srcs.loc[relation_mask, 'related'].to_list()\n    new_relation_vals = [x.copy() for x in relation_vals]\n    skyc1_srcs.loc[relation_mask, 'related'] = new_relation_vals\n\n    # Reorder so we don't mess up the dask metas.\n    skyc1_srcs = skyc1_srcs[[\n        'id', 'ra', 'uncertainty_ew', 'weight_ew', 'dec', 'uncertainty_ns',\n        'weight_ns', 'flux_int', 'flux_int_err', 'flux_int_isl_ratio',\n        'flux_peak', 'flux_peak_err', 'flux_peak_isl_ratio', 'forced',\n        'compactness', 'has_siblings', 'snr', 'image', 'datetime', 'source',\n        'ra_source', 'dec_source', 'd2d', 'dr', 'related', 'epoch'\n    ]].reset_index(drop=True)\n\n    # Finally move the source ra and dec back to the sources_df ra and dec\n    # columns\n    sources_df['ra'] = sources_df['ra_source']\n    sources_df['dec'] = sources_df['dec_source']\n    sources_df['uncertainty_ew'] = sources_df['uncertainty_ew_source']\n    sources_df['uncertainty_ns'] = sources_df['uncertainty_ns_source']\n\n    # Drop not needed columns for the sources_df.\n    sources_df = sources_df.drop([\n        'uncertainty_ew_source', 'uncertainty_ns_source'\n    ], axis=1).reset_index(drop=True)\n\n    return sources_df, skyc1_srcs\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.remove_duplicate_measurements","title":"<code>remove_duplicate_measurements(sources_df, dup_lim=None, ini_df=False)</code>","text":"<p>Remove perceived duplicate sources from a dataframe of loaded measurements. Duplicates are determined by their separation and whether this distances is within the 'dup_lim'.</p> <p>Parameters:</p> Name Type Description Default <code>sources_df</code> <code>DataFrame</code> <p>The loaded measurements from two or more images.</p> required <code>dup_lim</code> <code>Optional[Angle]</code> <p>The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used (usual ASKAP pixel size).</p> <code>None</code> <code>ini_df</code> <code>bool</code> <p>Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The input sources_df with duplicate sources removed.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def remove_duplicate_measurements(\n    sources_df: pd.DataFrame,\n    dup_lim: Optional[Angle] = None,\n    ini_df: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove perceived duplicate sources from a dataframe of loaded\n    measurements. Duplicates are determined by their separation and whether\n    this distances is within the 'dup_lim'.\n\n    Args:\n        sources_df:\n            The loaded measurements from two or more images.\n        dup_lim:\n            The separation limit of when a source is considered a duplicate.\n            Defaults to None in which case 2.5 arcsec is used (usual ASKAP\n            pixel size).\n        ini_df:\n            Boolean to indicate whether these sources are part of the initial\n            source list creation for association. If 'True' the source ids are\n            reset ready for the first iteration. Defaults to 'False'.\n\n    Returns:\n        The input sources_df with duplicate sources removed.\n    \"\"\"\n    logger.debug('Cleaning duplicate sources from epoch...')\n\n    if dup_lim is None:\n        dup_lim = Angle(2.5 * u.arcsec)\n\n    logger.debug(\n        'Using duplicate crossmatch radius of %.2f arcsec.', dup_lim.arcsec\n    )\n\n    # sort by the distance from the image centre so we know\n    # that the first source is always the one to keep\n    sources_df = sources_df.sort_values(by='dist_from_centre')\n\n    sources_sc = SkyCoord(\n        sources_df['ra'],\n        sources_df['dec'],\n        unit=(u.deg, u.deg)\n    )\n\n    # perform search around sky to get all self matches\n    idxc, idxcatalog, *_ = sources_sc.search_around_sky(\n        sources_sc, dup_lim\n    )\n\n    # create df from results\n    results = pd.DataFrame(\n        data={\n            'source_id': idxc,\n            'match_id': idxcatalog,\n            'source_image': sources_df.iloc[idxc]['image'].tolist(),\n            'match_image': sources_df.iloc[idxcatalog]['image'].tolist()\n        }\n    )\n\n    # Drop those that are matched from the same image\n    matching_image_mask = (\n        results['source_image'] != results['match_image']\n    )\n\n    results = (\n        results.loc[matching_image_mask]\n        .drop(['source_image', 'match_image'], axis=1)\n    )\n\n    # create a pair column defining each pair ith index\n    results['pair'] = results.apply(tuple, 1).apply(sorted).apply(tuple)\n    # Drop the duplicate pairs (pairs are sorted so this works)\n    results = results.drop_duplicates('pair')\n    # No longer need pair\n    results = results.drop('pair', axis=1)\n    # Drop all self matches and we are left with those to drop\n    # in the match id column.\n    to_drop = results.loc[\n        results['source_id'] != results['match_id'],\n        'match_id'\n    ]\n    # Get the index values from the ith values\n    to_drop_indexes = sources_df.iloc[to_drop].index.values\n    logger.debug(\n        \"Dropping %i duplicate measurements.\", to_drop_indexes.shape[0]\n    )\n    # Drop them from sources\n    sources_df = sources_df.drop(to_drop_indexes).sort_values(by='ra')\n\n    # reset the source_df index\n    sources_df = sources_df.reset_index(drop=True)\n\n    # Reset the source number\n    if ini_df:\n        sources_df['source'] = sources_df.index + 1\n\n    del results\n\n    return sources_df\n</code></pre>"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.write_parquets","title":"<code>write_parquets(images, skyregions, bands, run_path)</code>","text":"<p>This function saves images, skyregions and bands to parquet files. It also returns a DataFrame containing containing the information of the sky regions associated with the current run.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List[Image]</code> <p>list of image Django ORM objects.</p> required <code>skyregions</code> <code>List[SkyRegion]</code> <p>list sky region Django ORM objects.</p> required <code>bands</code> <code>List[Band]</code> <p>list of band Django ORM objects.</p> required <code>run_path</code> <code>str</code> <p>directory to save parquets to.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Sky regions as pandas DataFrame.</p> Source code in <code>vast_pipeline/pipeline/utils.py</code> <pre><code>def write_parquets(\n    images: List[Image],\n    skyregions: List[SkyRegion],\n    bands: List[Band],\n    run_path: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    This function saves images, skyregions and bands to parquet files.\n    It also returns a DataFrame containing containing the information\n    of the sky regions associated with the current run.\n\n    Args:\n        images: list of image Django ORM objects.\n        skyregions: list sky region Django ORM objects.\n        bands: list of band Django ORM objects.\n        run_path: directory to save parquets to.\n\n    Returns:\n        Sky regions as pandas DataFrame.\n    \"\"\"\n    # write images parquet file under pipeline run folder\n    images_df = pd.DataFrame(map(lambda x: x.__dict__, images))\n    images_df = images_df.drop('_state', axis=1)\n    images_df.to_parquet(\n        os.path.join(run_path, 'images.parquet'),\n        index=False\n    )\n    # write skyregions parquet file under pipeline run folder\n    skyregs_df = pd.DataFrame(map(lambda x: x.__dict__, skyregions))\n    skyregs_df = skyregs_df.drop('_state', axis=1)\n    skyregs_df.to_parquet(\n        os.path.join(run_path, 'skyregions.parquet'),\n        index=False\n    )\n    # write skyregions parquet file under pipeline run folder\n    bands_df = pd.DataFrame(map(lambda x: x.__dict__, bands))\n    bands_df = bands_df.drop('_state', axis=1)\n    bands_df.to_parquet(\n        os.path.join(run_path, 'bands.parquet'),\n        index=False\n    )\n\n    return skyregs_df\n</code></pre>"},{"location":"reference/survey/translators/","title":"translators.py","text":"<p>Translators to map the table column names in the input files into the required column names in our db. The flux/ang scale needs to be the multiplicative factor that converts the input flux into mJy and a/b into arcsec.</p>"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_aegean","title":"<code>tr_aegean = {'prefix': 'IM', 'ra': 'ra', 'err_ra': 'er_ra', 'dec': 'dec', 'err_dec': 'err_dec', 'pos_err_ang_scale': 1, 'peak_flux': 'peak_flux', 'err_peak_flux': 'err_peak_flux', 'total_flux': 'int_flux', 'err_total_flux': 'err_int_flux', 'flux_scale': 1000.0, 'bmaj': 'a', 'bmin': 'b', 'ang_scale': 3600, 'pa': 'pa', 'freq': 999}</code>  <code>module-attribute</code>","text":"<p>The translator dictionary for a Agean catalogue input. Not complete.</p>"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_gleam","title":"<code>tr_gleam = {'prefix': 'MWA', 'ra': 'RAJ2000', 'err_ra': 'err_RAJ2000', 'dec': 'DEJ2000', 'err_dec': 'err_DEJ2000', 'pos_err_ang_scale': 1, 'peak_flux': 'peak_flux_wide', 'err_peak_flux': 'err_peak_flux_wide', 'total_flux': 'int_flux_wide', 'err_total_flux': 'err_int_flux_wide', 'flux_scale': 1000.0, 'bmaj': 'a_wide', 'bmin': 'b_wide', 'ang_scale': 1.0 / 3600, 'pa': 'pa_wide', 'freq': 200}</code>  <code>module-attribute</code>","text":"<p>The translator dictionary for a GLEAM catalogue input. Not complete.</p>"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_mwacs","title":"<code>tr_mwacs = {'prefix': 'MWACS', 'ra': 'RAJ2000', 'err_ra': 'e_RAJ2000', 'dec': 'DEJ2000', 'err_dec': 'e_DEJ2000', 'pos_err_ang_scale': 1, 'peak_flux': 'S180', 'err_peak_flux': 'e_S180', 'total_flux': 'S180', 'err_total_flux': 'e_S180', 'flux_scale': 1000.0, 'bmaj': 'MajAxis', 'bmin': 'MinAxis', 'ang_scale': 60, 'pa': 'PABeam', 'freq': 180}</code>  <code>module-attribute</code>","text":"<p>The translator dictionary for a MWACS catalogue input. Not complete.</p>"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_nvss","title":"<code>tr_nvss = {'prefix': 'NVSS', 'ra': '_RAJ2000', 'err_ra': 'e_RAJ2000', 'dec': '_DEJ2000', 'err_dec': 'e_DEJ2000', 'pos_err_ang_scale': 1.0 / 3600, 'peak_flux': 'S1.4', 'err_peak_flux': 'e_S1.4', 'total_flux': 'S1.4', 'err_total_flux': 'e_S1.4', 'flux_scale': 1, 'bmaj': 'MajAxis', 'bmin': 'MinAxis', 'ang_scale': 1.0 / 3600, 'pa': 'PA', 'freq': 1400}</code>  <code>module-attribute</code>","text":"<p>The translator dictionary for a NVSS catalogue input. Not complete.</p>"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_selavy","title":"<code>tr_selavy = {'island_id': {'name': 'island_id', 'dtype': np.dtype(str)}, 'component_id': {'name': 'component_id', 'dtype': np.dtype(str)}, 'rms_image': {'name': 'local_rms', 'dtype': np.dtype(float)}, 'ra_deg_cont': {'name': 'ra', 'dtype': np.dtype(float)}, 'ra_err': {'name': 'ra_err', 'dtype': np.dtype(float)}, 'dec_deg_cont': {'name': 'dec', 'dtype': np.dtype(float)}, 'dec_err': {'name': 'dec_err', 'dtype': np.dtype(float)}, 'flux_peak': {'name': 'flux_peak', 'dtype': np.dtype(float)}, 'flux_peak_err': {'name': 'flux_peak_err', 'dtype': np.dtype(float)}, 'flux_int': {'name': 'flux_int', 'dtype': np.dtype(float)}, 'flux_int_err': {'name': 'flux_int_err', 'dtype': np.dtype(float)}, 'maj_axis': {'name': 'bmaj', 'dtype': np.dtype(float)}, 'maj_axis_err': {'name': 'err_bmaj', 'dtype': np.dtype(float)}, 'min_axis': {'name': 'bmin', 'dtype': np.dtype(float)}, 'min_axis_err': {'name': 'err_bmin', 'dtype': np.dtype(float)}, 'pos_ang': {'name': 'pa', 'dtype': np.dtype(float)}, 'pos_ang_err': {'name': 'err_pa', 'dtype': np.dtype(float)}, 'maj_axis_deconv': {'name': 'psf_bmaj', 'dtype': np.dtype(float)}, 'min_axis_deconv': {'name': 'psf_bmin', 'dtype': np.dtype(float)}, 'pos_ang_deconv': {'name': 'psf_pa', 'dtype': np.dtype(float)}, 'flag_c4': {'name': 'flag_c4', 'dtype': np.dtype(bool)}, 'chi_squared_fit': {'name': 'chi_squared_fit', 'dtype': np.dtype(float)}, 'spectral_index': {'name': 'spectral_index', 'dtype': np.dtype(float)}, 'spectral_index_from_TT': {'name': 'spectral_index_from_TT', 'dtype': np.dtype(bool)}, 'has_siblings': {'name': 'has_siblings', 'dtype': np.dtype(bool)}, 'rms_image': {'name': 'local_rms', 'dtype': np.dtype(float)}}</code>  <code>module-attribute</code>","text":"<p>The translator dictionary for a Selavy catalogue input.</p>"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_sumss","title":"<code>tr_sumss = {'prefix': 'SUMSS', 'ra': '_RAJ2000', 'err_ra': 'e_RAJ2000', 'dec': '_DEJ2000', 'err_dec': 'e_DEJ2000', 'pos_err_ang_scale': 1.0 / 3600, 'peak_flux': 'Sp', 'err_peak_flux': 'e_Sp', 'total_flux': 'St', 'err_total_flux': 'e_St', 'flux_scale': 1, 'bmaj': 'MajAxis', 'bmin': 'MinAxis', 'ang_scale': 1.0 / 3600, 'pa': 'PA', 'freq': 843}</code>  <code>module-attribute</code>","text":"<p>The translator dictionary for a SUMSS catalogue input. Not complete.</p>"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.translators","title":"<code>translators = {'MWACS': tr_mwacs, 'AEGEAN': tr_aegean, 'GLEAM': tr_gleam, 'SUMSS': tr_sumss, 'NVSS': tr_nvss, 'SELAVY': tr_selavy, 'DEFAULT': tr_aegean}</code>  <code>module-attribute</code>","text":"<p>Dictionary containing all the translators defined in this module.</p>"},{"location":"reference/utils/auth/","title":"auth.py","text":""},{"location":"reference/utils/auth/#vast_pipeline.utils.auth.create_admin_user","title":"<code>create_admin_user(uid, response, details, user, social, *args, **kwargs)</code>","text":"<p>Give Django admin privileges to a user who login via GitHub and belong to a specific team. The parameters are as per python-social-auth docs https://python-social-auth.readthedocs.io/en/latest/pipeline.html#extending-the-pipeline</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>int</code> <p>user id</p> required <code>response</code> <code>Dict</code> <p>request dictionary</p> required <code>details</code> <code>Dict</code> <p>user details generated by the backend</p> required <code>user</code> <code>User</code> <p>Django user model object</p> required <code>social</code> <code>UserSocialAuth</code> <p>Social auth user model object</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>A dictionary with the Django User object in it or empty if no action is taken.</p> Source code in <code>vast_pipeline/utils/auth.py</code> <pre><code>def create_admin_user(uid: int, response: Dict, details: Dict, user: User,\n    social: UserSocialAuth , *args, **kwargs) -&gt; Dict:\n    \"\"\"\n    Give Django admin privileges to a user who login via GitHub and belong to\n    a specific team. The parameters are as per python-social-auth docs\n    https://python-social-auth.readthedocs.io/en/latest/pipeline.html#extending-the-pipeline\n\n    Args:\n        uid:\n            user id\n        response:\n            request dictionary\n        details:\n            user details generated by the backend\n        user:\n            Django user model object\n        social:\n            Social auth user model object\n\n    Returns:\n        A dictionary with the Django User object in it or empty if\n            no action is taken.\n    \"\"\"\n    # assume github-org backend, add &lt;if backend.name == 'github-org'&gt;\n    # if other backend are implemented\n    admin_team = settings.SOCIAL_AUTH_GITHUB_ADMIN_TEAM\n    usr = response.get('login', '')\n    if (usr != '' and admin_team != '' and user and not user.is_staff and\n        not user.is_superuser):\n        logger.info('Trying to add Django admin privileges to user')\n        # check if github user belong to admin team\n        org = settings.SOCIAL_AUTH_GITHUB_ORG_NAME\n        header = {\n            'Authorization': f\"token {response.get('access_token', '')}\"\n        }\n        url = (\n            f'https://api.github.com/orgs/{org}/teams/{admin_team}'\n            f'/memberships/{usr}'\n        )\n        resp = requests.get(url, headers=header)\n        if resp.ok:\n            # add user to admin\n            user.is_superuser = True\n            user.is_staff = True\n            user.save()\n            logger.info('Django admin privileges successfully added to user')\n            return {'user': user}\n        logger.info(f'GitHub request failed, reason: {resp.reason}')\n\n        return {}\n\n    return {}\n</code></pre>"},{"location":"reference/utils/auth/#vast_pipeline.utils.auth.load_github_avatar","title":"<code>load_github_avatar(response, social, *args, **kwargs)</code>","text":"<p>Add GitHub avatar url to the extra data stored by social_django app</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Dict</code> <p>request dictionary</p> required <code>social</code> <code>UserSocialAuth</code> <p>Social auth user model object</p> required <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict</code> <p>A dictionary with the Social auth user object in it or empty if no action is taken.</p> Source code in <code>vast_pipeline/utils/auth.py</code> <pre><code>def load_github_avatar(response: Dict, social: UserSocialAuth, *args,\n    **kwargs) -&gt; Dict:\n    \"\"\"\n    Add GitHub avatar url to the extra data stored by social_django app\n\n    Args:\n        response:\n            request dictionary\n        social:\n            Social auth user model object\n        *args:\n            Variable length argument list.\n        **kwargs:\n            Arbitrary keyword arguments.\n\n    Returns:\n        A dictionary with the Social auth user object in it or empty if\n            no action is taken.\n    \"\"\"\n    # assume github-org backend, add &lt;if backend.name == 'github-org'&gt;\n    # if other backend are implemented\n    # if social and social.get('extra_data', None)\n    # print(vars(social))\n    if 'avatar_url' not in social.extra_data:\n        logger.info('Adding GitHub avatar url to user extra data')\n        social.extra_data['avatar_url'] = response['avatar_url']\n        social.save()\n        return {'social': social}\n\n    return {}\n</code></pre>"},{"location":"reference/utils/delete_run/","title":"Delete run","text":""},{"location":"reference/utils/external_query/","title":"external_query.py","text":""},{"location":"reference/utils/external_query/#vast_pipeline.utils.external_query.ned","title":"<code>ned(coord, radius)</code>","text":"<p>Perform a cone search for sources with NED.</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>SkyCoord</code> <p>The coordinate of the centre of the cone.</p> required <code>radius</code> <code>Angle</code> <p>The radius of the cone in angular units.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dicts, where each dict is a query result row with the following keys:</p> <ul> <li>object_name: the name of the astronomical object.</li> <li>database: the source of the result, i.e. NED.</li> <li>separation_arcsec: separation to the query coordinate in arcsec.</li> <li>otype: object type.</li> <li>otype_long: long form of the object type.</li> <li>ra_hms: RA coordinate string in hms format.</li> <li>dec_dms: Dec coordinate string in \u00b1dms format.</li> </ul> Source code in <code>vast_pipeline/utils/external_query.py</code> <pre><code>def ned(coord: SkyCoord, radius: Angle) -&gt; List[Dict[str, Any]]:\n    \"\"\"Perform a cone search for sources with NED.\n\n    Args:\n        coord: The coordinate of the centre of the cone.\n        radius: The radius of the cone in angular units.\n\n    Returns:\n        A list of dicts, where each dict is a query result row with the following keys:\n\n            - object_name: the name of the astronomical object.\n            - database: the source of the result, i.e. NED.\n            - separation_arcsec: separation to the query coordinate in arcsec.\n            - otype: object type.\n            - otype_long: long form of the object type.\n            - ra_hms: RA coordinate string in hms format.\n            - dec_dms: Dec coordinate string in \u00b1dms format.\n    \"\"\"\n    # NED API doesn't supply the long-form object types.\n    # Copied from https://ned.ipac.caltech.edu/Documents/Guides/Database\n    NED_OTYPES = {\n        \"*\": \"Star or Point Source\",\n        \"**\": \"Double star\",\n        \"*Ass\": \"Stellar association\",\n        \"*Cl\": \"Star cluster\",\n        \"AbLS\": \"Absorption line system\",\n        \"Blue*\": \"Blue star\",\n        \"C*\": \"Carbon star\",\n        \"EmLS\": \"Emission line source\",\n        \"EmObj\": \"Emission object\",\n        \"exG*\": \"Extragalactic star (not a member of an identified galaxy)\",\n        \"Flare*\": \"Flare star\",\n        \"G\": \"Galaxy\",\n        \"GammaS\": \"Gamma ray source\",\n        \"GClstr\": \"Cluster of galaxies\",\n        \"GGroup\": \"Group of galaxies\",\n        \"GPair\": \"Galaxy pair\",\n        \"GTrpl\": \"Galaxy triple\",\n        \"G_Lens\": \"Lensed image of a galaxy\",\n        \"HII\": \"HII region\",\n        \"IrS\": \"Infrared source\",\n        \"MCld\": \"Molecular cloud\",\n        \"Neb\": \"Nebula\",\n        \"Nova\": \"Nova\",\n        \"Other\": \"Other classification (e.g. comet; plate defect)\",\n        \"PN\": \"Planetary nebula\",\n        \"PofG\": \"Part of galaxy\",\n        \"Psr\": \"Pulsar\",\n        \"QGroup\": \"Group of QSOs\",\n        \"QSO\": \"Quasi-stellar object\",\n        \"Q_Lens\": \"Lensed image of a QSO\",\n        \"RadioS\": \"Radio source\",\n        \"Red*\": \"Red star\",\n        \"RfN\": \"Reflection nebula\",\n        \"SN\": \"Supernova\",\n        \"SNR\": \"Supernova remnant\",\n        \"UvES\": \"Ultraviolet excess source\",\n        \"UvS\": \"Ultraviolet source\",\n        \"V*\": \"Variable star\",\n        \"VisS\": \"Visual source\",\n        \"WD*\": \"White dwarf\",\n        \"WR*\": \"Wolf-Rayet star\",\n        \"XrayS\": \"X-ray source\",\n        \"!*\": \"Galactic star\",\n        \"!**\": \"Galactic double star\",\n        \"!*Ass\": \"Galactic star association\",\n        \"!*Cl\": \"Galactic Star cluster\",\n        \"!Blue*\": \"Galactic blue star\",\n        \"!C*\": \"Galactic carbon star\",\n        \"!EmObj\": \"Galactic emission line object\",\n        \"!Flar*\": \"Galactic flare star\",\n        \"!HII\": \"Galactic HII region\",\n        \"!MCld\": \"Galactic molecular cloud\",\n        \"!Neb\": \"Galactic nebula\",\n        \"!Nova\": \"Galactic nova\",\n        \"!PN\": \"Galactic planetary nebula\",\n        \"!Psr\": \"Galactic pulsar\",\n        \"!RfN\": \"Galactic reflection nebula\",\n        \"!Red*\": \"Galactic red star\",\n        \"!SN\": \"Galactic supernova\",\n        \"!SNR\": \"Galactic supernova remnant\",\n        \"!V*\": \"Galactic variable star\",\n        \"!WD*\": \"Galactic white dwarf\",\n        \"!WR*\": \"Galactic Wolf-Rayet star\",\n    }\n    ned_result_table = Ned.query_region(coord, radius=radius)\n    if ned_result_table is None or len(ned_result_table) == 0:\n        ned_results_dict_list = []\n    else:\n        ned_results_df = ned_result_table[\n            [\"Object Name\", \"Separation\", \"Type\", \"RA\", \"DEC\"]\n        ].to_pandas()\n        ned_results_df = ned_results_df.rename(\n            columns={\n                \"Object Name\": \"object_name\",\n                \"Separation\": \"separation_arcsec\",\n                \"Type\": \"otype\",\n                \"RA\": \"ra_hms\",\n                \"DEC\": \"dec_dms\",\n            }\n        )\n        ned_results_df[\"otype_long\"] = ned_results_df.otype.replace(NED_OTYPES)\n        # convert NED result separation (arcmin) to arcsec\n        ned_results_df[\"separation_arcsec\"] = ned_results_df[\"separation_arcsec\"] * 60\n        # convert coordinates to RA (hms) Dec (dms) strings\n        ned_results_df[\"ra_hms\"] = Longitude(\n            ned_results_df[\"ra_hms\"], unit=\"deg\"\n        ).to_string(unit=\"hourangle\")\n        ned_results_df[\"dec_dms\"] = Latitude(\n            ned_results_df[\"dec_dms\"], unit=\"deg\"\n        ).to_string(unit=\"deg\")\n        ned_results_df[\"database\"] = \"NED\"\n        # convert dataframe to dict and replace float NaNs with None for JSON encoding\n        ned_results_dict_list = ned_results_df.sort_values(\"separation_arcsec\").to_dict(\n            orient=\"records\"\n        )\n    return ned_results_dict_list\n</code></pre>"},{"location":"reference/utils/external_query/#vast_pipeline.utils.external_query.simbad","title":"<code>simbad(coord, radius)</code>","text":"<p>Perform a cone search for sources with SIMBAD.</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>SkyCoord</code> <p>The coordinate of the centre of the cone.</p> required <code>radius</code> <code>Angle</code> <p>The radius of the cone in angular units.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dicts, where each dict is a query result row with the following keys:</p> <ul> <li>object_name: the name of the astronomical object.</li> <li>database: the source of the result, i.e. SIMBAD.</li> <li>separation_arcsec: separation to the query coordinate in arcsec.</li> <li>otype: object type.</li> <li>otype_long: long form of the object type.</li> <li>ra_hms: RA coordinate string in hms format.</li> <li>dec_dms: Dec coordinate string in \u00b1dms format.</li> </ul> Source code in <code>vast_pipeline/utils/external_query.py</code> <pre><code>def simbad(coord: SkyCoord, radius: Angle) -&gt; List[Dict[str, Any]]:\n    \"\"\"Perform a cone search for sources with SIMBAD.\n\n    Args:\n        coord: The coordinate of the centre of the cone.\n        radius: The radius of the cone in angular units.\n\n    Returns:\n        A list of dicts, where each dict is a query result row with the following keys:\n\n            - object_name: the name of the astronomical object.\n            - database: the source of the result, i.e. SIMBAD.\n            - separation_arcsec: separation to the query coordinate in arcsec.\n            - otype: object type.\n            - otype_long: long form of the object type.\n            - ra_hms: RA coordinate string in hms format.\n            - dec_dms: Dec coordinate string in \u00b1dms format.\n    \"\"\"\n    CustomSimbad = Simbad()\n    CustomSimbad.add_votable_fields(\n        \"distance_result\",\n        \"otype(S)\",\n        \"otype(V)\",\n        \"otypes\",\n    )\n    try:\n        simbad_result_table = CustomSimbad.query_region(coord, radius=radius)\n    except requests.HTTPError:\n        # try the Harvard mirror\n        CustomSimbad.SIMBAD_URL = \"https://simbad.harvard.edu/simbad/sim-script\"\n        simbad_result_table = CustomSimbad.query_region(coord, radius=radius)\n    if simbad_result_table is None:\n        simbad_results_dict_list = []\n    else:\n        simbad_results_df = simbad_result_table[\n            [\"MAIN_ID\", \"DISTANCE_RESULT\", \"OTYPE_S\", \"OTYPE_V\", \"RA\", \"DEC\"]\n        ].to_pandas()\n        simbad_results_df = simbad_results_df.rename(\n            columns={\n                \"MAIN_ID\": \"object_name\",\n                \"DISTANCE_RESULT\": \"separation_arcsec\",\n                \"OTYPE_S\": \"otype\",\n                \"OTYPE_V\": \"otype_long\",\n                \"RA\": \"ra_hms\",\n                \"DEC\": \"dec_dms\",\n            }\n        )\n        simbad_results_df[\"database\"] = \"SIMBAD\"\n        # convert coordinates to RA (hms) Dec (dms) strings\n        simbad_results_df[\"ra_hms\"] = Longitude(\n            simbad_results_df[\"ra_hms\"], unit=\"hourangle\"\n        ).to_string(unit=\"hourangle\")\n        simbad_results_df[\"dec_dms\"] = Latitude(\n            simbad_results_df[\"dec_dms\"], unit=\"deg\"\n        ).to_string(unit=\"deg\")\n        simbad_results_dict_list = simbad_results_df.to_dict(orient=\"records\")\n    return simbad_results_dict_list\n</code></pre>"},{"location":"reference/utils/external_query/#vast_pipeline.utils.external_query.tns","title":"<code>tns(coord, radius)</code>","text":"<p>Perform a cone search for sources with the Transient Name Server (TNS).</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>SkyCoord</code> <p>The coordinate of the centre of the cone.</p> required <code>radius</code> <code>Angle</code> <p>The radius of the cone in angular units.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dicts, where each dict is a query result row with the following keys:</p> <ul> <li>object_name: the name of the transient.</li> <li>database: the source of the result, i.e. TNS.</li> <li>separation_arcsec: separation to the query coordinate in arcsec.</li> <li>otype: object type.</li> <li>otype_long: long form of the object type. Not given by TNS, will always be     an empty string.</li> <li>ra_hms: RA coordinate string in hms format.</li> <li>dec_dms: Dec coordinate string in \u00b1dms format.</li> </ul> Source code in <code>vast_pipeline/utils/external_query.py</code> <pre><code>def tns(coord: SkyCoord, radius: Angle) -&gt; List[Dict[str, Any]]:\n    \"\"\"Perform a cone search for sources with the Transient Name Server (TNS).\n\n    Args:\n        coord: The coordinate of the centre of the cone.\n        radius: The radius of the cone in angular units.\n\n    Returns:\n        A list of dicts, where each dict is a query result row with the following keys:\n\n            - object_name: the name of the transient.\n            - database: the source of the result, i.e. TNS.\n            - separation_arcsec: separation to the query coordinate in arcsec.\n            - otype: object type.\n            - otype_long: long form of the object type. Not given by TNS, will always be\n                an empty string.\n            - ra_hms: RA coordinate string in hms format.\n            - dec_dms: Dec coordinate string in \u00b1dms format.\n    \"\"\"\n    TNS_API_URL = \"https://www.wis-tns.org/api/\"\n    headers = {\n        \"user-agent\": settings.TNS_USER_AGENT,\n    }\n\n    search_dict = {\n        \"ra\": coord.ra.to_string(unit=\"hourangle\", sep=\":\", pad=True),\n        \"dec\": coord.dec.to_string(unit=\"deg\", sep=\":\", alwayssign=True, pad=True),\n        \"radius\": str(radius.value),\n        \"units\": radius.unit.name,\n    }\n    r = requests.post(\n        urljoin(TNS_API_URL, \"get/search\"),\n        data={\"api_key\": settings.TNS_API_KEY, \"data\": json.dumps(search_dict)},\n        headers=headers,\n    )\n    tns_results_dict_list: List[Dict[str, Any]]\n    if r.ok:\n        tns_results_dict_list = r.json()[\"data\"][\"reply\"]\n        # Get details for each object result. TNS API doesn't support doing this in one\n        # request, so we iterate.\n        for result in tns_results_dict_list:\n            search_dict = {\n                \"objname\": result[\"objname\"],\n            }\n            r = requests.post(\n                urljoin(TNS_API_URL, \"get/object\"),\n                data={\"api_key\": settings.TNS_API_KEY, \"data\": json.dumps(search_dict)},\n                headers=headers,\n            )\n            if r.ok:\n                object_dict = r.json()[\"data\"][\"reply\"]\n                object_coord = SkyCoord(\n                    ra=object_dict[\"radeg\"], dec=object_dict[\"decdeg\"], unit=\"deg\"\n                )\n                result[\"otype\"] = object_dict[\"object_type\"][\"name\"]\n                if result[\"otype\"] is None:\n                    result[\"otype\"] = \"\"\n                result[\"otype_long\"] = \"\"\n                result[\"separation_arcsec\"] = coord.separation(object_coord).arcsec\n                result[\"ra_hms\"] = object_coord.ra.to_string(unit=\"hourangle\")\n                result[\"dec_dms\"] = object_coord.dec.to_string(unit=\"deg\")\n                result[\"database\"] = \"TNS\"\n                result[\"object_name\"] = object_dict[\"objname\"]\n    return tns_results_dict_list\n</code></pre>"},{"location":"reference/utils/unit_tags/","title":"unit_tags.py","text":""},{"location":"reference/utils/unit_tags/#vast_pipeline.utils.unit_tags.deg_to_arcmin","title":"<code>deg_to_arcmin(angle)</code>","text":"<p>Convert degrees to arcminutes.</p> <p>Parameters:</p> Name Type Description Default <code>angle</code> <code>float</code> <p>Angle in units of degrees.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Angle in units of arcminutes.</p> Source code in <code>vast_pipeline/utils/unit_tags.py</code> <pre><code>@register.filter\ndef deg_to_arcmin(angle: float) -&gt; float:\n    \"\"\"\n    Convert degrees to arcminutes.\n\n    Args:\n        angle: Angle in units of degrees.\n\n    Returns:\n        Angle in units of arcminutes.\n    \"\"\"\n    return float(angle) * 60.\n</code></pre>"},{"location":"reference/utils/unit_tags/#vast_pipeline.utils.unit_tags.deg_to_arcsec","title":"<code>deg_to_arcsec(angle)</code>","text":"<p>Convert degrees to arcseconds.</p> <p>Parameters:</p> Name Type Description Default <code>angle</code> <code>float</code> <p>Angle in units of degrees.</p> required <p>Returns:</p> Name Type Description <code>angle</code> <code>float</code> <p>Angle in units of arcseconds.</p> Source code in <code>vast_pipeline/utils/unit_tags.py</code> <pre><code>@register.filter\ndef deg_to_arcsec(angle: float) -&gt; float:\n    \"\"\"\n    Convert degrees to arcseconds.\n\n    Args:\n        angle: Angle in units of degrees.\n\n    Returns:\n        angle: Angle in units of arcseconds.\n    \"\"\"\n    return float(angle) * 3600.\n</code></pre>"},{"location":"reference/utils/utils/","title":"utils.py","text":"<p>This module contains general pipeline utility functions.</p>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.StopWatch","title":"<code>StopWatch</code>","text":"<p>A simple stopwatch to simplify timing code.</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>class StopWatch:\n    \"\"\"\n    A simple stopwatch to simplify timing code.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Initialise the StopWatch\n\n        Returns:\n            None.\n        \"\"\"\n        self._init = datetime.now()\n        self._last = self._init\n\n    def reset(self) -&gt; float:\n        \"\"\"\n        Reset the stopwatch and return the time since last reset (seconds).\n\n        Returns:\n            The time in seconds since the last reset.\n        \"\"\"\n        now = datetime.now()\n        diff = (now - self._last).total_seconds()\n        self._last = now\n\n        return diff\n\n    def reset_init(self) -&gt; float:\n        \"\"\"\n        Reset the stopwatch and return the total time since initialisation.\n\n        Returns:\n            The time in seconds since the initialisation.\n        \"\"\"\n        now = datetime.now()\n        diff = (now - self._init).total_seconds()\n        self._last = self._init = now\n\n        return diff\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.StopWatch.__init__","title":"<code>__init__()</code>","text":"<p>Initialise the StopWatch</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initialise the StopWatch\n\n    Returns:\n        None.\n    \"\"\"\n    self._init = datetime.now()\n    self._last = self._init\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.StopWatch.reset","title":"<code>reset()</code>","text":"<p>Reset the stopwatch and return the time since last reset (seconds).</p> <p>Returns:</p> Type Description <code>float</code> <p>The time in seconds since the last reset.</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def reset(self) -&gt; float:\n    \"\"\"\n    Reset the stopwatch and return the time since last reset (seconds).\n\n    Returns:\n        The time in seconds since the last reset.\n    \"\"\"\n    now = datetime.now()\n    diff = (now - self._last).total_seconds()\n    self._last = now\n\n    return diff\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.StopWatch.reset_init","title":"<code>reset_init()</code>","text":"<p>Reset the stopwatch and return the total time since initialisation.</p> <p>Returns:</p> Type Description <code>float</code> <p>The time in seconds since the initialisation.</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def reset_init(self) -&gt; float:\n    \"\"\"\n    Reset the stopwatch and return the total time since initialisation.\n\n    Returns:\n        The time in seconds since the initialisation.\n    \"\"\"\n    now = datetime.now()\n    diff = (now - self._init).total_seconds()\n    self._last = self._init = now\n\n    return diff\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.calculate_n_partitions","title":"<code>calculate_n_partitions(df, n_cpu, partition_size_mb=15)</code>","text":"<p>This function will calculate how many partitions a dataframe should be split into.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The pandas dataframe to be partitionined.</p> required <code>n_cpu</code> <p>The number of available CPUs.</p> required <code>partition_size</code> <p>The optimal partition size in MB. NOTE: The default partition size of 15MB is chosen because     many of the parallelised operations on partitioned     DataFrames can consume a much larger amount of memory     than the size of the partition. 15MB avoids consuming     too much memory for significant amounts of parallelism     (e.g. n_cpu &gt; 10) without significant cost to processing     speed.</p> required <p>Returns:</p> Type Description <p>The optimal number of partitions.</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def calculate_n_partitions(df, n_cpu, partition_size_mb=15):\n    \"\"\"\n    This function will calculate how many partitions a dataframe should be\n    split into.\n\n    Args:\n        df: The pandas dataframe to be partitionined.\n        n_cpu: The number of available CPUs.\n        partition_size: The optimal partition size in MB.\n            NOTE: The default partition size of 15MB is chosen because\n                many of the parallelised operations on partitioned\n                DataFrames can consume a much larger amount of memory\n                than the size of the partition. 15MB avoids consuming\n                too much memory for significant amounts of parallelism\n                (e.g. n_cpu &gt; 10) without significant cost to processing\n                speed.\n\n    Returns:\n        The optimal number of partitions.\n    \"\"\"\n    mem_usage_mb = df.memory_usage(deep=True).sum() / 1e6\n    n_partitions = int(np.ceil(mem_usage_mb / partition_size_mb))\n\n    # n_partitions should be &gt;= n_cpu for optimal parallel processing\n    if n_partitions &lt; n_cpu:\n        n_partitions = n_cpu\n\n    partition_size_mb = int(np.ceil(mem_usage_mb / n_partitions))\n\n    logger.debug(\"Using %d partions of %dMB\", n_partitions, partition_size_mb)\n\n    return n_partitions\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.calculate_workers_and_partitions","title":"<code>calculate_workers_and_partitions(df, n_cpu=None, max_partition_mb=15)</code>","text":"<p>Return number of workers and the number of partitions for Dask</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The pandas dataframe to be partitionined. Don't calculate partitions if df is None</p> required <code>num_cpu_max</code> <p>The maximum number of workers to allocate.          The default of None means use one less than all available cores</p> required <code>max_partition_mb</code> <p>The maximum partition size in MB.</p> <code>15</code> <p>Returns:</p> Type Description <code>(num_workers, n_partitions)</code> <p>Calculated workers and partitions.</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def calculate_workers_and_partitions(df, n_cpu=None, max_partition_mb=15):\n    \"\"\"\n    Return number of workers and the number of partitions for Dask\n\n    Args:\n        df: The pandas dataframe to be partitionined.\n            Don't calculate partitions if df is None\n        num_cpu_max: The maximum number of workers to allocate.\n                     The default of None means use one less than all available cores\n        max_partition_mb: The maximum partition size in MB.\n\n    Returns:\n        (num_workers, n_partitions): Calculated workers and partitions.\n    \"\"\"\n    num_cpu = cpu_count() - 1\n    num_workers = num_cpu if n_cpu is None else n_cpu\n    if num_workers &gt; num_cpu:\n        logger.debug(\"%d desired workers is greater than available cores. Limiting to %s.\",\n                     num_workers, num_cpu)\n        num_workers = num_cpu\n    n_partitions = 0\n    if df is not None:\n        n_partitions = calculate_n_partitions(df, num_workers,\n                                              partition_size_mb=max_partition_mb)\n\n    return num_workers, n_partitions\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.check_read_write_perm","title":"<code>check_read_write_perm(path, perm='W')</code>","text":"<p>Assess the file permission on a path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The system path to assess.</p> required <code>perm</code> <code>str</code> <p>The permission to check for.</p> <code>'W'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>IOError</code> <p>The permission is not valid on the checked directory.</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def check_read_write_perm(path: str, perm: str = \"W\") -&gt; None:\n    \"\"\"\n    Assess the file permission on a path.\n\n    Args:\n        path: The system path to assess.\n        perm: The permission to check for.\n\n    Returns:\n        None\n\n    Raises:\n        IOError: The permission is not valid on the checked directory.\n    \"\"\"\n    assert perm in (\"R\", \"W\", \"X\"), \"permission not supported\"\n\n    perm_map = {\"R\": os.R_OK, \"W\": os.W_OK, \"X\": os.X_OK}\n    if not os.access(path, perm_map[perm]):\n        msg = f\"permission not valid on folder: {path}\"\n        logger.error(msg)\n        raise IOError(msg)\n\n    pass\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.deg2dms","title":"<code>deg2dms(deg, dms_format=False, precision=2, truncate=False, latitude=True)</code>","text":"<p>Convert angle in degrees into a DMS formatted string.</p> <p>Parameters:</p> Name Type Description Default <code>deg</code> <code>float</code> <p>The angle to convert in degrees.</p> required <code>dms_format</code> <code>optional</code> <p>If <code>True</code>, use \"d\", \"m\", and \"s\" as the coorindate separator, otherwise use \":\". Defaults to False.</p> <code>False</code> <code>precision</code> <code>optional</code> <p>Floating point precision of the arcseconds component. Can be 0 or a positive integer. Negative values will be interpreted as 0. Defaults to 2.</p> <code>2</code> <code>truncate</code> <code>optional</code> <p>Truncate values after the decimal point instead of rounding. Defaults to False (rounding).</p> <code>False</code> <code>latitude</code> <code>optional</code> <p>The input <code>deg</code> value should be intrepreted as a latitude. Otherwise, it will be interpreted as a longitude. Defaults to True (latitude).</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p><code>deg</code> formatted as a DMS string.</p> Example <p>deg2dms(12.582438888888889) '+12:34:56.78' deg2dms(2.582438888888889, dms_format=True) '+02d34m56.78s' deg2dms(-12.582438888888889, precision=1) '-12:34:56.8' deg2dms(-12.582438888888889, precision=1, truncate=True) '-12:34:56.7'</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def deg2dms(\n    deg: float,\n    dms_format: bool = False,\n    precision: int = 2,\n    truncate: bool = False,\n    latitude: bool = True,\n) -&gt; str:\n    \"\"\"Convert angle in degrees into a DMS formatted string.\n\n    Args:\n        deg: The angle to convert in degrees.\n        dms_format (optional): If `True`, use \"d\", \"m\", and \"s\" as the coorindate\n            separator, otherwise use \":\". Defaults to False.\n        precision (optional): Floating point precision of the arcseconds component.\n            Can be 0 or a positive integer. Negative values will be interpreted as 0.\n            Defaults to 2.\n        truncate (optional): Truncate values after the decimal point instead of rounding.\n            Defaults to False (rounding).\n        latitude (optional): The input `deg` value should be intrepreted as a latitude.\n            Otherwise, it will be interpreted as a longitude.\n            Defaults to True (latitude).\n\n    Returns:\n        `deg` formatted as a DMS string.\n\n    Example:\n        &gt;&gt;&gt; deg2dms(12.582438888888889)\n        '+12:34:56.78'\n        &gt;&gt;&gt; deg2dms(2.582438888888889, dms_format=True)\n        '+02d34m56.78s'\n        &gt;&gt;&gt; deg2dms(-12.582438888888889, precision=1)\n        '-12:34:56.8'\n        &gt;&gt;&gt; deg2dms(-12.582438888888889, precision=1, truncate=True)\n        '-12:34:56.7'\n    \"\"\"\n    AngleClass = Latitude if latitude else Longitude\n    angle = AngleClass(deg, unit=\"deg\")\n    precision = precision if precision &gt;= 0 else 0\n\n    output_str: str = angle.to_string(\n        unit=\"deg\",\n        sep=\"fromunit\" if dms_format else \":\",\n        precision=precision if not truncate else None,\n        alwayssign=True,\n        pad=True,\n    )\n    if truncate:\n        # find the decimal point char position and the number of decimal places in the\n        # rendered input coordinate (in DMS format, not decimal deg)\n        dp_pos = output_str.find(\".\")\n        n_dp = len(output_str[dp_pos + 1:]) if dp_pos &gt;= 0 else 0\n\n        # if the input coordinate precision is less than the requsted output precision,\n        # pad the end with zeroes\n        if n_dp &lt; precision:\n            seconds_str = \"\"\n            # account for rendered input coord having precision = 0\n            if dp_pos &lt; 0:\n                seconds_str += \".\"\n            seconds_str += \"0\" * (precision - n_dp)\n            output_str += seconds_str\n        # otherwise, cut off the excess decimal places\n        elif n_dp &gt; precision:\n            if precision &gt; 0:\n                # account for the decimal point char\n                precision += 1\n            output_str = output_str[: dp_pos + precision]\n        # in the n_dp == precision case, do nothing\n    return output_str\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.deg2hms","title":"<code>deg2hms(deg, hms_format=False, precision=2, truncate=False, longitude=True)</code>","text":"<p>Convert angle in degrees into a HMS formatted string.</p> <p>Parameters:</p> Name Type Description Default <code>deg</code> <code>float</code> <p>The angle to convert in degrees.</p> required <code>hms_format</code> <code>optional</code> <p>If <code>True</code>, use \"h\", \"m\", and \"s\" as the coorindate separator, otherwise use \":\". Defaults to False.</p> <code>False</code> <code>precision</code> <code>optional</code> <p>Floating point precision of the seconds component. Can be 0 or a positive integer. Negative values will be interpreted as 0. Defaults to 2.</p> <code>2</code> <code>truncate</code> <code>optional</code> <p>Truncate values after the decimal point instead of rounding. Defaults to False (rounding).</p> <code>False</code> <code>longitude</code> <code>optional</code> <p>The input <code>deg</code> value should be intrepreted as a longitude. Otherwise, it will be interpreted as a latitude. Defaults to True (longitude).</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p><code>deg</code> formatted as an HMS string.</p> Example <p>deg2hms(188.73658333333333) '12:34:56.78' deg2hms(-188.73658333333333, hms_format=True) '12h34m56.78s' deg2hms(188.73658333333333, precision=1) '12:34:56.8' deg2hms(188.73658333333333, precision=1, truncate=True) '12:34:56.7'</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def deg2hms(\n    deg: float,\n    hms_format: bool = False,\n    precision: int = 2,\n    truncate: bool = False,\n    longitude: bool = True,\n) -&gt; str:\n    \"\"\"Convert angle in degrees into a HMS formatted string.\n\n    Args:\n        deg: The angle to convert in degrees.\n        hms_format (optional): If `True`, use \"h\", \"m\", and \"s\" as the coorindate\n            separator, otherwise use \":\". Defaults to False.\n        precision (optional): Floating point precision of the seconds component.\n            Can be 0 or a positive integer. Negative values will be interpreted as 0.\n            Defaults to 2.\n        truncate (optional): Truncate values after the decimal point instead of rounding.\n            Defaults to False (rounding).\n        longitude (optional): The input `deg` value should be intrepreted as a longitude.\n            Otherwise, it will be interpreted as a latitude.\n            Defaults to True (longitude).\n\n    Returns:\n        `deg` formatted as an HMS string.\n\n    Example:\n        &gt;&gt;&gt; deg2hms(188.73658333333333)\n        '12:34:56.78'\n        &gt;&gt;&gt; deg2hms(-188.73658333333333, hms_format=True)\n        '12h34m56.78s'\n        &gt;&gt;&gt; deg2hms(188.73658333333333, precision=1)\n        '12:34:56.8'\n        &gt;&gt;&gt; deg2hms(188.73658333333333, precision=1, truncate=True)\n        '12:34:56.7'\n    \"\"\"\n    # use the deg2dms formatter, replace d with h, and cut off the leading \u00b1\n    # sign\n    return deg2dms(\n        deg / 15.0,\n        dms_format=hms_format,\n        precision=precision,\n        truncate=truncate,\n        latitude=not longitude,\n    ).replace(\"d\", \"h\")[1:]\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.dict_merge","title":"<code>dict_merge(dct, merge_dct, add_keys=True)</code>","text":"<p>Recursive dict merge. Inspired by dict.update(), instead of updating only top-level keys, dict_merge recurses down into dicts nested to an arbitrary depth, updating keys. The <code>merge_dct</code> is merged into <code>dct</code>.</p> <p>This version will return a copy of the dictionary and leave the original arguments untouched.</p> <p>The optional argument <code>add_keys</code>, determines whether keys which are present in <code>merge_dict</code> but not <code>dct</code> should be included in the new dict.</p> <p>Parameters:</p> Name Type Description Default <code>dct</code> <code>dict</code> <p>onto which the merge is executed</p> required <code>merge_dct</code> <code>dict</code> <p>dct merged into dct</p> required <code>add_keys</code> <code>bool</code> <p>whether to add new keys</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[Any, Any]</code> <p>Updated dict.</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def dict_merge(\n    dct: Dict[Any, Any], merge_dct: Dict[Any, Any], add_keys=True\n) -&gt; Dict[Any, Any]:\n    \"\"\"Recursive dict merge. Inspired by dict.update(), instead of\n    updating only top-level keys, dict_merge recurses down into dicts nested\n    to an arbitrary depth, updating keys. The `merge_dct` is merged into\n    `dct`.\n\n    This version will return a copy of the dictionary and leave the original\n    arguments untouched.\n\n    The optional argument `add_keys`, determines whether keys which are\n    present in `merge_dict` but not `dct` should be included in the\n    new dict.\n\n    Args:\n        dct (dict): onto which the merge is executed\n        merge_dct (dict): dct merged into dct\n        add_keys (bool): whether to add new keys\n\n    Returns:\n        Updated dict.\n    \"\"\"\n    dct = dct.copy()\n    if not add_keys:\n        merge_dct = {k: merge_dct[k]\n                     for k in set(dct).intersection(set(merge_dct))}\n\n    for k, v in merge_dct.items():\n        if (\n            k in dct\n            and isinstance(dct[k], dict)\n            and isinstance(merge_dct[k], collections.abc.Mapping)\n        ):\n            dct[k] = dict_merge(dct[k], merge_dct[k], add_keys=add_keys)\n        else:\n            dct[k] = merge_dct[k]\n\n    return dct\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.eq_to_cart","title":"<code>eq_to_cart(ra, dec)</code>","text":"<p>Find the cartesian co-ordinates on the unit sphere given the eq. co-ords. ra, dec should be in degrees.</p> <p>Parameters:</p> Name Type Description Default <code>ra</code> <code>float</code> <p>The right ascension coordinate, in degrees, to convert.</p> required <code>dec</code> <code>float</code> <p>The declination coordinate, in degrees, to convert.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The cartesian x coordinate.</p> <code>float</code> <p>The cartesian y coordinate.</p> <code>float</code> <p>The cartesian z coordinate.</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def eq_to_cart(ra: float, dec: float) -&gt; Tuple[float, float, float]:\n    \"\"\"\n    Find the cartesian co-ordinates on the unit sphere given the eq.\n    co-ords. ra, dec should be in degrees.\n\n    Args:\n        ra: The right ascension coordinate, in degrees, to convert.\n        dec: The declination coordinate, in degrees, to convert.\n\n    Returns:\n        The cartesian x coordinate.\n        The cartesian y coordinate.\n        The cartesian z coordinate.\n    \"\"\"\n    # TODO: This part of the code can probably be removed along with the\n    # storage of these coodinates on the image.\n    return (\n        m.cos(m.radians(dec)) * m.cos(m.radians(ra)),  # Cartesian x\n        m.cos(m.radians(dec)) * m.sin(m.radians(ra)),  # Cartesian y\n        m.sin(m.radians(dec)),  # Cartesian z\n    )\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.equ2gal","title":"<code>equ2gal(ra, dec)</code>","text":"<p>Convert equatorial coordinates to galactic</p> <p>Parameters:</p> Name Type Description Default <code>ra</code> <code>float</code> <p>Right ascension in units of degrees.</p> required <code>dec</code> <code>float</code> <p>Declination in units of degrees.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Galactic longitude in degrees.</p> <code>float</code> <p>Galactic latitude in degrees.</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def equ2gal(ra: float, dec: float) -&gt; Tuple[float, float]:\n    \"\"\"\n    Convert equatorial coordinates to galactic\n\n    Args:\n        ra (float): Right ascension in units of degrees.\n        dec (float): Declination in units of degrees.\n\n    Returns:\n        Galactic longitude in degrees.\n        Galactic latitude in degrees.\n    \"\"\"\n    c = SkyCoord(\n        np.float(ra),\n        np.float(dec),\n        unit=(\n            u.deg,\n            u.deg),\n        frame=\"icrs\")\n    l = c.galactic.l.deg\n    b = c.galactic.b.deg\n\n    return l, b\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.gal2equ","title":"<code>gal2equ(l, b)</code>","text":"<p>Convert galactic coordinates to equatorial.</p> <p>Parameters:</p> Name Type Description Default <code>l</code> <code>float</code> <p>Galactic longitude in degrees.</p> required <code>b</code> <code>float</code> <p>Galactic latitude in degrees.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Right ascension in degrees.</p> <code>float</code> <p>Declination in degrees.</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def gal2equ(l: float, b: float) -&gt; Tuple[float, float]:\n    \"\"\"\n    Convert galactic coordinates to equatorial.\n\n    Args:\n        l (float): Galactic longitude in degrees.\n        b (float): Galactic latitude in degrees.\n\n    Returns:\n        Right ascension in degrees.\n        Declination in degrees.\n    \"\"\"\n    c = SkyCoord(\n        l=np.float(l) * u.deg,\n        b=np.float(b) * u.deg,\n        frame=\"galactic\")\n    ra = c.icrs.ra.deg\n    dec = c.icrs.dec.deg\n\n    return ra, dec\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.optimize_floats","title":"<code>optimize_floats(df)</code>","text":"<p>Downcast float columns in a pd.DataFrame to the smallest data type without losing any information.</p> <p>Credit to Robbert van der Gugten.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>input dataframe, no specific columns.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The input dataframe with the <code>float64</code> type columns downcasted.</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def optimize_floats(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Downcast float columns in a pd.DataFrame to the smallest\n    data type without losing any information.\n\n    Credit to Robbert van der Gugten.\n\n    Args:\n        df:\n            input dataframe, no specific columns.\n\n    Returns:\n        The input dataframe with the `float64` type columns downcasted.\n    \"\"\"\n    floats = df.select_dtypes(include=[\"float64\"]).columns.tolist()\n    df[floats] = df[floats].apply(pd.to_numeric, downcast=\"float\")\n\n    return df\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.optimize_ints","title":"<code>optimize_ints(df)</code>","text":"<p>Downcast integer columns in a pd.DataFrame to the smallest data type without losing any information.</p> <p>Credit to Robbert van der Gugten.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe, no specific columns.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The input dataframe with the <code>int64</code> type columns downcasted.</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def optimize_ints(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Downcast integer columns in a pd.DataFrame to the smallest\n    data type without losing any information.\n\n    Credit to Robbert van der Gugten.\n\n    Args:\n        df:\n            Input dataframe, no specific columns.\n\n    Returns:\n        The input dataframe with the `int64` type columns downcasted.\n    \"\"\"\n    ints = df.select_dtypes(include=[\"int64\"]).columns.tolist()\n    df[ints] = df[ints].apply(pd.to_numeric, downcast=\"integer\")\n\n    return df\n</code></pre>"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.parse_coord","title":"<code>parse_coord(coord_string, coord_frame='icrs')</code>","text":"<p>Parse a coordinate string and return a SkyCoord. The units may be expressed within <code>coord_string</code> e.g. \"21h52m03.1s -62d08m19.7s\", \"18.4d +43.1d\". If no units are given, the following assumptions are made:     - if both coordinate components are decimals, they are assumed to be in degrees.     - if a sexagesimal coordinate is given and the frame is galactic, both components         are assumed to be in degrees. For any other frame, the first component is         assumed to be in hourangles and the second in degrees. Will raise a ValueError if SkyCoord is unable to parse <code>coord_string</code>.</p> <p>Parameters:</p> Name Type Description Default <code>coord_string</code> <code>str</code> <p>The coordinate string to parse.</p> required <code>coord_frame</code> <code>str</code> <p>The frame of <code>coord_string</code>. Defaults to \"icrs\".</p> <code>'icrs'</code> <p>Returns:</p> Type Description <code>SkyCoord</code> <p>The SkyCoord object.</p> Source code in <code>vast_pipeline/utils/utils.py</code> <pre><code>def parse_coord(coord_string: str, coord_frame: str = \"icrs\") -&gt; SkyCoord:\n    \"\"\"Parse a coordinate string and return a SkyCoord. The units may be expressed within\n    `coord_string` e.g. \"21h52m03.1s -62d08m19.7s\", \"18.4d +43.1d\". If no units are given,\n    the following assumptions are made:\n        - if both coordinate components are decimals, they are assumed to be in degrees.\n        - if a sexagesimal coordinate is given and the frame is galactic, both components\n            are assumed to be in degrees. For any other frame, the first component is\n            assumed to be in hourangles and the second in degrees.\n    Will raise a ValueError if SkyCoord is unable to parse `coord_string`.\n\n    Args:\n        coord_string (str): The coordinate string to parse.\n        coord_frame (str, optional): The frame of `coord_string`. Defaults to \"icrs\".\n\n    Returns:\n        The SkyCoord object.\n    \"\"\"\n    # if both coord components are decimals, assume they're in degrees, otherwise assume\n    # hourangles and degrees. Note that the unit parameter is ignored if the units are\n    # not ambiguous i.e. if coord_string contains the units (e.g. 18.4d,\n    # 5h35m, etc)\n    try:\n        _ = [float(x) for x in coord_string.split()]\n        unit = \"deg\"\n    except ValueError:\n        if coord_frame == \"galactic\":\n            unit = \"deg\"\n        else:\n            unit = \"hourangle,deg\"\n\n    coord = SkyCoord(coord_string, unit=unit, frame=coord_frame)\n\n    return coord\n</code></pre>"},{"location":"reference/utils/view/","title":"view.py","text":"<p>Functions and variables used in pipeline/views.py.</p>"},{"location":"reference/utils/view/#vast_pipeline.utils.view.generate_colsfields","title":"<code>generate_colsfields(fields, url_prefix_dict, not_orderable_col=None, not_searchable_col=None)</code>","text":"<p>Generate data to be included in context for datatables.</p> <p>Example of url_prefix_dict format: api_col_dict = {     'source.name': reverse('vast_pipeline:source_detail', args=[1])[:-2],     'source.run.name': reverse('vast_pipeline:run_detail', args=[1])[:-2] }</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list</code> <p>List of fields to include as columns.</p> required <code>url_prefix_dict</code> <code>dict</code> <p>Dict containing the url prefix to form href links in the datatables.</p> required <code>not_orderable_col</code> <code>list</code> <p>List of columns that should be set to be not orderable in the final table.</p> <code>None</code> <code>not_searchable_col</code> <code>list</code> <p>List of columns that should be set to be not searchable in the final table.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List containing the JSON object.</p> Source code in <code>vast_pipeline/utils/view.py</code> <pre><code>def generate_colsfields(\n    fields: List[str], url_prefix_dict: Dict[str, str],\n    not_orderable_col: Optional[List[str]]=None,\n    not_searchable_col: Optional[List[str]]=None,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Generate data to be included in context for datatables.\n\n    Example of url_prefix_dict format:\n    api_col_dict = {\n        'source.name': reverse('vast_pipeline:source_detail', args=[1])[:-2],\n        'source.run.name': reverse('vast_pipeline:run_detail', args=[1])[:-2]\n    }\n\n    Args:\n        fields (list): List of fields to include as columns.\n        url_prefix_dict (dict): Dict containing the url prefix to form\n            href links in the datatables.\n        not_orderable_col (list): List of columns that should be set to\n            be not orderable in the final table.\n        not_searchable_col (list): List of columns that should be set to\n            be not searchable in the final table.\n\n    Returns:\n        List containing the JSON object.\n    \"\"\"\n    colsfields = []\n\n    if not_orderable_col is None:\n        not_orderable_col = []\n    if not_searchable_col is None:\n        not_searchable_col = []\n    for col in fields:\n        field2append = {}\n        if col == 'name':\n            field2append = {\n                'data': col, 'render': {\n                    'url': {\n                        'prefix': url_prefix_dict[col],\n                        'col': 'name'\n                    }\n                }\n            }\n        elif '.name' in col:\n            # this is for nested fields to build a render with column name\n            # and id in url. The API results should look like:\n            # {... , main_col : {'name': value, 'id': value, ... }}\n            main_col = col.rsplit('.', 1)[0]\n            field2append = {\n                'data': col,\n                'render': {\n                    'url': {\n                        'prefix': url_prefix_dict[col],\n                        'col': main_col,\n                        'nested': True,\n                    }\n                }\n            }\n        elif col == 'n_sibl':\n            field2append = {\n                'data': col, 'render': {\n                    'contains_sibl': {\n                        'col': col\n                    }\n                }\n            }\n        elif col in FLOAT_FIELDS:\n            field2append = {\n                'data': col,\n                'render': {\n                    'float': {\n                        'col': col,\n                        'precision': FLOAT_FIELDS[col]['precision'],\n                        'scale': FLOAT_FIELDS[col]['scale'],\n                    }\n                }\n            }\n        else:\n            field2append = {'data': col}\n\n        if col in not_orderable_col:\n            field2append['orderable'] = False\n\n        if col in not_searchable_col:\n            field2append['searchable'] = False\n\n        colsfields.append(field2append)\n\n    return colsfields\n</code></pre>"},{"location":"reference/utils/view/#vast_pipeline.utils.view.get_skyregions_collection","title":"<code>get_skyregions_collection(run_id=None)</code>","text":"<p>Produce Sky region geometry shapes JSON object for d3-celestial.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>int</code> <p>Run ID to filter on if not None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary representing a JSON object containing the sky regions.</p> Source code in <code>vast_pipeline/utils/view.py</code> <pre><code>def get_skyregions_collection(run_id: Optional[int]=None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Produce Sky region geometry shapes JSON object for d3-celestial.\n\n    Args:\n        run_id (int, optional): Run ID to filter on if not None.\n\n    Returns:\n        Dictionary representing a JSON object containing the sky regions.\n    \"\"\"\n    skyregions = SkyRegion.objects.all()\n    if run_id is not None:\n        skyregions = skyregions.filter(run=run_id)\n\n    features = []\n\n    for skr in skyregions:\n        ra_fix = 360. if skr.centre_ra &gt; 180. else 0.\n        ra = skr.centre_ra - ra_fix\n        dec = skr.centre_dec\n        width_ra = skr.width_ra / 2.\n        width_dec = skr.width_dec / 2.\n        id = skr.id\n        features.append(\n            {\n                \"type\": \"Feature\",\n                \"id\": f\"SkyRegion{id}\",\n                \"properties\": {\n                    \"n\": f\"{id:02d}\",\n                    \"loc\": [ra, dec]\n                },\n                \"geometry\": {\n                    \"type\": \"MultiLineString\",\n                    \"coordinates\": [[\n                        [ra+width_ra, dec+width_dec],\n                        [ra+width_ra, dec-width_dec],\n                        [ra-width_ra, dec-width_dec],\n                        [ra-width_ra, dec+width_dec],\n                        [ra+width_ra, dec+width_dec]\n                    ]]\n                }\n            }\n        )\n\n    skyregions_collection = {\n        \"type\": \"FeatureCollection\",\n        \"features\" : features\n    }\n\n    return skyregions_collection\n</code></pre>"},{"location":"using/access/","title":"Accessing the Pipeline","text":"<p>Access to the pipeline website is done using GitHub as the authentification method. In particular it checks organisation membership to confirm that the user is allowed access. For example for those wanting to access the VAST instance hosted on Nimbus must make sure they are a member of the askap-vast GitHub organisation.</p> <p>Note</p> <p>If you are attempting to access an instance of the VAST pipeline not hosted by the VAST group, confirm with the administrator what GitHub organisation membership is required.</p> <p>Adminstrators can refer to Pipeline Login for information on how to configure the login system.</p>"},{"location":"using/access/#logging-in","title":"Logging In","text":"<ol> <li> <p>Navigate to the VAST Pipeline (or other hosted instance) and the following page will appear. </p> </li> <li> <p>Click the <code>Login with GitHub</code> button and you will be presented with the following page to enter you GitHub details. Note that if you are already logged into GitHub in your browser then you will likely not see this page. </p> </li> <li> <p>After a successful login you will then be redirected to the Pipeline homepage. </p> </li> </ol>"},{"location":"using/access/#troubleshooting","title":"Troubleshooting","text":"<p>Failures will commonly be caused by the user not being a member of the correct GitHub organisation or an error in the configuration of the login system by the administrator. Contact the administrator of the pipeline instance if you encounter problems logging in.</p>"},{"location":"using/addtorun/","title":"Adding Images to a Run","text":"<p>This page describes how to add images to a completed run, including how to restore the run to the previous state if an addition goes wrong.</p> <p>Note</p> <p>Adding images to an existing run will update the sources already present from the respective run, such that the existing source IDs, comments, and tags are retained. If a full re-run was used instead then new IDs would be created and the comments and tags lost.</p> <p>There is no limit on how many times images may be added to a run.</p>"},{"location":"using/addtorun/#step-by-step-guide","title":"Step-by-step Guide","text":"<p>Warning</p> <p>A run must have a <code>Completed</code> status before images can be added to it.</p> <p>No other settings other than the input data can be changed in the config.</p>"},{"location":"using/addtorun/#1-navigate-to-the-run-detail-page","title":"1. Navigate to the Run Detail Page","text":"<p>Navigate to the detail page of the run you wish to process, and confirm that the job is marked as <code>Completed</code>.</p> <p></p>"},{"location":"using/addtorun/#2-add-the-new-images-to-the-configuration","title":"2. Add the New Images to the Configuration","text":"<p>Scroll to the configuration editor, enter edit mode and add the new images to the existing data inputs. If using epoch mode notation, add a new epoch(s) to the file.</p> <p>Once all the images, selavy files, rms images and background images have been added, select the <code>Write Current Config</code> option to save the file.</p> <p>Warning</p> <p>Do not remove the previous images from the configuration inputs or change any other options!</p> <p>Remember to make sure the order of the new input data is consistent between types!</p> <p></p>"},{"location":"using/addtorun/#3-perform-a-config-validation","title":"3. Perform a Config Validation","text":"<p>Check that the configuration file is still valid by selecting <code>Validate Config</code> from the same menu as shown in the previous step 2 screenshot.</p>"},{"location":"using/addtorun/#4-process-the-run","title":"4. Process the Run","text":"<p>Select the <code>Add Images or Re-Process Run</code> button at the top right of the run detail page to open the processing modal window. Select whether to turn debugging log output <code>On</code> or <code>Off</code> and when ready select the <code>Schedule Run</code>.</p> <p>Warning</p> <p>Do not toggle <code>Full Re-Run</code> to <code>On</code>!</p> <p>You can refresh the page to check the status of the run. You can confirm that the images have been added correctly by consulting the log output found below the configuration file. New images should have been ingested and output similar to the following should be present:</p> <p>2021-04-02-21-14-31_log.txt</p> <p> <pre><code>2021-04-02 21:17:00,628 association INFO Starting association.\n2021-04-02 21:17:00,628 association INFO Association mode selected: basic.\n2021-04-02 21:17:00,708 association INFO Found 7 images to add to the run.\n</code></pre></p> <p>Once the processing has <code>Completed</code> the run detail page will now show the updated statistics and information of the run.</p> <p></p>"},{"location":"using/addtorun/#restore-run-to-pre-add-version","title":"Restore Run to Pre-Add Version","text":"<p>When images are added to a run, a backup is made of the run before proceeding which can be used to restore the run to the pre-addition version.  For example, perhaps the wrong images were added or an error occurred mid-addition that could not be resolved.</p> <p>For full details see the documentation page for restoring a run here.</p> <p>Warning</p> <p>Do not add any further images if you wish to restore otherwise the backup version will be lost!</p>"},{"location":"using/deleterun/","title":"Deleting a Run","text":"<p>This page describes how to delete a pipeline run through the website interface.</p> <p>Deleting a run means that all outputs such as sources and associations are deleted from the database and the run itself is also removed. Images and the accompanying measurements are only removed if they were used solely by the deleted run. The run directory is also deleted that contains the output files and configuration files.</p> <p>A pipeline run can only be deleted by the creator or an administrator.</p> <p>Warning</p> <p>As stated above, deleting a run through the website will also delete the full run directory, which includes the configuration files. Please manually back up the configuration file if you think you are likely to revisit that particular run configuration in the future.</p> <p>Admin Tip</p> <p>Administrators can refer to the <code>clearpiperun</code> command for details on how to reset a pipeline run via the command line.</p>"},{"location":"using/deleterun/#step-by-step-guide","title":"Step-by-step Guide","text":""},{"location":"using/deleterun/#1-navigate-to-the-run-detail-page","title":"1. Navigate to the Run Detail Page","text":"<p>Navigate to the detail page of the run you wish to delete.</p> <p></p>"},{"location":"using/deleterun/#2-click-on-the-delete-run-button","title":"2. Click on the Delete Run Button","text":"<p>Click on the <code>Delete Run</code> button at the top right of the page, to open the confirmation modal.</p> <p> </p>"},{"location":"using/deleterun/#3-confirm-deletion","title":"3. Confirm Deletion","text":"<p>To confirm the deletion click on the <code>Delete Run</code> button in the modal. Once pressed the website will direct back to the <code>Pipeline Runs</code> page and a confirmation message will appear in the top right.</p> <p>Note</p> <p>Runs with lots of database entries may take a short time to delete. Hence, the run may still appear in the pipeline runs list for a short time following the request with a status of <code>Deleting</code>.</p> <p></p> <p>Refreshing the page will show a deleting status if the process is still running:</p> <p></p>"},{"location":"using/genarrow/","title":"Generating Arrow Files","text":"<p>This page describes how to generate the measurement arrow files for a pipeline run if the option in the configuration file to create them was turned off.</p> <p>Arrow files only be generated by the creator or an administrator.</p> <p>Two files are produced by the method:</p> File Description <code>measurements.arrow</code> An Apache Arrow format file containing all the measurements associated with the pipeline run (see Arrow Files). Extra processing is performed in the creation of this file such that source ids are already in place for the measurements. <code>measurement_pairs.arrow</code> An Apache Arrow format file containing all the measurement pair metrics (see Arrow Files). <p>Arrow Files Available</p> <p>Users can see if arrow files are present for the run of interest by checking the respective run detail page. </p> <p>Admin Tip</p> <p>The arrow files can be generated using the command line using the command <code>createmaeasarrow</code>).</p>"},{"location":"using/genarrow/#why-create-arrow-files","title":"Why Create Arrow Files?","text":"<p>Large pipeline runs (hundreds of images) mean that to read the measurements, hundreds of parquet files need to be read in, and can contain millions of rows. This can be slow using libraries such as pandas, and also consumes a lot of system memory.</p> <p>Instead, if the measurements are saved in the Apache Arrow format, libraries such as <code>vaex</code> are able to open <code>.arrow</code> files in an out-of-core context so the memory footprint is hugely reduced along with the reading of the file being very fast. The two-epoch measurement pairs are also saved to arrow format due to the same reasons.</p> <p>See Reading with vaex for further details on using <code>vaex</code>.</p>"},{"location":"using/genarrow/#step-by-step-guide","title":"Step-by-step Guide","text":""},{"location":"using/genarrow/#1-navigate-to-the-run-detail-page","title":"1. Navigate to the Run Detail Page","text":"<p>Navigate to the detail page of the run you wish to generate arrow files for.</p> <p></p>"},{"location":"using/genarrow/#2-select-the-generate-arrow-files-option","title":"2. Select the Generate Arrow Files Option","text":"<p>Click the <code>Generate Arrow Files</code> option at the top-right of the page.</p> <p></p> <p>This will open the generate arrow files modal.</p> <p></p>"},{"location":"using/genarrow/#3-submit-generate-arrow-files-request","title":"3. Submit Generate Arrow Files Request","text":"<p>It is possible to overwrite existing arrow files by toggling the <code>Overwrite Current Files</code> option.</p> <p>When ready, click the <code>Generate Arrow Files</code> button on the modal to submit the generate request. A notification will show to indicate whether the submission was successful.</p> <p></p>"},{"location":"using/genarrow/#4-refresh-and-check-the-generate-arrow-files-log-file","title":"4. Refresh and Check the Generate Arrow Files Log File","text":"<p>It is possible to check the progress by looking at the Generate Arrow Files Log File which can be found on the run detail page. The log will not be refreshed automatically and instead the page needs to be manually refreshed.</p> <p>Once completed the arrow files will be available for use.</p> <p></p>"},{"location":"using/initrun/","title":"Initialising a Pipeline Run","text":"<p>This page outlines the steps required to create a pipeline run through the web interface. A description of the run configuration options can be found in the next section.</p> <p>Note</p> <p>Administrators please refer to this section in the admin documentation for instructions on how to initialise a pipeline run via the command line interface.</p> <p>Warning</p> <p>No data quality control is performed by the pipeline. Make sure your input data is clean and error free before processing using your preferred method.</p>"},{"location":"using/initrun/#step-by-step-guide","title":"Step-by-step Guide","text":""},{"location":"using/initrun/#1-navigate-to-the-pipeline-runs-overview-page","title":"1. Navigate to the Pipeline Runs Overview Page","text":"<p>Navigate to the Pipeline Runs overview page by clicking on the <code>Pipeline Runs</code> option in the left hand side navigation bar, as highlighted below.</p> <p></p>"},{"location":"using/initrun/#2-select-the-new-pipeline-run-option","title":"2. Select the New Pipeline Run Option","text":"<p>From the Pipeline Runs overview page, select the <code>New Pipeline Run</code> button as highlighted in the screenshot below. This will open up a modal window to begin the run initialisation process.</p> <p> </p>"},{"location":"using/initrun/#3-fill-in-the-run-details","title":"3. Fill in the Run Details","text":"<p>Fill in the name and description of the run and then press next to navigate to the next form to enter and select the configuration options.</p> <p>For full details on how to configure a pipeline run see the Run Configuration page, but a few notes here:</p> <ul> <li>Any settings entered here are not final, they can still be changed once the run is created.</li> <li>The order of the input files must match between the data types - i.e. the first selavy file, rms image and background image must all be the products of the first image, and so on.</li> <li>If you have a high number of images, selavy files, rms images and background images, it may be easier to leave these empty and instead use the text editor on the run detail page to directly enter the list to the configuration file.</li> <li>By default, non-admin users have a 200 image limit.</li> </ul> <p>Once you have finished filling in the configuration options, press the <code>create</code> button.</p> <p> </p>"},{"location":"using/initrun/#4-the-run-detail-page","title":"4. The Run Detail Page","text":"<p>After pressing create, the run will be initialised in the pipeline, which means the required configuration files have been created and the run is ready to be processed. You will be navigated to the detail page of the created run (shown below). On this page you can:</p> <ul> <li>View details of the run.</li> <li>View and edit the configuration file.</li> <li>Leave a comment about the run.</li> <li>View tables of associated images and measurements (once processed).</li> <li>Submit the run to be processed.</li> </ul> <p>For full details on:</p> <ul> <li>how to use the text editor to edit the configuration file see Run Configuration,</li> <li>how to submit the job to be processed see Processing a Run,</li> <li>and how to add images to a run that has already been processed Adding Images to a Run.</li> </ul> <p></p>"},{"location":"using/processrun/","title":"Processing a Run","text":"<p>This page describes how to submit a pipeline run for processing.</p> <p>Admin Tip</p> <p>Administrators please refer to this section in the admin documentation for instructions on how to process a pipeline run via the command line interface.</p> <p>Admin Warning</p> <p>The <code>Django Q</code> service must be running in order for pipeline runs to be processed. See the Deployment page for further details.</p> <p>Tip</p> <p>Use the editor window on the run detail page to make adjustments to the run configuration file before processing.</p>"},{"location":"using/processrun/#step-by-step-guide","title":"Step-by-step Guide","text":""},{"location":"using/processrun/#1-navigate-to-the-run-detail-page","title":"1. Navigate to the Run Detail Page","text":"<p>Navigate to the detail page of the run you wish to process.</p> <p></p>"},{"location":"using/processrun/#2-run-a-config-validation","title":"2. Run a config validation","text":"<p>Before processing it is recommended to check that the configuration file is valid. This is done by scrolling down to the config file card on the page and selecting the <code>Validate Config</code> option accessed by clicking the three dots menu button. Doing this will check if the configuration contains any errors prior to processing.</p> <p>Feedback will be provided on whether the configuration file is valid. In the event of an error, this can be corrected by using the edit option found in the same menu.</p> <p></p> <p></p>"},{"location":"using/processrun/#3-confirm-processing","title":"3. Confirm Processing","text":"<p>With a successful configuration validation, scroll back up to the top of the page and click the <code>Process Run</code> button. This will open a modal window for you to confirm processing.</p> <p>For a newly initialised run, the only option that requires attention is whether to toggle the <code>Debug Log Output</code> on. This can be helpful if processing a new set of images which the pipeline hasn't seen before.</p> <p>If you are processing a run that has errored in the initial processing then the <code>Full Re-Run</code> option should be toggled to <code>On</code>.</p> <p>Once ready, press the <code>Schedule Run</code> button which will send the run to the queue for processing.</p> <p>Warning</p> <p>For non-admin users, by default there is a run image limit of 200.</p> <p></p> <p></p> <p></p>"},{"location":"using/processrun/#monitoring-the-run","title":"Monitoring the Run","text":"<p>You can check the status of the run by refreshing the run detail page and seeing if the <code>Run Status</code> field has been updated. You can also check the log output by scrolling down to the log file card found below the configuration file.</p> <p>There is currently no automated notification on completion or errors.</p>"},{"location":"using/processrun/#full-re-run","title":"Full Re-Run","text":"<p>A full re-run will be required if the run configuration needs to be changed, or in the event that an initial run has errored.</p> <p>Warning</p> <p>The <code>Full Re-Run</code> option will remove all associated existing data for that run.</p> <p>Note</p> <p>If images have been added to a run and the processing errors, there is a one time undo option that may avoid having to use the <code>Full Re-Run</code> command.</p>"},{"location":"using/processrun/#adding-images-to-a-run","title":"Adding Images to a Run","text":"<p>See the dedicated documentation page here.</p>"},{"location":"using/requireddata/","title":"Required Data","text":"<p>This page gives an overview of what data is required to run the pipeline along with how to obtain the data.</p>"},{"location":"using/requireddata/#acquiring-askap-data","title":"Acquiring ASKAP Data","text":"<p>Note: VAST Data Releases</p> <p>If you are a member of the VAST collaboration you will have access to the VAST data release which contains the data for the VAST Pilot Survey. Refer to the VAST wiki for more details.</p> <p>Data produced by the ASKAP telescope can be accessed by using The CSIRO ASKAP Science Data Archive (CASDA). CASDA provides a web form for users to search for the data they are interested in and request for the data to be staged for download. Note that a form of account or registration is required to download image cube products.</p> <p>All data products from CASDA should be compatible without any modifications. Please report an issue if you find this to not be the case.</p> <p>Tip: CASDA Data Products</p> <p>Descriptions of the data products available on CASDA can be found on this page.</p>"},{"location":"using/requireddata/#pipeline-required-data","title":"Pipeline Required Data","text":"<p>Warning: Correcting Data</p> <p>Currently, the pipeline does not contain any processes to correct the data as it is ingested by the pipeline. For example, if corrections to the flux or positions need to be applied, these should be done to the data directly before they are processed by the pipeline.</p> <p>If an image has been previously ingested before corrections were applied, the filename of the corrected image must be changed. Doing so will make the pipeline see the corrected image as a new image and reingest the corrected data.</p> <p>A pipeline run requires a minimum of two ASKAP observational images to process. For each image, the following table provides a summary of the files that are required.</p> Data File Type Description Primary image .fits The primary, Stokes I, taylor 0, image of the observation. Component Catalogue .xml, .csv, .txt The component source catalogue produced by the source finder <code>selavy</code> from the primary image. Noise map of primary image .fits The noise (or rms) map produced by the source finder <code>selavy</code> from the primary image. Background map of primary image (optional) .fits The background (or mean) map produced by the source finder <code>selavy</code> from the primary image. The background image is only required when source monitoring is enabled. <p>Refer to the respective sections on this page for more details on these inputs.</p>"},{"location":"using/requireddata/#image-file","title":"Image File","text":"<p>This is the primary image file produced by the ASKAP pipeline. The pipeline requires the Stokes I, total intensity (taylor 0) image file. It is also recommended to use the convolved final image, where each of the 36 individual beams have been convolved to a common resolution prior to the creation of the combined mosaic of the field. These files are denoted by a <code>.conv</code> in the file name.</p> <p>Must be in the <code>FITS</code> file format.</p> <p>Example CASDA Filename</p> <pre><code>image.i.SB25597.cont.taylor.0.restored.conv.fits\n</code></pre>"},{"location":"using/requireddata/#component-catalogues","title":"Component Catalogues","text":"<p>Currently, the pipeline does not contain any source finding capabilities so the source catalogue must be provided. In particular, the pipeline requires the continuum component catalogue produced by the <code>selavy</code> source finder.</p> <p>The files can be one of three formats:</p> <ul> <li><code>XML</code> The VOTable XML format of the selavy components catalogue.     Provided by CASDA.      Must have a file extension of <code>.xml</code>.</li> <li><code>CSV</code> The csv format of the selavy components catalogue.      Provided by CASDA.      Must have a file extension of <code>.csv</code>.</li> <li><code>TXT</code> 'Fixed-width' formatted output produced by the selavy source finder directly.      These are not available through CASDA.      Must have a file extension of <code>.txt</code>.</li> </ul> <p>Example CASDA Filenames</p> xmlcsv <pre><code>selavy-image.i.SB25597.cont.taylor.0.restored.conv.components.xml\n</code></pre> <pre><code>AS107_Continuum_Component_Catalogue_25597_3792.csv\n</code></pre> <p>Note that CASDA does not provide the fixed width text format selavy output.</p> <p>File format examples</p> xmlcsvtxt <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;VOTABLE version=\"1.3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n xmlns=\"http://www.ivoa.net/xml/VOTable/v1.3\"\n xmlns:stc=\"http://www.ivoa.net/xml/STC/v1.30\" &gt;\n  &lt;COOSYS ID=\"J2000\" equinox=\"J2000\" system=\"eq_FK5\"/&gt;\n  &lt;RESOURCE name=\"Component catalogue from Selavy source finding\"&gt;\n    &lt;TABLE name=\"Component catalogue\"&gt;\n      &lt;DESCRIPTION&gt;&lt;/DESCRIPTION&gt;\n      &lt;PARAM name=\"table_version\" ucd=\"meta.version\" datatype=\"char\" arraysize=\"43\" value=\"casda.continuum_component_description_v1.9\"/&gt;\n      &lt;PARAM name=\"imageFile\" ucd=\"meta.file;meta.fits\" datatype=\"char\" arraysize=\"48\" value=\"image.i.SB25600.cont.taylor.0.restored.conv.fits\"/&gt;\n      &lt;PARAM name=\"flagSubsection\" ucd=\"meta.code\" datatype=\"boolean\" value=\"1\"/&gt;\n      &lt;PARAM name=\"subsection\" ucd=\"\" datatype=\"char\" arraysize=\"25\" value=\"[1:16449,1:14759,1:1,1:1]\"/&gt;\n      &lt;PARAM name=\"flagStatSec\" ucd=\"meta.code\" datatype=\"boolean\" value=\"0\"/&gt;\n      &lt;PARAM name=\"StatSec\" ucd=\"\" datatype=\"char\" arraysize=\"7\" value=\"[*,*,*]\"/&gt;\n      &lt;PARAM name=\"searchType\" ucd=\"meta.note\" datatype=\"char\" arraysize=\"7\" value=\"spatial\"/&gt;\n      &lt;PARAM name=\"flagNegative\" ucd=\"meta.code\" datatype=\"boolean\" value=\"0\"/&gt;\n      &lt;PARAM name=\"flagBaseline\" ucd=\"meta.code\" datatype=\"boolean\" value=\"0\"/&gt;\n      &lt;PARAM name=\"flagRobustStats\" ucd=\"meta.code\" datatype=\"boolean\" value=\"1\"/&gt;\n      &lt;PARAM name=\"flagFDR\" ucd=\"meta.code\" datatype=\"boolean\" value=\"0\"/&gt;\n      &lt;PARAM name=\"threshold\" ucd=\"phot.flux;stat.min\" datatype=\"float\" value=\"5\"/&gt;\n      &lt;PARAM name=\"flagGrowth\" ucd=\"meta.code\" datatype=\"boolean\" value=\"1\"/&gt;\n      &lt;PARAM name=\"growthThreshold\" ucd=\"phot.flux;stat.min\" datatype=\"float\" value=\"3\"/&gt;\n      &lt;PARAM name=\"minPix\" ucd=\"\" datatype=\"int\" value=\"3\"/&gt;\n      &lt;PARAM name=\"minChannels\" ucd=\"\" datatype=\"int\" value=\"0\"/&gt;\n      &lt;PARAM name=\"minVoxels\" ucd=\"\" datatype=\"int\" value=\"3\"/&gt;\n      &lt;PARAM name=\"flagAdjacent\" ucd=\"meta.code\" datatype=\"boolean\" value=\"1\"/&gt;\n      &lt;PARAM name=\"threshVelocity\" ucd=\"\" datatype=\"float\" value=\"7\"/&gt;\n      &lt;PARAM name=\"flagRejectBeforeMerge\" ucd=\"\" datatype=\"boolean\" value=\"0\"/&gt;\n      &lt;PARAM name=\"flagTwoStageMerging\" ucd=\"\" datatype=\"boolean\" value=\"1\"/&gt;\n      &lt;PARAM name=\"pixelCentre\" ucd=\"\" datatype=\"char\" arraysize=\"8\" value=\"centroid\"/&gt;\n      &lt;PARAM name=\"flagSmooth\" ucd=\"meta.code\" datatype=\"boolean\" value=\"0\"/&gt;\n      &lt;PARAM name=\"flagATrous\" ucd=\"meta.code\" datatype=\"boolean\" value=\"0\"/&gt;\n      &lt;PARAM name=\"Reference frequency\" ucd=\"em.freq;meta.main\" datatype=\"float\" unit=\"Hz\" value=\"1.36749e+09\"/&gt;\n      &lt;PARAM name=\"thresholdActual\" ucd=\"\" datatype=\"float\" unit=\"Jy/beam\" value=\"5\"/&gt;\n      &lt;FIELD name=\"island_id\" ID=\"col_island_id\" ucd=\"meta.id.parent\" datatype=\"char\" unit=\"--\" arraysize=\"20\"/&gt;\n      &lt;FIELD name=\"component_id\" ID=\"col_component_id\" ucd=\"meta.id;meta.main\" datatype=\"char\" unit=\"--\" arraysize=\"24\"/&gt;\n      &lt;FIELD name=\"component_name\" ID=\"col_component_name\" ucd=\"meta.id\" datatype=\"char\" unit=\"\" arraysize=\"26\"/&gt;\n      &lt;FIELD name=\"ra_hms_cont\" ID=\"col_ra_hms_cont\" ucd=\"pos.eq.ra\" ref=\"J2000\" datatype=\"char\" unit=\"\" arraysize=\"12\"/&gt;\n      &lt;FIELD name=\"dec_dms_cont\" ID=\"col_dec_dms_cont\" ucd=\"pos.eq.dec\" ref=\"J2000\" datatype=\"char\" unit=\"\" arraysize=\"13\"/&gt;\n      &lt;FIELD name=\"ra_deg_cont\" ID=\"col_ra_deg_cont\" ucd=\"pos.eq.ra;meta.main\" ref=\"J2000\" datatype=\"double\" unit=\"deg\" width=\"12\" precision=\"6\"/&gt;\n      &lt;FIELD name=\"dec_deg_cont\" ID=\"col_dec_deg_cont\" ucd=\"pos.eq.dec;meta.main\" ref=\"J2000\" datatype=\"double\" unit=\"deg\" width=\"13\" precision=\"6\"/&gt;\n      &lt;FIELD name=\"ra_err\" ID=\"col_ra_err\" ucd=\"stat.error;pos.eq.ra\" ref=\"J2000\" datatype=\"float\" unit=\"arcsec\" width=\"11\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"dec_err\" ID=\"col_dec_err\" ucd=\"stat.error;pos.eq.dec\" ref=\"J2000\" datatype=\"float\" unit=\"arcsec\" width=\"11\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"freq\" ID=\"col_freq\" ucd=\"em.freq\" datatype=\"float\" unit=\"MHz\" width=\"11\" precision=\"1\"/&gt;\n      &lt;FIELD name=\"flux_peak\" ID=\"col_flux_peak\" ucd=\"phot.flux.density;stat.max;em.radio;stat.fit\" datatype=\"float\" unit=\"mJy/beam\" width=\"11\" precision=\"3\"/&gt;\n      &lt;FIELD name=\"flux_peak_err\" ID=\"col_flux_peak_err\" ucd=\"stat.error;phot.flux.density;stat.max;em.radio;stat.fit\" datatype=\"float\" unit=\"mJy/beam\" width=\"14\" precision=\"3\"/&gt;\n      &lt;FIELD name=\"flux_int\" ID=\"col_flux_int\" ucd=\"phot.flux.density;em.radio;stat.fit\" datatype=\"float\" unit=\"mJy\" width=\"10\" precision=\"3\"/&gt;\n      &lt;FIELD name=\"flux_int_err\" ID=\"col_flux_int_err\" ucd=\"stat.error;phot.flux.density;em.radio;stat.fit\" datatype=\"float\" unit=\"mJy\" width=\"13\" precision=\"3\"/&gt;\n      &lt;FIELD name=\"maj_axis\" ID=\"col_maj_axis\" ucd=\"phys.angSize.smajAxis;em.radio;stat.fit\" datatype=\"float\" unit=\"arcsec\" width=\"9\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"min_axis\" ID=\"col_min_axis\" ucd=\"phys.angSize.sminAxis;em.radio;stat.fit\" datatype=\"float\" unit=\"arcsec\" width=\"9\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"pos_ang\" ID=\"col_pos_ang\" ucd=\"phys.angSize;pos.posAng;em.radio;stat.fit\" datatype=\"float\" unit=\"deg\" width=\"8\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"maj_axis_err\" ID=\"col_maj_axis_err\" ucd=\"stat.error;phys.angSize.smajAxis;em.radio\" datatype=\"float\" unit=\"arcsec\" width=\"13\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"min_axis_err\" ID=\"col_min_axis_err\" ucd=\"stat.error;phys.angSize.sminAxis;em.radio\" datatype=\"float\" unit=\"arcsec\" width=\"13\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"pos_ang_err\" ID=\"col_pos_ang_err\" ucd=\"stat.error;phys.angSize;pos.posAng;em.radio\" datatype=\"float\" unit=\"deg\" width=\"12\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"maj_axis_deconv\" ID=\"col_maj_axis_deconv\" ucd=\"phys.angSize.smajAxis;em.radio;askap:meta.deconvolved\" datatype=\"float\" unit=\"arcsec\" width=\"18\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"min_axis_deconv\" ID=\"col_min_axis_deconv\" ucd=\"phys.angSize.sminAxis;em.radio;askap:meta.deconvolved\" datatype=\"float\" unit=\"arcsec\" width=\"16\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"pos_ang_deconv\" ID=\"col_pos_ang_deconv\" ucd=\"phys.angSize;pos.posAng;em.radio;askap:meta.deconvolved\" datatype=\"float\" unit=\"deg\" width=\"15\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"maj_axis_deconv_err\" ID=\"col_maj_axis_deconv_err\" ucd=\"stat.error;phys.angSize.smajAxis;em.radio;askap:meta.deconvolved\" datatype=\"float\" unit=\"arcsec\" width=\"13\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"min_axis_deconv_err\" ID=\"col_min_axis_deconv_err\" ucd=\"stat.error;phys.angSize.sminAxis;em.radio;askap:meta.deconvolved\" datatype=\"float\" unit=\"arcsec\" width=\"13\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"pos_ang_deconv_err\" ID=\"col_pos_ang_deconv_err\" ucd=\"stat.error;phys.angSize;pos.posAng;em.radio;askap:meta.deconvolved\" datatype=\"float\" unit=\"deg\" width=\"12\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"chi_squared_fit\" ID=\"col_chi_squared_fit\" ucd=\"stat.fit.chi2\" datatype=\"float\" unit=\"--\" width=\"17\" precision=\"3\"/&gt;\n      &lt;FIELD name=\"rms_fit_gauss\" ID=\"col_rms_fit_gauss\" ucd=\"stat.stdev;stat.fit\" datatype=\"float\" unit=\"mJy/beam\" width=\"15\" precision=\"3\"/&gt;\n      &lt;FIELD name=\"spectral_index\" ID=\"col_spectral_index\" ucd=\"spect.index;em.radio\" datatype=\"float\" unit=\"--\" width=\"15\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"spectral_curvature\" ID=\"col_spectral_curvature\" ucd=\"askap:spect.curvature;em.radio\" datatype=\"float\" unit=\"--\" width=\"19\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"spectral_index_err\" ID=\"col_spectral_index_err\" ucd=\"stat.error;spect.index;em.radio\" datatype=\"float\" unit=\"--\" width=\"15\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"spectral_curvature_err\" ID=\"col_spectral_curvature_err\" ucd=\"stat.error;askap:spect.curvature;em.radio\" datatype=\"float\" unit=\"--\" width=\"19\" precision=\"2\"/&gt;\n      &lt;FIELD name=\"rms_image\" ID=\"col_rms_image\" ucd=\"stat.stdev;phot.flux.density\" datatype=\"float\" unit=\"mJy/beam\" width=\"12\" precision=\"3\"/&gt;\n      &lt;FIELD name=\"has_siblings\" ID=\"col_has_siblings\" ucd=\"meta.code\" datatype=\"int\" unit=\"\" width=\"8\"/&gt;\n      &lt;FIELD name=\"fit_is_estimate\" ID=\"col_fit_is_estimate\" ucd=\"meta.code\" datatype=\"int\" unit=\"\" width=\"8\"/&gt;\n      &lt;FIELD name=\"spectral_index_from_TT\" ID=\"col_spectral_index_from_TT\" ucd=\"meta.code\" datatype=\"int\" unit=\"\" width=\"8\"/&gt;\n      &lt;FIELD name=\"flag_c4\" ID=\"col_flag_c4\" ucd=\"meta.code\" datatype=\"int\" unit=\"\" width=\"8\"/&gt;\n      &lt;FIELD name=\"comment\" ID=\"col_comment\" ucd=\"meta.note\" datatype=\"char\" unit=\"\" arraysize=\"100\"/&gt;\n      &lt;DATA&gt;\n        &lt;TABLEDATA&gt;\n        &lt;TR&gt;\n          &lt;TD&gt;    SB25600_island_1&lt;/TD&gt;&lt;TD&gt;    SB25600_component_1a&lt;/TD&gt;&lt;TD&gt;            J053527-691611&lt;/TD&gt;&lt;TD&gt;  05:35:27.9&lt;/TD&gt;&lt;TD&gt;    -69:16:11&lt;/TD&gt;&lt;TD&gt;   83.866441&lt;/TD&gt;&lt;TD&gt;   -69.269927&lt;/TD&gt;&lt;TD&gt;       0.00&lt;/TD&gt;&lt;TD&gt;       0.00&lt;/TD&gt;&lt;TD&gt;     1367.5&lt;/TD&gt;&lt;TD&gt;    780.068&lt;/TD&gt;&lt;TD&gt;         0.721&lt;/TD&gt;&lt;TD&gt;   843.563&lt;/TD&gt;&lt;TD&gt;        1.692&lt;/TD&gt;&lt;TD&gt;    11.80&lt;/TD&gt;&lt;TD&gt;     8.40&lt;/TD&gt;&lt;TD&gt;  176.22&lt;/TD&gt;&lt;TD&gt;         0.01&lt;/TD&gt;&lt;TD&gt;         0.01&lt;/TD&gt;&lt;TD&gt;        0.11&lt;/TD&gt;&lt;TD&gt;              3.09&lt;/TD&gt;&lt;TD&gt;            1.82&lt;/TD&gt;&lt;TD&gt;          56.44&lt;/TD&gt;&lt;TD&gt;         0.78&lt;/TD&gt;&lt;TD&gt;         2.35&lt;/TD&gt;&lt;TD&gt;        1.49&lt;/TD&gt;&lt;TD&gt;          596.911&lt;/TD&gt;&lt;TD&gt;       1811.003&lt;/TD&gt;&lt;TD&gt;          -0.93&lt;/TD&gt;&lt;TD&gt;             -99.00&lt;/TD&gt;&lt;TD&gt;           0.00&lt;/TD&gt;&lt;TD&gt;               0.00&lt;/TD&gt;&lt;TD&gt;       1.038&lt;/TD&gt;&lt;TD&gt;       0&lt;/TD&gt;&lt;TD&gt;       0&lt;/TD&gt;&lt;TD&gt;       1&lt;/TD&gt;&lt;TD&gt;       0&lt;/TD&gt;&lt;TD&gt;                                                                                                    &lt;/TD&gt;\n        &lt;/TR&gt;\n        &lt;TR&gt;\n          &lt;TD&gt;    SB25600_island_2&lt;/TD&gt;&lt;TD&gt;    SB25600_component_2a&lt;/TD&gt;&lt;TD&gt;            J060004-703833&lt;/TD&gt;&lt;TD&gt;  06:00:04.9&lt;/TD&gt;&lt;TD&gt;    -70:38:33&lt;/TD&gt;&lt;TD&gt;   90.020608&lt;/TD&gt;&lt;TD&gt;   -70.642664&lt;/TD&gt;&lt;TD&gt;       0.01&lt;/TD&gt;&lt;TD&gt;       0.01&lt;/TD&gt;&lt;TD&gt;     1367.5&lt;/TD&gt;&lt;TD&gt;    438.885&lt;/TD&gt;&lt;TD&gt;         1.202&lt;/TD&gt;&lt;TD&gt;   494.222&lt;/TD&gt;&lt;TD&gt;        2.918&lt;/TD&gt;&lt;TD&gt;    11.65&lt;/TD&gt;&lt;TD&gt;     8.86&lt;/TD&gt;&lt;TD&gt;    9.51&lt;/TD&gt;&lt;TD&gt;         0.03&lt;/TD&gt;&lt;TD&gt;         0.04&lt;/TD&gt;&lt;TD&gt;        0.40&lt;/TD&gt;&lt;TD&gt;              5.22&lt;/TD&gt;&lt;TD&gt;            0.00&lt;/TD&gt;&lt;TD&gt;          58.20&lt;/TD&gt;&lt;TD&gt;         0.09&lt;/TD&gt;&lt;TD&gt;         0.00&lt;/TD&gt;&lt;TD&gt;        0.82&lt;/TD&gt;&lt;TD&gt;         5351.118&lt;/TD&gt;&lt;TD&gt;       4419.234&lt;/TD&gt;&lt;TD&gt;          -1.06&lt;/TD&gt;&lt;TD&gt;             -99.00&lt;/TD&gt;&lt;TD&gt;           0.00&lt;/TD&gt;&lt;TD&gt;               0.00&lt;/TD&gt;&lt;TD&gt;       0.715&lt;/TD&gt;&lt;TD&gt;       1&lt;/TD&gt;&lt;TD&gt;       0&lt;/TD&gt;&lt;TD&gt;       1&lt;/TD&gt;&lt;TD&gt;       1&lt;/TD&gt;&lt;TD&gt;                                                                                                    &lt;/TD&gt;\n        &lt;/TR&gt;\n        &lt;TR&gt;\n          &lt;TD&gt;    SB25600_island_2&lt;/TD&gt;&lt;TD&gt;    SB25600_component_2b&lt;/TD&gt;&lt;TD&gt;            J060006-703854&lt;/TD&gt;&lt;TD&gt;  06:00:06.1&lt;/TD&gt;&lt;TD&gt;    -70:38:54&lt;/TD&gt;&lt;TD&gt;   90.025359&lt;/TD&gt;&lt;TD&gt;   -70.648364&lt;/TD&gt;&lt;TD&gt;       0.19&lt;/TD&gt;&lt;TD&gt;       0.24&lt;/TD&gt;&lt;TD&gt;     1367.5&lt;/TD&gt;&lt;TD&gt;     24.719&lt;/TD&gt;&lt;TD&gt;         1.326&lt;/TD&gt;&lt;TD&gt;    24.048&lt;/TD&gt;&lt;TD&gt;        2.986&lt;/TD&gt;&lt;TD&gt;    10.66&lt;/TD&gt;&lt;TD&gt;     8.37&lt;/TD&gt;&lt;TD&gt;  167.45&lt;/TD&gt;&lt;TD&gt;         0.59&lt;/TD&gt;&lt;TD&gt;         0.80&lt;/TD&gt;&lt;TD&gt;        9.41&lt;/TD&gt;&lt;TD&gt;              3.00&lt;/TD&gt;&lt;TD&gt;            0.00&lt;/TD&gt;&lt;TD&gt;         -86.64&lt;/TD&gt;&lt;TD&gt;         5.43&lt;/TD&gt;&lt;TD&gt;         0.00&lt;/TD&gt;&lt;TD&gt;       14.18&lt;/TD&gt;&lt;TD&gt;         5351.118&lt;/TD&gt;&lt;TD&gt;       4419.234&lt;/TD&gt;&lt;TD&gt;         -99.00&lt;/TD&gt;&lt;TD&gt;             -99.00&lt;/TD&gt;&lt;TD&gt;           0.00&lt;/TD&gt;&lt;TD&gt;               0.00&lt;/TD&gt;&lt;TD&gt;       0.715&lt;/TD&gt;&lt;TD&gt;       1&lt;/TD&gt;&lt;TD&gt;       0&lt;/TD&gt;&lt;TD&gt;       1&lt;/TD&gt;&lt;TD&gt;       1&lt;/TD&gt;&lt;TD&gt;                                                                                                    &lt;/TD&gt;\n        &lt;/TR&gt;\n        &lt;TR&gt;\n          &lt;TD&gt;    SB25600_island_3&lt;/TD&gt;&lt;TD&gt;    SB25600_component_3a&lt;/TD&gt;&lt;TD&gt;            J061931-681533&lt;/TD&gt;&lt;TD&gt;  06:19:31.4&lt;/TD&gt;&lt;TD&gt;    -68:15:33&lt;/TD&gt;&lt;TD&gt;   94.880875&lt;/TD&gt;&lt;TD&gt;   -68.259404&lt;/TD&gt;&lt;TD&gt;       0.49&lt;/TD&gt;&lt;TD&gt;       0.65&lt;/TD&gt;&lt;TD&gt;     1367.5&lt;/TD&gt;&lt;TD&gt;    258.844&lt;/TD&gt;&lt;TD&gt;        72.505&lt;/TD&gt;&lt;TD&gt;   302.138&lt;/TD&gt;&lt;TD&gt;       84.967&lt;/TD&gt;&lt;TD&gt;    11.83&lt;/TD&gt;&lt;TD&gt;     9.04&lt;/TD&gt;&lt;TD&gt;    0.90&lt;/TD&gt;&lt;TD&gt;         0.21&lt;/TD&gt;&lt;TD&gt;         0.18&lt;/TD&gt;&lt;TD&gt;        3.84&lt;/TD&gt;&lt;TD&gt;              4.77&lt;/TD&gt;&lt;TD&gt;            1.40&lt;/TD&gt;&lt;TD&gt;          63.44&lt;/TD&gt;&lt;TD&gt;         1.39&lt;/TD&gt;&lt;TD&gt;        19.02&lt;/TD&gt;&lt;TD&gt;        9.15&lt;/TD&gt;&lt;TD&gt;         1284.683&lt;/TD&gt;&lt;TD&gt;       2467.498&lt;/TD&gt;&lt;TD&gt;          -0.74&lt;/TD&gt;&lt;TD&gt;             -99.00&lt;/TD&gt;&lt;TD&gt;           0.01&lt;/TD&gt;&lt;TD&gt;               0.00&lt;/TD&gt;&lt;TD&gt;       0.277&lt;/TD&gt;&lt;TD&gt;       1&lt;/TD&gt;&lt;TD&gt;       0&lt;/TD&gt;&lt;TD&gt;       1&lt;/TD&gt;&lt;TD&gt;       0&lt;/TD&gt;&lt;TD&gt;                                                                                                    &lt;/TD&gt;\n        &lt;/TR&gt;\n        &lt;TR&gt;\n          &lt;TD&gt;    SB25600_island_3&lt;/TD&gt;&lt;TD&gt;    SB25600_component_3b&lt;/TD&gt;&lt;TD&gt;            J061931-681531&lt;/TD&gt;&lt;TD&gt;  06:19:31.0&lt;/TD&gt;&lt;TD&gt;    -68:15:31&lt;/TD&gt;&lt;TD&gt;   94.879037&lt;/TD&gt;&lt;TD&gt;   -68.258688&lt;/TD&gt;&lt;TD&gt;       0.21&lt;/TD&gt;&lt;TD&gt;       0.57&lt;/TD&gt;&lt;TD&gt;     1367.5&lt;/TD&gt;&lt;TD&gt;    153.160&lt;/TD&gt;&lt;TD&gt;        84.478&lt;/TD&gt;&lt;TD&gt;   157.853&lt;/TD&gt;&lt;TD&gt;       87.200&lt;/TD&gt;&lt;TD&gt;    11.40&lt;/TD&gt;&lt;TD&gt;     8.28&lt;/TD&gt;&lt;TD&gt;    6.90&lt;/TD&gt;&lt;TD&gt;         0.20&lt;/TD&gt;&lt;TD&gt;         0.21&lt;/TD&gt;&lt;TD&gt;        1.40&lt;/TD&gt;&lt;TD&gt;              4.07&lt;/TD&gt;&lt;TD&gt;            0.00&lt;/TD&gt;&lt;TD&gt;          55.21&lt;/TD&gt;&lt;TD&gt;         1.06&lt;/TD&gt;&lt;TD&gt;         0.00&lt;/TD&gt;&lt;TD&gt;        5.16&lt;/TD&gt;&lt;TD&gt;         1284.683&lt;/TD&gt;&lt;TD&gt;       2467.498&lt;/TD&gt;&lt;TD&gt;          -1.17&lt;/TD&gt;&lt;TD&gt;             -99.00&lt;/TD&gt;&lt;TD&gt;           0.02&lt;/TD&gt;&lt;TD&gt;               0.00&lt;/TD&gt;&lt;TD&gt;       0.277&lt;/TD&gt;&lt;TD&gt;       1&lt;/TD&gt;&lt;TD&gt;       0&lt;/TD&gt;&lt;TD&gt;       1&lt;/TD&gt;&lt;TD&gt;       0&lt;/TD&gt;&lt;TD&gt;                                                                                                    &lt;/TD&gt;\n        &lt;/TR&gt;\n        &lt;/TABLEDATA&gt;\n      &lt;/DATA&gt;\n    &lt;/TABLE&gt;\n  &lt;/RESOURCE&gt;\n&lt;/VOTABLE&gt;\n</code></pre> <pre><code>id,catalogue_id,first_sbid,other_sbids,project_id,island_id,component_id,component_name,ra_hms_cont,dec_dms_cont,ra_deg_cont,dec_deg_cont,ra_err,dec_err,freq,flux_peak,flux_peak_err,flux_int,flux_int_err,maj_axis,min_axis,pos_ang,maj_axis_err,min_axis_err,pos_ang_err,maj_axis_deconv,min_axis_deconv,maj_axis_deconv_err,pos_ang_deconv,min_axis_deconv_err,pos_ang_deconv_err,chi_squared_fit,rms_fit_gauss,spectral_index,spectral_index_err,spectral_curvature,spectral_curvature_err,rms_image,has_siblings,fit_is_estimate,spectral_index_from_tt,flag_c4,comment,quality_level,released_date\n20793260,3790,25600,,18,SB25600_island_2768,SB25600_component_2768a,J055644-690942,05:56:44.5,-69:09:42,89.185558,-69.161889,0.04,0.05,1367.5,0.834,0.009,0.68,0.037,9.63,7.76,165.12,0.23,0.33,3.9,0.0,0.0,0.0,-89.09,0.0,3.48,0.034,52.957,-99.0,0.0,-99.0,0.0,0.166,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z\n20793259,3790,25600,,18,SB25600_island_2767,SB25600_component_2767a,J060047-692500,06:00:47.7,-69:25:00,90.198913,-69.416437,0.09,0.37,1367.5,0.889,0.016,2.642,0.2,29.12,9.35,2.08,2.0,0.99,1.35,26.73,4.84,0.03,2.91,6.03,1.5,1.869,234.431,-99.0,0.0,-99.0,0.0,0.175,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z\n20793258,3790,25600,,18,SB25600_island_2766,SB25600_component_2766a,J055849-690051,05:58:49.4,-69:00:51,89.705808,-69.014253,0.14,0.24,1367.5,0.885,0.025,1.181,0.191,13.04,9.38,6.04,1.3,1.45,8.83,6.87,3.72,3.2,36.6,14.03,34.31,0.561,193.467,-99.0,0.0,-99.0,0.0,0.177,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z\n20793257,3790,25600,,18,SB25600_island_2765,SB25600_component_2765a,J055339-683120,05:53:39.5,-68:31:20,88.414758,-68.522436,0.08,0.1,1367.5,0.906,0.014,0.895,0.063,12.78,7.08,10.0,0.43,0.43,2.33,6.37,0.0,0.54,28.11,0.0,4.65,0.138,99.302,-99.0,0.0,-99.0,0.0,0.173,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z\n</code></pre> <pre><code>#           island_id            component_id component_name ra_hms_cont dec_dms_cont ra_deg_cont dec_deg_cont   ra_err  dec_err  freq  flux_peak flux_peak_err flux_int flux_int_err maj_axis min_axis pos_ang maj_axis_err min_axis_err pos_ang_err maj_axis_deconv min_axis_deconv pos_ang_deconv maj_axis_deconv_err min_axis_deconv_err pos_ang_deconv_err chi_squared_fit rms_fit_gauss spectral_index spectral_curvature spectral_index_err spectral_curvature_err  rms_image has_siblings fit_is_estimate spectral_index_from_TT flag_c4                                                                                             comment\n#                  --                      --                                               [deg]        [deg] [arcsec] [arcsec] [MHz] [mJy/beam]    [mJy/beam]    [mJy]        [mJy] [arcsec] [arcsec]   [deg]     [arcsec]     [arcsec]       [deg]        [arcsec]        [arcsec]          [deg]            [arcsec]            [arcsec]              [deg]              --    [mJy/beam]             --                 --                 --                     -- [mJy/beam]                                                                                                                                                                \n  SB10342_island_1000 SB10342_component_1000a     B2337-0423  23:37:09.9    -04:23:13  354.291208    -4.387204     0.03     0.02  -0.0     15.907         0.067   19.273        0.112    17.29    13.32   86.15         0.08         0.01        0.66            7.68            2.86         -35.37                0.08                0.72               0.71          57.119       678.701          -0.79             -99.00               0.00                   0.00      0.304            0               0                      1       0                                                                                                    \n  SB10342_island_1001 SB10342_component_1001a     B0001-0346  00:01:57.6    -03:46:12    0.490036    -3.770075     0.19     0.15  -0.0     12.139         0.425   24.396        0.914    24.14    15.82   61.05         0.45         0.04        2.12           17.96           10.07          50.17                0.06                0.24               4.91         690.545      1639.191          -0.58             -99.00               0.00                   0.00      0.339            1               0                      1       0                                                                                                    \n  SB10342_island_1001 SB10342_component_1001b     B0001-0346  00:01:57.8    -03:46:04    0.491002    -3.767781     1.13     1.35  -0.0      4.044         0.387   10.072        1.053    50.40     9.39   42.19         3.33         0.04        0.73           48.09            0.00          40.49                0.08                0.00              31.48         690.545      1639.191          -0.36             -99.00               0.00                   0.00      0.339            1               0                      1       0                                                                                                    \n  SB10342_island_1002 SB10342_component_1002a     B2333-0141  23:33:29.4    -01:41:04  353.372617    -1.684465     0.13     0.17  -0.0     14.516         0.128   47.232        0.561    35.56    17.39  140.82         0.51         0.02        0.52           33.30            7.00         -35.89                0.02                0.39               3.49         926.099      1383.267          -0.53             -99.00               0.00                   0.00      0.459            1               0                      1       0                                                                                                    \n  SB10342_island_1002 SB10342_component_1002b     B2333-0141  23:33:30.8    -01:41:38  353.378227    -1.693919     0.20     0.26  -0.0      8.051         0.134   25.654        0.550    28.02    21.61  133.92         0.67         0.06        3.37           24.95           14.85         -35.94                0.05                0.13               4.34         926.099      1383.267          -0.21             -99.00               0.00                   0.00      0.459            1               0                      1       0                                                                                                    \n</code></pre>"},{"location":"using/requireddata/#noise-image-file","title":"Noise Image File","text":"<p>This is the noise map that is created from the primary image file during source finding by <code>selavy</code>.</p> <p>Must be in the <code>FITS</code> file format.</p> <p>Example CASDA Filename</p> <pre><code>noiseMap.image.i.SB25600.cont.taylor.0.restored.conv.fits\n</code></pre>"},{"location":"using/requireddata/#background-image-file","title":"Background Image File","text":"<p>This is the mean map that is created from the primary image file during source finding by <code>selavy</code>. The background image files are only required when source monitoring is enabled in the pipeline run.</p> <p>Must be in the <code>FITS</code> file format.</p> <p>Example CASDA Filename</p> <pre><code>meanMap.image.i.SB25600.cont.taylor.0.restored.conv.fits\n</code></pre>"},{"location":"using/requireddata/#data-location","title":"Data Location","text":"<p>Data should be placed in the directories denoted by <code>RAW_IMAGE_DIR</code> and <code>HOME_DATA_DIR</code> in the pipeline configuration. The <code>HOME_DATA_DIR</code> is designed to allow users to upload their own data to their home directory on the system where the pipeline is installed. By default, the <code>HOME_DATA_DIR</code> is set to scan the directory called <code>vast-pipeline-extra-data</code> in the users home area.</p> <p>Refer to the Pipeline Configuration section for more information.</p>"},{"location":"using/restorerun/","title":"Restoring a Run","text":"<p>This page details the process of restoring a pipeline run to the previous successful version.</p> <p>When images are added to a run, a backup is made of the run before proceeding which can be used to restore the run to the pre-addition version.  For example, perhaps the wrong images were added or an error occurred mid-addition that could not be resolved.</p> <p>A pipeline run can only be restored by the creator or an administrator.</p> <p>Admin Tip</p> <p>This process can also be launched via the command line using the <code>restorepiperun</code> command.  It is described in the admin section here.</p> <p>Warning: One time use</p> <p>This process can only be used to restore the run once.  I.e. it is not possible to restore the run to an even earlier version.</p>"},{"location":"using/restorerun/#step-by-step-guide","title":"Step-by-step Guide","text":"<p>In this example, the <code>docs_example_run</code> will be restored to the state before the images were added in the Adding Images to a Run example.</p>"},{"location":"using/restorerun/#1-navigate-to-the-run-detail-page","title":"1. Navigate to the Run Detail Page","text":"<p>Navigate to the detail page of the run you wish to restore.</p> <p></p>"},{"location":"using/restorerun/#2-select-the-restore-run-option","title":"2. Select the Restore Run Option","text":"<p>Click the <code>Restore Run</code> option at the top-right of the page.</p> <p></p> <p>This will open the restore confirmation modal.</p> <p></p>"},{"location":"using/restorerun/#3-check-the-restore-configuration","title":"3. Check the Restore Configuration","text":"<p>Shown in the modal is the configuration file of the previous successful run. This can be used to check that the images listed here are those that are expected.</p> <p>Debug level logging can also be turned on using the toggle button.</p> <p>When ready, click the <code>Restore Run</code> button on the modal to submit the restore request. A notification will show to indicate whether the submission was successful.</p> <p></p>"},{"location":"using/restorerun/#4-refresh-and-check-the-restore-log-file","title":"4. Refresh and Check the Restore Log File","text":"<p>While restoring the pipeline run will show a status of <code>Restoring</code>. It is possible to check the progress by looking at the Restore Log File which can be found on the run detail page. The log will not be refreshed automatically and instead the page needs to be manually refreshed.</p> <p>Upon a successful restoration the status will be changed back to <code>Completed</code>.</p> <p></p>"},{"location":"using/runconfig/","title":"Run Configuration","text":"<p>This page gives an overview of the configuration options available for a pipeline run.</p>"},{"location":"using/runconfig/#default-configuration-file","title":"Default Configuration File","text":"<p>Below is an example of a default <code>config.yaml</code> file. Note that no images or other input files have been provided. The file can be either edited directly or through the editor available on the run detail page.</p> <p>Warning</p> <p>Similarly to Python files, the indentation in the run configuration YAML file is important as it defines nested parameters.</p> <p>config.yaml</p> <pre><code># This file specifies the pipeline configuration for the current pipeline run.\n# You should review these settings before processing any images - some of the default\n# values will probably not be appropriate.\n\nrun:\n  # Path of the pipeline run\n  path: ... # auto-filled by pipeline initpiperun command\n\n  # Hide astropy warnings during the run execution.\n  suppress_astropy_warnings: True\n\ninputs:\n  # NOTE: all the inputs must match with each other, i.e. the catalogue for the first\n  # input image (inputs.image[0]) must be the first input catalogue (inputs.selavy[0])\n  # and so on.\n  image:\n  # list input images here, e.g. (note the leading hyphens)\n  # - /path/to/image1.fits\n  # - /path/to/image2.fits\n\n  selavy:\n  # list input selavy catalogues here, as above with the images\n\n  noise:\n  # list input noise (rms) images here, as above with the images\n\n  # Required only if source_monitoring.monitor is true, otherwise optional. If not providing\n  # background images, remove the entire background section below.\n  background:\n  # list input background images here, as above with the images\n\n\nsource_monitoring:\n  # Source monitoring can be done both forward and backward in 'time'.\n  # Monitoring backward means re-opening files that were previously processed and can be slow.\n  monitor: True\n\n  # Minimum SNR ratio a source has to be if it was placed in the area of minimum rms in\n  # the image from which it is to be extracted from. If lower than this value it is skipped\n  min_sigma: 3.0\n\n  # Multiplicative scaling factor to the buffer size of the forced photometry from the\n  # image edge\n  edge_buffer_scale: 1.2\n\n  # Passed to forced-phot as `cluster_threshold`. See docs for details. If unsure, leave\n  # as default.\n  cluster_threshold: 3.0\n\n  # Attempt forced-phot fit even if there are NaN's present in the rms or background maps.\n  allow_nan: False\n\nsource_association:\n  # basic, advanced, or deruiter\n  method: basic\n\n  # Maximum source separation allowed during basic and advanced association in arcsec\n  radius: 10.0\n\n  # Options that apply only to deruiter association\n  deruiter_radius: 5.68  # unitless\n  deruiter_beamwidth_limit: 1.5  # multiplicative factor\n\n  # Split input images into sky region groups and run the association on these groups in\n  # parallel. Best used when there are a large number of input images with multiple\n  # non-overlapping patches of the sky.\n  # Not recommended for smaller searches of &lt;= 3 sky regions.\n  parallel: False\n\n  # If images have been submitted in epoch dictionaries then an attempt will be made by\n  # the pipeline to remove duplicate sources. To do this a crossmatch is made between\n  # catalgoues to match 'the same' measurements from different catalogues. This\n  # parameter governs the distance for which a match is made in arcsec. Default is 2.5\n  # arcsec which is typically 1 pixel in ASKAP images.\n  epoch_duplicate_radius: 2.5  # arcsec\n\nnew_sources:\n  # Controls when a source is labelled as a new source. The source in question must meet\n  # the requirement of: min sigma &gt; (source_peak_flux / lowest_previous_image_min_rms)\n  min_sigma: 5.0\n\nmeasurements:\n  # Source finder used to produce input catalogues. Only selavy is currently supported.\n  source_finder: selavy\n\n  # Minimum error to apply to all flux measurements. The actual value used will either\n  # be the catalogued value or this value, whichever is greater. This is a fraction, e.g.\n  # 0.05 = 5% error, 0 = no minimum error.\n  flux_fractional_error: 0.0\n\n  # Replace the selavy errors with Condon (1997) errors.\n  condon_errors: True\n\n  # Sometimes the local rms for a source is reported as 0 by selavy.\n  # Choose a value to use for the local rms in these cases in mJy/beam.\n  selavy_local_rms_fill_value: 0.2\n\n  # Create 'measurements.arrow' and 'measurement_pairs.arrow' files at the end of \n  # a successful run.\n  write_arrow_files: False\n\n  # The positional uncertainty of a measurement is in reality the fitting errors and the\n  # astrometric uncertainty of the image/survey/instrument combined in quadrature.\n  # These two parameters are the astrometric uncertainty in RA/Dec and they may be different.\n  ra_uncertainty: 1.0 # arcsec\n  dec_uncertainty: 1.0  # arcsec\n\nvariability:\n  # For each source, calculate the measurement pair metrics (Vs and m) for each unique\n  # combination of measurements.\n  pair_metrics: True\n\n  # Only measurement pairs where the Vs metric exceeds this value are selected for the\n  # aggregate pair metrics that are stored in Source objects.\n  source_aggregate_pair_metrics_min_abs_vs: 4.3\n\nprocessing:\n  # Options to control use of Dask parallelism\n  # NOTE: These are advanced options and you should only change them if you know what you are doing.\n\n  # The total number of workers available to Dask ('null' means use one less than all cores)\n  num_workers: null\n\n  # The number of workers to use for disk IO operations (e.g. when reading images for forced extraction)\n  num_workers_io: 5\n\n  # The default maximum size (in MB) to allow per partition of Dask DataFrames\n  # Increasing this will create fewer partitions and will potentially increase the memory footprint\n  # of parallel tasks.\n  max_partition_mb: 15\n</code></pre> <p>Note</p> <p>Throughout the documentation we use dot-notation to refer to nested parameters, for example <code>inputs.image</code> refers to the list of input images. This page on YAML syntax from the Ansible documentation is a good brief primer on the basics.</p>"},{"location":"using/runconfig/#configuration-options","title":"Configuration Options","text":""},{"location":"using/runconfig/#general-run-options","title":"General Run Options","text":"<p><code>run.path</code> Path to the directory for the pipeline run. This parameter will be automatically filled if the configuration file is generate with the <code>initpiperun</code> management command or if the run was created with the web interface.</p> <p><code>run.suppress_astropy_warnings</code> Boolean. Astropy warnings are suppressed in the logging output if set to <code>True</code>. Defaults to <code>True</code>.</p>"},{"location":"using/runconfig/#input-images-and-selavy-files","title":"Input Images and Selavy Files","text":"<p>Warning: Entry Order</p> <p>The order of the the inputs must be consistent between the different input types. I.e. if <code>image1.fits</code> is the first listed image then <code>image1_selavy.txt</code> must be the first selavy input listed.</p> <p><code>inputs.image</code> Line entries or epoch headed entries.  The full paths to the image FITS files to be processed - these can be regular FITS files, or FITS files that use a <code>CompImageHDU</code>. In principle the pipeline also supports <code>.fits.fz</code> files, although this is not officially supported. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association.</p> <p>config.yaml</p> Normal modeEpoch mode <pre><code>inputs:\n  image:\n  - /full/path/to/image1.fits\n  - /full/path/to/image2.fits\n  - /full/path/to/image3.fits\n</code></pre> <p><pre><code>inputs:\n  image:\n    epoch01:\n    - /full/path/to/image1.fits\n    - /full/path/to/image2.fits\n    epoch02:\n    - /full/path/to/image3.fits\n</code></pre> Make sure that the epoch names are orderable in the correct order, for example, use: <pre><code>epoch01:\n...\nepoch09:\nepoch10:\n</code></pre> and not: <pre><code>epoch1:\n...\nepoch9:\nepoch10:\n</code></pre></p> <p><code>inputs.selavy</code> Line entries or epoch headed entries.  The full paths to the selavy text files to be processed.  Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association.</p> <p>config.yaml</p> Normal modeEpoch mode <pre><code>inputs:\n  selavy:\n  - /full/path/to/image1_selavy.txt\n  - /full/path/to/image2_selavy.txt\n  - /full/path/to/image3_selavy.txt\n</code></pre> <p><pre><code>inputs:\n  selavy:\n    epoch01:\n    - /full/path/to/image1_selavy.txt\n    - /full/path/to/image2_selavy.txt\n    epoch02:\n    - /full/path/to/image3_selavy.txt\n</code></pre> Make sure that the epoch names are orderable in the correct order, for example, use: <pre><code>epoch01:\n...\nepoch09:\nepoch10:\n</code></pre> and not: <pre><code>epoch1:\n...\nepoch9:\nepoch10:\n</code></pre></p> <p><code>inputs.noise</code> Line entries or epoch headed entries.  The full paths to the image noise (RMS) FITS files to be processed.  Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association.</p> <p>config.yaml</p> Normal modeEpoch mode <pre><code>inputs:\n  noise:\n  - /full/path/to/image1_rms.fits\n  - /full/path/to/image2_rms.fits\n  - /full/path/to/image3_rms.fits\n</code></pre> <p><pre><code>inputs:\n  noise:\n    epoch01:\n    - /full/path/to/image1_rms.fits\n    - /full/path/to/image2_rms.fits\n    epoch02:\n    - /full/path/to/image3_rms.fits\n</code></pre> Make sure that the epoch names are orderable in the correct order, for example, use: <pre><code>epoch01:\n...\nepoch09:\nepoch10:\n</code></pre> and not: <pre><code>epoch1:\n...\nepoch9:\nepoch10:\n</code></pre></p> <p><code>inputs.background</code> Line entries or epoch headed entries. The full paths to the image background (mean) FITS files to be processed.  Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. The background images are only required to be defined if <code>source_monitoring.monitor</code> is set to <code>True</code>.</p> <p>config.yaml</p> Normal modeEpoch mode <pre><code>inputs:\n  background:\n  - /full/path/to/image1_bkg.fits\n  - /full/path/to/image2_bkg.fits\n  - /full/path/to/image3_bkg.fits\n</code></pre> <p><pre><code>inputs:\n  background:\n    epoch01:\n    - /full/path/to/image1_bkg.fits\n    - /full/path/to/image2_bkg.fits\n    epoch02:\n    - /full/path/to/image3_bkg.fits\n</code></pre> Make sure that the epoch names are orderable in the correct order, for example, use: <pre><code>epoch01:\n...\nepoch09:\nepoch10:\n</code></pre> and not: <pre><code>epoch1:\n...\nepoch9:\nepoch10:\n</code></pre></p>"},{"location":"using/runconfig/#using-glob-expressions","title":"Using glob expressions","text":"<p>Instead of providing each input file explicitly, the inputs can be given as glob expressions which are resolved and sorted. Glob expressions must be provided as a mapping with the key <code>glob</code>. Both normal and epoch mode are supported.</p> <p>For example, the image input examples given above can be equivalently specified with the following glob expressions.</p> <p>config.yaml</p> Normal modeEpoch mode <pre><code>inputs:\n  image:\n    glob: /full/path/to/image*.fits\n</code></pre> <pre><code>inputs:\n  image:\n    epoch01:\n      glob: /full/path/to/image[12].fits\n    epoch02:\n    - /full/path/to/image3.fits\n</code></pre> <p>Multiple glob expressions can also be provided as a list, in which case they are resolved and sorted in the order they are given. For example:</p> <p>config.yaml</p> <pre><code>inputs:\n  image:\n    glob:\n    - /full/path/to/A/image*.fits\n    - /full/path/to/B/image*.fits\n</code></pre> <p>Note that it is not valid YAML to mix a sequence/list and a mapping/dictionary, meaning that for each input type (or epoch if using epoch mode), the files may be given either as glob expressions or explicit file paths. For example, the following is invalid:</p> <p>Invalid config.yaml</p> <pre><code>inputs:\n  image:\n    # Invalid! Thou shalt not mix sequences and mappings in YAML\n    - /full/path/to/A/image1.fits\n    glob: /full/path/to/B/image*.fits\n</code></pre> <p>However, an explicit file path is a valid glob expression, so adding explicit paths alongside glob expressions is still possible by simply including the path in a list of glob expressions. For example, the following is valid:</p> <p>config.yaml</p> <pre><code>inputs:\n  image:\n    glob:\n    - /full/path/to/A/image1.fits\n    - /full/path/to/B/image*.fits\n</code></pre> <p>In the above example, the final resolved image input list would contain the image <code>/full/path/to/A/image1.fits</code>, followed by all files matching <code>image*.fits</code> in <code>/full/path/to/B</code>.</p>"},{"location":"using/runconfig/#source-monitoring","title":"Source Monitoring","text":"<p><code>source_monitoring.monitor</code> Boolean. Turns on or off forced extractions for non detections. If set to <code>True</code> then <code>inputs.background</code> must also be defined. Defaults to <code>False</code>.</p> <p><code>source_monitoring.min_sigma</code> Float. For forced extractions to be performed they must meet a minimum signal-to-noise threshold with respect to the minimum rms value of the respective image. If the proposed forced measurement does not meet the threshold then it is not performed. I.e.</p> \\[ \\frac{f_{peak}}{\\text{rms}_{image, min}} \\geq \\small{\\text{source_monitoring.min_sigma}}\\text{,} \\] <p>where \\(f_{peak}\\) is the initial detection peak flux measurement of the source in question and \\(\\text{rms}_{image, min}\\) is the minimum RMS value of the image where the forced extraction is to take place. Defaults to <code>3.0</code>.</p> <p><code>source_monitoring.edge_buffer_scale</code> Float. Monitor forced extractions are not performed when the location is within 3 beamwidths of the image edge. This parameter scales this distance by the value set, which can help avoid errors when the 3 beamwidth limit is insufficient to avoid extraction failures. Defaults to 1.2.</p> <p><code>source_monitoring.cluster_threshold</code> Float. A argument directly passed to the forced photometry package used by the pipeline. It defines the multiple of <code>major_axes</code> to use for identifying clusters. Defaults to 3.0.</p> <p><code>source_monitoring.allow_nan</code> Boolean. A argument directly passed to the forced photometry package used by the pipeline. It defines whether <code>NaN</code> values are allowed to be present in the extraction area in the rms or background maps. <code>True</code> would mean that <code>NaN</code> values are allowed. Defaults to False.</p>"},{"location":"using/runconfig/#association","title":"Association","text":"<p>Tip</p> <p>Refer to the association documentation for full details on the association methods.</p> <p><code>source_association.method</code> String. Select whether to use the <code>basic</code>, <code>advanced</code> or <code>deruiter</code> association method, entered as a string of the method name. Defaults to <code>\"basic\"</code>.</p> <p><code>source_association.radius</code> Float. The distance limit to use during <code>basic</code> and <code>advanced</code> association. Unit is arcseconds. Defaults to <code>10.0</code>.</p> <p><code>source_association.deruiter_radius</code> Float. The de Ruiter radius limit to use during <code>deruiter</code> association only. The parameter is unitless. Defaults to <code>5.68</code>.</p> <p><code>source_association.deruiter_beamwidth_limit</code> Float. The beamwidth limit to use during <code>deruiter</code> association only. Multiplicative factor. Defaults to <code>1.5</code>.</p> <p><code>source_association.parallel</code> Boolean. When <code>True</code>, association is performed in parallel on non-overlapping groups of sky regions. Defaults to <code>False</code>.</p> <p><code>source_association.epoch_duplicate_radius</code> Float. Applies to epoch based association only. Defines the limit at which a duplicate source is identified. Unit is arcseconds. Defaults to <code>2.5</code> (commonly one pixel for ASKAP images).</p>"},{"location":"using/runconfig/#new-sources","title":"New Sources","text":"<p><code>new_sources.min_sigma</code> Float. Defines the limit at which a source is classed as a new source based upon the would-be significance of detections in previous images where no detection was made. i.e.</p> \\[ \\frac{f_{peak}}{\\text{rms}_{image, min}} \\geq \\small{\\text{new_sources.min_sigma}}\\text{,} \\] <p>where \\(f_{peak}\\) is the initial detection peak flux measurement of the source in question and \\(\\text{rms}_{image, min}\\) is the minimum RMS value of the previous image(s) where no detection was made. If the requirement is met in any previous image then the source is flagged as new. Defaults to <code>5.0</code>.</p>"},{"location":"using/runconfig/#measurements","title":"Measurements","text":"<p><code>measurements.source_finder</code> String. Signifies the format of the source finder text file read by the pipeline. Currently only supports <code>\"selavy\"</code>.</p> <p>Warning</p> <p>Source finding is not performed by the pipeline and must be completed prior to processing.</p> <p><code>measurements.flux_fractional_error</code> Define a fractional flux error that will be added in quadrature to the extracted sources. Note that this will be reflected in the final source statistics and will not be applied directly to the measurements. Entered as a float between 0 - 1.0 which represents 0 - 100%. Defaults to <code>0.0</code>.</p> <p><code>measurements.condon_errors</code> Boolean. Calculate the Condon errors of the extractions when read in from the source extraction file. If <code>False</code> then the errors directly from the source finder output are used. Recommended to set to <code>True</code> for selavy extractions. Defaults to <code>True</code>.</p> <p><code>measurements.selavy_local_rms_fill_value</code> Float. Value to substitute for the <code>local_rms</code> parameter in selavy extractions if a <code>0.0</code> value is found. Unit is mJy. Defaults to <code>0.2</code>.</p> <p><code>measurements.write_arrow_files</code> Boolean. When <code>True</code> then two <code>arrow</code> format files are produced:</p> <ul> <li><code>measurements.arrow</code> - an arrow file containing all the measurements associated with the run.</li> <li><code>measurement_pairs.arrow</code> -  an arrow file containing the measurement pairs information pre-merged with extra information from the measurements. Only output if <code>variability.pair_metrics</code> is also set to <code>True</code>.</li> </ul> <p>Producing these files for large runs (200+ images) is recommended for post-processing. Defaults to <code>False</code>.</p> <p>Note</p> <p>The arrow files can optionally be produced after the run has completed. See the Generating Arrow Files page.</p> <p><code>measurements.ra_uncertainty</code> Float. Defines an uncertainty error to the RA that will be added in quadrature to the existing source extraction error. Used to represent a systematic positional error. Unit is arcseconds. Defaults to 1.0.</p> <p><code>measurements.dec_uncertainty</code> Float. Defines an uncertainty error to the Dec that will be added in quadrature to the existing source extraction error. Used to represent systematic positional error. Unit is arcseconds. Defaults to 1.0.</p>"},{"location":"using/runconfig/#variability","title":"Variability","text":"<p><code>variability.pair_metrics</code> Boolean. When <code>True</code> then the two-epoch metrics are calculated for each source. It is recommended that users set this to <code>False</code> to skip calculating the pair metrics for runs that contain many input images per source. Defaults to <code>True</code>.</p> <p><code>variability.source_aggregate_pair_metrics_min_abs_vs</code> Float. Defines the minimum \\(V_s\\) two-epoch metric value threshold used to attach the most significant pair value to the source. Defaults to <code>4.3</code>.</p>"}]}