{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"VAST Pipeline \u00b6 This repository holds the code of the Radio Transient detection pipeline for the VAST project. Please read the Installation Instructions . If you have any questions or feedback, we welcome you to open an issue . If you are interested in contributing to the code, please read and follow the Contributing and Developing Guidelines . Features \u00b6 Code base in Python 3.7+ Source association/manipulations using Astropy4+ and Pandas1+ dataframe Association methods: basic ( Astropy crossmatch), advanced (search with a fixed distance), De Ruiter Flagging of \"New Source\" and \"Related Source\" Forced Extraction (Monitor) backward and forward in time Parallelization and scalability using Dask API ( Dask 2+ ) Data exploration in modern Django3+ Web App (Bootstrap 4) Accessing raw pipeline output data using .parquet and .arrow files Pipeline interface from command line (CLI) and via Web App Web App is served by Postgres12+ with Q3C plugin Screenshots and Previews \u00b6 Credits & Acknowledgements \u00b6 This tool was developed by Sergio Pintaldi from the Sydney Informatics Hub \u2013 a core research facility of The University of Sydney \u2013 together with Adam Stewart from the Sydney Institute for Astronomy , Andrew O'Brien and David Kaplan from the Department of Physics, University of Wisconsin-Milwaukee . Substantial contributions have been made by the ADACS team Shibli Saleheen, David Liptai, Ella Xi Wang. The developers thank the creators of SB Admin 2 to make the dashboard template freely available. If using this tool in your research, please acknowledge the Sydney Informatics Hub in publications. / /\\ ___ /__/\\ / /:/_ / /\\ \\ \\:\\ / /:/ /\\ / /:/ \\__\\:\\ / /:/ /::\\ /__/::\\ ___ / /::\\ /__/:/ /:/\\:\\ \\__\\/\\:\\__ /__/\\ /:/\\:\\ \\ \\:\\/:/~/:/ \\ \\:\\/\\ \\ \\:\\/:/__\\/ \\ \\::/ /:/ \\__\\::/ \\ \\::/ \\__\\/ /:/ /__/:/ \\ \\:\\ /__/:/ please \\__\\/ \\ \\:\\ \\__\\/ acknowledge your use\\__\\/","title":"Home"},{"location":"#vast-pipeline","text":"This repository holds the code of the Radio Transient detection pipeline for the VAST project. Please read the Installation Instructions . If you have any questions or feedback, we welcome you to open an issue . If you are interested in contributing to the code, please read and follow the Contributing and Developing Guidelines .","title":"VAST Pipeline"},{"location":"#features","text":"Code base in Python 3.7+ Source association/manipulations using Astropy4+ and Pandas1+ dataframe Association methods: basic ( Astropy crossmatch), advanced (search with a fixed distance), De Ruiter Flagging of \"New Source\" and \"Related Source\" Forced Extraction (Monitor) backward and forward in time Parallelization and scalability using Dask API ( Dask 2+ ) Data exploration in modern Django3+ Web App (Bootstrap 4) Accessing raw pipeline output data using .parquet and .arrow files Pipeline interface from command line (CLI) and via Web App Web App is served by Postgres12+ with Q3C plugin","title":"Features"},{"location":"#screenshots-and-previews","text":"","title":"Screenshots and Previews"},{"location":"#credits-acknowledgements","text":"This tool was developed by Sergio Pintaldi from the Sydney Informatics Hub \u2013 a core research facility of The University of Sydney \u2013 together with Adam Stewart from the Sydney Institute for Astronomy , Andrew O'Brien and David Kaplan from the Department of Physics, University of Wisconsin-Milwaukee . Substantial contributions have been made by the ADACS team Shibli Saleheen, David Liptai, Ella Xi Wang. The developers thank the creators of SB Admin 2 to make the dashboard template freely available. If using this tool in your research, please acknowledge the Sydney Informatics Hub in publications. / /\\ ___ /__/\\ / /:/_ / /\\ \\ \\:\\ / /:/ /\\ / /:/ \\__\\:\\ / /:/ /::\\ /__/::\\ ___ / /::\\ /__/:/ /:/\\:\\ \\__\\/\\:\\__ /__/\\ /:/\\:\\ \\ \\:\\/:/~/:/ \\ \\:\\/\\ \\ \\:\\/:/__\\/ \\ \\::/ /:/ \\__\\::/ \\ \\::/ \\__\\/ /:/ /__/:/ \\ \\:\\ /__/:/ please \\__\\/ \\ \\:\\ \\__\\/ acknowledge your use\\__\\/","title":"Credits &amp; Acknowledgements"},{"location":"changelog/","text":"Change Log \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , with an added List of PRs section and links to the relevant PRs on the individal updates. This project adheres to Semantic Versioning . Unreleased \u00b6 Added \u00b6 Added script to auto-generate code reference documentation pages #480 . Added code reference section to documentation #480 . Added new pages and sections to documentation #471 Added requirements/environment.yml so make it easier for Miniconda users to get the non-Python dependencies #472 . Added pyproject.toml and poetry.lock #472 . Added init-tools/init-db.py #472 . Added image add mode run restore command 'restorepiperun' #463 Added documentation folder and files for mkdocs and CI #433 Added add image to existing run feature #443 Added networkx to base reqiurements #460 . Added CI/CD workflow to run tests on pull requests #446 Added basic regression tests #425 Added image length validation for config #425 Changed \u00b6 Bumped Browsersync from 2.26.13 to 2.26.14 #481 . Dependabot: Bump prismjs from 1.22.0 to 1.23.0 #469 . Changed non-google format docstrings to google format #480 . Changed some documentation layout and updated content #471 . Changed the vaex dependency to vaex-arrow #472 . Set CREATE_MEASUREMENTS_ARROW_FILES = True in the basic association test config #472 . Bumped minimum Python version to 3.7.1 #472 . Replaced npm package gulp-sass with @mr-hope/gulp-sass , a fork which drops the dependency on the deprecated node-sass which is difficult to install #472 . Changed the installation documentation to instruct users to use a PostgreSQL Docker image with Q3C already installed #472 . Changed 'cmd' flag in run pipeline to 'cli' #466 . Changed CONTRIBUTING.md and README.md #433 Changed forced extraction name suffix to run id rather than datetime #443 Changed tests to run on smaller cutouts #443 Changed particles style on login page #459 . Dependabot: Bump ini from 1.3.5 to 1.3.8 #436 Fixed \u00b6 Fixed the default Dask multiprocessing context to \"fork\" #472 . Fixed Selavy catalogue ingest to discard the unit row before reading the data #473 . Fixed initial job processing from the UI #466 . Fixed links in README.md #464 . Fixed basic association new sources created through relations #443 Fixed tests running pipeline multiple times #443 Fixed particles canvas sizing on login page #459 . Fixed breadcrumb new line on small resolutitons #459 . Fixed config files in tests #430 Fixed sources table on measurement detail page #429 . Fixed missing meta columns in parallel association #427 . Removed \u00b6 Removed requirements/*.txt files. Development dependency management moved to Poetry #472 . Removed init-tools/init-db.sh #472 . Removed INSTALL.md , PROFILE.md and static/README.md #433 Removed aplpy from base requirements #460 . List of PRs \u00b6 #481 dep: Bump Browsersync from 2.26.13 to 2.26.14. #469 dep: Bump prismjs from 1.22.0 to 1.23.0. #480 feat: Code reference documentation update. #471 feat: Documentation update. #472 feat: Simplify install. #473 fix: discard the selavy unit row before reading. #466 fix: Fixed initial job processing from the UI. #463 feat: Added image add mode run restore command. #433 doc: add documentation GitHub pages website with CI. #443 feat, fix: Adds the ability to add images to an existing run. #460 dep: Removed aplpy from base requirements. #446 feat: CI/CD workflow. #459 fix: Fix particles and breadcrumb issues on mobile. #436 dep: Bump ini from 1.3.5 to 1.3.8. #430 fix: Test config files. #425 feat: Basic regression tests. #429 fix: Fixed sources table on measurement detail page. #427 fix: Fixed missing meta columns in parallel association. 0.2.0 (2020-11-30) \u00b6 Added \u00b6 Added a check in the UI running that the job is not already running or queued #421 . Added the deletion of all parquet and arrow files upon a re-run #421 . Added source selection by name or ID on source query page #401 . Added test cases #412 Added askap-vast/forced_phot to pip requirements #408 . Added pipeline configuration parameter, SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS , to filter measurement pairs before calculating aggregate metrics #407 . Added custom 404.html and 500.html templates for error pages #415 Added ability to export measurement_pairs.parqyet as an arrow file #393 . Added new fields to detail pages and source and measurement tables #406 . Added new fields to source query page (island flux ratio, min and max fluxes) #406 . Added min, max flux values to sources and agg min island flux ratio field #406 . Added island flux ratio column to measurements, component flux divided by total island flux (peak and int) #406 . Added a maximum number of images for runs through the UI #404 . Added the ability to run a pipeline run through the UI #404 . Added Queued status to the list of pipeline run statuses #404 . Added the dependancy django-q that enables scheduled tasks to be processed #404 . Added source tagging #396 . Added link to measurement table from the lightcurve to source detail page #387 . Added 'epoch based' parameter to pipeline run detail page #387 . Adds basic commenting functionality for sources, measurements, images, and runs #368 . Custom CSS now processed with Sass: Bootstrap and sb-admin-2 theme are compiled into a single stylesheet #370 . Added vast_pipeline/pipeline/generators.py which contains generator functions #382 . Range and NaN check on new source analysis to match forced extraction #374 . Added the ability for the pipeline to read in groups of images which are defined as a single epoch #277 . Added the ability of the pipeline to remove duplicated measurements from an epoch #277 . Added option to control separation measurements which are defined as a duplicate #277 . Added the ability of the pipeline to separate images to associate into unique sky region groups #277 . Added option to perform assocication of separate sky region groups in parallel #277 . Added new options to webinterface pipeline run creation #277 . Added epoch_based column run model #277 . Added links to tables and postage stamps on source detail page #379 . Updates image background_path from current run when not originally provided #377 . Added csv export button to datatables on webinterface #363 . Added support for Excel export button to datatables on webinterface (waiting on datatables buttons fix) #363 . Added column visibility button to datatables on webinterface #363 . Added dependancy datatables-buttons 1.6.4 #363 . Added dependancy jszip (required for Excel export) #363 . Adds n_selavy_measurements and n_forced_measurements to run model #362 . Adds steps to populate new measurement count fields in pipeline run #362 . Source order from the query is preserved on source detail view #364 . Setting HOME_DATA_DIR to specify a directory relative to the user's home directory to scan for FITS and text files to use in a Run initialised with the UI #361 . Adds a node graph to accompany the lightcurve that shows which measurement pairs exceed the default variability metric thresholds ( Vs >= 4.3 , |m| >= 0.26 ) #305 . Adds the MeasurementPair model to store two variability metrics for each flux type: Vs, the t-statistic; and m, the modulation index. The maximum of these metrics are also added to the Source model for joinless queries. These metrics are calculated during the pipeline run #305 . Adds radio buttons to change the lightcurve data points between peak and integrated fluxes #305 . Fills out information on all webinterface detail pages #345 . Adds frequency information the measurements and images webinterface tables. #345 . Adds celestial plot and tables to webinterface pipeline detail page #345 . Adds useful links to webinterface navbar #345 . Adds tool tips to webinterface source query #345 . Adds hash reading to webinterface source query to allow filling from URL hash parameters #345 . Add links to number cards on webinterface #345 . Added home icon on hover on webinterface #345 . Added copy-to-clipboard functionality on coordinates on webinterface #345 . Changed \u00b6 Renamed 'alert-wrapper' container to 'toast-wrapper' #419 . Changed alerts to use the Bootstrap toasts system #419 . Bumped some npm package versions to address dependabot security alerts #411 . Images table on pipeline run detail page changed to order by datetime by default #417 . Changed config argument CREATE_MEASUREMENTS_ARROW_FILE -> CREATE_MEASUREMENTS_ARROW_FILES #393 . Naming of average flux query fields to account for other min max flux fields #406 . Expanded README.md to include DjangoQ and UI job scheduling information #404 . Shifted alerts location to the top right #404 . Log file card now expanded by default on pipeline run detail page #404 . Changed user comments on source detail pages to incorporate tagging feature #396 . Updated RACS HiPS URL in Aladin #399 . Changed home page changelog space to welcome/help messages #387 . The comment field in the Run model has been renamed to description . A comment many-to-many relationship was added to permit user comments on Run instances #368 . Moved sb-admin-2 Bootstrap theme static assets to NPM package dependency #370 . Refactored bulk uploading to use iterable generator objects #382 . Updated validation of config file to check that all options are present and valid #373 . Rewritten relation functions to improve speed #307 . Minor changes to association to increase speed #307 . Changes to decrease memory usage during the calculation of the ideal coverage dataframe #307 . Updated the get_src_skyregion_merged_df logic to account for epochs #277 . Updated the job creation modal layout #277 . Bumped datatables-buttons to 1.6.5 and enabled excel export buttton #380 . Bumped datatables to 1.10.22 #363 . Changed dom layout on datatables #363 . Changed external results table pagination buttons on source detail webinterface page pagination to include less numbers to avoid overlap #363 . Changes measurement counts view on website to use new model parameters #362 . Lightcurve plot now generated using Bokeh #305 . Multiple changes to webinterface page layouts #345 . Changes source names to the format ASKAP_hhmmss.ss(+/-)ddmmss.ss #345 . Simplified webinterface navbar #345 . Excludes sources and pipeline runs from being listed in the source query page that are not complete on the webinterface #345 . Clarifies number of measurements on webinterface detail pages #345 . Changed N.A. labels to N/A on the webinterface #345 . Fixed \u00b6 Fixed pipeline run DB loading in command line runpipeline command #401 . Fixed nodejs version #412 Fixed npm start failure #412 All queries using the 2-epoch metric Vs now operate on abs(Vs) . The original Vs stored in MeasurementPair objects is still signed #407 . Changed aggregate 2-epoch metric calculation for Source objects to ensure they come from the same pair #407 . Fixed new sources rms measurement returns when no measurements are valid #417 . Fixed measuring rms values from selavy created NAXIS=3 FITS images #417 . Fixed rms value calculation in non-cluster forced extractions #402 . Increase request limit for gunicorn #398 . Fixed max source Vs metric to being an absolute value #391 . Fixed misalignment of lightcurve card header text and the flux type radio buttons #386 . Fixes incorrently named GitHub social-auth settings variable that prevented users from logging in with GitHub #372 . Fixes webinterface navbar overspill at small sizes #345 . Fixes webinterface favourite source table #345 . Removed \u00b6 Removed/Disabled obsolete test cases #412 Removed vast_pipeline/pipeline/forced_phot.py #408 . Removed 'selavy' from homepage measurements count label #391 . Removed leftover pipeline/plots.py file #391 . Removed static/css/pipeline.css , this file is now produced by compiling the Sass ( scss/**/*.scss ) files with Gulp #370 . Removed any storage of meas_dj_obj or src_dj_obj in the pipeline #382 . Removed static/vendor/chart-js package #305 . Removed static/css/collapse-box.css , content moved to pipeline.css #345 . List of PRs \u00b6 #421 feat: Delete output files on re-run & UI run check. #401 feat: Added source selection by name or id to query page. #412 feat: added some unit tests. #419 feat: Update alerts to use toasts. #408 feat: use forced_phot dependency instead of copied code. #407 fix, model: modified 2-epoch metric calculation. #411 fix: updated npm deps to fix security vulnerabilities. #415 feat: Added custom 404 and 500 templates. #393 feat: Added measurement_pairs arrow export. #406 feat, model: Added island flux ratio columns. #402 fix: Fixed rms value calculation in non-cluster forced extractions. #404 feat, dep, model: Completed schedule pipe run. #396 feat: added source tagging. #398 fix: gunicorn request limit #399 fix: Updated RACS HiPS path. #391 fix: Vs metric fix and removed pipeline/plots.py. #387 feat: Minor website updates. #386 fix: fix lightcurve header floats. #368 feat: vast-candidates merger: Add user commenting #370 feat: moved sb-admin-2 assets to dependencies. #382 feat: Refactored bulk uploading of objects. #374 feat, fix: Bring new source checks inline with forced extraction. #373 fix: Check all options are valid and present in validate_cfg. #307 feat: Improve relation functions and general association speed ups. #277 feat,model: Parallel and epoch based association. #380 feat, dep: Enable Excel export button. #379 feat: Add links to source detail template. #377 fix: Update image bkg path when not originally provided. #363 feat, dep: Add export and column visibility buttons to tables. #362 feat, model: Added number of measurements to Run DB model. #364 feat: preserve source query order on detail view. #361 feat, fix: restrict home dir scan to specified directory. #372 fix: fix social auth scope setting name. #305 feat: 2 epoch metrics #345 feat, fix: Website improvements. 0.1.0 (2020-09-27) \u00b6 First release of the Vast Pipeline. This was able to process 707 images (EPOCH01 to EPOCH11x) on a machine with 64 GB of RAM. List of PRs \u00b6 #347 feat: Towards first release #354 fix, model: Updated Band model fields to floats #346 fix: fix JS9 overflow in measurement detail view #349 dep: Bump lodash from 4.17.15 to 4.17.20 #348 dep: Bump django from 3.0.5 to 3.0.7 in /requirements #344 fix: fixed aladin init for all pages #340 break: rename pipeline folder to vast_pipeline #342 fix: Hotfix - fixed parquet path on job detail view #336 feat: Simbad/NED async cone search #284 fix: Update Aladin surveys with RACS and VAST #333 feat: auth to GitHub org, add logging and docstring #325 fix, feat: fix forced extraction using Dask bags backend #334 doc: better migration management explanation #332 fix: added clean to build task, removed commented lines #322 fix, model: add unique to image name, remove timestamp from image folder #321 feat: added css and js sourcemaps #314 feat: query form redesign, sesame resolver, coord validator #318 feat: Suppress astropy warnings #317 fix: Forced photometry fixes for #298 and #312 #316 fix: fix migration file 0001_initial.py #310 fix: Fix run detail number of measurements display #309 fix: Added JS9 overlay filters and changed JS9 overlay behaviour on sources and measurements #303 fix: Fix write config feedback and validation #306 feat: Add config validation checks #302 fix: Fix RA correction for d3 celestial #300 fix: increase line limit for gunicorn server #299 fix: fix admin \"view site\" redirect #294 fix: Make lightcurves start at zero #268 feat: Production set up with static files and command #291 fix: Bug fix for forced_photom cluster allow_nan #289 fix: Fix broken UI run creation #287 fix: Fix forced measurement parquet files write #286 fix: compile JS9 without helper option #285 fix: Fix removing forced parquet and clear images from piperun","title":"Changelog"},{"location":"changelog/#change-log","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , with an added List of PRs section and links to the relevant PRs on the individal updates. This project adheres to Semantic Versioning .","title":"Change Log"},{"location":"changelog/#unreleased","text":"","title":"Unreleased"},{"location":"changelog/#added","text":"Added script to auto-generate code reference documentation pages #480 . Added code reference section to documentation #480 . Added new pages and sections to documentation #471 Added requirements/environment.yml so make it easier for Miniconda users to get the non-Python dependencies #472 . Added pyproject.toml and poetry.lock #472 . Added init-tools/init-db.py #472 . Added image add mode run restore command 'restorepiperun' #463 Added documentation folder and files for mkdocs and CI #433 Added add image to existing run feature #443 Added networkx to base reqiurements #460 . Added CI/CD workflow to run tests on pull requests #446 Added basic regression tests #425 Added image length validation for config #425","title":"Added"},{"location":"changelog/#changed","text":"Bumped Browsersync from 2.26.13 to 2.26.14 #481 . Dependabot: Bump prismjs from 1.22.0 to 1.23.0 #469 . Changed non-google format docstrings to google format #480 . Changed some documentation layout and updated content #471 . Changed the vaex dependency to vaex-arrow #472 . Set CREATE_MEASUREMENTS_ARROW_FILES = True in the basic association test config #472 . Bumped minimum Python version to 3.7.1 #472 . Replaced npm package gulp-sass with @mr-hope/gulp-sass , a fork which drops the dependency on the deprecated node-sass which is difficult to install #472 . Changed the installation documentation to instruct users to use a PostgreSQL Docker image with Q3C already installed #472 . Changed 'cmd' flag in run pipeline to 'cli' #466 . Changed CONTRIBUTING.md and README.md #433 Changed forced extraction name suffix to run id rather than datetime #443 Changed tests to run on smaller cutouts #443 Changed particles style on login page #459 . Dependabot: Bump ini from 1.3.5 to 1.3.8 #436","title":"Changed"},{"location":"changelog/#fixed","text":"Fixed the default Dask multiprocessing context to \"fork\" #472 . Fixed Selavy catalogue ingest to discard the unit row before reading the data #473 . Fixed initial job processing from the UI #466 . Fixed links in README.md #464 . Fixed basic association new sources created through relations #443 Fixed tests running pipeline multiple times #443 Fixed particles canvas sizing on login page #459 . Fixed breadcrumb new line on small resolutitons #459 . Fixed config files in tests #430 Fixed sources table on measurement detail page #429 . Fixed missing meta columns in parallel association #427 .","title":"Fixed"},{"location":"changelog/#removed","text":"Removed requirements/*.txt files. Development dependency management moved to Poetry #472 . Removed init-tools/init-db.sh #472 . Removed INSTALL.md , PROFILE.md and static/README.md #433 Removed aplpy from base requirements #460 .","title":"Removed"},{"location":"changelog/#list-of-prs","text":"#481 dep: Bump Browsersync from 2.26.13 to 2.26.14. #469 dep: Bump prismjs from 1.22.0 to 1.23.0. #480 feat: Code reference documentation update. #471 feat: Documentation update. #472 feat: Simplify install. #473 fix: discard the selavy unit row before reading. #466 fix: Fixed initial job processing from the UI. #463 feat: Added image add mode run restore command. #433 doc: add documentation GitHub pages website with CI. #443 feat, fix: Adds the ability to add images to an existing run. #460 dep: Removed aplpy from base requirements. #446 feat: CI/CD workflow. #459 fix: Fix particles and breadcrumb issues on mobile. #436 dep: Bump ini from 1.3.5 to 1.3.8. #430 fix: Test config files. #425 feat: Basic regression tests. #429 fix: Fixed sources table on measurement detail page. #427 fix: Fixed missing meta columns in parallel association.","title":"List of PRs"},{"location":"changelog/#020-2020-11-30","text":"","title":"0.2.0 (2020-11-30)"},{"location":"changelog/#added_1","text":"Added a check in the UI running that the job is not already running or queued #421 . Added the deletion of all parquet and arrow files upon a re-run #421 . Added source selection by name or ID on source query page #401 . Added test cases #412 Added askap-vast/forced_phot to pip requirements #408 . Added pipeline configuration parameter, SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS , to filter measurement pairs before calculating aggregate metrics #407 . Added custom 404.html and 500.html templates for error pages #415 Added ability to export measurement_pairs.parqyet as an arrow file #393 . Added new fields to detail pages and source and measurement tables #406 . Added new fields to source query page (island flux ratio, min and max fluxes) #406 . Added min, max flux values to sources and agg min island flux ratio field #406 . Added island flux ratio column to measurements, component flux divided by total island flux (peak and int) #406 . Added a maximum number of images for runs through the UI #404 . Added the ability to run a pipeline run through the UI #404 . Added Queued status to the list of pipeline run statuses #404 . Added the dependancy django-q that enables scheduled tasks to be processed #404 . Added source tagging #396 . Added link to measurement table from the lightcurve to source detail page #387 . Added 'epoch based' parameter to pipeline run detail page #387 . Adds basic commenting functionality for sources, measurements, images, and runs #368 . Custom CSS now processed with Sass: Bootstrap and sb-admin-2 theme are compiled into a single stylesheet #370 . Added vast_pipeline/pipeline/generators.py which contains generator functions #382 . Range and NaN check on new source analysis to match forced extraction #374 . Added the ability for the pipeline to read in groups of images which are defined as a single epoch #277 . Added the ability of the pipeline to remove duplicated measurements from an epoch #277 . Added option to control separation measurements which are defined as a duplicate #277 . Added the ability of the pipeline to separate images to associate into unique sky region groups #277 . Added option to perform assocication of separate sky region groups in parallel #277 . Added new options to webinterface pipeline run creation #277 . Added epoch_based column run model #277 . Added links to tables and postage stamps on source detail page #379 . Updates image background_path from current run when not originally provided #377 . Added csv export button to datatables on webinterface #363 . Added support for Excel export button to datatables on webinterface (waiting on datatables buttons fix) #363 . Added column visibility button to datatables on webinterface #363 . Added dependancy datatables-buttons 1.6.4 #363 . Added dependancy jszip (required for Excel export) #363 . Adds n_selavy_measurements and n_forced_measurements to run model #362 . Adds steps to populate new measurement count fields in pipeline run #362 . Source order from the query is preserved on source detail view #364 . Setting HOME_DATA_DIR to specify a directory relative to the user's home directory to scan for FITS and text files to use in a Run initialised with the UI #361 . Adds a node graph to accompany the lightcurve that shows which measurement pairs exceed the default variability metric thresholds ( Vs >= 4.3 , |m| >= 0.26 ) #305 . Adds the MeasurementPair model to store two variability metrics for each flux type: Vs, the t-statistic; and m, the modulation index. The maximum of these metrics are also added to the Source model for joinless queries. These metrics are calculated during the pipeline run #305 . Adds radio buttons to change the lightcurve data points between peak and integrated fluxes #305 . Fills out information on all webinterface detail pages #345 . Adds frequency information the measurements and images webinterface tables. #345 . Adds celestial plot and tables to webinterface pipeline detail page #345 . Adds useful links to webinterface navbar #345 . Adds tool tips to webinterface source query #345 . Adds hash reading to webinterface source query to allow filling from URL hash parameters #345 . Add links to number cards on webinterface #345 . Added home icon on hover on webinterface #345 . Added copy-to-clipboard functionality on coordinates on webinterface #345 .","title":"Added"},{"location":"changelog/#changed_1","text":"Renamed 'alert-wrapper' container to 'toast-wrapper' #419 . Changed alerts to use the Bootstrap toasts system #419 . Bumped some npm package versions to address dependabot security alerts #411 . Images table on pipeline run detail page changed to order by datetime by default #417 . Changed config argument CREATE_MEASUREMENTS_ARROW_FILE -> CREATE_MEASUREMENTS_ARROW_FILES #393 . Naming of average flux query fields to account for other min max flux fields #406 . Expanded README.md to include DjangoQ and UI job scheduling information #404 . Shifted alerts location to the top right #404 . Log file card now expanded by default on pipeline run detail page #404 . Changed user comments on source detail pages to incorporate tagging feature #396 . Updated RACS HiPS URL in Aladin #399 . Changed home page changelog space to welcome/help messages #387 . The comment field in the Run model has been renamed to description . A comment many-to-many relationship was added to permit user comments on Run instances #368 . Moved sb-admin-2 Bootstrap theme static assets to NPM package dependency #370 . Refactored bulk uploading to use iterable generator objects #382 . Updated validation of config file to check that all options are present and valid #373 . Rewritten relation functions to improve speed #307 . Minor changes to association to increase speed #307 . Changes to decrease memory usage during the calculation of the ideal coverage dataframe #307 . Updated the get_src_skyregion_merged_df logic to account for epochs #277 . Updated the job creation modal layout #277 . Bumped datatables-buttons to 1.6.5 and enabled excel export buttton #380 . Bumped datatables to 1.10.22 #363 . Changed dom layout on datatables #363 . Changed external results table pagination buttons on source detail webinterface page pagination to include less numbers to avoid overlap #363 . Changes measurement counts view on website to use new model parameters #362 . Lightcurve plot now generated using Bokeh #305 . Multiple changes to webinterface page layouts #345 . Changes source names to the format ASKAP_hhmmss.ss(+/-)ddmmss.ss #345 . Simplified webinterface navbar #345 . Excludes sources and pipeline runs from being listed in the source query page that are not complete on the webinterface #345 . Clarifies number of measurements on webinterface detail pages #345 . Changed N.A. labels to N/A on the webinterface #345 .","title":"Changed"},{"location":"changelog/#fixed_1","text":"Fixed pipeline run DB loading in command line runpipeline command #401 . Fixed nodejs version #412 Fixed npm start failure #412 All queries using the 2-epoch metric Vs now operate on abs(Vs) . The original Vs stored in MeasurementPair objects is still signed #407 . Changed aggregate 2-epoch metric calculation for Source objects to ensure they come from the same pair #407 . Fixed new sources rms measurement returns when no measurements are valid #417 . Fixed measuring rms values from selavy created NAXIS=3 FITS images #417 . Fixed rms value calculation in non-cluster forced extractions #402 . Increase request limit for gunicorn #398 . Fixed max source Vs metric to being an absolute value #391 . Fixed misalignment of lightcurve card header text and the flux type radio buttons #386 . Fixes incorrently named GitHub social-auth settings variable that prevented users from logging in with GitHub #372 . Fixes webinterface navbar overspill at small sizes #345 . Fixes webinterface favourite source table #345 .","title":"Fixed"},{"location":"changelog/#removed_1","text":"Removed/Disabled obsolete test cases #412 Removed vast_pipeline/pipeline/forced_phot.py #408 . Removed 'selavy' from homepage measurements count label #391 . Removed leftover pipeline/plots.py file #391 . Removed static/css/pipeline.css , this file is now produced by compiling the Sass ( scss/**/*.scss ) files with Gulp #370 . Removed any storage of meas_dj_obj or src_dj_obj in the pipeline #382 . Removed static/vendor/chart-js package #305 . Removed static/css/collapse-box.css , content moved to pipeline.css #345 .","title":"Removed"},{"location":"changelog/#list-of-prs_1","text":"#421 feat: Delete output files on re-run & UI run check. #401 feat: Added source selection by name or id to query page. #412 feat: added some unit tests. #419 feat: Update alerts to use toasts. #408 feat: use forced_phot dependency instead of copied code. #407 fix, model: modified 2-epoch metric calculation. #411 fix: updated npm deps to fix security vulnerabilities. #415 feat: Added custom 404 and 500 templates. #393 feat: Added measurement_pairs arrow export. #406 feat, model: Added island flux ratio columns. #402 fix: Fixed rms value calculation in non-cluster forced extractions. #404 feat, dep, model: Completed schedule pipe run. #396 feat: added source tagging. #398 fix: gunicorn request limit #399 fix: Updated RACS HiPS path. #391 fix: Vs metric fix and removed pipeline/plots.py. #387 feat: Minor website updates. #386 fix: fix lightcurve header floats. #368 feat: vast-candidates merger: Add user commenting #370 feat: moved sb-admin-2 assets to dependencies. #382 feat: Refactored bulk uploading of objects. #374 feat, fix: Bring new source checks inline with forced extraction. #373 fix: Check all options are valid and present in validate_cfg. #307 feat: Improve relation functions and general association speed ups. #277 feat,model: Parallel and epoch based association. #380 feat, dep: Enable Excel export button. #379 feat: Add links to source detail template. #377 fix: Update image bkg path when not originally provided. #363 feat, dep: Add export and column visibility buttons to tables. #362 feat, model: Added number of measurements to Run DB model. #364 feat: preserve source query order on detail view. #361 feat, fix: restrict home dir scan to specified directory. #372 fix: fix social auth scope setting name. #305 feat: 2 epoch metrics #345 feat, fix: Website improvements.","title":"List of PRs"},{"location":"changelog/#010-2020-09-27","text":"First release of the Vast Pipeline. This was able to process 707 images (EPOCH01 to EPOCH11x) on a machine with 64 GB of RAM.","title":"0.1.0 (2020-09-27)"},{"location":"changelog/#list-of-prs_2","text":"#347 feat: Towards first release #354 fix, model: Updated Band model fields to floats #346 fix: fix JS9 overflow in measurement detail view #349 dep: Bump lodash from 4.17.15 to 4.17.20 #348 dep: Bump django from 3.0.5 to 3.0.7 in /requirements #344 fix: fixed aladin init for all pages #340 break: rename pipeline folder to vast_pipeline #342 fix: Hotfix - fixed parquet path on job detail view #336 feat: Simbad/NED async cone search #284 fix: Update Aladin surveys with RACS and VAST #333 feat: auth to GitHub org, add logging and docstring #325 fix, feat: fix forced extraction using Dask bags backend #334 doc: better migration management explanation #332 fix: added clean to build task, removed commented lines #322 fix, model: add unique to image name, remove timestamp from image folder #321 feat: added css and js sourcemaps #314 feat: query form redesign, sesame resolver, coord validator #318 feat: Suppress astropy warnings #317 fix: Forced photometry fixes for #298 and #312 #316 fix: fix migration file 0001_initial.py #310 fix: Fix run detail number of measurements display #309 fix: Added JS9 overlay filters and changed JS9 overlay behaviour on sources and measurements #303 fix: Fix write config feedback and validation #306 feat: Add config validation checks #302 fix: Fix RA correction for d3 celestial #300 fix: increase line limit for gunicorn server #299 fix: fix admin \"view site\" redirect #294 fix: Make lightcurves start at zero #268 feat: Production set up with static files and command #291 fix: Bug fix for forced_photom cluster allow_nan #289 fix: Fix broken UI run creation #287 fix: Fix forced measurement parquet files write #286 fix: compile JS9 without helper option #285 fix: Fix removing forced parquet and clear images from piperun","title":"List of PRs"},{"location":"code_of_conduct/","text":"Code Of Conduct \u00b6 By joining the VAST collaboration you agree to adhere to the Code of Conduct below. We are committed to making this collaboration productive and enjoyable for everyone, regardless of gender, sexual orientation, disability, physical appearance, body size, race, nationality or religion. We will not tolerate harassment of colleagues and students in any form. To achieve this, VAST members must endeavour to work together in a cooperative way on scientific projects that fall within the scope of VAST. In particular all members must: Exercise their best professional and ethical judgement and carry out their duties and functions with integrity and objectivity; Act fairly and reasonably, and treat colleagues and students with respect, impartiality, courtesy and sensitivity; Avoid conflicts of interest; Adhere to the VAST membership and publication policies. Please follow these guidelines in all your interactions (including online) within VAST: Behave professionally . Harassment and sexist, racist, or exclusionary comments or jokes are not appropriate. Harassment includes sustained disruption of talks or other events, inappropriate physical contact, sexual attention or innuendo, deliberate intimidation, stalking, and photography or recording of an individual without consent. It also includes offensive comments related to gender, sexual orientation, disability, physical appearance, body size, race or religion. All communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery is not appropriate. Be kind to others . Do not insult or put down other collaboration members. Participants asked to stop any inappropriate behaviour are expected to comply immediately. Members violating these rules may be asked to leave the collaboration at the discretion of the PIs. Any participant who wishes to report a violation of this policy is asked to speak, in confidence, to Tara Murphy (tara.murphy@sydney.edu.au) or David Kaplan (kaplan@uwm.edu). Extra Guidelines \u00b6 We are a community based on openness, as well as friendly and didactic discussions. We aspire to treat everybody equally, and value their contributions. Decisions are made based on technical merit and consensus. Code is not the only way to help the project. Reviewing pull requests, answering questions to help others on mailing lists or issues, organizing and teaching tutorials, working on the website, improving the documentation, are all priceless contributions. We abide by the principles of openness, respect, and consideration of others of the Python Software Foundation: https://www.python.org/psf/codeofconduct/ Acknowledgements \u00b6 We ask that all VAST publications (papers, ATELs, etc) include the line: This work was done as part of the ASKAP Variables and Slow Transients (VAST) collaboration (Murphy et al. 2013, PASA, 30, 6). Separately, all refereed publications should carry the standard CSIRO acknowledgement : The Australian SKA Pathfinder is part of the Australia Telescope National Facility which is managed by CSIRO. Operation of ASKAP is funded by the Australian Government with support from the National Collaborative Research Infrastructure Strategy. ASKAP uses the resources of the Pawsey Supercomputing Centre. Establishment of ASKAP, the Murchison Radio-astronomy Observatory and the Pawsey Supercomputing Centre are initiatives of the Australian Government, with support from the Government of Western Australia and the Science and Industry Endowment Fund. We acknowledge the Wajarri Yamatji people as the traditional owners of the Observatory site. This project is supported by the University of Sydney, the Australian Research Council, and CSIRO.","title":"Code of Conduct"},{"location":"code_of_conduct/#code-of-conduct","text":"By joining the VAST collaboration you agree to adhere to the Code of Conduct below. We are committed to making this collaboration productive and enjoyable for everyone, regardless of gender, sexual orientation, disability, physical appearance, body size, race, nationality or religion. We will not tolerate harassment of colleagues and students in any form. To achieve this, VAST members must endeavour to work together in a cooperative way on scientific projects that fall within the scope of VAST. In particular all members must: Exercise their best professional and ethical judgement and carry out their duties and functions with integrity and objectivity; Act fairly and reasonably, and treat colleagues and students with respect, impartiality, courtesy and sensitivity; Avoid conflicts of interest; Adhere to the VAST membership and publication policies. Please follow these guidelines in all your interactions (including online) within VAST: Behave professionally . Harassment and sexist, racist, or exclusionary comments or jokes are not appropriate. Harassment includes sustained disruption of talks or other events, inappropriate physical contact, sexual attention or innuendo, deliberate intimidation, stalking, and photography or recording of an individual without consent. It also includes offensive comments related to gender, sexual orientation, disability, physical appearance, body size, race or religion. All communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery is not appropriate. Be kind to others . Do not insult or put down other collaboration members. Participants asked to stop any inappropriate behaviour are expected to comply immediately. Members violating these rules may be asked to leave the collaboration at the discretion of the PIs. Any participant who wishes to report a violation of this policy is asked to speak, in confidence, to Tara Murphy (tara.murphy@sydney.edu.au) or David Kaplan (kaplan@uwm.edu).","title":"Code Of Conduct"},{"location":"code_of_conduct/#extra-guidelines","text":"We are a community based on openness, as well as friendly and didactic discussions. We aspire to treat everybody equally, and value their contributions. Decisions are made based on technical merit and consensus. Code is not the only way to help the project. Reviewing pull requests, answering questions to help others on mailing lists or issues, organizing and teaching tutorials, working on the website, improving the documentation, are all priceless contributions. We abide by the principles of openness, respect, and consideration of others of the Python Software Foundation: https://www.python.org/psf/codeofconduct/","title":"Extra Guidelines"},{"location":"code_of_conduct/#acknowledgements","text":"We ask that all VAST publications (papers, ATELs, etc) include the line: This work was done as part of the ASKAP Variables and Slow Transients (VAST) collaboration (Murphy et al. 2013, PASA, 30, 6). Separately, all refereed publications should carry the standard CSIRO acknowledgement : The Australian SKA Pathfinder is part of the Australia Telescope National Facility which is managed by CSIRO. Operation of ASKAP is funded by the Australian Government with support from the National Collaborative Research Infrastructure Strategy. ASKAP uses the resources of the Pawsey Supercomputing Centre. Establishment of ASKAP, the Murchison Radio-astronomy Observatory and the Pawsey Supercomputing Centre are initiatives of the Australian Government, with support from the Government of Western Australia and the Science and Industry Endowment Fund. We acknowledge the Wajarri Yamatji people as the traditional owners of the Observatory site. This project is supported by the University of Sydney, the Australian Research Council, and CSIRO.","title":"Acknowledgements"},{"location":"faqs/","text":"Frequently Asked Questions \u00b6 Can the VAST Pipeline be used with images from other telescopes? \u00b6 The base answer to this question is that the pipeline has been designed specifically for ASKAPsoft and ASKAPpipeline products, so compatibility with data from other telescopes is not supported. However, it's important to remember that the pipeline performs no source extraction itself, instead it reads in source catalogues that is expected to be in the format of the output of the Selavy source extractor. As seen from the Image Ingest page , the pipeline does not use any special or out of the ordinary FITS headers when reading the images, and the only inputs required are the images, catalogues, noise images and background images - which are standard products. Hence, the real answer to this question is yes, if one of the following is performed: Run the Selavy source extractor on the images to process. Convert the component output from a different source extractor to match that of the Selavy component file . The pipeline was also designed in a way such that other source extractor 'translators' could be plugged into the pipeline. So a further option is to develop new translators such that the pipeline can read in output from other source extractors. The translators can be found in vast_pipeline/surveys/translators.py . Please open a discussion or issue on GitHub if you intend to give this a go! Bug In reading the code recently I have a suspicion the FITS reading code is reliant on the TELESCOP FITS header being equal to ASKAP . This is unintentional as there is nothing special about the FITS headers being read. Worth to check if anyone goes down this path. - Adam, March 2021.","title":"FAQs"},{"location":"faqs/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faqs/#can-the-vast-pipeline-be-used-with-images-from-other-telescopes","text":"The base answer to this question is that the pipeline has been designed specifically for ASKAPsoft and ASKAPpipeline products, so compatibility with data from other telescopes is not supported. However, it's important to remember that the pipeline performs no source extraction itself, instead it reads in source catalogues that is expected to be in the format of the output of the Selavy source extractor. As seen from the Image Ingest page , the pipeline does not use any special or out of the ordinary FITS headers when reading the images, and the only inputs required are the images, catalogues, noise images and background images - which are standard products. Hence, the real answer to this question is yes, if one of the following is performed: Run the Selavy source extractor on the images to process. Convert the component output from a different source extractor to match that of the Selavy component file . The pipeline was also designed in a way such that other source extractor 'translators' could be plugged into the pipeline. So a further option is to develop new translators such that the pipeline can read in output from other source extractors. The translators can be found in vast_pipeline/surveys/translators.py . Please open a discussion or issue on GitHub if you intend to give this a go! Bug In reading the code recently I have a suspicion the FITS reading code is reliant on the TELESCOP FITS header being equal to ASKAP . This is unintentional as there is nothing special about the FITS headers being read. Worth to check if anyone goes down this path. - Adam, March 2021.","title":"Can the VAST Pipeline be used with images from other telescopes?"},{"location":"license/","text":"MIT License Copyright (c) 2020-2025 ASKAP VAST Organisation, The University of Sydney (Sydney Informatics Hub), Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"adminusage/app/","text":"Web App \u00b6 This section describes how to run the pipeline Django web app/server. Starting the Pipeline Web App \u00b6 Make sure you installed and compiled correctly the frontend assets see guide Start the Django development web server: ( pipeline_env ) $ ./manage.py runserver Test the webserver by pointing your browser at http://127.0.0.1:8000 or http://localhost:8000 . The webserver is independent of runpipeline and you can use the website while the pipeline commands are running. Running a pipeline run via the web server \u00b6 It is possible to launch the processing of a pipeline run by using the relevant option on the pipeline run detail page. This uses DjangoQ to schedule and process the runs and a cluster needs to be set up in order for the runs to process: Check the Q_CLUSTER options in ./webinterface/settings.py . Refer to the DjangoQ docs if you are unsure on the meaning of any parameters. Launch the cluster using the following command, making sure you are in the pipeline environment: ( pipeline_env ) $ ./manage.py qcluster If the pipeline is updated then the qcluster also needs to be be restarted. A warning that if you submit jobs before the cluster is set up, or is taken down, then these jobs will begin immediately once the cluster is back online.","title":"Web App"},{"location":"adminusage/app/#web-app","text":"This section describes how to run the pipeline Django web app/server.","title":"Web App"},{"location":"adminusage/app/#starting-the-pipeline-web-app","text":"Make sure you installed and compiled correctly the frontend assets see guide Start the Django development web server: ( pipeline_env ) $ ./manage.py runserver Test the webserver by pointing your browser at http://127.0.0.1:8000 or http://localhost:8000 . The webserver is independent of runpipeline and you can use the website while the pipeline commands are running.","title":"Starting the Pipeline Web App"},{"location":"adminusage/app/#running-a-pipeline-run-via-the-web-server","text":"It is possible to launch the processing of a pipeline run by using the relevant option on the pipeline run detail page. This uses DjangoQ to schedule and process the runs and a cluster needs to be set up in order for the runs to process: Check the Q_CLUSTER options in ./webinterface/settings.py . Refer to the DjangoQ docs if you are unsure on the meaning of any parameters. Launch the cluster using the following command, making sure you are in the pipeline environment: ( pipeline_env ) $ ./manage.py qcluster If the pipeline is updated then the qcluster also needs to be be restarted. A warning that if you submit jobs before the cluster is set up, or is taken down, then these jobs will begin immediately once the cluster is back online.","title":"Running a pipeline run via the web server"},{"location":"adminusage/cli/","text":"Command Line Interface (CLI) \u00b6 This section describes the commands available to the administrators of the pipelines. Pipeline Usage \u00b6 All the pipeline commands are run using the Django global ./manage.py <command> interface. Therefore you need to activate the Python environment. You can have a look at the available commands for the pipeline app: ( pipeline_env ) $ ./manage.py help Output: ... [ vast_pipeline ] clearpiperun createmeasarrow debugrun importsurvey initpiperun restorepiperun runpipeline ... There are 7 commands, described in detail below. clearpiperun \u00b6 Detailed commands for resetting the database can be found in Contributing and Developing Guidelines . Resetting a pipeline run can be done using the clearpiperun command. This will delete all images and related objects such as sources associated with that pipeline run. Images that have been used in other pipeline runs will not be deleted. ( pipeline_env ) $ ./manage.py clearpiperun path/to/my_pipe_run # or ( pipeline_env ) $ ./manage.py clearpiperun my_pipe_run More details on the clearpiperun command can be found in the Contributing and Developing Guidelines . createmeasarrow \u00b6 This command allows for the creation of the measurements.arrow and measurement_pairs.arrow files after a run has been successfully completed. See Arrow Files for more information. ./manage.py createmeasarrow --help usage: manage.py createmeasarrow [-h] [--overwrite] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperun Create `measurements.arrow` and `measurement_pairs.arrow` files for a completed pipeline run. positional arguments: piperun Path or name of the pipeline run. optional arguments: -h, --help show this help message and exit --overwrite Overwrite previous 'measurements.arrow' file. --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the co Example usage: ./manage.py createmeasarrow docs_example_run 2021-03-30 10:48:40,952 createmeasarrow INFO Creating measurements arrow file for 'docs_example_run'. 2021-03-30 10:48:40,952 utils INFO Creating measurements.arrow for run docs_example_run. 2021-03-30 10:48:41,829 createmeasarrow INFO Creating measurement pairs arrow file for 'docs_example_run'. 2021-03-30 10:48:41,829 utils INFO Creating measurement_pairs.arrow for run docs_example_run. debugrun \u00b6 The debugrun command is used to print out a summary of the pipeline run to the terminal. A single pipeline run can be entered as an argument or all can be entered to print the statistics of all the pipeline runs in the database. ./manage.py debugrun --help usage: manage.py debugrun [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperuns [piperuns ...] Print out total metrics such as nr of measurements for runs positional arguments: piperuns Name or path of pipeline run(s) to debug.Pass \"all\" to print summary data of all the runs. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. Example usage: ./manage.py debugrun docs_example_run * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Printing summary data of pipeline run \"docs_example_run\" Nr of images: 14 Nr of measurements: 4312 Nr of forced measurements: 2156 Nr of sources: 557 Nr of association: 3276 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * importsurvey \u00b6 This functionality is not yet available. initpiperun \u00b6 In order to process the images in the pipeline, you must create/initialise a pipeline run first. The pipeline run creation is done using the initpiperun django command, which requires a pipeline run folder. The command creates a folder with the pipeline run name under the settings PROJECT_WORKING_DIR defined in settings . ( pipeline_env ) $ ./manage.py initpiperun --help Output: usage: manage.py initpiperun [ -h ] [ --version ] [ -v { 0 ,1,2,3 }] [ --settings SETTINGS ] [ --pythonpath PYTHONPATH ] [ --traceback ] [ --no-color ] [ --force-color ] runname Create the pipeline run folder structure to run a pipeline instance positional arguments: runname Name of the pipeline run. optional arguments: -h, --help show this help message and exit --version show program 's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn' t provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\" . --traceback Raise on CommandError exceptions --no-color Don ' t colorize the command output. --force-color Force colorization of the command output. The command yields the following folder structure: ( pipeline_env ) $ ./manage.py initpiperun my_pipe_run Output: 2020 -02-27 23 :04:33,344 initpiperun INFO creating pipeline run folder 2020 -02-27 23 :04:33,344 initpiperun INFO copying default config in pipeline run folder 2020 -02-27 23 :04:33,344 initpiperun INFO pipeline run initialisation successful! Please modify the \"config.py\" restorepiperun \u00b6 Details on the add images feature can be found here . It allows for a pipeline run that has had an image added to the run to be restored to the state it was in before the image addition was made. By default the command will ask for confirmation that the run is to be restored (the option --no-confirm skips this). ./manage.py restorepiperun --help usage: manage.py restorepiperun [ -h ] [ --no-confirm ] [ --version ] [ -v { 0 ,1,2,3 }] [ --settings SETTINGS ] [ --pythonpath PYTHONPATH ] [ --traceback ] [ --no-color ] [ --force-color ] [ --skip-checks ] piperuns [ piperuns ... ] Restore a pipeline run to the previous person after image add mode has been used. positional arguments: piperuns Name or path of pipeline run ( s ) to restore. optional arguments: -h, --help show this help message and exit --no-confirm Flag to skip the confirmation stage and proceed to restore the pipeline run. --version show program 's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn' t provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\" . --traceback Raise on CommandError exceptions --no-color Don ' t colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. ( pipeline_env ) $ ./manage.py restorepiperun path/to/my_pipe_run # or ( pipeline_env ) $ ./manage.py restorepiperun my_pipe_run Example usage: ./manage.py restorepiperun docs_example_run 2021 -01-31 12 :45:31,039 restorepiperun INFO Will restore the run to the following config: PIPE_RUN_PATH...................................../Users/adam/GitHub/vast-pipeline/pipeline-runs/docs_example_run IMAGE_FILES....................................... [ '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout.fits' ] SELAVY_FILES...................................... [ '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout.components.txt' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout.components.txt' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout.components.txt' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout.components.txt' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout.components.txt' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout.components.txt' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout.components.txt' ] BACKGROUND_FILES.................................. [ '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout_bkg.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout_bkg.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout_bkg.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout_bkg.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout_bkg.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout_bkg.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout_bkg.fits' ] NOISE_FILES....................................... [ '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout_rms.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout_rms.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout_rms.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout_rms.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout_rms.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout_rms.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout_rms.fits' ] SOURCE_FINDER.....................................selavy MONITOR...........................................True MONITOR_MIN_SIGMA.................................3.0 MONITOR_EDGE_BUFFER_SCALE.........................1.2 MONITOR_CLUSTER_THRESHOLD.........................3.0 MONITOR_ALLOW_NAN.................................False ASTROMETRIC_UNCERTAINTY_RA........................1 ASTROMETRIC_UNCERTAINTY_DEC.......................1 ASSOCIATION_PARALLEL..............................True ASSOCIATION_EPOCH_DUPLICATE_RADIUS................2.5 ASSOCIATION_METHOD................................basic ASSOCIATION_RADIUS................................15.0 ASSOCIATION_DE_RUITER_RADIUS......................5.68 ASSOCIATION_BEAMWIDTH_LIMIT.......................1.5 NEW_SOURCE_MIN_SIGMA..............................5.0 DEFAULT_SURVEY....................................None FLUX_PERC_ERROR...................................0 USE_CONDON_ERRORS.................................True SELAVY_LOCAL_RMS_ZERO_FILL_VALUE..................0.2 CREATE_MEASUREMENTS_ARROW_FILES...................False SUPPRESS_ASTROPY_WARNINGS.........................True SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS..........4.3 Would you like to restore the run ? ( y/n ) : y 2021 -01-31 12 :45:38,567 restorepiperun INFO Restoring 'docs_example_run' from backup parquet files. 2021 -01-31 12 :45:39,216 restorepiperun INFO Deleting new sources and associated objects to restore run Total objects deleted: 2475 2021 -01-31 12 :45:39,532 restorepiperun INFO Deleting forced measurement and associated objects to restore run. Total objects deleted: 3769 2021 -01-31 12 :45:39,576 restorepiperun INFO Restoring metrics for 459 sources. 2021 -01-31 12 :45:39,631 restorepiperun INFO Removing 7 images from the run. 2021 -01-31 12 :45:39,826 restorepiperun INFO Deleting associations to restore run. Total objects deleted: 3239 2021 -01-31 12 :45:40,651 restorepiperun INFO Deleting measurement pairs to restore run. Total objects deleted: 14692 2021 -01-31 12 :45:40,653 restorepiperun INFO Restoring run metrics. 2021 -01-31 12 :45:40,662 restorepiperun INFO Restoring parquet files and removing .bak files. 2021 -01-31 12 :45:40,669 restorepiperun INFO Restore complete. runpipeline \u00b6 The pipeline is run using runpipeline django command. ( pipeline_env ) $ ./manage.py runpipeline --help Output: usage: manage.py runpipeline [ -h ] [ --version ] [ -v { 0 ,1,2,3 }] [ --settings SETTINGS ] [ --pythonpath PYTHONPATH ] [ --traceback ] [ --no-color ] [ --force-color ] [ --skip-checks ] piperun Process the pipeline for a list of images and Selavy catalogs positional arguments: piperun Path or name of the pipeline run. optional arguments: -h, --help show this help message and exit --version show program 's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn' t provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\" . --traceback Raise on CommandError exceptions --no-color Don ' t colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. General usage: ( pipeline_env ) $ ./manage.py runpipeline path/to/my_pipe_run","title":"CLI"},{"location":"adminusage/cli/#command-line-interface-cli","text":"This section describes the commands available to the administrators of the pipelines.","title":"Command Line Interface (CLI)"},{"location":"adminusage/cli/#pipeline-usage","text":"All the pipeline commands are run using the Django global ./manage.py <command> interface. Therefore you need to activate the Python environment. You can have a look at the available commands for the pipeline app: ( pipeline_env ) $ ./manage.py help Output: ... [ vast_pipeline ] clearpiperun createmeasarrow debugrun importsurvey initpiperun restorepiperun runpipeline ... There are 7 commands, described in detail below.","title":"Pipeline Usage"},{"location":"adminusage/cli/#clearpiperun","text":"Detailed commands for resetting the database can be found in Contributing and Developing Guidelines . Resetting a pipeline run can be done using the clearpiperun command. This will delete all images and related objects such as sources associated with that pipeline run. Images that have been used in other pipeline runs will not be deleted. ( pipeline_env ) $ ./manage.py clearpiperun path/to/my_pipe_run # or ( pipeline_env ) $ ./manage.py clearpiperun my_pipe_run More details on the clearpiperun command can be found in the Contributing and Developing Guidelines .","title":"clearpiperun"},{"location":"adminusage/cli/#createmeasarrow","text":"This command allows for the creation of the measurements.arrow and measurement_pairs.arrow files after a run has been successfully completed. See Arrow Files for more information. ./manage.py createmeasarrow --help usage: manage.py createmeasarrow [-h] [--overwrite] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperun Create `measurements.arrow` and `measurement_pairs.arrow` files for a completed pipeline run. positional arguments: piperun Path or name of the pipeline run. optional arguments: -h, --help show this help message and exit --overwrite Overwrite previous 'measurements.arrow' file. --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the co Example usage: ./manage.py createmeasarrow docs_example_run 2021-03-30 10:48:40,952 createmeasarrow INFO Creating measurements arrow file for 'docs_example_run'. 2021-03-30 10:48:40,952 utils INFO Creating measurements.arrow for run docs_example_run. 2021-03-30 10:48:41,829 createmeasarrow INFO Creating measurement pairs arrow file for 'docs_example_run'. 2021-03-30 10:48:41,829 utils INFO Creating measurement_pairs.arrow for run docs_example_run.","title":"createmeasarrow"},{"location":"adminusage/cli/#debugrun","text":"The debugrun command is used to print out a summary of the pipeline run to the terminal. A single pipeline run can be entered as an argument or all can be entered to print the statistics of all the pipeline runs in the database. ./manage.py debugrun --help usage: manage.py debugrun [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperuns [piperuns ...] Print out total metrics such as nr of measurements for runs positional arguments: piperuns Name or path of pipeline run(s) to debug.Pass \"all\" to print summary data of all the runs. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. Example usage: ./manage.py debugrun docs_example_run * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Printing summary data of pipeline run \"docs_example_run\" Nr of images: 14 Nr of measurements: 4312 Nr of forced measurements: 2156 Nr of sources: 557 Nr of association: 3276 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *","title":"debugrun"},{"location":"adminusage/cli/#importsurvey","text":"This functionality is not yet available.","title":"importsurvey"},{"location":"adminusage/cli/#initpiperun","text":"In order to process the images in the pipeline, you must create/initialise a pipeline run first. The pipeline run creation is done using the initpiperun django command, which requires a pipeline run folder. The command creates a folder with the pipeline run name under the settings PROJECT_WORKING_DIR defined in settings . ( pipeline_env ) $ ./manage.py initpiperun --help Output: usage: manage.py initpiperun [ -h ] [ --version ] [ -v { 0 ,1,2,3 }] [ --settings SETTINGS ] [ --pythonpath PYTHONPATH ] [ --traceback ] [ --no-color ] [ --force-color ] runname Create the pipeline run folder structure to run a pipeline instance positional arguments: runname Name of the pipeline run. optional arguments: -h, --help show this help message and exit --version show program 's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn' t provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\" . --traceback Raise on CommandError exceptions --no-color Don ' t colorize the command output. --force-color Force colorization of the command output. The command yields the following folder structure: ( pipeline_env ) $ ./manage.py initpiperun my_pipe_run Output: 2020 -02-27 23 :04:33,344 initpiperun INFO creating pipeline run folder 2020 -02-27 23 :04:33,344 initpiperun INFO copying default config in pipeline run folder 2020 -02-27 23 :04:33,344 initpiperun INFO pipeline run initialisation successful! Please modify the \"config.py\"","title":"initpiperun"},{"location":"adminusage/cli/#restorepiperun","text":"Details on the add images feature can be found here . It allows for a pipeline run that has had an image added to the run to be restored to the state it was in before the image addition was made. By default the command will ask for confirmation that the run is to be restored (the option --no-confirm skips this). ./manage.py restorepiperun --help usage: manage.py restorepiperun [ -h ] [ --no-confirm ] [ --version ] [ -v { 0 ,1,2,3 }] [ --settings SETTINGS ] [ --pythonpath PYTHONPATH ] [ --traceback ] [ --no-color ] [ --force-color ] [ --skip-checks ] piperuns [ piperuns ... ] Restore a pipeline run to the previous person after image add mode has been used. positional arguments: piperuns Name or path of pipeline run ( s ) to restore. optional arguments: -h, --help show this help message and exit --no-confirm Flag to skip the confirmation stage and proceed to restore the pipeline run. --version show program 's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn' t provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\" . --traceback Raise on CommandError exceptions --no-color Don ' t colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. ( pipeline_env ) $ ./manage.py restorepiperun path/to/my_pipe_run # or ( pipeline_env ) $ ./manage.py restorepiperun my_pipe_run Example usage: ./manage.py restorepiperun docs_example_run 2021 -01-31 12 :45:31,039 restorepiperun INFO Will restore the run to the following config: PIPE_RUN_PATH...................................../Users/adam/GitHub/vast-pipeline/pipeline-runs/docs_example_run IMAGE_FILES....................................... [ '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout.fits' ] SELAVY_FILES...................................... [ '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout.components.txt' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout.components.txt' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout.components.txt' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout.components.txt' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout.components.txt' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout.components.txt' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout.components.txt' ] BACKGROUND_FILES.................................. [ '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout_bkg.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout_bkg.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout_bkg.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout_bkg.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout_bkg.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout_bkg.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout_bkg.fits' ] NOISE_FILES....................................... [ '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout_rms.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout_rms.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout_rms.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout_rms.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout_rms.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout_rms.fits' , '/Users/adam/askap/pipeline-test-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout_rms.fits' ] SOURCE_FINDER.....................................selavy MONITOR...........................................True MONITOR_MIN_SIGMA.................................3.0 MONITOR_EDGE_BUFFER_SCALE.........................1.2 MONITOR_CLUSTER_THRESHOLD.........................3.0 MONITOR_ALLOW_NAN.................................False ASTROMETRIC_UNCERTAINTY_RA........................1 ASTROMETRIC_UNCERTAINTY_DEC.......................1 ASSOCIATION_PARALLEL..............................True ASSOCIATION_EPOCH_DUPLICATE_RADIUS................2.5 ASSOCIATION_METHOD................................basic ASSOCIATION_RADIUS................................15.0 ASSOCIATION_DE_RUITER_RADIUS......................5.68 ASSOCIATION_BEAMWIDTH_LIMIT.......................1.5 NEW_SOURCE_MIN_SIGMA..............................5.0 DEFAULT_SURVEY....................................None FLUX_PERC_ERROR...................................0 USE_CONDON_ERRORS.................................True SELAVY_LOCAL_RMS_ZERO_FILL_VALUE..................0.2 CREATE_MEASUREMENTS_ARROW_FILES...................False SUPPRESS_ASTROPY_WARNINGS.........................True SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS..........4.3 Would you like to restore the run ? ( y/n ) : y 2021 -01-31 12 :45:38,567 restorepiperun INFO Restoring 'docs_example_run' from backup parquet files. 2021 -01-31 12 :45:39,216 restorepiperun INFO Deleting new sources and associated objects to restore run Total objects deleted: 2475 2021 -01-31 12 :45:39,532 restorepiperun INFO Deleting forced measurement and associated objects to restore run. Total objects deleted: 3769 2021 -01-31 12 :45:39,576 restorepiperun INFO Restoring metrics for 459 sources. 2021 -01-31 12 :45:39,631 restorepiperun INFO Removing 7 images from the run. 2021 -01-31 12 :45:39,826 restorepiperun INFO Deleting associations to restore run. Total objects deleted: 3239 2021 -01-31 12 :45:40,651 restorepiperun INFO Deleting measurement pairs to restore run. Total objects deleted: 14692 2021 -01-31 12 :45:40,653 restorepiperun INFO Restoring run metrics. 2021 -01-31 12 :45:40,662 restorepiperun INFO Restoring parquet files and removing .bak files. 2021 -01-31 12 :45:40,669 restorepiperun INFO Restore complete.","title":"restorepiperun"},{"location":"adminusage/cli/#runpipeline","text":"The pipeline is run using runpipeline django command. ( pipeline_env ) $ ./manage.py runpipeline --help Output: usage: manage.py runpipeline [ -h ] [ --version ] [ -v { 0 ,1,2,3 }] [ --settings SETTINGS ] [ --pythonpath PYTHONPATH ] [ --traceback ] [ --no-color ] [ --force-color ] [ --skip-checks ] piperun Process the pipeline for a list of images and Selavy catalogs positional arguments: piperun Path or name of the pipeline run. optional arguments: -h, --help show this help message and exit --version show program 's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn' t provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\" . --traceback Raise on CommandError exceptions --no-color Don ' t colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. General usage: ( pipeline_env ) $ ./manage.py runpipeline path/to/my_pipe_run","title":"runpipeline"},{"location":"architecture/database/","text":"Database Schema \u00b6 This section describes the relationships between the objects/tables stored in the database. Django Web App Schema \u00b6 The following figure shows a detailed schematics of the schema and relationships as well as tables parameters of the Django App. Pipeline Detailed Schema \u00b6 A details to the pipeline schema is shown below: Important points \u00b6 Some of the key points of the above relationship diagram are: each image object is indipendent from the others and can belong to multiple pipeline runs to avoid duplication. An image can belong to multiple pipeline run objects and a run object can have multiple images. If a user want to upload an image object with different characteristic (i.e. using a custom source extraction tool), is free to do so but the image name need to be unique . So we suggest to assign a custom name to your image files. Each image is linked to a set of source measurement objects by means of a foreign key. Therefore those objects can belong to multiple source objects. A source object can have multiple measurements and a measurements can belong to multiple source objects. The pipeline schema has been mainly designed to allow for completely disjoint run objects so that each users can run their own processing with their specific settings, defined in the configuration file.","title":"Database Schema"},{"location":"architecture/database/#database-schema","text":"This section describes the relationships between the objects/tables stored in the database.","title":"Database Schema"},{"location":"architecture/database/#django-web-app-schema","text":"The following figure shows a detailed schematics of the schema and relationships as well as tables parameters of the Django App.","title":"Django Web App Schema"},{"location":"architecture/database/#pipeline-detailed-schema","text":"A details to the pipeline schema is shown below:","title":"Pipeline Detailed Schema"},{"location":"architecture/database/#important-points","text":"Some of the key points of the above relationship diagram are: each image object is indipendent from the others and can belong to multiple pipeline runs to avoid duplication. An image can belong to multiple pipeline run objects and a run object can have multiple images. If a user want to upload an image object with different characteristic (i.e. using a custom source extraction tool), is free to do so but the image name need to be unique . So we suggest to assign a custom name to your image files. Each image is linked to a set of source measurement objects by means of a foreign key. Therefore those objects can belong to multiple source objects. A source object can have multiple measurements and a measurements can belong to multiple source objects. The pipeline schema has been mainly designed to allow for completely disjoint run objects so that each users can run their own processing with their specific settings, defined in the configuration file.","title":"Important points"},{"location":"architecture/intro/","text":"VAST Pipeline Architecture \u00b6 The pipeline is essentially a Django app in which the pipeline is run as a Django admin command. The main structure of the pipeline is described in this schematics: Design Philosophy \u00b6 We design the pipeline in order to make it easy to use but at the same time powerful and fast. We decided to use the familiar Pandas Dataframe structure to wrap all the data manipulations, including the association operations, in the back-end. The Python community, as well as the research and scientific communities (including the astro-physicists) are very familiar with Pandas, and they should be able to understand, mantain and develop the code base. Usually in the \"Big Data\" world the commond tools adopted by the industry and research are Apache Hadoop and Spark . We decided to use Dask which is similar to Spark in same ways, but it integrates well with Pandas Dataframe and its syntax is quite similar to Pandas. Further it provides scalability by means of clustering and integrating with HPC (High Performance Comptuing) stacks. The pipeline code itself and the web app are integrated into one code base, for the sake of simplicity, easy to develop using one central repository. The user can still run the pipeline via CLI (Command Line Interface), using Django Admin Commands , as well as thorugh the web app itself. The integration avoid duplication in code, especially on regards the declaration of the schema in the ORM (Object Relational Mapping), and add user and permission management on the underlyng data, through the in-built functionality of Django framework. The front-end is built in simple HTML, CSS and Javascript using a freely available Bootstrap 4 template. The developers know best practices in the web development are focusing mostly on single page applications using framework such as ReactJS and AngularJS . The choice of using just the basic web stack (HTML + CSS + JS) was driven by the fact that future developers do not need to learn modern web frameworks such as React and Angular, but the fundamental web programming which is still the core of those tools. Technology Stack \u00b6 Back-End \u00b6 Astropy 4+ Astroquery 0.4+ Bokeh 2+ Dask 2+ Django 3+ Django Rest Framework Rest Framework Datatables Django Q Python Social Auth - Django Django Crispy Forms Django Tagulous Pandas 1+ Python 3.7+ Pyarrow 0.17+ Postgres 10+ Q3C Vaex 3+ Front-End \u00b6 Aladin Lite Bokeh Bootstrap 4 DataTables D3 Celestial Jquery JS9 ParticleJS PrismJS SB Admin 2 template Additional \u00b6 Docker node 12+ npm 6+ gulp 4+ GitHub Actions","title":"Architecture Intro"},{"location":"architecture/intro/#vast-pipeline-architecture","text":"The pipeline is essentially a Django app in which the pipeline is run as a Django admin command. The main structure of the pipeline is described in this schematics:","title":"VAST Pipeline Architecture"},{"location":"architecture/intro/#design-philosophy","text":"We design the pipeline in order to make it easy to use but at the same time powerful and fast. We decided to use the familiar Pandas Dataframe structure to wrap all the data manipulations, including the association operations, in the back-end. The Python community, as well as the research and scientific communities (including the astro-physicists) are very familiar with Pandas, and they should be able to understand, mantain and develop the code base. Usually in the \"Big Data\" world the commond tools adopted by the industry and research are Apache Hadoop and Spark . We decided to use Dask which is similar to Spark in same ways, but it integrates well with Pandas Dataframe and its syntax is quite similar to Pandas. Further it provides scalability by means of clustering and integrating with HPC (High Performance Comptuing) stacks. The pipeline code itself and the web app are integrated into one code base, for the sake of simplicity, easy to develop using one central repository. The user can still run the pipeline via CLI (Command Line Interface), using Django Admin Commands , as well as thorugh the web app itself. The integration avoid duplication in code, especially on regards the declaration of the schema in the ORM (Object Relational Mapping), and add user and permission management on the underlyng data, through the in-built functionality of Django framework. The front-end is built in simple HTML, CSS and Javascript using a freely available Bootstrap 4 template. The developers know best practices in the web development are focusing mostly on single page applications using framework such as ReactJS and AngularJS . The choice of using just the basic web stack (HTML + CSS + JS) was driven by the fact that future developers do not need to learn modern web frameworks such as React and Angular, but the fundamental web programming which is still the core of those tools.","title":"Design Philosophy"},{"location":"architecture/intro/#technology-stack","text":"","title":"Technology Stack"},{"location":"architecture/intro/#back-end","text":"Astropy 4+ Astroquery 0.4+ Bokeh 2+ Dask 2+ Django 3+ Django Rest Framework Rest Framework Datatables Django Q Python Social Auth - Django Django Crispy Forms Django Tagulous Pandas 1+ Python 3.7+ Pyarrow 0.17+ Postgres 10+ Q3C Vaex 3+","title":"Back-End"},{"location":"architecture/intro/#front-end","text":"Aladin Lite Bokeh Bootstrap 4 DataTables D3 Celestial Jquery JS9 ParticleJS PrismJS SB Admin 2 template","title":"Front-End"},{"location":"architecture/intro/#additional","text":"Docker node 12+ npm 6+ gulp 4+ GitHub Actions","title":"Additional"},{"location":"design/association/","text":"Source Association \u00b6 This page details the association stage of a pipeline run. There are three association methods available which are summarised in the table below, and detailed in the following sections. Tip For complex fields and large surveys the De Ruiter method is recommended. Method Fixed Assoc. Radius Astropy function Possible Relation Types Basic Yes match_coordinates_sky one-to-many Advanced Yes search_around_sky many-to-many, many-to-one, one-to-many de Ruiter (TraP) No search_around_sky many-to-many, many-to-one, one-to-many General Association Notes \u00b6 Terminology \u00b6 During association, measurements are associated into unique sources . Association Process \u00b6 By default, association is performed on an image-by-image basis, ordered by the observational date. The only time this isn't the case is when Epoch Based Association is used. Note Epoch Based Association is not an association method, rather it changes how the measurements are handled when passed to one of the three methods for association. Weighted Average Coordinates \u00b6 After every iteration of each association method, the average RA and Dec, weighted by the positional uncertainty, are calculated for each source. These weighted averages are then used as the base catalogue for the next association iteration. In other words, as the measurements are associated, new measurements are associated against the weighted average of the sources identified to that point in the process. Sources positions are reported using the weighted averages. Association Methods \u00b6 Tip For a better understanding on the underlying process, see this page in the astropy documentation for examples on matching catalogues. Basic \u00b6 The most basic association method uses the astropy match_coordinates_sky function which: Associates measurements using only the nearest neighbour for each source when comparing catalogues. Uses a fixed association radius as a threshold for a 'match'. Only one-to-many relations are possible. Advanced \u00b6 This method uses the same process as Basic , however the astropy function search_around_sky is used instead. This means: All possible matches between the two catalogues are found, rather than only the nearest neighbour. A fixed association radius is still applied as the threshold. All types of relations are possible. de Ruiter \u00b6 The de Ruiter method is a translation of the association method used by the LOFAR Transients Pipeline (TraP) , which uses the de Ruiter radius in order to define associations. The search_around_sky astropy method is still used, but the threshold for a potential match is first limited by a beamwidth limit value which is defined in the pipeline run configuration file ( ASSOCIATION_BEAMWIDTH_LIMIT ), such that the initial threshold separation distance is set to \\[ \\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,img}}}{2}, \\] where \\(\\theta_{\\text{bmaj,img}}\\) is the major axis of the restoring beam of the image being associated. Then, the de Ruiter radius is calculated for all potential matches which is defined as \\[ r_{i,j} = \\sqrt{ \\frac{ (\\alpha_{i} - \\alpha_{j})^{2}((\\delta_{i} + \\delta_{j})/2)}{\\sigma^{2}_{\\alpha_{i}} + \\sigma^{2}_{\\alpha_{j}}} + \\frac{(\\delta_{i} + \\delta_{j})^{2}}{\\sigma^{2}_{\\delta_{i}} + \\sigma^{2}_{\\delta_{j}}} } \\] where \\(\\alpha_{n}\\) is the right ascension of source n, \\(\\delta_{n}\\) is its declination, and \\(\\sigma_{y}\\) represents the error on the quantity y. Matches are then identified by applying a threshold maximum value to the de Ruiter radius which is defined by the user in the pipeline run configuration file ( ASSOCIATION_DE_RUITER_RADIUS ). All relation types are possible using this method. Relations \u00b6 Situations can arise where a source is associated with more than one source in the catalogue being cross-matched (or vice versa). Internally these types of associations are called: many-to-many one-to-many many-to-one a good explanation of these situations is presented in the TraP documentation here . The VAST Pipeline follows the TraP methods in handling these types of associations, which is also detailed in the linked documentation. In short: many-to-many associations are reduced to one-to-one or one-to-many associations. one-to-many and many-to-one associations create \"forked\" unique sources. I.e. an individual datapoint can belong to two different sources. The VAST Pipeline reports the one-to-many and many-to-one associations by relating sources. A source may have one or more relations which signifies the the source could be associated with more than one other source. This often happens for complex sources with many closely packed components. A read-through of the TraP documentation is highly encouraged on this point as it contains an excellent description. Epoch Based Association \u00b6 The pipeline is able to associate inputs on an epoch basis. What this means is that, for example, all VAST Pilot Epoch 1 measurements are grouped together and are associated with grouped together Epoch 2 measurements, and so on. In doing this, duplicate measurements from within the same epoch are cut with the measurement kept being that which is closest to the centre of its respective image. The separation distance that defines a duplicate is defined in the pipeline run configuration file ( ASSOCIATION_EPOCH_DUPLICATE_RADIUS ). The mode is activated by entering the images to be processed as dictionary objects, using an orderable string as the key and lists of images as the values, as demonstrated below. IMAGE_FILES = { \"epoch01\" : [ \"/full/path/to/image1.fits\" , \"/full/path/to/image2.fits\" ], \"epoch02\" : [ \"/full/path/to/image3.fits\" ], } The lightcurves below show the difference between 'regular' association (top) and 'epoch based' association (lower) for a source. For large surveys where transient and variablity searches on the epoch timescale is required, using this mode can greatly speed up the association stage. Warning Epoch based association does eliminate the full time resolution of your data! The base time resolution will be between the defined epochs. Parallel Association \u00b6 When parallel association is used, the images to process are analysed and grouped into distinct patches of the sky that do not overlap. These distinct regions are then processed through the source association in parallel. It is recommended to use parallel association when your dataset covers three or more distinct patches of sky.","title":"Source Association"},{"location":"design/association/#source-association","text":"This page details the association stage of a pipeline run. There are three association methods available which are summarised in the table below, and detailed in the following sections. Tip For complex fields and large surveys the De Ruiter method is recommended. Method Fixed Assoc. Radius Astropy function Possible Relation Types Basic Yes match_coordinates_sky one-to-many Advanced Yes search_around_sky many-to-many, many-to-one, one-to-many de Ruiter (TraP) No search_around_sky many-to-many, many-to-one, one-to-many","title":"Source Association"},{"location":"design/association/#general-association-notes","text":"","title":"General Association Notes"},{"location":"design/association/#terminology","text":"During association, measurements are associated into unique sources .","title":"Terminology"},{"location":"design/association/#association-process","text":"By default, association is performed on an image-by-image basis, ordered by the observational date. The only time this isn't the case is when Epoch Based Association is used. Note Epoch Based Association is not an association method, rather it changes how the measurements are handled when passed to one of the three methods for association.","title":"Association Process"},{"location":"design/association/#weighted-average-coordinates","text":"After every iteration of each association method, the average RA and Dec, weighted by the positional uncertainty, are calculated for each source. These weighted averages are then used as the base catalogue for the next association iteration. In other words, as the measurements are associated, new measurements are associated against the weighted average of the sources identified to that point in the process. Sources positions are reported using the weighted averages.","title":"Weighted Average Coordinates"},{"location":"design/association/#association-methods","text":"Tip For a better understanding on the underlying process, see this page in the astropy documentation for examples on matching catalogues.","title":"Association Methods"},{"location":"design/association/#basic","text":"The most basic association method uses the astropy match_coordinates_sky function which: Associates measurements using only the nearest neighbour for each source when comparing catalogues. Uses a fixed association radius as a threshold for a 'match'. Only one-to-many relations are possible.","title":"Basic"},{"location":"design/association/#advanced","text":"This method uses the same process as Basic , however the astropy function search_around_sky is used instead. This means: All possible matches between the two catalogues are found, rather than only the nearest neighbour. A fixed association radius is still applied as the threshold. All types of relations are possible.","title":"Advanced"},{"location":"design/association/#de-ruiter","text":"The de Ruiter method is a translation of the association method used by the LOFAR Transients Pipeline (TraP) , which uses the de Ruiter radius in order to define associations. The search_around_sky astropy method is still used, but the threshold for a potential match is first limited by a beamwidth limit value which is defined in the pipeline run configuration file ( ASSOCIATION_BEAMWIDTH_LIMIT ), such that the initial threshold separation distance is set to \\[ \\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,img}}}{2}, \\] where \\(\\theta_{\\text{bmaj,img}}\\) is the major axis of the restoring beam of the image being associated. Then, the de Ruiter radius is calculated for all potential matches which is defined as \\[ r_{i,j} = \\sqrt{ \\frac{ (\\alpha_{i} - \\alpha_{j})^{2}((\\delta_{i} + \\delta_{j})/2)}{\\sigma^{2}_{\\alpha_{i}} + \\sigma^{2}_{\\alpha_{j}}} + \\frac{(\\delta_{i} + \\delta_{j})^{2}}{\\sigma^{2}_{\\delta_{i}} + \\sigma^{2}_{\\delta_{j}}} } \\] where \\(\\alpha_{n}\\) is the right ascension of source n, \\(\\delta_{n}\\) is its declination, and \\(\\sigma_{y}\\) represents the error on the quantity y. Matches are then identified by applying a threshold maximum value to the de Ruiter radius which is defined by the user in the pipeline run configuration file ( ASSOCIATION_DE_RUITER_RADIUS ). All relation types are possible using this method.","title":"de Ruiter"},{"location":"design/association/#relations","text":"Situations can arise where a source is associated with more than one source in the catalogue being cross-matched (or vice versa). Internally these types of associations are called: many-to-many one-to-many many-to-one a good explanation of these situations is presented in the TraP documentation here . The VAST Pipeline follows the TraP methods in handling these types of associations, which is also detailed in the linked documentation. In short: many-to-many associations are reduced to one-to-one or one-to-many associations. one-to-many and many-to-one associations create \"forked\" unique sources. I.e. an individual datapoint can belong to two different sources. The VAST Pipeline reports the one-to-many and many-to-one associations by relating sources. A source may have one or more relations which signifies the the source could be associated with more than one other source. This often happens for complex sources with many closely packed components. A read-through of the TraP documentation is highly encouraged on this point as it contains an excellent description.","title":"Relations"},{"location":"design/association/#epoch-based-association","text":"The pipeline is able to associate inputs on an epoch basis. What this means is that, for example, all VAST Pilot Epoch 1 measurements are grouped together and are associated with grouped together Epoch 2 measurements, and so on. In doing this, duplicate measurements from within the same epoch are cut with the measurement kept being that which is closest to the centre of its respective image. The separation distance that defines a duplicate is defined in the pipeline run configuration file ( ASSOCIATION_EPOCH_DUPLICATE_RADIUS ). The mode is activated by entering the images to be processed as dictionary objects, using an orderable string as the key and lists of images as the values, as demonstrated below. IMAGE_FILES = { \"epoch01\" : [ \"/full/path/to/image1.fits\" , \"/full/path/to/image2.fits\" ], \"epoch02\" : [ \"/full/path/to/image3.fits\" ], } The lightcurves below show the difference between 'regular' association (top) and 'epoch based' association (lower) for a source. For large surveys where transient and variablity searches on the epoch timescale is required, using this mode can greatly speed up the association stage. Warning Epoch based association does eliminate the full time resolution of your data! The base time resolution will be between the defined epochs.","title":"Epoch Based Association"},{"location":"design/association/#parallel-association","text":"When parallel association is used, the images to process are analysed and grouped into distinct patches of the sky that do not overlap. These distinct regions are then processed through the source association in parallel. It is recommended to use parallel association when your dataset covers three or more distinct patches of sky.","title":"Parallel Association"},{"location":"design/imageingest/","text":"Image & Selavy Catalogue Ingest \u00b6 This page details the stage of the pipeline that ingests the images to be processed. When the pipeline encounters an image for the first time (in any pipeline run), the image and accompanying selavy catalogue are uploaded to the pipeline database. The portion of a pipeline log file below shows the messages for the ingestion of three images. Note Once an image is uploaded then that image is available for all other runs to use without having to re-upload. 2021-03-11 12:59:49,751 loading INFO Reading image VAST_0127-73A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:49,756 utils INFO Adding new frequency band: 887 2021-03-11 12:59:49,771 utils INFO Created sky region 21.838, -73.121 2021-03-11 12:59:49,775 utils INFO Adding new-test-data to sky region 21.838, -73.121 2021-03-11 12:59:50,100 loading INFO Processed measurements dataframe of shape: (203, 40) 2021-03-11 12:59:50,273 loading INFO Bulk created #203 Measurement 2021-03-11 12:59:50,334 loading INFO Reading image VAST_2118+00A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:50,345 utils INFO Created sky region 322.439, -3.987 2021-03-11 12:59:50,347 utils INFO Adding new-test-data to sky region 322.439, -3.987 2021-03-11 12:59:50,577 loading INFO Processed measurements dataframe of shape: (148, 40) 2021-03-11 12:59:50,708 loading INFO Bulk created #148 Measurement 2021-03-11 12:59:50,736 loading INFO Reading image VAST_2118-06A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:50,749 utils INFO Created sky region 322.439, -4.487 2021-03-11 12:59:50,752 utils INFO Adding new-test-data to sky region 322.439, -4.487 2021-03-11 12:59:50,977 loading INFO Processed measurements dataframe of shape: (159, 40) 2021-03-11 12:59:51,111 loading INFO Bulk created #159 Measurement Ingest Steps Summary \u00b6 The FITS file is opened and read (the header is used to obtain metadata) along with the selavy component catalogue text file. The selavy component file is cleaned for erroneous components along with the calculation of extra measurements metrics such as signal-to-noise ratio , compactness and positional uncertainties. Also, optionally, flux errors are recalculated using the Condon (1997) method. See the Selavy Measurements Processing section below for further details. Median, minimum and maximum root-mean-square (RMS) values are read from the accompanying RMS image provided by the user and these values are attached to the image. The image is also attached to a sky region and a frequency band based on its properties (see Sky Region and Frequency Band ). The cleaned measurements (selavy components) are saved to a parquet file for repeated easy access. The overall image, band and sky region information for the pipeline run are written to a parquet file. See Ingest Steps Details for further details on the steps. Uniqueness \u00b6 The image uniqueness is defined by the filename. If you wish to upload a different version of the same image, e.g. a version where different Selavy settings were used in the source extraction, then you would have to make sure the image filename was different to the previously ingested image. Ingest Steps Details \u00b6 Selavy Measurements Processing \u00b6 Cleaning \u00b6 The selavy measurements are checked for erroneous values that could cause issues with the source association. Any sources that are found to have the following properties are removed: Sources that have a peak or integrated flux value of 0. Sources that have a bmaj or bmin value of 0. Sources that have a bmaj or bmin value less than half of the respective values of the image restoring beam. In addition, components are also checked for zero values that can be corrected, where the correction values to apply are defined in either the user or overall pipeline configuration files. The field names of these zero checks are defined in the table below. Field Name Correct with Location flux_int_err FLUX_DEFAULT_MIN_ERROR settings.py flux_peak_err FLUX_DEFAULT_MIN_ERROR settings.py ra_err POS_DEFAULT_MIN_ERROR settings.py dec_err POS_DEFAULT_MIN_ERROR settings.py local_rms SELAVY_LOCAL_RMS_ZERO_FILL_VALUE config.py Note settings.py refers to the pipeline configuration file webinterface/settings.py which is configured by the system administrator and cannot be modified by regular users. config.py refers to a pipeline run configuration file which is set by the user. Condon (1997) Flux & Positional Errors \u00b6 If selected in the pipeline run configuration file, the flux and positional errors are recalculated using the Condon (1997) method. The following errors are replaced with those that are recalculated: flux_peak_err flux_int_err err_bmaj err_bmin err_pa ra_err dec_err Positional Errors (de Ruiter method) \u00b6 Firstly, the systematic astrometry error from the user pipeline run configuration file ( ASTROMETRIC_UNCERTAINTY_RA and ASTROMETRIC_UNCERTAINTY_DEC ) are applied to the measurement. These values are saved as ew_sys_err and ns_sys_err . Warning Currently the systematic errors applied at the pipeline run stage are then permanently fixed to the measurements, meaning that all subsequent runs using these measurements will use the fixed astrometic error. It is recommended to leave the values to the default value of 1.0. In order to apply the TraP de Ruiter association method, some extra positional error values are calculated. Firstly the ra_err and dec_err are used to estimate the largest angular uncertainty of the measurement which is recorded as the error_radius . It is estimated by finding the largest angular separation between the measurement coordinate and every coordinate combination of \\(ra \\pm \\delta ra\\) and \\(dec \\pm \\delta dec\\) . The final uncertainties are then defined as the hypotenuse values of ew_sys_err / ns_sys_err and the error_radius . These are defined as the uncertainty_ew and uncertainty_ns , respectively. The weights of the errors are defined as \\(\\frac{1}{\\text{uncertainty_x}^{2}}\\) where x is either ew or ns . Other Metrics \u00b6 The table below defines extra metrics that are added to the measurements. Field Name Description time The image datetime applied to the measurement. snr \\(\\frac{\\text{flux_peak}}{\\text{local_rms}}\\) . compactness \\(\\frac{\\text{flux_int}}{\\text{flux_peak}}\\) . flux_int_isl_ratio \\(\\frac{\\text{flux_int}}{\\text{total_island_int_flux}}\\) . flux_peak_isl_ratio \\(\\frac{\\text{flux_peak}}{\\text{total_island_peak_flux}}\\) . Sky Region \u00b6 The pipeline defines sky regions that are used to easily find images that cover the same region of the sky. A sky region is defined by: The central coodinate. The width in both ra and dec (the physical_bmaj and physical_bmin values are used here, see Uploaded Image Information ). An extraction radius ( xtr_radius ; fov_bmin is used here, again see Uploaded Image Information ). Hence, images that cover the exact same patch of sky will be assigned to the same sky region. Frequency Band \u00b6 The image is associated to a frequency object in the pipeline that represents the observational frequency information of the image. The frequency and the bandwidth are recorded. Image RMS Values \u00b6 The median, minimum and maximum values are calculated directly from the RMS map supplied by the user as a required input. This is achieved by loading the data from the FITS file and using the respective numpy operations on the data array to obtain the values. FITS Headers Used \u00b6 The table below defines which header fields are used to read the image information. Header Field Used For DATE-OBS The date and time of the observation. TIMESYS The timezone of the date and time. DURATION Duration of the observation in seconds. STOKES Stokes parameter of the image. TELESCOP Telescope name. BMAJ Major axis size of the restoring beam. BMIN Minor axis size of the restoring beam. BPA Position angle of the restoring beam. NAXIS1 Size of the image RA axis in pixels. NAXIS2 Size of the image Dec axis in pixels. CTYPE3(or 4) Check if equal to FREQ to use for frequency information. CRVAL3(or 4) Central frequency. CDELT3(or 4) Bandwidth. RESTFREQ and RESTBW can also be used as fallback options for frequency detection. The pixel scales are obtained with astropy.wcs.utils.proj_plane_pixel_scales . Uploaded Image Information \u00b6 The table below defines what is defined and uploaded using the meta data (FITS header) and other inputs. Field Name Default Description measurements_path n/a The system path to the corresponding selavy components file (saved as a parquet file by the pipeline) polarisation I The polarisation of the image (currently only Stokes I is supported). name n/a The name of the image which is taken from the filename. path n/a The system path to the image FITS file. noise_path '' The system path to the related noise image FITS file. background_path '' The system path to the related background image FITS file. datetime n/a Observational datetime of the image. jd n/a Observational datetime of the image in Julian days format. duration 0 Duration of the observation (if found in header). Seconds. ra n/a The Right Ascension of the image pointing centre. Degrees. dec n/a The Declination of the image pointing centre. Degrees. fov_bmaj n/a The estimated major axis field-of-view value - the radius_pixels multiplied by the major axis pixel size. Degrees. fov_bmin n/a The estimated minor axis field-of-view value - the radius_pixels multiplied by the minor axis pixel size. Degrees. physical_bmaj n/a The actual major axis on-sky size - the number of pixels on the major axis multiplied by the major axis pixel size. Degrees. physical_bmin n/a The actual minor axis on-sky size - the number of pixels on the minor axis multiplied by the minor axis pixel size. Degrees. radius_pixels n/a Estimated 'diameter' of the useable image area. Pixels. beam_bmaj n/a The size of the major axis of the image restoring beam. Degrees. beam_bmin n/a The size of the minor axis of the image restoring beam. Degrees. beam_bpa n/a The position angle of the image restoring beam. Degrees East of North. rms_median n/a The median RMS value from the RMS map. mJy/beam. rms_min n/a The minimum RMS value from the RMS map (pixel value). mJy/beam. rms_max n/a The maximum RMS value from the RMS map (pixel value). mJy/beam.","title":"Image & Selavy Catalogue Ingest"},{"location":"design/imageingest/#image-selavy-catalogue-ingest","text":"This page details the stage of the pipeline that ingests the images to be processed. When the pipeline encounters an image for the first time (in any pipeline run), the image and accompanying selavy catalogue are uploaded to the pipeline database. The portion of a pipeline log file below shows the messages for the ingestion of three images. Note Once an image is uploaded then that image is available for all other runs to use without having to re-upload. 2021-03-11 12:59:49,751 loading INFO Reading image VAST_0127-73A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:49,756 utils INFO Adding new frequency band: 887 2021-03-11 12:59:49,771 utils INFO Created sky region 21.838, -73.121 2021-03-11 12:59:49,775 utils INFO Adding new-test-data to sky region 21.838, -73.121 2021-03-11 12:59:50,100 loading INFO Processed measurements dataframe of shape: (203, 40) 2021-03-11 12:59:50,273 loading INFO Bulk created #203 Measurement 2021-03-11 12:59:50,334 loading INFO Reading image VAST_2118+00A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:50,345 utils INFO Created sky region 322.439, -3.987 2021-03-11 12:59:50,347 utils INFO Adding new-test-data to sky region 322.439, -3.987 2021-03-11 12:59:50,577 loading INFO Processed measurements dataframe of shape: (148, 40) 2021-03-11 12:59:50,708 loading INFO Bulk created #148 Measurement 2021-03-11 12:59:50,736 loading INFO Reading image VAST_2118-06A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:50,749 utils INFO Created sky region 322.439, -4.487 2021-03-11 12:59:50,752 utils INFO Adding new-test-data to sky region 322.439, -4.487 2021-03-11 12:59:50,977 loading INFO Processed measurements dataframe of shape: (159, 40) 2021-03-11 12:59:51,111 loading INFO Bulk created #159 Measurement","title":"Image &amp; Selavy Catalogue Ingest"},{"location":"design/imageingest/#ingest-steps-summary","text":"The FITS file is opened and read (the header is used to obtain metadata) along with the selavy component catalogue text file. The selavy component file is cleaned for erroneous components along with the calculation of extra measurements metrics such as signal-to-noise ratio , compactness and positional uncertainties. Also, optionally, flux errors are recalculated using the Condon (1997) method. See the Selavy Measurements Processing section below for further details. Median, minimum and maximum root-mean-square (RMS) values are read from the accompanying RMS image provided by the user and these values are attached to the image. The image is also attached to a sky region and a frequency band based on its properties (see Sky Region and Frequency Band ). The cleaned measurements (selavy components) are saved to a parquet file for repeated easy access. The overall image, band and sky region information for the pipeline run are written to a parquet file. See Ingest Steps Details for further details on the steps.","title":"Ingest Steps Summary"},{"location":"design/imageingest/#uniqueness","text":"The image uniqueness is defined by the filename. If you wish to upload a different version of the same image, e.g. a version where different Selavy settings were used in the source extraction, then you would have to make sure the image filename was different to the previously ingested image.","title":"Uniqueness"},{"location":"design/imageingest/#ingest-steps-details","text":"","title":"Ingest Steps Details"},{"location":"design/imageingest/#selavy-measurements-processing","text":"","title":"Selavy Measurements Processing"},{"location":"design/imageingest/#cleaning","text":"The selavy measurements are checked for erroneous values that could cause issues with the source association. Any sources that are found to have the following properties are removed: Sources that have a peak or integrated flux value of 0. Sources that have a bmaj or bmin value of 0. Sources that have a bmaj or bmin value less than half of the respective values of the image restoring beam. In addition, components are also checked for zero values that can be corrected, where the correction values to apply are defined in either the user or overall pipeline configuration files. The field names of these zero checks are defined in the table below. Field Name Correct with Location flux_int_err FLUX_DEFAULT_MIN_ERROR settings.py flux_peak_err FLUX_DEFAULT_MIN_ERROR settings.py ra_err POS_DEFAULT_MIN_ERROR settings.py dec_err POS_DEFAULT_MIN_ERROR settings.py local_rms SELAVY_LOCAL_RMS_ZERO_FILL_VALUE config.py Note settings.py refers to the pipeline configuration file webinterface/settings.py which is configured by the system administrator and cannot be modified by regular users. config.py refers to a pipeline run configuration file which is set by the user.","title":"Cleaning"},{"location":"design/imageingest/#condon-1997-flux-positional-errors","text":"If selected in the pipeline run configuration file, the flux and positional errors are recalculated using the Condon (1997) method. The following errors are replaced with those that are recalculated: flux_peak_err flux_int_err err_bmaj err_bmin err_pa ra_err dec_err","title":"Condon (1997) Flux &amp; Positional Errors"},{"location":"design/imageingest/#positional-errors-de-ruiter-method","text":"Firstly, the systematic astrometry error from the user pipeline run configuration file ( ASTROMETRIC_UNCERTAINTY_RA and ASTROMETRIC_UNCERTAINTY_DEC ) are applied to the measurement. These values are saved as ew_sys_err and ns_sys_err . Warning Currently the systematic errors applied at the pipeline run stage are then permanently fixed to the measurements, meaning that all subsequent runs using these measurements will use the fixed astrometic error. It is recommended to leave the values to the default value of 1.0. In order to apply the TraP de Ruiter association method, some extra positional error values are calculated. Firstly the ra_err and dec_err are used to estimate the largest angular uncertainty of the measurement which is recorded as the error_radius . It is estimated by finding the largest angular separation between the measurement coordinate and every coordinate combination of \\(ra \\pm \\delta ra\\) and \\(dec \\pm \\delta dec\\) . The final uncertainties are then defined as the hypotenuse values of ew_sys_err / ns_sys_err and the error_radius . These are defined as the uncertainty_ew and uncertainty_ns , respectively. The weights of the errors are defined as \\(\\frac{1}{\\text{uncertainty_x}^{2}}\\) where x is either ew or ns .","title":"Positional Errors (de Ruiter method)"},{"location":"design/imageingest/#other-metrics","text":"The table below defines extra metrics that are added to the measurements. Field Name Description time The image datetime applied to the measurement. snr \\(\\frac{\\text{flux_peak}}{\\text{local_rms}}\\) . compactness \\(\\frac{\\text{flux_int}}{\\text{flux_peak}}\\) . flux_int_isl_ratio \\(\\frac{\\text{flux_int}}{\\text{total_island_int_flux}}\\) . flux_peak_isl_ratio \\(\\frac{\\text{flux_peak}}{\\text{total_island_peak_flux}}\\) .","title":"Other Metrics"},{"location":"design/imageingest/#sky-region","text":"The pipeline defines sky regions that are used to easily find images that cover the same region of the sky. A sky region is defined by: The central coodinate. The width in both ra and dec (the physical_bmaj and physical_bmin values are used here, see Uploaded Image Information ). An extraction radius ( xtr_radius ; fov_bmin is used here, again see Uploaded Image Information ). Hence, images that cover the exact same patch of sky will be assigned to the same sky region.","title":"Sky Region"},{"location":"design/imageingest/#frequency-band","text":"The image is associated to a frequency object in the pipeline that represents the observational frequency information of the image. The frequency and the bandwidth are recorded.","title":"Frequency Band"},{"location":"design/imageingest/#image-rms-values","text":"The median, minimum and maximum values are calculated directly from the RMS map supplied by the user as a required input. This is achieved by loading the data from the FITS file and using the respective numpy operations on the data array to obtain the values.","title":"Image RMS Values"},{"location":"design/imageingest/#fits-headers-used","text":"The table below defines which header fields are used to read the image information. Header Field Used For DATE-OBS The date and time of the observation. TIMESYS The timezone of the date and time. DURATION Duration of the observation in seconds. STOKES Stokes parameter of the image. TELESCOP Telescope name. BMAJ Major axis size of the restoring beam. BMIN Minor axis size of the restoring beam. BPA Position angle of the restoring beam. NAXIS1 Size of the image RA axis in pixels. NAXIS2 Size of the image Dec axis in pixels. CTYPE3(or 4) Check if equal to FREQ to use for frequency information. CRVAL3(or 4) Central frequency. CDELT3(or 4) Bandwidth. RESTFREQ and RESTBW can also be used as fallback options for frequency detection. The pixel scales are obtained with astropy.wcs.utils.proj_plane_pixel_scales .","title":"FITS Headers Used"},{"location":"design/imageingest/#uploaded-image-information","text":"The table below defines what is defined and uploaded using the meta data (FITS header) and other inputs. Field Name Default Description measurements_path n/a The system path to the corresponding selavy components file (saved as a parquet file by the pipeline) polarisation I The polarisation of the image (currently only Stokes I is supported). name n/a The name of the image which is taken from the filename. path n/a The system path to the image FITS file. noise_path '' The system path to the related noise image FITS file. background_path '' The system path to the related background image FITS file. datetime n/a Observational datetime of the image. jd n/a Observational datetime of the image in Julian days format. duration 0 Duration of the observation (if found in header). Seconds. ra n/a The Right Ascension of the image pointing centre. Degrees. dec n/a The Declination of the image pointing centre. Degrees. fov_bmaj n/a The estimated major axis field-of-view value - the radius_pixels multiplied by the major axis pixel size. Degrees. fov_bmin n/a The estimated minor axis field-of-view value - the radius_pixels multiplied by the minor axis pixel size. Degrees. physical_bmaj n/a The actual major axis on-sky size - the number of pixels on the major axis multiplied by the major axis pixel size. Degrees. physical_bmin n/a The actual minor axis on-sky size - the number of pixels on the minor axis multiplied by the minor axis pixel size. Degrees. radius_pixels n/a Estimated 'diameter' of the useable image area. Pixels. beam_bmaj n/a The size of the major axis of the image restoring beam. Degrees. beam_bmin n/a The size of the minor axis of the image restoring beam. Degrees. beam_bpa n/a The position angle of the image restoring beam. Degrees East of North. rms_median n/a The median RMS value from the RMS map. mJy/beam. rms_min n/a The minimum RMS value from the RMS map (pixel value). mJy/beam. rms_max n/a The maximum RMS value from the RMS map (pixel value). mJy/beam.","title":"Uploaded Image Information"},{"location":"design/monitor/","text":"Forced Measurements \u00b6 This page details the forced measurements obtained by the pipeline. Definition \u00b6 When MONITOR = True is set in the pipeline run configuration file, any sources that have non-detections in their lightcurve will have these measurements 'filled in' by performing forced measurements . This means that the flux at the source's current average position in the non-detection image is forcefully measured by fitting a Gaussian with the same shape as the respective image restoring beam. Forced measurements are labelled in the measurements table and parquet files by the column forced . Note Forced measurements are local to a pipeline run - they will not appear in any other pipeline run. Warning Forced measurements are not performed within 3 beamwidths of the image edge. Minimum Sigma Filter \u00b6 Before forced measurements are processed, a minimum sigma check is made to make sure that the forced measurements would provide useful information. For example, a dataset may contain an image that has significantly less sensitivity than the other images. In this case a faint source in the more sensitive images will not be expected to be seen in the less sensitive image. To avoid unnecessary computation, this source is not forcefully measured. The check is performed like that which is made in the New Sources process where the signal-to-noise ratio is calculated using the rms \\(_{min}\\) of the image it is to be extracted from. Hence, for a forced measurement to take place the following condition must be met: \\[ \\frac{f_{peak,det}}{\\text{rms}_{min,i}} > \\text{MONITOR_MIN_SIGMA}, \\] where \\(i\\) is the image for which the measurement is to be forcefully measured. MONITOR_MIN_SIGMA is able to be controlled by the user in the pipeline run configuration file. By default the value is set to 3.0. Tip Setting MONITOR_MIN_SIGMA = 0.0 will ensure that all forced measurements are performed regardless of signal-to-noise. Configuration File Options \u00b6 The following options are present in the pipeline run configuration file to users along with their defaults: MONITOR = False MONITOR_MIN_SIGMA = 3.0 MONITOR_EDGE_BUFFER_SCALE = 1.2 MONITOR_CLUSTER_THRESHOLD = 3.0 MONITOR_ALLOW_NAN = False MONITOR turns forced measurements on ( True ) or off ( False ). MONITOR_MIN_SIGMA controls the the minimum sigma check threshold as explained in Minimum Sigma Filter . MONITOR_EDGE_BUFFER_SCALE controls the size of the buffer from the image edge where forced measurements are not performed ( MONITOR_EDGE_BUFFER_SCALE \\(\\times 3\\theta_{beam}\\) ). An error can sometimes occur that increasing this value can solve. MONITOR_CLUSTER_THRESHOLD is directly passed to the forced photometry package used by the pipeline. It defines the multiple of major_axes to use for identifying clusters. MONITOR_ALLOW_NAN is directly passed to the forced photometry package used by the pipeline. It defines whether NaN values are allowed to be present in the extraction area in the rms or background maps. True would mean that NaN values are allowed. Software - forced_phot \u00b6 The software used to perform the forced measurements, forced_phot , was written by David Kaplan and can be found on the VAST GitHub here .","title":"Forced Measurements"},{"location":"design/monitor/#forced-measurements","text":"This page details the forced measurements obtained by the pipeline.","title":"Forced Measurements"},{"location":"design/monitor/#definition","text":"When MONITOR = True is set in the pipeline run configuration file, any sources that have non-detections in their lightcurve will have these measurements 'filled in' by performing forced measurements . This means that the flux at the source's current average position in the non-detection image is forcefully measured by fitting a Gaussian with the same shape as the respective image restoring beam. Forced measurements are labelled in the measurements table and parquet files by the column forced . Note Forced measurements are local to a pipeline run - they will not appear in any other pipeline run. Warning Forced measurements are not performed within 3 beamwidths of the image edge.","title":"Definition"},{"location":"design/monitor/#minimum-sigma-filter","text":"Before forced measurements are processed, a minimum sigma check is made to make sure that the forced measurements would provide useful information. For example, a dataset may contain an image that has significantly less sensitivity than the other images. In this case a faint source in the more sensitive images will not be expected to be seen in the less sensitive image. To avoid unnecessary computation, this source is not forcefully measured. The check is performed like that which is made in the New Sources process where the signal-to-noise ratio is calculated using the rms \\(_{min}\\) of the image it is to be extracted from. Hence, for a forced measurement to take place the following condition must be met: \\[ \\frac{f_{peak,det}}{\\text{rms}_{min,i}} > \\text{MONITOR_MIN_SIGMA}, \\] where \\(i\\) is the image for which the measurement is to be forcefully measured. MONITOR_MIN_SIGMA is able to be controlled by the user in the pipeline run configuration file. By default the value is set to 3.0. Tip Setting MONITOR_MIN_SIGMA = 0.0 will ensure that all forced measurements are performed regardless of signal-to-noise.","title":"Minimum Sigma Filter"},{"location":"design/monitor/#configuration-file-options","text":"The following options are present in the pipeline run configuration file to users along with their defaults: MONITOR = False MONITOR_MIN_SIGMA = 3.0 MONITOR_EDGE_BUFFER_SCALE = 1.2 MONITOR_CLUSTER_THRESHOLD = 3.0 MONITOR_ALLOW_NAN = False MONITOR turns forced measurements on ( True ) or off ( False ). MONITOR_MIN_SIGMA controls the the minimum sigma check threshold as explained in Minimum Sigma Filter . MONITOR_EDGE_BUFFER_SCALE controls the size of the buffer from the image edge where forced measurements are not performed ( MONITOR_EDGE_BUFFER_SCALE \\(\\times 3\\theta_{beam}\\) ). An error can sometimes occur that increasing this value can solve. MONITOR_CLUSTER_THRESHOLD is directly passed to the forced photometry package used by the pipeline. It defines the multiple of major_axes to use for identifying clusters. MONITOR_ALLOW_NAN is directly passed to the forced photometry package used by the pipeline. It defines whether NaN values are allowed to be present in the extraction area in the rms or background maps. True would mean that NaN values are allowed.","title":"Configuration File Options"},{"location":"design/monitor/#software-forced_phot","text":"The software used to perform the forced measurements, forced_phot , was written by David Kaplan and can be found on the VAST GitHub here .","title":"Software - forced_phot"},{"location":"design/newsources/","text":"New Sources \u00b6 This page details the new source analysis performed by the pipeline. Definition \u00b6 A new source is defined as a source that is detected during the pipeline run that was not detected in any previous epoch of the location of the source, and has a peak flux such that it should have been detected. Note Remember that pipeline runs are self-contained - i.e. a run does not have any knowledge of another run, hence new sources are local to their pipeline run. New Source Process \u00b6 The pipeline identifies new sources by using the following steps: Sources are found that have 'incomplete' light curves, i.e. there are epochs in the pipeline run of the source location where the source is not detected. The would-be 'ideal' coverage is then calculated to determine which images contain the source location but have a non-detection. Remove sources where the epoch of the first detection is also the earliest possible detection epoch. For the remaining sources a general rms check is made to answer the question of should this source be expected to be detected at all in the previous epochs. This is done by taking the minimum \\(\\text{rms}_{min}\\) of all the non-detection images and making sure that \\[ \\frac{f_{peak,det}}{\\text{minimum rms}_{min}} > \\text{NEW_SOURCE_MIN_SIGMA}, \\] where \\(f_{peak,det}\\) is the peak flux density of the first detection of the source. The default value of NEW_SOURCE_MIN_SIGMA is 5.0, but the parameter can be controlled by the user in the pipeline run configuration file. The sources that meet the above criteria are marked as a new source . The new source high sigma value is calculated for all new sources. New Source High Sigma \u00b6 In the process detailed above, the rms check is made against the minimum rms of the previous non-detection images. This might not be an accurate representation of the rms at the source's actual location in the image, for example, the rms might be high at the source location such that a detection of the source wouldn't be expected at the \\(5\\sigma\\) level. To account for this the new source high sigma value is calculated for all new sources. For each non-detection image for a source, the true signal-to-noise ratio the source would have in the non-detection image is calculated, i.e. \\[ \\text{new source true sigma}_i = \\frac{f_{peak,det}}{\\text{rms}_{location,i}} \\] where \\(\\text{rms}_{location,i}\\) is the rms at the source location for each non-detection image, \\(i\\) . The new source high sigma is then equal to the maximum \\(\\text{rms}_{location,i}\\) . The value can be used to filter those new sources which would be borderline detections, or not expected to be detected at all, at the actual location in the previous images. This allows users to concentrate on the significant new sources. Viewing New Sources \u00b6 New sources are marked as new on the website interface (see Source Pages ) and in the source parquet file output there is a boolean column named new .","title":"New Sources"},{"location":"design/newsources/#new-sources","text":"This page details the new source analysis performed by the pipeline.","title":"New Sources"},{"location":"design/newsources/#definition","text":"A new source is defined as a source that is detected during the pipeline run that was not detected in any previous epoch of the location of the source, and has a peak flux such that it should have been detected. Note Remember that pipeline runs are self-contained - i.e. a run does not have any knowledge of another run, hence new sources are local to their pipeline run.","title":"Definition"},{"location":"design/newsources/#new-source-process","text":"The pipeline identifies new sources by using the following steps: Sources are found that have 'incomplete' light curves, i.e. there are epochs in the pipeline run of the source location where the source is not detected. The would-be 'ideal' coverage is then calculated to determine which images contain the source location but have a non-detection. Remove sources where the epoch of the first detection is also the earliest possible detection epoch. For the remaining sources a general rms check is made to answer the question of should this source be expected to be detected at all in the previous epochs. This is done by taking the minimum \\(\\text{rms}_{min}\\) of all the non-detection images and making sure that \\[ \\frac{f_{peak,det}}{\\text{minimum rms}_{min}} > \\text{NEW_SOURCE_MIN_SIGMA}, \\] where \\(f_{peak,det}\\) is the peak flux density of the first detection of the source. The default value of NEW_SOURCE_MIN_SIGMA is 5.0, but the parameter can be controlled by the user in the pipeline run configuration file. The sources that meet the above criteria are marked as a new source . The new source high sigma value is calculated for all new sources.","title":"New Source Process"},{"location":"design/newsources/#new-source-high-sigma","text":"In the process detailed above, the rms check is made against the minimum rms of the previous non-detection images. This might not be an accurate representation of the rms at the source's actual location in the image, for example, the rms might be high at the source location such that a detection of the source wouldn't be expected at the \\(5\\sigma\\) level. To account for this the new source high sigma value is calculated for all new sources. For each non-detection image for a source, the true signal-to-noise ratio the source would have in the non-detection image is calculated, i.e. \\[ \\text{new source true sigma}_i = \\frac{f_{peak,det}}{\\text{rms}_{location,i}} \\] where \\(\\text{rms}_{location,i}\\) is the rms at the source location for each non-detection image, \\(i\\) . The new source high sigma is then equal to the maximum \\(\\text{rms}_{location,i}\\) . The value can be used to filter those new sources which would be borderline detections, or not expected to be detected at all, at the actual location in the previous images. This allows users to concentrate on the significant new sources.","title":"New Source High Sigma"},{"location":"design/newsources/#viewing-new-sources","text":"New sources are marked as new on the website interface (see Source Pages ) and in the source parquet file output there is a boolean column named new .","title":"Viewing New Sources"},{"location":"design/overview/","text":"Pipeline Steps Overview \u00b6 This page gives an overview of the processing steps of a pipeline run. Each section contains a link to feature page that contains more details. Terminology \u00b6 Run A single pipeline dataset defined by a configuration file. Image A FITS image that is being processed as part of a pipeline run. It also has related inputs of the selavy source catalogue, and the noise and background images also produced by selavy. Measurement An extracted measurement read from the selavy source catalogue from an associated image. The only measurements produced by the pipeline are forced measurements which are performed when monitoring is used. Source A group of measurements that have been identified as the same astrophysical source by a pipeline association method. Pipeline Processing Steps \u00b6 Note Each pipeline run is self-aware only, which means that each run does not draw on the results of other runs. However, since images and their measurements don't change, subsequent runs that use any image that was ingested as part of a previous run will not be ingested again. 1. Image & Selavy Catalogue Ingest \u00b6 Full details: Image & Selavy Catalogue Ingest . The first stage of the pipeline is to read and ingest to the database the input data that has been provided in the configuration file. This includes determing statistics about the image footprint and properties, and also importing and cleaning the associated measurements from the selavy file. The errors on the measurements can also be recalculated at this stage based upon the Condon (1997) method. Image uniqueness is determined by the filename, and once the image is ingested, it is available for other pipeline runs to use without having to re-ingest. 2. Source Association \u00b6 Full details: Source Association . Once all images and measurments have been ingested the source association step is performed, where measurements over time are associated to a unique astrophysical source. Images are arranged chronologically and association is performed on an image by image basis, or as a grouped \"epoch\" if epoch based association is used. The association is performed as per the settings entered in the run configuration file. 3. Ideal Coverage & New Source Analysis \u00b6 Full details: New Sources . With the measurements associated the sources are analysed to check for non-detections over time and whether the source should have been seen in any non-detection images. The ideal coverage calculation is also used to determine any sources that should be marked as new , i.e. a source that has appeared over time that was not detected in the first image of its location on the sky. The non-detections are then passed to the forced monitoring step. 4. Monitoring Forced Measurements \u00b6 Full details: Forced Measurements . This step is optional. The non-detections which form gaps in the lightcurves of each source are filled in by forcefully extracting a flux measurement at the location of the source. 5. Source Statistics Calculation \u00b6 Full details: Source Statistics . Statistics are calculated for each source such as the weighted average sky position, average flux values, variability metrics (including two-epoch pair metrics) and various counts. 6. Database Upload \u00b6 All the results from the pipeline run are uploaded to the database. Specifically at the end of the run the following is written to the database: Sources and their statistics. Relations between sources. Associations. Measurement two-epoch pairs. For large runs this can be a substantional component of the pipeline run time. Bulk upload statements will be seen in the pipeline run log file such as these shown below: 2021-03-11 13:00:04,893 loading INFO Bulk created #557 Source 2021-03-11 13:00:04,910 loading INFO Populate \"related\" field of sources... 2021-03-11 13:00:04,919 loading INFO Bulk created #29 RelatedSource 2021-03-11 13:00:04,943 loading INFO Upload associations... 2021-03-11 13:00:05,650 loading INFO Bulk created #3276 Association 2021-03-11 13:00:08,374 loading INFO Bulk created #10000 MeasurementPair 2021-03-11 13:00:08,663 loading INFO Bulk created #1150 MeasurementPair","title":"Pipeline Steps Overview"},{"location":"design/overview/#pipeline-steps-overview","text":"This page gives an overview of the processing steps of a pipeline run. Each section contains a link to feature page that contains more details.","title":"Pipeline Steps Overview"},{"location":"design/overview/#terminology","text":"Run A single pipeline dataset defined by a configuration file. Image A FITS image that is being processed as part of a pipeline run. It also has related inputs of the selavy source catalogue, and the noise and background images also produced by selavy. Measurement An extracted measurement read from the selavy source catalogue from an associated image. The only measurements produced by the pipeline are forced measurements which are performed when monitoring is used. Source A group of measurements that have been identified as the same astrophysical source by a pipeline association method.","title":"Terminology"},{"location":"design/overview/#pipeline-processing-steps","text":"Note Each pipeline run is self-aware only, which means that each run does not draw on the results of other runs. However, since images and their measurements don't change, subsequent runs that use any image that was ingested as part of a previous run will not be ingested again.","title":"Pipeline Processing Steps"},{"location":"design/overview/#1-image-selavy-catalogue-ingest","text":"Full details: Image & Selavy Catalogue Ingest . The first stage of the pipeline is to read and ingest to the database the input data that has been provided in the configuration file. This includes determing statistics about the image footprint and properties, and also importing and cleaning the associated measurements from the selavy file. The errors on the measurements can also be recalculated at this stage based upon the Condon (1997) method. Image uniqueness is determined by the filename, and once the image is ingested, it is available for other pipeline runs to use without having to re-ingest.","title":"1. Image &amp; Selavy Catalogue Ingest"},{"location":"design/overview/#2-source-association","text":"Full details: Source Association . Once all images and measurments have been ingested the source association step is performed, where measurements over time are associated to a unique astrophysical source. Images are arranged chronologically and association is performed on an image by image basis, or as a grouped \"epoch\" if epoch based association is used. The association is performed as per the settings entered in the run configuration file.","title":"2. Source Association"},{"location":"design/overview/#3-ideal-coverage-new-source-analysis","text":"Full details: New Sources . With the measurements associated the sources are analysed to check for non-detections over time and whether the source should have been seen in any non-detection images. The ideal coverage calculation is also used to determine any sources that should be marked as new , i.e. a source that has appeared over time that was not detected in the first image of its location on the sky. The non-detections are then passed to the forced monitoring step.","title":"3. Ideal Coverage &amp; New Source Analysis"},{"location":"design/overview/#4-monitoring-forced-measurements","text":"Full details: Forced Measurements . This step is optional. The non-detections which form gaps in the lightcurves of each source are filled in by forcefully extracting a flux measurement at the location of the source.","title":"4. Monitoring Forced Measurements"},{"location":"design/overview/#5-source-statistics-calculation","text":"Full details: Source Statistics . Statistics are calculated for each source such as the weighted average sky position, average flux values, variability metrics (including two-epoch pair metrics) and various counts.","title":"5. Source Statistics Calculation"},{"location":"design/overview/#6-database-upload","text":"All the results from the pipeline run are uploaded to the database. Specifically at the end of the run the following is written to the database: Sources and their statistics. Relations between sources. Associations. Measurement two-epoch pairs. For large runs this can be a substantional component of the pipeline run time. Bulk upload statements will be seen in the pipeline run log file such as these shown below: 2021-03-11 13:00:04,893 loading INFO Bulk created #557 Source 2021-03-11 13:00:04,910 loading INFO Populate \"related\" field of sources... 2021-03-11 13:00:04,919 loading INFO Bulk created #29 RelatedSource 2021-03-11 13:00:04,943 loading INFO Upload associations... 2021-03-11 13:00:05,650 loading INFO Bulk created #3276 Association 2021-03-11 13:00:08,374 loading INFO Bulk created #10000 MeasurementPair 2021-03-11 13:00:08,663 loading INFO Bulk created #1150 MeasurementPair","title":"6. Database Upload"},{"location":"design/sourcestats/","text":"Source Statistics \u00b6 This page details the source statistics that are calculated by the pipeline. Overview \u00b6 The table below provides a summary of all the statistic and counts provided by the pipeline. See the Variability Statistics section for the table containing the variability metrics. Note Remember that all source statistics and counts are calculated from the individual measurements that are associated with the source. Parameter Includes Forced Meas. Description wavg_ra No The weighted average of the Right Ascension, degrees. wavg_dec No The weighted average of the Declination, degrees. wavg_uncertainty_ew No The weighted average uncertainty in the east-west (RA) direction, degrees. wavg_uncertainty_ns No The weighted average uncertainty in the north-south (Dec) direction, degrees. avg_flux_int Yes The average integrated flux, mJy. max_flux_int Yes The maximum integrated flux value, mJy. min_flux_int Yes The minimum integrated flux value, mJy. avg_flux_peak Yes The average peak flux, mJy/beam. max_flux_peak Yes The maximum peak flux value, mJy/beam. min_flux_peak Yes The minimum peak flux value, mJy/beam. min_flux_int_isl_ratio Yes The minimum integrated flux value island ratio (int_flux / total_isl_int_flux). min_flux_peak_isl_ratio Yes The minimum peak flux value island ratio (peak_flux / total_isl_peak_flux). avg_compactness No The average compactness of the source (compactness is defined by int_flux / peak_flux). min_snr No The minimum signal-to-noise ratio of the source. max_snr No The maximum signal-to-noise ratio of the source. n_neighbour_dist n/a On sky separation distance to the nearest neighbour within the same run, degrees (arcmin on webserver). new_high_sigma n/a The largest sigma value a new source would have if it was placed at its location in the previous images it was not detected in. See New Sources for more information. New sources only. n_meas Yes The total number of measurements associated to the source. Named Total Datapoints on the webserver. n_meas_sel No The total number of selavy measurements associated to the source. Named Selavy Datapoints on the webserver. n_meas_forced Yes The total number of forced measurements associated to the source. Named Forced Datapoints on the webserver. n_rel n/a The total number of relations the source has. See Source Association . Named Relations on the webserver. n_sibl n/a The total number measurements that has a sibling. On the webserver tables this is firstly presented as a boolean column of if the source contains measurements that have a sibling. Variability Statistics \u00b6 Below is a table describing the variability metrics of the source. See the following sections for further explanation of these metrics. Parameter Includes Forced Meas. Description v_int Yes The \\(V\\) metric for the integrated flux. v_peak Yes The \\(V\\) metric for the peak flux. eta_int Yes The \\(\\eta\\) metric for the integrated flux. eta_peak Yes The \\(\\eta\\) metric for the peak flux. vs_abs_significant_max_int Yes The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. m_abs_significant_max_int Yes The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. vs_abs_significant_max_peak Yes The \\(\\mid V_s \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. m_abs_significant_max_peak Yes The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. V and \u03b7 Metrics \u00b6 The \\(V\\) and \\(\\eta\\) metrics are the same as those used by the LOFAR Transients Pipeline (TraP) , for a complete description please refer to Swinbank et al. (2015) . In the VAST Pipeline, the metrics are calculated twice, for both the integrated and peak fluxes. \\(V\\) is the proportional flux variability of the source and is given by the ratio of the sample standard deviation ( \\(s\\) ) and mean of the flux, \\(I\\) : \\[ V = \\frac{s}{\\overline{I}} = \\frac{1}{\\overline{I}} \\sqrt{\\frac{N}{N - 1}\\left(\\overline{I^{2}}-\\overline{I}^{2}\\right)}. \\] The \\(\\eta\\) value is the significance of the variability, based on \\(\\chi^{2}\\) statistics, and is given by: \\[ \\eta = \\frac{N}{N - 1}\\left(\\overline{wI^{2}} - \\frac{\\overline{wI}^{2}}{\\overline{w}}\\right) \\] where \\(w\\) is the uncertainty ( \\(e\\) ) in \\(I\\) of a measurement, and is given by \\(w=\\frac{1}{e}\\) . Two-Epoch Metrics \u00b6 Alternative variability metrics, \\(V_s\\) and \\(m\\) , are also calculated which we refer to as the 'two-epoch metrics'. They are calculated for each unique pair of measurements assoicated with the source, with the most significant pair of values attached to the source (see section below). Please refer to Mooley et al. (2016) for further details. Note All the two-epoch pair \\(V_s\\) and \\(m\\) values for a run are saved in the output file measurement_pairs.parquet for offline analysis. \\(V_s\\) is a statistic to compare the flux densities of a source between two-epochs and is given by: \\[ V_s = \\frac{\\Delta S}{\\sigma} = \\frac{S_1 - S_2}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{2}^{2}}} \\] where \\(S\\) is the flux and \\(\\sigma\\) is the associated error. This metric is known to follow a Student-t distribution. Typically, in the literature, a source is defined as variable if this parameter is beyond the 95% confidence interval, i.e.: \\[ \\mid V_s \\mid \\geq 4.3. \\] \\(m\\) is a moduluation index variable given by: \\[ m = \\frac{\\Delta S}{\\overline{S}} \\] where \\(\\overline{S}\\) is the mean of the flux densities \\(S_1\\) and \\(S_2\\) . Typically, in the literature, the threshold for this value for a source to be considered variable is: \\[ \\mid m \\mid \\gt 0.26, \\] which equates to a variability of 30%. However the user is free to set their own level to define variablity. Significant Source Values \u00b6 The \\(V_s\\) and \\(m\\) metrics of the 'maximum signficant pair' is attached to the source. The maximum significant pair is determind by selecting the most significant \\(\\mid m \\mid\\) value given a minimum \\(V_s\\) threshold which is defined in the pipeline configuration file SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS . By default this value is set to 4.3. For example, if a source with three associatied measurements gave the following pair metrics: Pair \\(\\mid V_s \\mid\\) \\(\\mid m \\mid\\) A-B 4.5 0.1 B-C 2.5 0.05 A-C 4.3 0.4 then the A-C pair metrics are attached to the source as the most significant. This can be used to quickly determine significant two-epoch variability for a source. If there are no pair values above the minimum \\(V_s\\) threshold then these values attached to the source will be 0. The measurement_pairs.parquet file can be used to manually explore the measurement pairs if one wishes to lower the threshold.","title":"Source Statistics"},{"location":"design/sourcestats/#source-statistics","text":"This page details the source statistics that are calculated by the pipeline.","title":"Source Statistics"},{"location":"design/sourcestats/#overview","text":"The table below provides a summary of all the statistic and counts provided by the pipeline. See the Variability Statistics section for the table containing the variability metrics. Note Remember that all source statistics and counts are calculated from the individual measurements that are associated with the source. Parameter Includes Forced Meas. Description wavg_ra No The weighted average of the Right Ascension, degrees. wavg_dec No The weighted average of the Declination, degrees. wavg_uncertainty_ew No The weighted average uncertainty in the east-west (RA) direction, degrees. wavg_uncertainty_ns No The weighted average uncertainty in the north-south (Dec) direction, degrees. avg_flux_int Yes The average integrated flux, mJy. max_flux_int Yes The maximum integrated flux value, mJy. min_flux_int Yes The minimum integrated flux value, mJy. avg_flux_peak Yes The average peak flux, mJy/beam. max_flux_peak Yes The maximum peak flux value, mJy/beam. min_flux_peak Yes The minimum peak flux value, mJy/beam. min_flux_int_isl_ratio Yes The minimum integrated flux value island ratio (int_flux / total_isl_int_flux). min_flux_peak_isl_ratio Yes The minimum peak flux value island ratio (peak_flux / total_isl_peak_flux). avg_compactness No The average compactness of the source (compactness is defined by int_flux / peak_flux). min_snr No The minimum signal-to-noise ratio of the source. max_snr No The maximum signal-to-noise ratio of the source. n_neighbour_dist n/a On sky separation distance to the nearest neighbour within the same run, degrees (arcmin on webserver). new_high_sigma n/a The largest sigma value a new source would have if it was placed at its location in the previous images it was not detected in. See New Sources for more information. New sources only. n_meas Yes The total number of measurements associated to the source. Named Total Datapoints on the webserver. n_meas_sel No The total number of selavy measurements associated to the source. Named Selavy Datapoints on the webserver. n_meas_forced Yes The total number of forced measurements associated to the source. Named Forced Datapoints on the webserver. n_rel n/a The total number of relations the source has. See Source Association . Named Relations on the webserver. n_sibl n/a The total number measurements that has a sibling. On the webserver tables this is firstly presented as a boolean column of if the source contains measurements that have a sibling.","title":"Overview"},{"location":"design/sourcestats/#variability-statistics","text":"Below is a table describing the variability metrics of the source. See the following sections for further explanation of these metrics. Parameter Includes Forced Meas. Description v_int Yes The \\(V\\) metric for the integrated flux. v_peak Yes The \\(V\\) metric for the peak flux. eta_int Yes The \\(\\eta\\) metric for the integrated flux. eta_peak Yes The \\(\\eta\\) metric for the peak flux. vs_abs_significant_max_int Yes The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. m_abs_significant_max_int Yes The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. vs_abs_significant_max_peak Yes The \\(\\mid V_s \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. m_abs_significant_max_peak Yes The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair.","title":"Variability Statistics"},{"location":"design/sourcestats/#v-and-metrics","text":"The \\(V\\) and \\(\\eta\\) metrics are the same as those used by the LOFAR Transients Pipeline (TraP) , for a complete description please refer to Swinbank et al. (2015) . In the VAST Pipeline, the metrics are calculated twice, for both the integrated and peak fluxes. \\(V\\) is the proportional flux variability of the source and is given by the ratio of the sample standard deviation ( \\(s\\) ) and mean of the flux, \\(I\\) : \\[ V = \\frac{s}{\\overline{I}} = \\frac{1}{\\overline{I}} \\sqrt{\\frac{N}{N - 1}\\left(\\overline{I^{2}}-\\overline{I}^{2}\\right)}. \\] The \\(\\eta\\) value is the significance of the variability, based on \\(\\chi^{2}\\) statistics, and is given by: \\[ \\eta = \\frac{N}{N - 1}\\left(\\overline{wI^{2}} - \\frac{\\overline{wI}^{2}}{\\overline{w}}\\right) \\] where \\(w\\) is the uncertainty ( \\(e\\) ) in \\(I\\) of a measurement, and is given by \\(w=\\frac{1}{e}\\) .","title":"V and \u03b7 Metrics"},{"location":"design/sourcestats/#two-epoch-metrics","text":"Alternative variability metrics, \\(V_s\\) and \\(m\\) , are also calculated which we refer to as the 'two-epoch metrics'. They are calculated for each unique pair of measurements assoicated with the source, with the most significant pair of values attached to the source (see section below). Please refer to Mooley et al. (2016) for further details. Note All the two-epoch pair \\(V_s\\) and \\(m\\) values for a run are saved in the output file measurement_pairs.parquet for offline analysis. \\(V_s\\) is a statistic to compare the flux densities of a source between two-epochs and is given by: \\[ V_s = \\frac{\\Delta S}{\\sigma} = \\frac{S_1 - S_2}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{2}^{2}}} \\] where \\(S\\) is the flux and \\(\\sigma\\) is the associated error. This metric is known to follow a Student-t distribution. Typically, in the literature, a source is defined as variable if this parameter is beyond the 95% confidence interval, i.e.: \\[ \\mid V_s \\mid \\geq 4.3. \\] \\(m\\) is a moduluation index variable given by: \\[ m = \\frac{\\Delta S}{\\overline{S}} \\] where \\(\\overline{S}\\) is the mean of the flux densities \\(S_1\\) and \\(S_2\\) . Typically, in the literature, the threshold for this value for a source to be considered variable is: \\[ \\mid m \\mid \\gt 0.26, \\] which equates to a variability of 30%. However the user is free to set their own level to define variablity.","title":"Two-Epoch Metrics"},{"location":"design/sourcestats/#significant-source-values","text":"The \\(V_s\\) and \\(m\\) metrics of the 'maximum signficant pair' is attached to the source. The maximum significant pair is determind by selecting the most significant \\(\\mid m \\mid\\) value given a minimum \\(V_s\\) threshold which is defined in the pipeline configuration file SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS . By default this value is set to 4.3. For example, if a source with three associatied measurements gave the following pair metrics: Pair \\(\\mid V_s \\mid\\) \\(\\mid m \\mid\\) A-B 4.5 0.1 B-C 2.5 0.05 A-C 4.3 0.4 then the A-C pair metrics are attached to the source as the most significant. This can be used to quickly determine significant two-epoch variability for a source. If there are no pair values above the minimum \\(V_s\\) threshold then these values attached to the source will be 0. The measurement_pairs.parquet file can be used to manually explore the measurement pairs if one wishes to lower the threshold.","title":"Significant Source Values"},{"location":"developing/docsdev/","text":"Development Guidelines for Documentation \u00b6 The pipeline documentation has been developed using the python package mkdocs and the material theme . It is published as a static website using GitHub pages. Documentation development server \u00b6 This section describes how to set up a development server to live reload your changes to the pipeline documentation. The main code of the documentation is under the docs folder, with fews hacks: in order to keep the repository README.md , CHANGELOG.md , LICENSE.txt and CODE_OF_CONDUCT.md on the root path, relative soft links have been created under the docs folder: architecture changelog.md -> ../CHANGELOG.md code_of_conduct.md -> ../CODE_OF_CONDUCT.md developing img license.md -> ../LICENSE.txt quickstart README.md -> ../README.md theme usage Start the development server: ( pipeline_env ) $ mkdocs serve And start to modify/add the documentation by changing the markdown files under the docs folder. The structure of the site (see nav section) and the settings are in the mkdocs.yml in the root of the repository. mkdocs and the material theme have a lot of customizations and plugins, so please refer to their documentation. Python docstrings and source code can be rendered using the mkdocstrings plugin: for example rendering models.py functions docstrings can be done with: ::: vast_pipeline.models More details on how to display the source code and additional tweaks can be found in mkdocstrings documentation . Deployment to GitHub pages \u00b6 Automatic deployment to GitHub pages is set up using GitHub actions and workflows. See source code under the .github folder.","title":"Docs"},{"location":"developing/docsdev/#development-guidelines-for-documentation","text":"The pipeline documentation has been developed using the python package mkdocs and the material theme . It is published as a static website using GitHub pages.","title":"Development Guidelines for Documentation"},{"location":"developing/docsdev/#documentation-development-server","text":"This section describes how to set up a development server to live reload your changes to the pipeline documentation. The main code of the documentation is under the docs folder, with fews hacks: in order to keep the repository README.md , CHANGELOG.md , LICENSE.txt and CODE_OF_CONDUCT.md on the root path, relative soft links have been created under the docs folder: architecture changelog.md -> ../CHANGELOG.md code_of_conduct.md -> ../CODE_OF_CONDUCT.md developing img license.md -> ../LICENSE.txt quickstart README.md -> ../README.md theme usage Start the development server: ( pipeline_env ) $ mkdocs serve And start to modify/add the documentation by changing the markdown files under the docs folder. The structure of the site (see nav section) and the settings are in the mkdocs.yml in the root of the repository. mkdocs and the material theme have a lot of customizations and plugins, so please refer to their documentation. Python docstrings and source code can be rendered using the mkdocstrings plugin: for example rendering models.py functions docstrings can be done with: ::: vast_pipeline.models More details on how to display the source code and additional tweaks can be found in mkdocstrings documentation .","title":"Documentation development server"},{"location":"developing/docsdev/#deployment-to-github-pages","text":"Automatic deployment to GitHub pages is set up using GitHub actions and workflows. See source code under the .github folder.","title":"Deployment to GitHub pages"},{"location":"developing/github/","text":"GitHub platform guidelines \u00b6 This section explains how to interact with GitHub platform for opening Issues and Pull Requests, and some notes how to make a release of the pipeline if you are a mantainer of the code base. Issues Guidelines \u00b6 Please search for a similar issue before opening a new one by run a search on the issue page with specific key-words. When opening a new issue, please specify the issue type (e.g. bug, feature, etc.) and provide a detailed description with use cases if applied. Pull Request Guideline \u00b6 Opening a PR \u00b6 First consider . . . search among the issues for similar problems/bugs/etc opening an issue before creating/issuing the PR. So we can separate problems from solutions. Steps to issue a pull request: Open an issue (e.g. My issue blah , GitHub will assign a id e.g. #123 ). Branch off master by naming your branch fix-#123-my-issue-blah (keep it short please). Do your changes. Run test locally with ./manage.py test vast_pipeline (see the complete guide on the test for more details). Run the webserver and check the functionality. Commit and issue the PR. Assign the review to one or more reviewers if none are assigned by default. Warning PRs not branched off master will be rejected !. Pull Request Review Guidelines \u00b6 The guidelines to dealing with reviews and conversations on GitHub are essentially: Be nice with the review and do not offend the author of the PR: Nobody is a perfect developer or born so! The reviewers will in general mark the conversation as \"resolved\" (e.g. he/she is satisfied with the answer from the PR author). The PR author will re-request the review by clicking on the on the top right corner and might ping the reviewer on a comment if necessary with @github_name . When the PR is approved by at least one reviewer you might want to merge it to master (you should have that privileges), unless you want to make sure that such PR is reviewed by another reviewer (e.g. you are doing big changes or important changes or you want to make sure that other person is aware/updated about the changes in that PR). Releasing Guidelines \u00b6 In to order to make a release, please follow these steps (example: making the 0.1.0 release): Make sure that every new feature and PR will be merged to master, before continuing with the releasing process. Update the CHANGELOG.md on master directly (only admin can and need to force-push the changes) with the list of changes. An example of format can be found here . The 0.2.X branch will be updated by merging master into 0.2.X . Branch off 0.2.X and call it 0.2.1 , then change the package.json with the version of the release, commit and tag the commit. Push commit and tag to origin. Make a release in GitHub using that tag. NOTE : keep the version on master branch to something like 99.99.99dev and in 0.2.X branch to something like 0.2.99dev. In the release branch, change only the version in package.json .","title":"GitHub Platform"},{"location":"developing/github/#github-platform-guidelines","text":"This section explains how to interact with GitHub platform for opening Issues and Pull Requests, and some notes how to make a release of the pipeline if you are a mantainer of the code base.","title":"GitHub platform guidelines"},{"location":"developing/github/#issues-guidelines","text":"Please search for a similar issue before opening a new one by run a search on the issue page with specific key-words. When opening a new issue, please specify the issue type (e.g. bug, feature, etc.) and provide a detailed description with use cases if applied.","title":"Issues Guidelines"},{"location":"developing/github/#pull-request-guideline","text":"","title":"Pull Request Guideline"},{"location":"developing/github/#opening-a-pr","text":"First consider . . . search among the issues for similar problems/bugs/etc opening an issue before creating/issuing the PR. So we can separate problems from solutions. Steps to issue a pull request: Open an issue (e.g. My issue blah , GitHub will assign a id e.g. #123 ). Branch off master by naming your branch fix-#123-my-issue-blah (keep it short please). Do your changes. Run test locally with ./manage.py test vast_pipeline (see the complete guide on the test for more details). Run the webserver and check the functionality. Commit and issue the PR. Assign the review to one or more reviewers if none are assigned by default. Warning PRs not branched off master will be rejected !.","title":"Opening a PR"},{"location":"developing/github/#pull-request-review-guidelines","text":"The guidelines to dealing with reviews and conversations on GitHub are essentially: Be nice with the review and do not offend the author of the PR: Nobody is a perfect developer or born so! The reviewers will in general mark the conversation as \"resolved\" (e.g. he/she is satisfied with the answer from the PR author). The PR author will re-request the review by clicking on the on the top right corner and might ping the reviewer on a comment if necessary with @github_name . When the PR is approved by at least one reviewer you might want to merge it to master (you should have that privileges), unless you want to make sure that such PR is reviewed by another reviewer (e.g. you are doing big changes or important changes or you want to make sure that other person is aware/updated about the changes in that PR).","title":"Pull Request Review Guidelines"},{"location":"developing/github/#releasing-guidelines","text":"In to order to make a release, please follow these steps (example: making the 0.1.0 release): Make sure that every new feature and PR will be merged to master, before continuing with the releasing process. Update the CHANGELOG.md on master directly (only admin can and need to force-push the changes) with the list of changes. An example of format can be found here . The 0.2.X branch will be updated by merging master into 0.2.X . Branch off 0.2.X and call it 0.2.1 , then change the package.json with the version of the release, commit and tag the commit. Push commit and tag to origin. Make a release in GitHub using that tag. NOTE : keep the version on master branch to something like 99.99.99dev and in 0.2.X branch to something like 0.2.99dev. In the release branch, change only the version in package.json .","title":"Releasing Guidelines"},{"location":"developing/intro/","text":"Contributing and Developing Guidelines \u00b6 This section explains how to contribute to the project code base and collaborate on GitHub platform. A very exahustive set of general guidelines can be follow here , but I think the following will suffice for our purpose. The next sections describes: GitHub Platform Local Development Enviroment Tests Docs Bechmarks Terminology \u00b6 These below is the terminology used to identify pipeline objects. Pipeline run (or \"Run\") -> Pipeline run instance, also referred as run, p_run, piperun, pipe_run, ... in the code Measurement -> the extracted measurement from the source finder of a single astrophysical source from an image, referred in the code as measurement(s), meas, ... Source -> A collection of single measurements for the same astrophysical source, referred as src, source, ... in the code","title":"Intro"},{"location":"developing/intro/#contributing-and-developing-guidelines","text":"This section explains how to contribute to the project code base and collaborate on GitHub platform. A very exahustive set of general guidelines can be follow here , but I think the following will suffice for our purpose. The next sections describes: GitHub Platform Local Development Enviroment Tests Docs Bechmarks","title":"Contributing and Developing Guidelines"},{"location":"developing/intro/#terminology","text":"These below is the terminology used to identify pipeline objects. Pipeline run (or \"Run\") -> Pipeline run instance, also referred as run, p_run, piperun, pipe_run, ... in the code Measurement -> the extracted measurement from the source finder of a single astrophysical source from an image, referred in the code as measurement(s), meas, ... Source -> A collection of single measurements for the same astrophysical source, referred as src, source, ... in the code","title":"Terminology"},{"location":"developing/localdevenv/","text":"Pipeline Local Development Environment Guidelines \u00b6 This section describes how to set up a local development environment more in details. Back End \u00b6 Installation \u00b6 The installation instructions are the same as the ones describes in the quickstart section with one key difference. Rather than installing the Python dependencies with pip, you will need to install and use Poetry . After installing Poetry, running the command below will install the pipeline dependencies defined in poetry.lock into a virtual environment. The main difference between using Poetry and pip is that pip will only install the dependencies necessary for using the pipeline, whereas Poetry will also install development dependencies required for contributing (e.g. tools to build the documentation). poetry install Note Poetry will automatically create a virtual environment if it detects that your shell isn't currently using one. This should be fine for most users. If you prefer to use an alternative virtual environment manager (e.g. Miniconda), you can prevent Poetry from creating virtual environments . However, even if you are using something like Miniconda, allowing Poetry to manage the virtualenv (the default behaviour) is fine. The development team only uses this option during our automated testing since our test runner machines only contain a single Python environment so using virtualenvs is redundant. Solving your models.py /migrations issues \u00b6 First of all the makemigrations command must be run only if you modified the models.py or you pull down changes (i.e. git pull ) that modify models.py . So please refer to the cases below: 1. You modify models.py \u00b6 In this case there are 2 situations that arise: You currently don't have a production environment and/or you are comfortable in dropping all the data in the database. In this case, you don't need to update the production environment with multiple migration files even though Django docs promote making as many migrations as you need to ( see Django migration guidelines ). For such reasons please consider doing the following steps: 1.1. Make your changes to models.py . 1.2. Remove all the migration files including 0002_q3c.py . 1.3. Run ./manage.py makemigrations . This will generate a NEW 0001_initial.py file. 1.4. Commit the new migraton file 0001_initial.py as well as models.py within a single commit, and add an appropriate message (e.g. add field X to model Y). DO NOT commit the removal of 0002_q3c.py . 1.5. Get back the Q3C migration file git checkout pipeline/migrations/0002_q3c.py . You currently have a production environment and/or you do not want to lose all the data in the database. In this situation, you need to be careful not to mess up with the producation database, so please consider doing the following steps: 2.1. Make a copy (\"dump\") of the production database as it is, e.g. (by logging remotely to the server) pg_dump -h DBHOST_PROD -p DBPORT_PROD -U DBUSER_PROD -Fc -o DBNAME_PROD > prod-backup-$(date +\"%F_%H%M\").sql . 2.2. Upload the copy to your local development database, e.g. pg_restore -h DBHOST_DEV -p DBPORT_DEV --verbose --clean --no-acl --no-owner -U DBUSER_DEV -d DBNAME_DEV prod-backup.sql . 2.3. Make your changes to models.py . 2.4. Run ./manage.py makemigrations with optional but strongly recommended -n 'my_migration_name' . This will generate a new migration file 000X_my_migration_name.py where X is incremented by 1 with respect the last migration file. 2.5. Commit the new migraton file 000X_my_migration_name.py as well as models.py within a single commit, and add an appropriate message (e.g. add field X to model Y) Warning Do not modify the 0002_q3c.py file as it relates to migration operations for using Q3C plugin and related functions! 2. Someone else modified models.py and you pull the changes \u00b6 Situation: ~/vast-pipeline [master]$ git fetch && git pull Updating abc123..321cba Fast-forward pipeline/models.py | 4 +++- pipeline/migrations/0001_initial.py | 5 ++++- 2 file changed, 9 insertions(+), 2 deletions(-) You realise that you are in this situation when: In the command above you see changes (i.e. + or - ) in models.py and/or in migrations (i.e. XXXX_some_migration.py ) Running the webserver, a message reports You have unapplied migrations; your app may not work properly until they are applied. Run 'python manage.py migrate' to apply them. Running the pipeline you have errors related to the database models Solutions to such scenario: If you don't mind losing all the data in your database just follow the Reset the database instructions to drop all the data. But if you want to keep your data, you have to fix these changes by trying to run makemigrations and migrate . But ideally you should follow the following steps: Identify the previous commit before pulling the changes (when your migration and model files were working): ~/vast-pipeline [master]$ git show -1 pipeline/models.py #OR ~/vast-pipeline [master]$ git show -2 pipeline/models.py #OR ~/vast-pipeline [master]$ git show -1 pipeline/migrations/XXXX_my_migration.py Or even better ~/vast-pipeline [master]$ git log -p pipeline/models.py Take note of the commit hash of the old changes (i.e. before pulling down the new changes). Checkout ONLY your old migration files, for example like this: ~/vast-pipeline [master]$ git checkout 37cabac84785742437927c785b63a767aa8ac5ff pipeline/migrations/0001_initial.py Make the migrations ./manage.py makemigrations && ./manage.py migrate Run the pipeline and the webserver to see that everything is working fine Squash the migrations using Django migration guidelines Continue with the normal development cycle (i.e. branch off master, do changes, commit everything, including your changes in the models/migrations even done with the squashing! ) Removing/Clearing Data \u00b6 The following sub-sections show how to completely drop every data in the database and how to remove only the data related to one or more pipeline runs. Reset the database \u00b6 Make sure you installed the requirements dev.txt . And django_extensions is in EXTRA_APPS in your setting configuration file .env (e.g. EXTRA_APPS=django_extensions,another_app,... ). ( pipeline_env ) $ ./manage.py reset_db && ./manage.py migrate # use the following for no confirmation prompt ( pipeline_env ) $ ./manage.py reset_db --noinput && ./manage.py migrate Clearing Run Data \u00b6 It is sometimes convenient to remove the data belonging to one or more pipeline runs while developing the code base. This is particularly useful to save time by not having to re-upload the image data along with the measurements. The data related to the pipeline are the Sources, Associations, Forced extractions entries in database and the parquet files in the respective folder. By default the command will keep the run folder with the config and the log files. ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run To clear more than one run: ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run1 my-pipe-run2 path/to/my-pipe-run3 The command accept both a path or a name of the pipeline run(s). To remove all the runs, issue: ( pipeline_env ) $ ./manage.py clearpiperun clearall The command to keep the parquet files is: ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run --keep-parquet The remove completely the pipeline folder ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run --remove-all FrontEnd Assets Management and Guidelines \u00b6 This guide explain the installation, compilation and development of the front end assets (HTML, CSS, JS and relative modules). We make use of a node installation with npm and gulp tasks to build the front end assets. Installation of node packages \u00b6 After installing a node version and npm , install the node modules using from the base folder ( vast-pipeline ): npm ci Npm will install the node packages in a node_modules folder under the main root. ... \u251c\u2500\u2500 node_modules ... For installing future additional dependecies you can run npm install --save my-package or npm install --save-dev my-dev-package (to save a development module), and after that commit both package.json and package-lock.json files. For details about the installed packages and npm scripts see package.json . FrontEnd Tasks with gulp \u00b6 Using gulp and npm scripts you can: Install dependencies under the ./static/vendor folder. Building (e.g. minify/uglify) CSS and/or Javascript files. Run a developement server that \"hot-reload\" your web page when any HTML, CSS or Javascript file is modified. The command to list all the gulp \"tasks\" and sub-tasks is (you might need gulp-cli installed globally, i.e. npm i --global gulp-cli , more info here ): gulp --tasks Output: [11:55:30] Tasks for ~/PATH/TO/REPO/vast-pipeline/gulpfile.js [11:55:30] \u251c\u2500\u2500 clean [11:55:30] \u251c\u2500\u252c js9 [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u251c\u2500\u2500 css [11:55:30] \u251c\u2500\u2500 js [11:55:30] \u251c\u2500\u252c vendor [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u251c\u2500\u252c build [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u251c\u2500\u252c watch [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 watchFiles [11:55:30] \u2502 \u2514\u2500\u2500 browserSync [11:55:30] \u251c\u2500\u252c default [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u2514\u2500\u2500 debug Alternatively you can run gulp from the installed version in the node_modules folder with: ./node_modules/.bin/gulp --tasks For further details about tasks, see gulpfile . 1. Install Dependencies under vendor Folder \u00b6 Install the dependencies under the ./static/vendor folder, with: npm run vendor Or, using global gulp-cli : gulp vendor As seen in the tasks diagram above, the vendor task run the module task in parallel with the js9 tasks. JS9 has many task as these run with manual command that involve make/make install and then writing configuration to js9prefs.js file. You can run manually the installation of JS9 with gulp js9 . 2. Building CSS and Javascript files \u00b6 npm run build # or npm start # or gulp build # or gulp default # or gulp will run the vendor task and minify both CSS and Javascript files. By default, when no other tasks is specified, gulp runs the build task. You can run single tasks with: gulp css to run just the minification of the CSS files. 3. Run Developement Server \u00b6 Start your normal Django server with ( NOTE : do not change the default port!): ( pipeline_env ) $: ./manage.py runserver In another terminal run: npm run watch # or gulp watch The latter will open your dev server, that will auto reload and apply your latest changes in any CSS, Javascript and/or HTML files. As pointed out in the gulp task tree above the watch task run both the vendor and build tasks. 4. Debug Task \u00b6 This task is for debugging the paths used in the others task, but also serve as a palce hodler to debug commands. npm run debug # or gulp debug 5. Clean Task \u00b6 This task delete the vendor folder ( /static/vendor ) along with all the files. npm run clean # or gulp clean FrontEnd assets for Production \u00b6 In order to compile the frontend assets for production, activate the Python virtual environment, then run: ( pipeline_env ) $ npm run js9staticprod && ./manage.py collectstatic -c This command will collect all static assets (Javascript and CSS files) and copy them to STATIC_ROOT path in setting.py, so make sure you have permission to write to that. STATIC_ROOT is assigned to ./staticfiles by default, otherwise assigned to the path you defined in your .env file. The js9staticprod gulp task is necessary if you specify a STATIC_URL and a BASE_URL different than the default, for example if you need to prefix the site / with a base url because you are running another webserver (e.g. another web server is running on https://my-astro-platform.com/ so you want to run the pipeline on the same server/domain https://my-astro-platform.com/pipeline , so you need to set BASE_URL='/pipeline/' and STATIC_URL=/pipeline-static/ in settings.py ). We recommend to run this in any case! Then you can move that folder to where it can be served by the production static files server ( Ningx or Apache are usually good choices, in case refere to the Django documentation ).","title":"Local Development Environment"},{"location":"developing/localdevenv/#pipeline-local-development-environment-guidelines","text":"This section describes how to set up a local development environment more in details.","title":"Pipeline Local Development Environment Guidelines"},{"location":"developing/localdevenv/#back-end","text":"","title":"Back End"},{"location":"developing/localdevenv/#installation","text":"The installation instructions are the same as the ones describes in the quickstart section with one key difference. Rather than installing the Python dependencies with pip, you will need to install and use Poetry . After installing Poetry, running the command below will install the pipeline dependencies defined in poetry.lock into a virtual environment. The main difference between using Poetry and pip is that pip will only install the dependencies necessary for using the pipeline, whereas Poetry will also install development dependencies required for contributing (e.g. tools to build the documentation). poetry install Note Poetry will automatically create a virtual environment if it detects that your shell isn't currently using one. This should be fine for most users. If you prefer to use an alternative virtual environment manager (e.g. Miniconda), you can prevent Poetry from creating virtual environments . However, even if you are using something like Miniconda, allowing Poetry to manage the virtualenv (the default behaviour) is fine. The development team only uses this option during our automated testing since our test runner machines only contain a single Python environment so using virtualenvs is redundant.","title":"Installation"},{"location":"developing/localdevenv/#solving-your-modelspymigrations-issues","text":"First of all the makemigrations command must be run only if you modified the models.py or you pull down changes (i.e. git pull ) that modify models.py . So please refer to the cases below:","title":"Solving your models.py/migrations issues"},{"location":"developing/localdevenv/#1-you-modify-modelspy","text":"In this case there are 2 situations that arise: You currently don't have a production environment and/or you are comfortable in dropping all the data in the database. In this case, you don't need to update the production environment with multiple migration files even though Django docs promote making as many migrations as you need to ( see Django migration guidelines ). For such reasons please consider doing the following steps: 1.1. Make your changes to models.py . 1.2. Remove all the migration files including 0002_q3c.py . 1.3. Run ./manage.py makemigrations . This will generate a NEW 0001_initial.py file. 1.4. Commit the new migraton file 0001_initial.py as well as models.py within a single commit, and add an appropriate message (e.g. add field X to model Y). DO NOT commit the removal of 0002_q3c.py . 1.5. Get back the Q3C migration file git checkout pipeline/migrations/0002_q3c.py . You currently have a production environment and/or you do not want to lose all the data in the database. In this situation, you need to be careful not to mess up with the producation database, so please consider doing the following steps: 2.1. Make a copy (\"dump\") of the production database as it is, e.g. (by logging remotely to the server) pg_dump -h DBHOST_PROD -p DBPORT_PROD -U DBUSER_PROD -Fc -o DBNAME_PROD > prod-backup-$(date +\"%F_%H%M\").sql . 2.2. Upload the copy to your local development database, e.g. pg_restore -h DBHOST_DEV -p DBPORT_DEV --verbose --clean --no-acl --no-owner -U DBUSER_DEV -d DBNAME_DEV prod-backup.sql . 2.3. Make your changes to models.py . 2.4. Run ./manage.py makemigrations with optional but strongly recommended -n 'my_migration_name' . This will generate a new migration file 000X_my_migration_name.py where X is incremented by 1 with respect the last migration file. 2.5. Commit the new migraton file 000X_my_migration_name.py as well as models.py within a single commit, and add an appropriate message (e.g. add field X to model Y) Warning Do not modify the 0002_q3c.py file as it relates to migration operations for using Q3C plugin and related functions!","title":"1. You modify models.py"},{"location":"developing/localdevenv/#2-someone-else-modified-modelspy-and-you-pull-the-changes","text":"Situation: ~/vast-pipeline [master]$ git fetch && git pull Updating abc123..321cba Fast-forward pipeline/models.py | 4 +++- pipeline/migrations/0001_initial.py | 5 ++++- 2 file changed, 9 insertions(+), 2 deletions(-) You realise that you are in this situation when: In the command above you see changes (i.e. + or - ) in models.py and/or in migrations (i.e. XXXX_some_migration.py ) Running the webserver, a message reports You have unapplied migrations; your app may not work properly until they are applied. Run 'python manage.py migrate' to apply them. Running the pipeline you have errors related to the database models Solutions to such scenario: If you don't mind losing all the data in your database just follow the Reset the database instructions to drop all the data. But if you want to keep your data, you have to fix these changes by trying to run makemigrations and migrate . But ideally you should follow the following steps: Identify the previous commit before pulling the changes (when your migration and model files were working): ~/vast-pipeline [master]$ git show -1 pipeline/models.py #OR ~/vast-pipeline [master]$ git show -2 pipeline/models.py #OR ~/vast-pipeline [master]$ git show -1 pipeline/migrations/XXXX_my_migration.py Or even better ~/vast-pipeline [master]$ git log -p pipeline/models.py Take note of the commit hash of the old changes (i.e. before pulling down the new changes). Checkout ONLY your old migration files, for example like this: ~/vast-pipeline [master]$ git checkout 37cabac84785742437927c785b63a767aa8ac5ff pipeline/migrations/0001_initial.py Make the migrations ./manage.py makemigrations && ./manage.py migrate Run the pipeline and the webserver to see that everything is working fine Squash the migrations using Django migration guidelines Continue with the normal development cycle (i.e. branch off master, do changes, commit everything, including your changes in the models/migrations even done with the squashing! )","title":"2. Someone else modified models.py and you pull the changes"},{"location":"developing/localdevenv/#removingclearing-data","text":"The following sub-sections show how to completely drop every data in the database and how to remove only the data related to one or more pipeline runs.","title":"Removing/Clearing Data"},{"location":"developing/localdevenv/#reset-the-database","text":"Make sure you installed the requirements dev.txt . And django_extensions is in EXTRA_APPS in your setting configuration file .env (e.g. EXTRA_APPS=django_extensions,another_app,... ). ( pipeline_env ) $ ./manage.py reset_db && ./manage.py migrate # use the following for no confirmation prompt ( pipeline_env ) $ ./manage.py reset_db --noinput && ./manage.py migrate","title":"Reset the database"},{"location":"developing/localdevenv/#clearing-run-data","text":"It is sometimes convenient to remove the data belonging to one or more pipeline runs while developing the code base. This is particularly useful to save time by not having to re-upload the image data along with the measurements. The data related to the pipeline are the Sources, Associations, Forced extractions entries in database and the parquet files in the respective folder. By default the command will keep the run folder with the config and the log files. ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run To clear more than one run: ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run1 my-pipe-run2 path/to/my-pipe-run3 The command accept both a path or a name of the pipeline run(s). To remove all the runs, issue: ( pipeline_env ) $ ./manage.py clearpiperun clearall The command to keep the parquet files is: ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run --keep-parquet The remove completely the pipeline folder ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run --remove-all","title":"Clearing Run Data"},{"location":"developing/localdevenv/#frontend-assets-management-and-guidelines","text":"This guide explain the installation, compilation and development of the front end assets (HTML, CSS, JS and relative modules). We make use of a node installation with npm and gulp tasks to build the front end assets.","title":"FrontEnd Assets Management and Guidelines"},{"location":"developing/localdevenv/#installation-of-node-packages","text":"After installing a node version and npm , install the node modules using from the base folder ( vast-pipeline ): npm ci Npm will install the node packages in a node_modules folder under the main root. ... \u251c\u2500\u2500 node_modules ... For installing future additional dependecies you can run npm install --save my-package or npm install --save-dev my-dev-package (to save a development module), and after that commit both package.json and package-lock.json files. For details about the installed packages and npm scripts see package.json .","title":"Installation of node packages"},{"location":"developing/localdevenv/#frontend-tasks-with-gulp","text":"Using gulp and npm scripts you can: Install dependencies under the ./static/vendor folder. Building (e.g. minify/uglify) CSS and/or Javascript files. Run a developement server that \"hot-reload\" your web page when any HTML, CSS or Javascript file is modified. The command to list all the gulp \"tasks\" and sub-tasks is (you might need gulp-cli installed globally, i.e. npm i --global gulp-cli , more info here ): gulp --tasks Output: [11:55:30] Tasks for ~/PATH/TO/REPO/vast-pipeline/gulpfile.js [11:55:30] \u251c\u2500\u2500 clean [11:55:30] \u251c\u2500\u252c js9 [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u251c\u2500\u2500 css [11:55:30] \u251c\u2500\u2500 js [11:55:30] \u251c\u2500\u252c vendor [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u251c\u2500\u252c build [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u251c\u2500\u252c watch [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 watchFiles [11:55:30] \u2502 \u2514\u2500\u2500 browserSync [11:55:30] \u251c\u2500\u252c default [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u2514\u2500\u2500 debug Alternatively you can run gulp from the installed version in the node_modules folder with: ./node_modules/.bin/gulp --tasks For further details about tasks, see gulpfile .","title":"FrontEnd Tasks with gulp"},{"location":"developing/localdevenv/#1-install-dependencies-under-vendor-folder","text":"Install the dependencies under the ./static/vendor folder, with: npm run vendor Or, using global gulp-cli : gulp vendor As seen in the tasks diagram above, the vendor task run the module task in parallel with the js9 tasks. JS9 has many task as these run with manual command that involve make/make install and then writing configuration to js9prefs.js file. You can run manually the installation of JS9 with gulp js9 .","title":"1. Install Dependencies under vendor Folder"},{"location":"developing/localdevenv/#2-building-css-and-javascript-files","text":"npm run build # or npm start # or gulp build # or gulp default # or gulp will run the vendor task and minify both CSS and Javascript files. By default, when no other tasks is specified, gulp runs the build task. You can run single tasks with: gulp css to run just the minification of the CSS files.","title":"2. Building CSS and Javascript files"},{"location":"developing/localdevenv/#3-run-developement-server","text":"Start your normal Django server with ( NOTE : do not change the default port!): ( pipeline_env ) $: ./manage.py runserver In another terminal run: npm run watch # or gulp watch The latter will open your dev server, that will auto reload and apply your latest changes in any CSS, Javascript and/or HTML files. As pointed out in the gulp task tree above the watch task run both the vendor and build tasks.","title":"3. Run Developement Server"},{"location":"developing/localdevenv/#4-debug-task","text":"This task is for debugging the paths used in the others task, but also serve as a palce hodler to debug commands. npm run debug # or gulp debug","title":"4. Debug Task"},{"location":"developing/localdevenv/#5-clean-task","text":"This task delete the vendor folder ( /static/vendor ) along with all the files. npm run clean # or gulp clean","title":"5. Clean Task"},{"location":"developing/localdevenv/#frontend-assets-for-production","text":"In order to compile the frontend assets for production, activate the Python virtual environment, then run: ( pipeline_env ) $ npm run js9staticprod && ./manage.py collectstatic -c This command will collect all static assets (Javascript and CSS files) and copy them to STATIC_ROOT path in setting.py, so make sure you have permission to write to that. STATIC_ROOT is assigned to ./staticfiles by default, otherwise assigned to the path you defined in your .env file. The js9staticprod gulp task is necessary if you specify a STATIC_URL and a BASE_URL different than the default, for example if you need to prefix the site / with a base url because you are running another webserver (e.g. another web server is running on https://my-astro-platform.com/ so you want to run the pipeline on the same server/domain https://my-astro-platform.com/pipeline , so you need to set BASE_URL='/pipeline/' and STATIC_URL=/pipeline-static/ in settings.py ). We recommend to run this in any case! Then you can move that folder to where it can be served by the production static files server ( Ningx or Apache are usually good choices, in case refere to the Django documentation ).","title":"FrontEnd assets for Production"},{"location":"developing/profiling/","text":"Benchmarks \u00b6 Initial Profiling \u00b6 These profiling tests were run with the pipeline codebase correspondent to commit 373c2ce (some commits after the first release). Running on 12GB of data with 464MB peak memory usage takes 4 mins: performance: ~80% final_operations , ~10% get_src_skyregion_merged_df . final_operations calls other functions, out of these the largest is 50% in make_upload_sources which spends about 20% of time on utils <method 'execute' of 'psycopg2.extensions.cursor' objects> . The get_src_skyregion_merged_df time sink is in threading 15% of time is on wait ( final_operations spends some time on threading as well, hence 15% > 10%). memory: 40% pyarrow parquet write_table , rest is mostly fragmented, some more pyarrow and some pandas Running on 3MB of data with peak memory usage 176MB, takes 1.5s: performance: ~30% goes to pipeline (about 9% of this is pickle ), ~11% goes to read , rest goes to django I think memory: 30% memory is spent on django , 20% is spent on astropy/coordinates/matrix_utilities , 10% on importing other modules, rest is fragmented quite small Note that I didn't include the generation of the images/*/measurements.parquet or other files in these profiles. Database Update Operations \u00b6 Delete ( Model.objects.all().delete() ) and reupload ( bulk_upload ) (in seconds) columns\\rows 10 3 10 4 10 5 4 0.15 1.24 12.95 8 0.26 1.64 19.11 12 0.31 2.18 21.49 Per cell, 10 3 rows is slower than 10 4 and 10 5 rows, possibly due to overhead. Best to avoid uploading 10 3 rows each bulk_create call. Django bulk_update columns\\rows 10 3 10 4 10 5 4 3.39 na na 8 4.38 na na 12 5.50 na na I don't think there's any point testing 10 4 or 10 5 rows, it's obviously the worst performing function, and I've already had to force quit the terminal twice because keyboard interrupt didn't work. SQL join as ( SQL_update in vast_pipeline.pipeline.loading ) columns\\rows 10 3 10 4 10 5 4 0.016 0.11 3.08 8 0.019 0.32 4.31 12 0.027 0.38 5.39 10 5 is slower per cell than 10 4 and 10 3 , not sure why. Recommend updating 10 4 rows each time. This timing info does vary a bit on randomness. Sometimes the SQL join as takes as long as 1 second to complete 10 3 rows, I'm not sure what's causing this.","title":"Bechmarks"},{"location":"developing/profiling/#benchmarks","text":"","title":"Benchmarks"},{"location":"developing/profiling/#initial-profiling","text":"These profiling tests were run with the pipeline codebase correspondent to commit 373c2ce (some commits after the first release). Running on 12GB of data with 464MB peak memory usage takes 4 mins: performance: ~80% final_operations , ~10% get_src_skyregion_merged_df . final_operations calls other functions, out of these the largest is 50% in make_upload_sources which spends about 20% of time on utils <method 'execute' of 'psycopg2.extensions.cursor' objects> . The get_src_skyregion_merged_df time sink is in threading 15% of time is on wait ( final_operations spends some time on threading as well, hence 15% > 10%). memory: 40% pyarrow parquet write_table , rest is mostly fragmented, some more pyarrow and some pandas Running on 3MB of data with peak memory usage 176MB, takes 1.5s: performance: ~30% goes to pipeline (about 9% of this is pickle ), ~11% goes to read , rest goes to django I think memory: 30% memory is spent on django , 20% is spent on astropy/coordinates/matrix_utilities , 10% on importing other modules, rest is fragmented quite small Note that I didn't include the generation of the images/*/measurements.parquet or other files in these profiles.","title":"Initial Profiling"},{"location":"developing/profiling/#database-update-operations","text":"Delete ( Model.objects.all().delete() ) and reupload ( bulk_upload ) (in seconds) columns\\rows 10 3 10 4 10 5 4 0.15 1.24 12.95 8 0.26 1.64 19.11 12 0.31 2.18 21.49 Per cell, 10 3 rows is slower than 10 4 and 10 5 rows, possibly due to overhead. Best to avoid uploading 10 3 rows each bulk_create call. Django bulk_update columns\\rows 10 3 10 4 10 5 4 3.39 na na 8 4.38 na na 12 5.50 na na I don't think there's any point testing 10 4 or 10 5 rows, it's obviously the worst performing function, and I've already had to force quit the terminal twice because keyboard interrupt didn't work. SQL join as ( SQL_update in vast_pipeline.pipeline.loading ) columns\\rows 10 3 10 4 10 5 4 0.016 0.11 3.08 8 0.019 0.32 4.31 12 0.027 0.38 5.39 10 5 is slower per cell than 10 4 and 10 3 , not sure why. Recommend updating 10 4 rows each time. This timing info does vary a bit on randomness. Sometimes the SQL join as takes as long as 1 second to complete 10 3 rows, I'm not sure what's causing this.","title":"Database Update Operations"},{"location":"developing/tests/","text":"Tests Guidelines \u00b6 This section describes how to run the test suite of the VAST Pipeline. General Tests \u00b6 Test are found under the folder tests . Have a look and feel free to include new tests. Run the tests with the following: To run all tests: ( pipeline_env ) $ ./manage.py test To run one test file or class, use: ( pipeline_env ) $ ./manage.py test <path.to.test> for example, to run the test class CheckRunConfigValidationTest located in test_runpipeline.py , use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_runpipeline.CheckRunConfigValidationTest to run the tests located in test_webserver.py , use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_webserver Regression Tests \u00b6 Regression tests located in test_regression.py {:target=\" blank\"} require the use of the _VAST_2118-06A field test dataset which is not a part of the repository. This data is downloadable from cloudstor . You can use the script located in tests/regression-data/ : $ cd vast_pipeline/tests/regression-data/ && ./download.sh to download the VAST_2118-06A field test dataset into the regression-data folder. Or manually by clicking the button below: Download data for test and place the VAST_2118-06A field test dataset into the regression-data folder. These regression tests are skipped if the dataset is not present. All tests should be run before pushing to master. Running all the tests takes a few minutes, so it is not recommended to run them for every change. If you have made a minor change and would like to only run unit tests, skipping regression tests, use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_pipeline Note If changes are made to the default config keys, these changes need to be propagated to the test config files.","title":"Tests"},{"location":"developing/tests/#tests-guidelines","text":"This section describes how to run the test suite of the VAST Pipeline.","title":"Tests Guidelines"},{"location":"developing/tests/#general-tests","text":"Test are found under the folder tests . Have a look and feel free to include new tests. Run the tests with the following: To run all tests: ( pipeline_env ) $ ./manage.py test To run one test file or class, use: ( pipeline_env ) $ ./manage.py test <path.to.test> for example, to run the test class CheckRunConfigValidationTest located in test_runpipeline.py , use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_runpipeline.CheckRunConfigValidationTest to run the tests located in test_webserver.py , use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_webserver","title":"General Tests"},{"location":"developing/tests/#regression-tests","text":"Regression tests located in test_regression.py {:target=\" blank\"} require the use of the _VAST_2118-06A field test dataset which is not a part of the repository. This data is downloadable from cloudstor . You can use the script located in tests/regression-data/ : $ cd vast_pipeline/tests/regression-data/ && ./download.sh to download the VAST_2118-06A field test dataset into the regression-data folder. Or manually by clicking the button below: Download data for test and place the VAST_2118-06A field test dataset into the regression-data folder. These regression tests are skipped if the dataset is not present. All tests should be run before pushing to master. Running all the tests takes a few minutes, so it is not recommended to run them for every change. If you have made a minor change and would like to only run unit tests, skipping regression tests, use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_pipeline Note If changes are made to the default config keys, these changes need to be propagated to the test config files.","title":"Regression Tests"},{"location":"exploringwebsite/admintools/","text":"Website Admin Tools \u00b6 Accessing the Admin Tools \u00b6 Users designated as administrators of the pipeline instance being explored (controlled by GitHub team membership) will be able to see the admin button at the top of the navbar as shown below. Clicking this button and then selecting the Django open will take the user to the Django admin backend interface shown below. Authentification and Authorization \u00b6 This section allows for the management of the user accounts and groups. Here users can be made admins and details such as email address updated. Django Q Tasks \u00b6 This area allows for the management of the Django Q processing queue. Admins are able to cancel scheduled tasks, view failed tasks or schedule new tasks. Python Social Auth \u00b6 The area for managing aspects of the authentification system that allows users to log in via GitHub. VAST_PIPELINE \u00b6 Admins are able to interact with the pipeline results data that has been uploaded from pipeline runs. This includes editing and removing objects or fields in the data as well as tags and comments.","title":"Admin Tools"},{"location":"exploringwebsite/admintools/#website-admin-tools","text":"","title":"Website Admin Tools"},{"location":"exploringwebsite/admintools/#accessing-the-admin-tools","text":"Users designated as administrators of the pipeline instance being explored (controlled by GitHub team membership) will be able to see the admin button at the top of the navbar as shown below. Clicking this button and then selecting the Django open will take the user to the Django admin backend interface shown below.","title":"Accessing the Admin Tools"},{"location":"exploringwebsite/admintools/#authentification-and-authorization","text":"This section allows for the management of the user accounts and groups. Here users can be made admins and details such as email address updated.","title":"Authentification and Authorization"},{"location":"exploringwebsite/admintools/#django-q-tasks","text":"This area allows for the management of the Django Q processing queue. Admins are able to cancel scheduled tasks, view failed tasks or schedule new tasks.","title":"Django Q Tasks"},{"location":"exploringwebsite/admintools/#python-social-auth","text":"The area for managing aspects of the authentification system that allows users to log in via GitHub.","title":"Python Social Auth"},{"location":"exploringwebsite/admintools/#vast_pipeline","text":"Admins are able to interact with the pipeline results data that has been uploaded from pipeline runs. This includes editing and removing objects or fields in the data as well as tags and comments.","title":"VAST_PIPELINE"},{"location":"exploringwebsite/imagepages/","text":"Image Pages \u00b6 This page details the website pages for information on the images. List of Images \u00b6 Shown on this page is a list of images that have been ingested into the pipeline database from all pipeline runs, along with their statistics. From this page the full detail page of a specific image can be accessed by clicking on the image name. Explanation of the table options can be found on the overview page here . Image Detail Page \u00b6 This page presents all the information about the selected image. Previous & Next Buttons \u00b6 These buttons do the following: Previous : Navigates to the previous image by id value. Next : Navigates to the next image by id value. Details \u00b6 A text representation of details of the image. Aladin Lite Viewer \u00b6 Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the image central coordinates of the image. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS. The red square shows the footprint of the image sky region on the sky. User Comments \u00b6 Users are able to read and post comments on an image using this form. Measurements Table \u00b6 This table displays all the Selavy measurements that were ingested with the image (no forced measurements appear here as they are run specific). The measurement detail page can be reached by clicking the measurement name. Pipeline Runs Table \u00b6 This table displays all the pipeline runs that use the current image. The pipeline detail page can be reached by clicking the run name.","title":"Image Pages"},{"location":"exploringwebsite/imagepages/#image-pages","text":"This page details the website pages for information on the images.","title":"Image Pages"},{"location":"exploringwebsite/imagepages/#list-of-images","text":"Shown on this page is a list of images that have been ingested into the pipeline database from all pipeline runs, along with their statistics. From this page the full detail page of a specific image can be accessed by clicking on the image name. Explanation of the table options can be found on the overview page here .","title":"List of Images"},{"location":"exploringwebsite/imagepages/#image-detail-page","text":"This page presents all the information about the selected image.","title":"Image Detail Page"},{"location":"exploringwebsite/imagepages/#previous-next-buttons","text":"These buttons do the following: Previous : Navigates to the previous image by id value. Next : Navigates to the next image by id value.","title":"Previous &amp; Next Buttons"},{"location":"exploringwebsite/imagepages/#details","text":"A text representation of details of the image.","title":"Details"},{"location":"exploringwebsite/imagepages/#aladin-lite-viewer","text":"Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the image central coordinates of the image. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS. The red square shows the footprint of the image sky region on the sky.","title":"Aladin Lite Viewer"},{"location":"exploringwebsite/imagepages/#user-comments","text":"Users are able to read and post comments on an image using this form.","title":"User Comments"},{"location":"exploringwebsite/imagepages/#measurements-table","text":"This table displays all the Selavy measurements that were ingested with the image (no forced measurements appear here as they are run specific). The measurement detail page can be reached by clicking the measurement name.","title":"Measurements Table"},{"location":"exploringwebsite/imagepages/#pipeline-runs-table","text":"This table displays all the pipeline runs that use the current image. The pipeline detail page can be reached by clicking the run name.","title":"Pipeline Runs Table"},{"location":"exploringwebsite/measurementpages/","text":"Measurement Pages \u00b6 This page details the website pages for information on the measurements. List of Measurements \u00b6 A list of measurements that have been ingested into the pipeline database from all pipeline runs, along with their statistics, is shown on this page. From this page the full detail page of a specific measurement can be accessed by clicking on the name of the measurement. Explanation of the table options can be found on the overview page here . Measurement Detail Page \u00b6 This page presents all the information about the selected measurement, including a postage stamp cutout of the component. SIMBAD, NED, Previous & Next Buttons \u00b6 These buttons do the following: SIMBAD : Performs a cone search on SIMBAD with a radius of 10 arcmin centered on the measurement location. NED : Performs a cone search on NED with a radius of 10 arcmin centered on the measurement location. Previous : Navigates to the previous measurement by id value. Next : Navigates to the next measurement by id value. Details \u00b6 A text representation of details of the measurement. Aladin Lite Viewer \u00b6 Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the location of the measurement. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS. JS9 Viewer \u00b6 JS9 website . The right panel contains a JS9 viewer showing the postage stamp FITS image of the measurement loaded from its respective image FITS file. Note If the image data is removed from its location when the pipeline run was processed the JS9 viewer will no longer work. User Comments \u00b6 Users are able to read and post comments on a measurement using this form. Sources Table \u00b6 The sources table shows all the sources, from all pipeline runs, that the measurement is associated to. Explanation of the table options can be found on the overview page here . Siblings Table \u00b6 The siblings table displays all other measurements that are a sibling of the current measurement, i.e., the measurements belong to the same island (as determined by Selavy ).","title":"Measurement Pages"},{"location":"exploringwebsite/measurementpages/#measurement-pages","text":"This page details the website pages for information on the measurements.","title":"Measurement Pages"},{"location":"exploringwebsite/measurementpages/#list-of-measurements","text":"A list of measurements that have been ingested into the pipeline database from all pipeline runs, along with their statistics, is shown on this page. From this page the full detail page of a specific measurement can be accessed by clicking on the name of the measurement. Explanation of the table options can be found on the overview page here .","title":"List of Measurements"},{"location":"exploringwebsite/measurementpages/#measurement-detail-page","text":"This page presents all the information about the selected measurement, including a postage stamp cutout of the component.","title":"Measurement Detail Page"},{"location":"exploringwebsite/measurementpages/#simbad-ned-previous-next-buttons","text":"These buttons do the following: SIMBAD : Performs a cone search on SIMBAD with a radius of 10 arcmin centered on the measurement location. NED : Performs a cone search on NED with a radius of 10 arcmin centered on the measurement location. Previous : Navigates to the previous measurement by id value. Next : Navigates to the next measurement by id value.","title":"SIMBAD, NED, Previous &amp; Next Buttons"},{"location":"exploringwebsite/measurementpages/#details","text":"A text representation of details of the measurement.","title":"Details"},{"location":"exploringwebsite/measurementpages/#aladin-lite-viewer","text":"Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the location of the measurement. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS.","title":"Aladin Lite Viewer"},{"location":"exploringwebsite/measurementpages/#js9-viewer","text":"JS9 website . The right panel contains a JS9 viewer showing the postage stamp FITS image of the measurement loaded from its respective image FITS file. Note If the image data is removed from its location when the pipeline run was processed the JS9 viewer will no longer work.","title":"JS9 Viewer"},{"location":"exploringwebsite/measurementpages/#user-comments","text":"Users are able to read and post comments on a measurement using this form.","title":"User Comments"},{"location":"exploringwebsite/measurementpages/#sources-table","text":"The sources table shows all the sources, from all pipeline runs, that the measurement is associated to. Explanation of the table options can be found on the overview page here .","title":"Sources Table"},{"location":"exploringwebsite/measurementpages/#siblings-table","text":"The siblings table displays all other measurements that are a sibling of the current measurement, i.e., the measurements belong to the same island (as determined by Selavy ).","title":"Siblings Table"},{"location":"exploringwebsite/runpages/","text":"Pipeline Run Pages \u00b6 This page details the website pages for information on the pipeline runs. List of Pipeline Runs \u00b6 A list of the pipeline runs that have been processed or initialised are presented on this page along with basic statistics, including the run status. From this page the full detail page of a specific pipeline run can be accessed by clicking on the name of the pipeline run. Explanation of the table options can be found on the overview page here . Pipeline Run Detail Page \u00b6 This page presents all the information about the pipeline run, including options to edit the configuration file and to schedule the run for processing. Note For full details on how to process a run please refer to this page . Summary Cards \u00b6 The cards at the top of the page give a summary of the total numbers of: Images in the pipeline run. Measurements in the pipeline run. Sources in the pipeline run. New sources in the pipeline run. Clicking on the total number of images or measurements will navigate the user to the Image and Measurements tables on this page, where as the source cards will take the user to the Sources Query page. Warning When sent to the source query page, the user should make sure to click submit on the search. Details \u00b6 A text representation of details of the pipeline run. Run Sky Regions \u00b6 A sky map showing the area of sky covered by the images associated with the pipeline run. Configuration File \u00b6 Here the pipeline run configuration file can be viewed, edited and validated. Editing the Configuration File \u00b6 To edit the configuration file first select the Toggle on/off Config Edit option, that is shown in the screenshot to the right. This will enter edit mode on the configuration file as denoted by the --Edit Mode-- message shown in the screenshot below. Warning Do not toggle off edit mode without first selecting Wrtie Current Config otherwise changes will be lost. When all changes are applied, select the Write Current Config to save the changes. Validating the Configuration File \u00b6 From the configuration file menu select the Validate Config option. A feedback modal will then appear with feedback stating whether the configuration validation was successful or failed. The feeback may take a moment to appear as the check is performed. User Comments \u00b6 Users are able to read and post comments on a pipeline run using this form. Log File \u00b6 The full log file of the pipeline run is able to viewed. Image and Measurements Tables \u00b6 Two tables are on the pipeline run detail page displaying the images and measurements (including forced measurements) that are part of the pipeline run.","title":"Pipeline Run Pages"},{"location":"exploringwebsite/runpages/#pipeline-run-pages","text":"This page details the website pages for information on the pipeline runs.","title":"Pipeline Run Pages"},{"location":"exploringwebsite/runpages/#list-of-pipeline-runs","text":"A list of the pipeline runs that have been processed or initialised are presented on this page along with basic statistics, including the run status. From this page the full detail page of a specific pipeline run can be accessed by clicking on the name of the pipeline run. Explanation of the table options can be found on the overview page here .","title":"List of Pipeline Runs"},{"location":"exploringwebsite/runpages/#pipeline-run-detail-page","text":"This page presents all the information about the pipeline run, including options to edit the configuration file and to schedule the run for processing. Note For full details on how to process a run please refer to this page .","title":"Pipeline Run Detail Page"},{"location":"exploringwebsite/runpages/#summary-cards","text":"The cards at the top of the page give a summary of the total numbers of: Images in the pipeline run. Measurements in the pipeline run. Sources in the pipeline run. New sources in the pipeline run. Clicking on the total number of images or measurements will navigate the user to the Image and Measurements tables on this page, where as the source cards will take the user to the Sources Query page. Warning When sent to the source query page, the user should make sure to click submit on the search.","title":"Summary Cards"},{"location":"exploringwebsite/runpages/#details","text":"A text representation of details of the pipeline run.","title":"Details"},{"location":"exploringwebsite/runpages/#run-sky-regions","text":"A sky map showing the area of sky covered by the images associated with the pipeline run.","title":"Run Sky Regions"},{"location":"exploringwebsite/runpages/#configuration-file","text":"Here the pipeline run configuration file can be viewed, edited and validated.","title":"Configuration File"},{"location":"exploringwebsite/runpages/#editing-the-configuration-file","text":"To edit the configuration file first select the Toggle on/off Config Edit option, that is shown in the screenshot to the right. This will enter edit mode on the configuration file as denoted by the --Edit Mode-- message shown in the screenshot below. Warning Do not toggle off edit mode without first selecting Wrtie Current Config otherwise changes will be lost. When all changes are applied, select the Write Current Config to save the changes.","title":"Editing the Configuration File"},{"location":"exploringwebsite/runpages/#validating-the-configuration-file","text":"From the configuration file menu select the Validate Config option. A feedback modal will then appear with feedback stating whether the configuration validation was successful or failed. The feeback may take a moment to appear as the check is performed.","title":"Validating the Configuration File"},{"location":"exploringwebsite/runpages/#user-comments","text":"Users are able to read and post comments on a pipeline run using this form.","title":"User Comments"},{"location":"exploringwebsite/runpages/#log-file","text":"The full log file of the pipeline run is able to viewed.","title":"Log File"},{"location":"exploringwebsite/runpages/#image-and-measurements-tables","text":"Two tables are on the pipeline run detail page displaying the images and measurements (including forced measurements) that are part of the pipeline run.","title":"Image and Measurements Tables"},{"location":"exploringwebsite/sourcepages/","text":"Source Pages \u00b6 This page details the website pages for information on the sources. Source Query \u00b6 Users can filter and query the sources currently in the database by using the form located on this page. The form is submitted by clicking the blue button, the red button will reset the form by removing all entered values. Once the form is submitted the results are dynamically updated in the results table below the form (i.e. on the same page). The following sections provide further details on the form. Data Source \u00b6 Here a specific pipeline run can be selected from a dropdown list to filter sources to only those from the chosen run. By default sources from all runs are shown. Note Only successfully completed pipeline runs are available to select. This rule also applies when all is selected. Cone Search \u00b6 Users can choose whether to input their coordinates directly or use the object name resolver to attempt to fetch the coordinates. Manual Input \u00b6 The format of the coordinates should be in a standard format that is recognised by astropy, for example: 21 29 45.29 -04 29 11.9 21:29:45.29 -04:29:11.9 322.4387083 -4.4866389 Galactic coordinates can also be entered by selecting Galactic from the dropdown menu that is set to ICRS by default. Feedback will be given immediately whether the coordinates are valid, as shown in the screenshots below: Once the coordinates have been entered the radius value must also be specified as shown in the screenshot above. Use the dropdown menu to change the radius unit to be arcsec , arcmin or deg . Name Resolver \u00b6 To use the name resolver, the name of the source should be entered into the Object Name field (e.g. PSR J2129-04 ), select the name resolver service and then click the Resolve button. The coordinates will then be automatically filled on a successful match. if no match is found then this will be communicated by the form as below: Table Filters \u00b6 This section of the form allows the user to place thresholds and selections on specific metrics of the sources. Please refer to the Source Statistics page for details on the different metrics. There are also tooltips located on the form to offer explanations. The following options are not standard source metrics: Include and Exclude Tags \u00b6 Users can attach tags to sources (see Tags and Favourites ) and here tags can be selected to include or exclude in the source search. Source Selection \u00b6 Here specific sources can be searched for by entering the source names, or source database id values, in a comma-separated list. For example: ASKAP_011816.05-730747.77,ASKAP_011816.05-730747.77,ASKAP_213221.21-040900.42 1031,1280,52 are valid entries to this search field. Use the dropdown menu to declare whether name (default) or id values are being searched. Results Table \u00b6 Located directly below the form is the results table which is dynamically updated once the form is submitted. The full detail page of a specific source can be accessed by clicking on the source name in the table. Explanation of the table options can be found on the overview page here . Source Detail Page \u00b6 This page presents all the information about the selected source, including a light curve and cutouts of all the measurements that are associated to the source. Star, SIMBAD, NED, Previous & Next Buttons \u00b6 These buttons do the following: Star : Adds the source to the user's favourites, see Tags and Favourites . SIMBAD : Performs a cone search on SIMBAD with a radius of 10 arcmin centered on the source location. NED : Performs a cone search on NED with a radius of 10 arcmin centered on the source location. Previous : Navigates to the previous source that was returned in the source query. Next : Navigates to the next source that was returned in the source query Details \u00b6 A text representation of details of the measurement. User Comments & Tags \u00b6 Users are able to read and post comments on a measurement using this form, in addition to adding and removing tags, see Tags and Favourites . Aladin Lite Viewer \u00b6 Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the location of the source. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS. Light Curve \u00b6 The light curve of the source is shown. The peak or integrated flux can be selected by using the radio selection buttons. Two-epoch Node Graph \u00b6 The node graph is a visual representation of what two-epoch pairings have significant variability metric values. If an epoch pairing is significant then they are joined by a line on the graph. Hovering over the line will highlight the epoch pairing on the light curve plot. External Search Results Table \u00b6 This table shows the result of a query to the SIMBAD and NED services for astronomical sources within 1 arcmin of the source location. Along with the name and coordinate of the matches, the on-sky separation between the source is shown along with the source type. JS9 Viewer Postage Stamps \u00b6 JS9 website . The JS9 viewer is used to show the postage stamp FITS images of the measurements that are associated with the source, loaded from their respective image FITS files. Note If the image data is removed from its location when the pipeline run was processed the JS9 viewer will no longer work. Source Measurements Table \u00b6 This table displays the measurements that are associated with the source. The detail page for the measurement can be reached by clicking the name of the respective measurement. Related Sources Table \u00b6 This table displays the sources that are a relation of the source in question. For further information refer to the Relations section in the association documentation. Tags and Favourites \u00b6 Users are able to save a source as a favourite for later reference, in addition to adding tags to sources that can be used in source queries. Adding a Source to Favourites \u00b6 A source can be added to a user's favourites by: Selecting the 'star' button at the top of the source detail page as shown below. A modal window will open to confirm the saving of the source as a favourite. An optional comment can be entered. Select Add to Favourites and a confirmation alert will be shown to signify the source has been added successfully. Viewing Favourite Sources \u00b6 A user can access their favourite sources by selecting the Favourite Sources option from the menu when clicking on their username at the top right-hand corner of the page. The user will then be navigated to their favourite sources table as shown below. Adding a Tag \u00b6 Follow these steps to add a tag to a source: Type the tag to be added into the field tag field. If the tag has already been used it will appear in the dropdown text options and can be selected by clicking the text. To add a new tag, enter the complete text of the new tag and again click the text in the dropdown text. After clicking the text the tag will then show as a bordered tag in the input field. Finally, click the submit button (a comment is optional) and the tag will be saved as shown below. A comment will appear stating the addition of the tag. Removing a Tag \u00b6 Click the x on the tag to remove it and then click the Submit button to save the removal.","title":"Source Pages"},{"location":"exploringwebsite/sourcepages/#source-pages","text":"This page details the website pages for information on the sources.","title":"Source Pages"},{"location":"exploringwebsite/sourcepages/#source-query","text":"Users can filter and query the sources currently in the database by using the form located on this page. The form is submitted by clicking the blue button, the red button will reset the form by removing all entered values. Once the form is submitted the results are dynamically updated in the results table below the form (i.e. on the same page). The following sections provide further details on the form.","title":"Source Query"},{"location":"exploringwebsite/sourcepages/#data-source","text":"Here a specific pipeline run can be selected from a dropdown list to filter sources to only those from the chosen run. By default sources from all runs are shown. Note Only successfully completed pipeline runs are available to select. This rule also applies when all is selected.","title":"Data Source"},{"location":"exploringwebsite/sourcepages/#cone-search","text":"Users can choose whether to input their coordinates directly or use the object name resolver to attempt to fetch the coordinates.","title":"Cone Search"},{"location":"exploringwebsite/sourcepages/#manual-input","text":"The format of the coordinates should be in a standard format that is recognised by astropy, for example: 21 29 45.29 -04 29 11.9 21:29:45.29 -04:29:11.9 322.4387083 -4.4866389 Galactic coordinates can also be entered by selecting Galactic from the dropdown menu that is set to ICRS by default. Feedback will be given immediately whether the coordinates are valid, as shown in the screenshots below: Once the coordinates have been entered the radius value must also be specified as shown in the screenshot above. Use the dropdown menu to change the radius unit to be arcsec , arcmin or deg .","title":"Manual Input"},{"location":"exploringwebsite/sourcepages/#name-resolver","text":"To use the name resolver, the name of the source should be entered into the Object Name field (e.g. PSR J2129-04 ), select the name resolver service and then click the Resolve button. The coordinates will then be automatically filled on a successful match. if no match is found then this will be communicated by the form as below:","title":"Name Resolver"},{"location":"exploringwebsite/sourcepages/#table-filters","text":"This section of the form allows the user to place thresholds and selections on specific metrics of the sources. Please refer to the Source Statistics page for details on the different metrics. There are also tooltips located on the form to offer explanations. The following options are not standard source metrics:","title":"Table Filters"},{"location":"exploringwebsite/sourcepages/#include-and-exclude-tags","text":"Users can attach tags to sources (see Tags and Favourites ) and here tags can be selected to include or exclude in the source search.","title":"Include and Exclude Tags"},{"location":"exploringwebsite/sourcepages/#source-selection","text":"Here specific sources can be searched for by entering the source names, or source database id values, in a comma-separated list. For example: ASKAP_011816.05-730747.77,ASKAP_011816.05-730747.77,ASKAP_213221.21-040900.42 1031,1280,52 are valid entries to this search field. Use the dropdown menu to declare whether name (default) or id values are being searched.","title":"Source Selection"},{"location":"exploringwebsite/sourcepages/#results-table","text":"Located directly below the form is the results table which is dynamically updated once the form is submitted. The full detail page of a specific source can be accessed by clicking on the source name in the table. Explanation of the table options can be found on the overview page here .","title":"Results Table"},{"location":"exploringwebsite/sourcepages/#source-detail-page","text":"This page presents all the information about the selected source, including a light curve and cutouts of all the measurements that are associated to the source.","title":"Source Detail Page"},{"location":"exploringwebsite/sourcepages/#star-simbad-ned-previous-next-buttons","text":"These buttons do the following: Star : Adds the source to the user's favourites, see Tags and Favourites . SIMBAD : Performs a cone search on SIMBAD with a radius of 10 arcmin centered on the source location. NED : Performs a cone search on NED with a radius of 10 arcmin centered on the source location. Previous : Navigates to the previous source that was returned in the source query. Next : Navigates to the next source that was returned in the source query","title":"Star, SIMBAD, NED, Previous &amp; Next Buttons"},{"location":"exploringwebsite/sourcepages/#details","text":"A text representation of details of the measurement.","title":"Details"},{"location":"exploringwebsite/sourcepages/#user-comments-tags","text":"Users are able to read and post comments on a measurement using this form, in addition to adding and removing tags, see Tags and Favourites .","title":"User Comments &amp; Tags"},{"location":"exploringwebsite/sourcepages/#aladin-lite-viewer","text":"Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the location of the source. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS.","title":"Aladin Lite Viewer"},{"location":"exploringwebsite/sourcepages/#light-curve","text":"The light curve of the source is shown. The peak or integrated flux can be selected by using the radio selection buttons.","title":"Light Curve"},{"location":"exploringwebsite/sourcepages/#two-epoch-node-graph","text":"The node graph is a visual representation of what two-epoch pairings have significant variability metric values. If an epoch pairing is significant then they are joined by a line on the graph. Hovering over the line will highlight the epoch pairing on the light curve plot.","title":"Two-epoch Node Graph"},{"location":"exploringwebsite/sourcepages/#external-search-results-table","text":"This table shows the result of a query to the SIMBAD and NED services for astronomical sources within 1 arcmin of the source location. Along with the name and coordinate of the matches, the on-sky separation between the source is shown along with the source type.","title":"External Search Results Table"},{"location":"exploringwebsite/sourcepages/#js9-viewer-postage-stamps","text":"JS9 website . The JS9 viewer is used to show the postage stamp FITS images of the measurements that are associated with the source, loaded from their respective image FITS files. Note If the image data is removed from its location when the pipeline run was processed the JS9 viewer will no longer work.","title":"JS9 Viewer Postage Stamps"},{"location":"exploringwebsite/sourcepages/#source-measurements-table","text":"This table displays the measurements that are associated with the source. The detail page for the measurement can be reached by clicking the name of the respective measurement.","title":"Source Measurements Table"},{"location":"exploringwebsite/sourcepages/#related-sources-table","text":"This table displays the sources that are a relation of the source in question. For further information refer to the Relations section in the association documentation.","title":"Related Sources Table"},{"location":"exploringwebsite/sourcepages/#tags-and-favourites","text":"Users are able to save a source as a favourite for later reference, in addition to adding tags to sources that can be used in source queries.","title":"Tags and Favourites"},{"location":"exploringwebsite/sourcepages/#adding-a-source-to-favourites","text":"A source can be added to a user's favourites by: Selecting the 'star' button at the top of the source detail page as shown below. A modal window will open to confirm the saving of the source as a favourite. An optional comment can be entered. Select Add to Favourites and a confirmation alert will be shown to signify the source has been added successfully.","title":"Adding a Source to Favourites"},{"location":"exploringwebsite/sourcepages/#viewing-favourite-sources","text":"A user can access their favourite sources by selecting the Favourite Sources option from the menu when clicking on their username at the top right-hand corner of the page. The user will then be navigated to their favourite sources table as shown below.","title":"Viewing Favourite Sources"},{"location":"exploringwebsite/sourcepages/#adding-a-tag","text":"Follow these steps to add a tag to a source: Type the tag to be added into the field tag field. If the tag has already been used it will appear in the dropdown text options and can be selected by clicking the text. To add a new tag, enter the complete text of the new tag and again click the text in the dropdown text. After clicking the text the tag will then show as a bordered tag in the input field. Finally, click the submit button (a comment is optional) and the tag will be saved as shown below. A comment will appear stating the addition of the tag.","title":"Adding a Tag"},{"location":"exploringwebsite/sourcepages/#removing-a-tag","text":"Click the x on the tag to remove it and then click the Submit button to save the removal.","title":"Removing a Tag"},{"location":"exploringwebsite/websiteoverview/","text":"Website Overview \u00b6 This page gives an overview of the pipeline website, with links to main pages on features where appropriate. Refer to Accessing the Pipeline for details on how to access the pipeline instance that is hosted by the VAST collaboration. For admins, refer to the following pages for details of the configuration and set up of the web server: Configuration , Deployment and Web App Admin Usage . Homepage \u00b6 The homepage provides a summary of the data currently held in the pipeline instance that has been processed. The four cards at the top of the homepage provide total values for the amount of pipeline runs, images, measurements and sources that are stored in the database. Clicking any of them will take you to the respective overview page for the data type. Note The totals presented on the homepage are totals for all pipeline runs combined! Also displayed is a sky region map that shows all the areas of the sky that have had successful and completed pipeline runs performed. Navbar \u00b6 The navbar, shown to the right, acts as the main method in which to navigate around the website. The following sections link to the respective documentation pages explainging the features of each link. Note The admin button on the navbar is only seen when the user is designated as an administrator. Tip The navbar can be collapsed by pressing the menu (or hamburger) button next to it at the top of the page. Admin \u00b6 See the website admin tools page. Allows for admins to manage users, Django Q schedules and the data itself. Pipeline Runs \u00b6 See the Pipeline Run Pages doc. Navigates the user to the list of pipeline runs available, which in turn link to the detail page for each respective run. Sources Query \u00b6 See the Source Pages section. Takes the user to the source query page, where users can search for sources by defining a set of thresholds and feature requirements. From the results users can also access the detail page for individual sources. Measurements \u00b6 See the Measurement Pages section. Navigates the user to the measurements page that features a table containing all the measurements currently held in the database. From here users can also access the detail page for individual measurements. Images \u00b6 See the Image Pages section. Takes the user to the images page that features a table containing all the images currently held in the database. From here users can also access the detail page for individual images. External Links \u00b6 Documentation : Links to this documentation website. Pipeline Repository : A link to the GitHub pipeline repository. Raise an Issue : A link to open a new issue on the GitHub repository. Start a Discussion : A link to open a new discussion on the GitHub repository. VAST Links GitHub : A link to the VAST organisation GitHub page. JupyterHub : Links to the VAST hosted JupyterHub instance which includes access to the pipeline results and vast-tools . Website : Links to the VAST collaboration website. Wiki : Links to the VAST Wiki which is hosted on GitHub. Data Tables \u00b6 Much of the data is presented using tables that share consistent functionality across the website. An example of a table is shown below, note the interactive features across the top of the table, these are explained after the screenshot. Show 10 entries : A selectable limiter of how many rows to display at once (maximum 100). Column visibility : Enables the user to hide and show columns columns. In the screenshot below the compactness column is hidden by deselecting it in the presented list. CSV : Will download a CSV file of the data currently shown on screen. Excel : Will download an Excel file of the data currently shown on screen. Warning Note the statement currently shown on screen - only this data will be downloaded to the CSV and Excel files. All the records are not able to be downloaded in this manner - for this it is recommened to interact with the output parquet files . Search : A search bar for the user to filter the table to the row they require. The search will take into account all appropriate columns when searching.","title":"Overview"},{"location":"exploringwebsite/websiteoverview/#website-overview","text":"This page gives an overview of the pipeline website, with links to main pages on features where appropriate. Refer to Accessing the Pipeline for details on how to access the pipeline instance that is hosted by the VAST collaboration. For admins, refer to the following pages for details of the configuration and set up of the web server: Configuration , Deployment and Web App Admin Usage .","title":"Website Overview"},{"location":"exploringwebsite/websiteoverview/#homepage","text":"The homepage provides a summary of the data currently held in the pipeline instance that has been processed. The four cards at the top of the homepage provide total values for the amount of pipeline runs, images, measurements and sources that are stored in the database. Clicking any of them will take you to the respective overview page for the data type. Note The totals presented on the homepage are totals for all pipeline runs combined! Also displayed is a sky region map that shows all the areas of the sky that have had successful and completed pipeline runs performed.","title":"Homepage"},{"location":"exploringwebsite/websiteoverview/#navbar","text":"The navbar, shown to the right, acts as the main method in which to navigate around the website. The following sections link to the respective documentation pages explainging the features of each link. Note The admin button on the navbar is only seen when the user is designated as an administrator. Tip The navbar can be collapsed by pressing the menu (or hamburger) button next to it at the top of the page.","title":"Navbar"},{"location":"exploringwebsite/websiteoverview/#admin","text":"See the website admin tools page. Allows for admins to manage users, Django Q schedules and the data itself.","title":"Admin"},{"location":"exploringwebsite/websiteoverview/#pipeline-runs","text":"See the Pipeline Run Pages doc. Navigates the user to the list of pipeline runs available, which in turn link to the detail page for each respective run.","title":"Pipeline Runs"},{"location":"exploringwebsite/websiteoverview/#sources-query","text":"See the Source Pages section. Takes the user to the source query page, where users can search for sources by defining a set of thresholds and feature requirements. From the results users can also access the detail page for individual sources.","title":"Sources Query"},{"location":"exploringwebsite/websiteoverview/#measurements","text":"See the Measurement Pages section. Navigates the user to the measurements page that features a table containing all the measurements currently held in the database. From here users can also access the detail page for individual measurements.","title":"Measurements"},{"location":"exploringwebsite/websiteoverview/#images","text":"See the Image Pages section. Takes the user to the images page that features a table containing all the images currently held in the database. From here users can also access the detail page for individual images.","title":"Images"},{"location":"exploringwebsite/websiteoverview/#external-links","text":"Documentation : Links to this documentation website. Pipeline Repository : A link to the GitHub pipeline repository. Raise an Issue : A link to open a new issue on the GitHub repository. Start a Discussion : A link to open a new discussion on the GitHub repository. VAST Links GitHub : A link to the VAST organisation GitHub page. JupyterHub : Links to the VAST hosted JupyterHub instance which includes access to the pipeline results and vast-tools . Website : Links to the VAST collaboration website. Wiki : Links to the VAST Wiki which is hosted on GitHub.","title":"External Links"},{"location":"exploringwebsite/websiteoverview/#data-tables","text":"Much of the data is presented using tables that share consistent functionality across the website. An example of a table is shown below, note the interactive features across the top of the table, these are explained after the screenshot. Show 10 entries : A selectable limiter of how many rows to display at once (maximum 100). Column visibility : Enables the user to hide and show columns columns. In the screenshot below the compactness column is hidden by deselecting it in the presented list. CSV : Will download a CSV file of the data currently shown on screen. Excel : Will download an Excel file of the data currently shown on screen. Warning Note the statement currently shown on screen - only this data will be downloaded to the CSV and Excel files. All the records are not able to be downloaded in this manner - for this it is recommened to interact with the output parquet files . Search : A search bar for the user to filter the table to the row they require. The search will take into account all appropriate columns when searching.","title":"Data Tables"},{"location":"outputs/coldesc/","text":"Column Descriptions \u00b6 This page details the columns contained in each output file. associations \u00b6 Column Unit Description source_id n/a The database id of the source for the association. meas_id n/a The database id of the measurement for the association. d2d arcsec The on-sky separation of the measurement to the source at the iteration stage the association was created. dr n/a The de Ruiter radius of the measurement to the source at the iteration stage the association was created. Will be 0 if de Ruiter assocation is not being used. bands \u00b6 Column Unit Description id n/a The database id of the band. name n/a The string name of the band, equal to the frequency value. frequency MHz The band central frequency. bandwidth MHz The bandwidth of the frequency band, will be 0 if not known. images \u00b6 Column Unit Description id n/a The database id of the image. band_id n/a The database id of the associated band. skyreg_id n/a The database id of the associated sky region. measurements_path n/a The system path to the measurements parquet file. polarisation n/a The polarisation of the image. name n/a The name of the image, taken from the filename. path n/a The system path to the image FITS file. noise_path n/a The system path to the associated noise image FITS file. background_path n/a The system path to the associated background image FITS file. datetime n/a The date and time of the observation, read from the FITS header. jd days The date and time of the observation in Julian Days. duration s The duration of the observation taken from the FITS header, if available. ra deg The central Right Ascension coordinate of the image. dec deg The central Declination coordinate of the image. fov_bmaj deg The estimated major axis field-of-view value - the radius_pixels multipled by the major axis pixel size. fov_bmin deg The estimated minor axis field-of-view value - the radius_pixels multipled by the minor axis pixel size. physical_bmaj deg The actual major axis on-sky size - the number of pixels on the major axis multiplied by the major axis pixel size. physical_bmin deg The actual minor axis on-sky size - the number of pixels on the minor axis multiplied by the minor axis pixel size. radius_pixels pixels Estimated 'diameter' of the useable image area. beam_bmaj deg The size of the major axis of the image restoring beam. beam_bmin deg The size of the minor axis of the image restoring beam. beam_bpa deg The position angle of the image restoring beam. rms_median mJy/beam The median RMS value derrived from the RMS map. rms_min mJy/beam The minimum RMS value derrived from the RMS map (pixel value). rms_max mJy/beam The maximum RMS value derrived from the RMS map (pixel value). measurements \u00b6 Tip Some columns are the same as that defined in the Selavy source finder output . Column Unit Description island_id n/a The Selavy assigned island_id. component_id n/a The Selavy assigned component_id. local_rms mJy The rms value at the location of the measurement. ra deg The right ascension coordinate of the measurement. ra_err deg The error of the right ascension coordinate of the measurement. dec deg The declination coordinate of the measurement. dec_err deg The error of the declination coordinate of the measurement. flux_peak mJy/beam The measured peak flux of the component. flux_peak_err mJy/beam The error of the measured peak flux of the component. flux_int mJy The measured integrated flux of the component. flux_int_err mJy The error of the measured integrated flux of the component. bmaj arcsec The major axis size of the fitted Gaussian (FWHM). err_bmaj deg The error of the major axis size of the fitted Gaussian (FWHM). bmin arcsec The minor axis size of the fitted Gaussian (FWHM). err_bmin deg The error of the minor axis size of the fitted Gaussian (FWHM). pa deg The position angle of the fitted Gaussian (FWHM). err_pa deg The error of the position angle of the fitted Gaussian (FWHM). psf_bmaj arcsec The Selavy deconvolved size of the major axis of the fitted Gaussian. psf_bmin arcsec The Selavy deconvolved size of the minor axis of the fitted Gaussian. psf_pa deg The Selavy deconvolved position angle of the fitted Gaussian. flag_c4 n/a Selavy flag denoting whether the component is considered formally bad (doesn't meet chi-squared criterion). chi_squared_fit n/a The Selavy quality of the fit. spectral_index n/a The fitted Selavy spectral index of the component. spectral_index_from_TT n/a Selavy flag to denote if the spectral index has been derived from the Taylor-term images ( True ). has_siblings n/a Selavy flag to denote whether the component is one of many fitted to the same island. image_id n/a The database id of the image the measurement is from. time n/a The date and time of observation the measurement is from (obtained from the image). name n/a The string name of the measurement. snr n/a The signal-to-noise ratio of the measurement. compactness n/a The compactness of the measurement ( flux_int / flux_peak ). ew_sys_err deg The systematic right ascension error assigned to the measurement. ns_sys_err deg The systematic declination error assigned to the measurement. error_radius deg The pipeline estimated error radius of the measurement. uncertainty_ew deg Total RA positional error of the measurement. uncertainty_ns deg Total Dec positional error of the measurement. weight_ew deg \\(^{-1}\\) The weight of the RA error (1/e). weight_ns deg \\(^{-1}\\) The weight of the Dec error (1/e). forced n/a Flag to denote whether the measurement is produced from the forced fitting procedure ( True ). flux_int_isl_ratio n/a The ratio of the measurements integrated flux to the total island integrated flux. flux_peak_isl_ratio n/a The ratio of the measurements peak flux to the total island peak flux. id n/a The database id of the measurement. measurement_pairs \u00b6 Column Unit Description meas_id_a n/a The database id of measurement a of the pair. meas_id_b n/a The database id of measurement b of the pair. flux_int_a mJy The integrated flux of measurement a of the pair. flux_int_err_a mJy The error of the integrated flux of measurement a of the pair. flux_peak_a mJy/beam The peak flux of measurement a of the pair. flux_peak_err_a mJy/beam The error of the peak flux of measurement a of the pair. image_name_a n/a The image name of measurement a of the pair. flux_int_b mJy The integrated flux of measurement b of the pair. flux_int_err_b mJy The error of the integrated flux of measurement b of the pair. flux_peak_b mJy/beam The peak flux of measurement b of the pair. flux_peak_err_b mJy/beam The error of the peak flux of measurement b of the pair. image_name_b n/a The image name of measurement b of the pair. vs_peak n/a The pair \\(V_s\\) value using the peak flux. vs_int n/a The pair \\(V_s\\) value using the integrated flux. m_peak n/a The pair \\(m\\) value using the peak flux. m_int n/a The pair \\(m\\) value using the integrated flux. source_id n/a The database id of the source the pair is associated to. id n/a The database id of the measurement pair. relations \u00b6 Column Unit Description from_source_id n/a The database id of the first source in the relation pair. to_source_id n/a The database id of the second source in the relation pair. skyregions \u00b6 Column Unit Description id n/a The database id of the sky region. centre_ra deg The right ascension value of the sky region central coordinate. centre_dec deg The declination value of the sky region central coordinate. width_ra deg The width of the area covered by the sky region. width_dec deg The height of the area covered by the sky region. xtr_radius deg The hypotenuse radius of the sky region. x rad The central cartesian x coordinate of the sky region. y rad The central cartesian y coordinate of the sky region. z rad The central cartesian z coordinate of the sky region. sources \u00b6 Note The index column of the sources parquet is set to the database id of the source. Column Unit Description n_meas n/a The total number of measurements associated to the source (selavy and forced). n_meas_sel n/a The total number of selavy measurements associated to the source. n_meas_forced n/a The total number of forced measurements associated to the source. n_sibl n/a The total number of measurements that have a has_sibling value of True . n_rel n/a The total number of relations the source has. wavg_ra deg The weighted average of the Right Ascension of the measurements, that acts as the source position. wavg_dec deg The weighted average of the Declination of the measurements, that acts as the source position. wavg_uncertainty_ew deg The error of the weighted average right ascension value. wavg_uncertainty_ns deg The error of the weighted average declination value. new n/a Flag to signify the source is classed as a new source ( True ). new_high_sigma n/a The largest sigma value a new source would have if it was placed at its location in the previous images it was not detected in. See New Sources for more information. Set to 0 for non-new sources. n_neighbour_dist deg The on-sky separation to the nearest source with in the same pipeline run. avg_compactness n/a The average compactness value of the associated measurements. min_snr n/a The minimum signal-to-noise ratio of the associated measurements. max_snr n/a The maximum signal-to-noise ratio of the associated measurements. avg_flux_int mJy The average integrated flux value of the measurements associated to the source (inc. forced measurements). max_flux_int mJy The maximum integrated flux value of the measurements associated to the source (inc. forced measurements). min_flux_int mJy The minimum integrated flux value of the measurements associated to the source (inc. forced measurements). avg_flux_peak mJy/beam The average peak flux value of the measurements associated to the source (inc. forced measurements). max_flux_peak mJy/beam The maximum peak flux value of the measurements associated to the source (inc. forced measurements). min_flux_peak mJy/beam The minimum peak flux value of the measurements associated to the source (inc. forced measurements). min_flux_peak_isl_ratio n/a The minimum ratio of the peak flux to the total island peak flux of the measurements associated to the source. min_flux_int_isl_ratio n/a The minimum ratio of the integrated flux to the total island integrated flux of the measurements associated to the source. v_int n/a The calculated variability \\(V\\) metric using the integrated flux values. See variability statistics . v_peak n/a The calculated variability \\(V\\) metric using the peak flux values. See variability statistics . eta_int n/a The calculated variability \\(\\eta\\) metric using the integrated flux. See variability statistics . eta_peak n/a The calculated variability \\(\\eta\\) metric using the peak flux. See variability statistics . vs_abs_significant_max_peak n/a The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. See variability statistics . m_abs_significant_max_peak n/a The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. See variability statistics . vs_abs_significant_max_int n/a The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. See variability statistics . m_abs_significant_max_int n/a The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. See variability statistics .","title":"Column Descriptions"},{"location":"outputs/coldesc/#column-descriptions","text":"This page details the columns contained in each output file.","title":"Column Descriptions"},{"location":"outputs/coldesc/#associations","text":"Column Unit Description source_id n/a The database id of the source for the association. meas_id n/a The database id of the measurement for the association. d2d arcsec The on-sky separation of the measurement to the source at the iteration stage the association was created. dr n/a The de Ruiter radius of the measurement to the source at the iteration stage the association was created. Will be 0 if de Ruiter assocation is not being used.","title":"associations"},{"location":"outputs/coldesc/#bands","text":"Column Unit Description id n/a The database id of the band. name n/a The string name of the band, equal to the frequency value. frequency MHz The band central frequency. bandwidth MHz The bandwidth of the frequency band, will be 0 if not known.","title":"bands"},{"location":"outputs/coldesc/#images","text":"Column Unit Description id n/a The database id of the image. band_id n/a The database id of the associated band. skyreg_id n/a The database id of the associated sky region. measurements_path n/a The system path to the measurements parquet file. polarisation n/a The polarisation of the image. name n/a The name of the image, taken from the filename. path n/a The system path to the image FITS file. noise_path n/a The system path to the associated noise image FITS file. background_path n/a The system path to the associated background image FITS file. datetime n/a The date and time of the observation, read from the FITS header. jd days The date and time of the observation in Julian Days. duration s The duration of the observation taken from the FITS header, if available. ra deg The central Right Ascension coordinate of the image. dec deg The central Declination coordinate of the image. fov_bmaj deg The estimated major axis field-of-view value - the radius_pixels multipled by the major axis pixel size. fov_bmin deg The estimated minor axis field-of-view value - the radius_pixels multipled by the minor axis pixel size. physical_bmaj deg The actual major axis on-sky size - the number of pixels on the major axis multiplied by the major axis pixel size. physical_bmin deg The actual minor axis on-sky size - the number of pixels on the minor axis multiplied by the minor axis pixel size. radius_pixels pixels Estimated 'diameter' of the useable image area. beam_bmaj deg The size of the major axis of the image restoring beam. beam_bmin deg The size of the minor axis of the image restoring beam. beam_bpa deg The position angle of the image restoring beam. rms_median mJy/beam The median RMS value derrived from the RMS map. rms_min mJy/beam The minimum RMS value derrived from the RMS map (pixel value). rms_max mJy/beam The maximum RMS value derrived from the RMS map (pixel value).","title":"images"},{"location":"outputs/coldesc/#measurements","text":"Tip Some columns are the same as that defined in the Selavy source finder output . Column Unit Description island_id n/a The Selavy assigned island_id. component_id n/a The Selavy assigned component_id. local_rms mJy The rms value at the location of the measurement. ra deg The right ascension coordinate of the measurement. ra_err deg The error of the right ascension coordinate of the measurement. dec deg The declination coordinate of the measurement. dec_err deg The error of the declination coordinate of the measurement. flux_peak mJy/beam The measured peak flux of the component. flux_peak_err mJy/beam The error of the measured peak flux of the component. flux_int mJy The measured integrated flux of the component. flux_int_err mJy The error of the measured integrated flux of the component. bmaj arcsec The major axis size of the fitted Gaussian (FWHM). err_bmaj deg The error of the major axis size of the fitted Gaussian (FWHM). bmin arcsec The minor axis size of the fitted Gaussian (FWHM). err_bmin deg The error of the minor axis size of the fitted Gaussian (FWHM). pa deg The position angle of the fitted Gaussian (FWHM). err_pa deg The error of the position angle of the fitted Gaussian (FWHM). psf_bmaj arcsec The Selavy deconvolved size of the major axis of the fitted Gaussian. psf_bmin arcsec The Selavy deconvolved size of the minor axis of the fitted Gaussian. psf_pa deg The Selavy deconvolved position angle of the fitted Gaussian. flag_c4 n/a Selavy flag denoting whether the component is considered formally bad (doesn't meet chi-squared criterion). chi_squared_fit n/a The Selavy quality of the fit. spectral_index n/a The fitted Selavy spectral index of the component. spectral_index_from_TT n/a Selavy flag to denote if the spectral index has been derived from the Taylor-term images ( True ). has_siblings n/a Selavy flag to denote whether the component is one of many fitted to the same island. image_id n/a The database id of the image the measurement is from. time n/a The date and time of observation the measurement is from (obtained from the image). name n/a The string name of the measurement. snr n/a The signal-to-noise ratio of the measurement. compactness n/a The compactness of the measurement ( flux_int / flux_peak ). ew_sys_err deg The systematic right ascension error assigned to the measurement. ns_sys_err deg The systematic declination error assigned to the measurement. error_radius deg The pipeline estimated error radius of the measurement. uncertainty_ew deg Total RA positional error of the measurement. uncertainty_ns deg Total Dec positional error of the measurement. weight_ew deg \\(^{-1}\\) The weight of the RA error (1/e). weight_ns deg \\(^{-1}\\) The weight of the Dec error (1/e). forced n/a Flag to denote whether the measurement is produced from the forced fitting procedure ( True ). flux_int_isl_ratio n/a The ratio of the measurements integrated flux to the total island integrated flux. flux_peak_isl_ratio n/a The ratio of the measurements peak flux to the total island peak flux. id n/a The database id of the measurement.","title":"measurements"},{"location":"outputs/coldesc/#measurement_pairs","text":"Column Unit Description meas_id_a n/a The database id of measurement a of the pair. meas_id_b n/a The database id of measurement b of the pair. flux_int_a mJy The integrated flux of measurement a of the pair. flux_int_err_a mJy The error of the integrated flux of measurement a of the pair. flux_peak_a mJy/beam The peak flux of measurement a of the pair. flux_peak_err_a mJy/beam The error of the peak flux of measurement a of the pair. image_name_a n/a The image name of measurement a of the pair. flux_int_b mJy The integrated flux of measurement b of the pair. flux_int_err_b mJy The error of the integrated flux of measurement b of the pair. flux_peak_b mJy/beam The peak flux of measurement b of the pair. flux_peak_err_b mJy/beam The error of the peak flux of measurement b of the pair. image_name_b n/a The image name of measurement b of the pair. vs_peak n/a The pair \\(V_s\\) value using the peak flux. vs_int n/a The pair \\(V_s\\) value using the integrated flux. m_peak n/a The pair \\(m\\) value using the peak flux. m_int n/a The pair \\(m\\) value using the integrated flux. source_id n/a The database id of the source the pair is associated to. id n/a The database id of the measurement pair.","title":"measurement_pairs"},{"location":"outputs/coldesc/#relations","text":"Column Unit Description from_source_id n/a The database id of the first source in the relation pair. to_source_id n/a The database id of the second source in the relation pair.","title":"relations"},{"location":"outputs/coldesc/#skyregions","text":"Column Unit Description id n/a The database id of the sky region. centre_ra deg The right ascension value of the sky region central coordinate. centre_dec deg The declination value of the sky region central coordinate. width_ra deg The width of the area covered by the sky region. width_dec deg The height of the area covered by the sky region. xtr_radius deg The hypotenuse radius of the sky region. x rad The central cartesian x coordinate of the sky region. y rad The central cartesian y coordinate of the sky region. z rad The central cartesian z coordinate of the sky region.","title":"skyregions"},{"location":"outputs/coldesc/#sources","text":"Note The index column of the sources parquet is set to the database id of the source. Column Unit Description n_meas n/a The total number of measurements associated to the source (selavy and forced). n_meas_sel n/a The total number of selavy measurements associated to the source. n_meas_forced n/a The total number of forced measurements associated to the source. n_sibl n/a The total number of measurements that have a has_sibling value of True . n_rel n/a The total number of relations the source has. wavg_ra deg The weighted average of the Right Ascension of the measurements, that acts as the source position. wavg_dec deg The weighted average of the Declination of the measurements, that acts as the source position. wavg_uncertainty_ew deg The error of the weighted average right ascension value. wavg_uncertainty_ns deg The error of the weighted average declination value. new n/a Flag to signify the source is classed as a new source ( True ). new_high_sigma n/a The largest sigma value a new source would have if it was placed at its location in the previous images it was not detected in. See New Sources for more information. Set to 0 for non-new sources. n_neighbour_dist deg The on-sky separation to the nearest source with in the same pipeline run. avg_compactness n/a The average compactness value of the associated measurements. min_snr n/a The minimum signal-to-noise ratio of the associated measurements. max_snr n/a The maximum signal-to-noise ratio of the associated measurements. avg_flux_int mJy The average integrated flux value of the measurements associated to the source (inc. forced measurements). max_flux_int mJy The maximum integrated flux value of the measurements associated to the source (inc. forced measurements). min_flux_int mJy The minimum integrated flux value of the measurements associated to the source (inc. forced measurements). avg_flux_peak mJy/beam The average peak flux value of the measurements associated to the source (inc. forced measurements). max_flux_peak mJy/beam The maximum peak flux value of the measurements associated to the source (inc. forced measurements). min_flux_peak mJy/beam The minimum peak flux value of the measurements associated to the source (inc. forced measurements). min_flux_peak_isl_ratio n/a The minimum ratio of the peak flux to the total island peak flux of the measurements associated to the source. min_flux_int_isl_ratio n/a The minimum ratio of the integrated flux to the total island integrated flux of the measurements associated to the source. v_int n/a The calculated variability \\(V\\) metric using the integrated flux values. See variability statistics . v_peak n/a The calculated variability \\(V\\) metric using the peak flux values. See variability statistics . eta_int n/a The calculated variability \\(\\eta\\) metric using the integrated flux. See variability statistics . eta_peak n/a The calculated variability \\(\\eta\\) metric using the peak flux. See variability statistics . vs_abs_significant_max_peak n/a The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. See variability statistics . m_abs_significant_max_peak n/a The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. See variability statistics . vs_abs_significant_max_int n/a The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. See variability statistics . m_abs_significant_max_int n/a The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. See variability statistics .","title":"sources"},{"location":"outputs/outputs/","text":"Outputs Overview \u00b6 This page gives details on the output files that the pipeline writes to disk. Pipeline Run Output Overview \u00b6 The output for a pipeline run will be located in the pipeline working directory, which is defined at the pipeline configuration stage (see Pipeline Configuration ). A sub-directory will exist for each pipeline run that contains the output products for the run. Note If you do not administrate your system or do not have access to a vast-tools notebook interface, please contact your system admin to confirm the working directory and how to best access the files. The pipeline uses the Apache Parquet file format to write results to disk. Details on how to read these files can be found below in Reading the Outputs . Below is the output structure for a pipeline run named new-test-data when the pipeline run option CREATE_MEASUREMENTS_ARROW_FILES has been set to True and the working directory is named pipeline-runs (see File Details for descriptions): pipeline-runs \u251c\u2500\u2500 new-test-data \u2502 \u251c\u2500\u2500 associations.parquet \u2502 \u251c\u2500\u2500 bands.parquet \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 config_prev.py \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH02_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH03x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH02_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH03x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH12_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 images.parquet \u2502 \u251c\u2500\u2500 log.txt \u2502 \u251c\u2500\u2500 measurements.arrow \u2502 \u251c\u2500\u2500 measurement_pairs.arrow \u2502 \u251c\u2500\u2500 measurement_pairs.parquet \u2502 \u251c\u2500\u2500 relations.parquet \u2502 \u251c\u2500\u2500 skyregions.parquet \u2502 \u2514\u2500\u2500 sources.parquet Arrow Files \u00b6 Large pipeline runs (hundreds of images) mean that to read the measurements, hundreds of parquet files need to be read in, and can contain millions of rows. This can be slow using libraries such as pandas, and also consumes a lot of system memory. A solution to this is to save all the measurements associated with the pipeline run into one single file in the Apache Arrow format. The library vaex is able to open .arrow files in an out-of-core context so the memory footprint is hugely reduced along with the reading of the file being very fast. The two-epoch measurement pairs are also saved to arrow format due to the same reasons. See Reading with vaex for further details on using vaex . Note At the time of development vaex could not open parquets in an out-of-core context. This will be reviewed in the future if such functionality is added to vaex . Bug Currently the arrow files cannot be generated by runs that are processed through the website. In this case please contact the pipeline administrator who can generate the arrow files after a run has successfully completed (refer to the admin command createmaeasarrow ). To enable the arrow files to be produced, the option CREATE_MEASUREMENTS_ARROW_FILES is required to be set to True in the pipeline run config. Image Data \u00b6 The data for the images ingested into the pipeline is also stored in the pipeline working directory under the subdirectory images : pipeline-runs \u251c\u2500\u2500 images \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH02_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH03x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH02_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH03x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u2514\u2500\u2500 VAST_2118-06A_EPOCH12_I_cutout_fits \u2502 \u2514\u2500\u2500 measurements.parquet Here, for each image, the selavy measurements that have been ingested are stored in the parquet format under a subdirectory of the respective image name. File Details \u00b6 File Description associations.parquet Contains the association information between sources and measurements. bands.parquet Contains the information of the bands associated with the pipeline run. config.py The pipeline run configuration file. config_prev.py The previous pipeline run configuration file used by the add image mode. forced_measurements*.parquet Multiple files that contain the forced measurements extracted from the respective image denoted in the filename. images.parquet Contains the information of the images processed in the pipeline run. log.txt The log file of the pipeline run. measurements.arrow An Apache Arrow format file containing all the measurements associated with the pipeline run (see Arrow Files ). measurement_pairs.arrow An Apache Arrow format file containing all the measurement pair metrics (see Arrow Files ). measurement_pairs.parquet Contains all the measurement pairs metrics. relations.parquet Contains the relation information between sources. skyregions.parquet Contains the sky region information of the pipeline run. sources.parquet Contains all the sources resulting from teh pipeline run.","title":"Outputs Overview"},{"location":"outputs/outputs/#outputs-overview","text":"This page gives details on the output files that the pipeline writes to disk.","title":"Outputs Overview"},{"location":"outputs/outputs/#pipeline-run-output-overview","text":"The output for a pipeline run will be located in the pipeline working directory, which is defined at the pipeline configuration stage (see Pipeline Configuration ). A sub-directory will exist for each pipeline run that contains the output products for the run. Note If you do not administrate your system or do not have access to a vast-tools notebook interface, please contact your system admin to confirm the working directory and how to best access the files. The pipeline uses the Apache Parquet file format to write results to disk. Details on how to read these files can be found below in Reading the Outputs . Below is the output structure for a pipeline run named new-test-data when the pipeline run option CREATE_MEASUREMENTS_ARROW_FILES has been set to True and the working directory is named pipeline-runs (see File Details for descriptions): pipeline-runs \u251c\u2500\u2500 new-test-data \u2502 \u251c\u2500\u2500 associations.parquet \u2502 \u251c\u2500\u2500 bands.parquet \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 config_prev.py \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH02_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH03x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH02_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH03x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH12_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 images.parquet \u2502 \u251c\u2500\u2500 log.txt \u2502 \u251c\u2500\u2500 measurements.arrow \u2502 \u251c\u2500\u2500 measurement_pairs.arrow \u2502 \u251c\u2500\u2500 measurement_pairs.parquet \u2502 \u251c\u2500\u2500 relations.parquet \u2502 \u251c\u2500\u2500 skyregions.parquet \u2502 \u2514\u2500\u2500 sources.parquet","title":"Pipeline Run Output Overview"},{"location":"outputs/outputs/#arrow-files","text":"Large pipeline runs (hundreds of images) mean that to read the measurements, hundreds of parquet files need to be read in, and can contain millions of rows. This can be slow using libraries such as pandas, and also consumes a lot of system memory. A solution to this is to save all the measurements associated with the pipeline run into one single file in the Apache Arrow format. The library vaex is able to open .arrow files in an out-of-core context so the memory footprint is hugely reduced along with the reading of the file being very fast. The two-epoch measurement pairs are also saved to arrow format due to the same reasons. See Reading with vaex for further details on using vaex . Note At the time of development vaex could not open parquets in an out-of-core context. This will be reviewed in the future if such functionality is added to vaex . Bug Currently the arrow files cannot be generated by runs that are processed through the website. In this case please contact the pipeline administrator who can generate the arrow files after a run has successfully completed (refer to the admin command createmaeasarrow ). To enable the arrow files to be produced, the option CREATE_MEASUREMENTS_ARROW_FILES is required to be set to True in the pipeline run config.","title":"Arrow Files"},{"location":"outputs/outputs/#image-data","text":"The data for the images ingested into the pipeline is also stored in the pipeline working directory under the subdirectory images : pipeline-runs \u251c\u2500\u2500 images \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH02_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH03x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH02_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH03x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u2514\u2500\u2500 VAST_2118-06A_EPOCH12_I_cutout_fits \u2502 \u2514\u2500\u2500 measurements.parquet Here, for each image, the selavy measurements that have been ingested are stored in the parquet format under a subdirectory of the respective image name.","title":"Image Data"},{"location":"outputs/outputs/#file-details","text":"File Description associations.parquet Contains the association information between sources and measurements. bands.parquet Contains the information of the bands associated with the pipeline run. config.py The pipeline run configuration file. config_prev.py The previous pipeline run configuration file used by the add image mode. forced_measurements*.parquet Multiple files that contain the forced measurements extracted from the respective image denoted in the filename. images.parquet Contains the information of the images processed in the pipeline run. log.txt The log file of the pipeline run. measurements.arrow An Apache Arrow format file containing all the measurements associated with the pipeline run (see Arrow Files ). measurement_pairs.arrow An Apache Arrow format file containing all the measurement pair metrics (see Arrow Files ). measurement_pairs.parquet Contains all the measurement pairs metrics. relations.parquet Contains the relation information between sources. skyregions.parquet Contains the sky region information of the pipeline run. sources.parquet Contains all the sources resulting from teh pipeline run.","title":"File Details"},{"location":"outputs/usingoutputs/","text":"Using the Outputs \u00b6 This page gives details on how to open and use the pipeline output files. It is recommended to use pandas or vaex to read the pipeline results from the parquet files. See the sections below for more information on using each library. Note It is also possible to use Dask to read the parquets in an out-of-core context but the general performance can sometimes be poor with many parquet files. vaex is the preferred out-of-core method. Tip Be sure to look at vast-tools , a ready-made library for exploring pipeline results! Reading with pandas \u00b6 pandas documentation . Warning pyarrow will be required to open parquets with pandas . We recommend using this instead of fastparquet . To open a parquet using pandas use the read_parquet method: import pandas as pd sources = pd . read_parquet ( 'pipeline-runs/new-test-data/sources.parquet' ) sources . head () n_meas_forced n_meas ... vs_abs_significant_max_int m_abs_significant_max_int id ... 1 0 3 ... 55.050146 0.191083 2 1 3 ... 29.367098 0.525999 3 0 3 ... 4.388447 0.199877 4 1 3 ... 20.000058 1.047998 5 0 3 ... 0.000000 0.000000 [ 5 rows x 31 columns ] To read multiple parquets at once using pandas a loop must be used: import glob import pandas as pd files = glob . glob ( \"pipeline-runs/images/*/measurements.parquet\" ) data = [ pd . read_parquet ( f ) for f in files ] measurements = pd . concat ( data , ignore_index = True ) Tip If you don't require all the columns you can specify which columes to read using the columns variable. sources = pd . read_parquet ( 'pipeline-runs/new-test-data/sources.parquet' , columns = [ 'id' , 'n_meas' ]) Reading with vaex \u00b6 vaex documentation . Warning vaex is a young project so bugs may be expected along with frequent updates. It has currently been tested with version 3.0.0 . Version 4.0.0 promises opening parquet files in an out-of-core context. Warning Some pipeline parquet format files do not open with vaex 3.0.0. arrow format files should open successfully. A parquet, or arrow file, can be opened using the open() method: import vaex measurements = vaex . open ( 'pipeline-runs/new-test-data/measurements.arrow' ) measurements . head () # source island_id component_id local_rms ra ra_err dec dec_err flux_peak flux_peak_err flux_int flux_int_err bmaj err_bmaj bmin err_bmin pa err_pa psf_bmaj psf_bmin psf_pa flag_c4 chi_squared_fit spectral_index spectral_index_from_TT has_siblings image_id time name snr compactness ew_sys_err ns_sys_err error_radius uncertainty_ew uncertainty_ns weight_ew weight_ns forced flux_int_isl_ratio flux_peak_isl_ratio id 0 730 SB00004_island_1 SB00004_component_1a 0.463596 321.902 4.42819e-06 - 4.20097 2.2637e-06 307.991 0.4742 425.175 1.04728 20.95 1.08838e-05 12.28 4.32359e-06 108.19 0.00160065 15.58 0 - 73.86 False 3516.59 - 99 True True 2 2019 - 08 - 27 13 : 38 : 38.810000000 VAST_2118 + 00 A_SB00004_component_1a 664.352 1.38048 0.000277778 0.000277778 5.05099e-06 0.000277824 0.000277824 1.29557e+07 1.29557e+07 False 0.651019 0.709182 204 1 730 SB00009_island_1 SB00009_component_1a 0.463422 321.902 3.52062e-06 - 4.20103 2.45968e-06 318.544 0.472982 349.471 0.87264 21.4 8.50698e-06 12.78 5.46908e-06 107.02 0.0020224 5.72 0 46.03 True 15427.1 - 99 True True 3 2019 - 08 - 27 18 : 52 : 00.556000000 VAST_2118 - 06 A_SB00009_component_1a 687.374 1.09709 0.000277778 0.000277778 4.26887e-06 0.000277811 0.000277811 1.29569e+07 1.29569e+07 False 0.721399 0.7483 352 2 730 SB00006_island_1 SB00006_component_1a 0.627357 321.901 4.58076e-06 - 4.20086 2.92861e-06 310.503 0.662503 421.137 1.4171 17.05 1.07873e-05 12.42 6.89559e-06 90.71 0.00438684 10.51 4.24 - 82.52 False 4483.74 - 99 True True 5 2019 - 10 - 29 10 : 28 : 07.911000000 VAST_2118 + 00 A_SB00006_component_1a 494.938 1.35631 0.000277778 0.000277778 5.46682e-06 0.000277832 0.000277832 1.2955e+07 1.2955e+07 False 0.677562 0.721427 670 3 730 SB00011_island_1 SB00011_component_1a 0.627496 321.901 3.92144e-06 - 4.20087 3.21715e-06 310.998 0.643049 350.901 1.21083 17.06 9.23494e-06 12.43 7.57501e-06 91.19 0.00481863 6.43 0 27.62 False 4405.81 - 99 True True 4 2019 - 10 - 29 13 : 39 : 33.996000000 VAST_2118 - 06 A_SB00011_component_1a 495.618 1.12831 0.000277778 0.000277778 5.05099e-06 0.000277824 0.000277824 1.29557e+07 1.29557e+07 False 0.677576 0.721816 511 4 730 SB00005_island_1 SB00005_component_1a 0.346147 321.901 1.83032e-06 - 4.20051 1.71797e-06 299.072 0.342783 288.462 0.579893 14.13 4.37963e-06 12.14 3.97013e-06 65.14 0.00546325 2.86 0 6.42 False 2661.49 - 99 True True 7 2019 - 10 - 30 09 : 10 : 04.340000000 VAST_2118 + 00 A_SB00005_component_1a 864.004 0.964524 0.000277778 0.000277778 2.69987e-06 0.000277791 0.000277791 1.29588e+07 1.29588e+07 False 0.659887 0.719556 976 5 730 SB00010_island_1 SB00010_component_1a 0.347692 321.901 1.97328e-06 - 4.20052 1.75466e-06 300.969 0.360198 353.643 0.695754 14.16 4.76862e-06 12.16 3.99059e-06 65.77 0.00546515 6.12 3.95 49.18 False 2412.18 - 99 True True 6 2019 - 10 - 30 10 : 11 : 56.913000000 VAST_2118 - 06 A_SB00010_component_1a 865.62 1.17502 0.000277778 0.000277778 2.56132e-06 0.00027779 0.00027779 1.29589e+07 1.29589e+07 False 0.664605 0.723765 816 6 730 SB00007_island_1 SB00007_component_1a 0.387605 321.901 2.03947e-06 - 4.20032 1.77854e-06 317.014 0.392106 332.662 0.701451 14.56 4.96382e-06 11.54 3.99573e-06 64.78 0.00375775 4.81 0 24.08 False 1486.99 - 99 True True 10 2020 - 01 - 11 05 : 27 : 24.605000000 VAST_2118 + 00 A_SB00007_component_1a 817.88 1.04936 0.000277778 0.000277778 2.83165e-06 0.000277792 0.000277792 1.29587e+07 1.29587e+07 False 0.666924 0.716339 1493 7 730 SB00012_island_1 SB00012_component_1a 0.391978 321.901 2.12442e-06 - 4.20032 1.81129e-06 318.042 0.404457 365.987 0.770202 14.57 5.18203e-06 11.53 4.0454e-06 65.33 0.00378202 5.75 3.63 46.53 False 1328.58 - 99 True True 9 2020 - 01 - 11 05 : 40 : 11.007000000 VAST_2118 - 06 A_SB00012_component_1a 811.376 1.15075 0.000277778 0.000277778 2.69987e-06 0.000277791 0.000277791 1.29588e+07 1.29588e+07 False 0.667317 0.717746 1330 8 730 SB00008_island_1 SB00008_component_1a 0.432726 321.901 2.98443e-06 - 4.20052 2.32201e-06 293.737 0.436863 309.072 0.784657 18.35 7.14713e-06 12.12 5.31097e-06 105.78 0.00261377 4.82 0 18.62 False 2448.82 - 99 True True 13 2020 - 01 - 12 05 : 23 : 07.478000000 VAST_2118 + 00 A_SB00008_component_1a 678.807 1.05221 0.000277778 0.000277778 3.81819e-06 0.000277804 0.000277804 1.29576e+07 1.29576e+07 False 0.63994 0.725001 1889 9 730 SB00013_island_1 SB00013_component_1a 0.437279 321.901 3.14407e-06 - 4.20052 2.36161e-06 294.141 0.451346 340.92 0.864347 18.38 7.55055e-06 12.12 5.36009e-06 106.18 0.00262701 6.01 4 51.55 False 2368.93 - 99 True True 12 2020 - 01 - 12 05 : 36 : 03.834000000 VAST_2118 - 06 A_SB00013_component_1a 672.663 1.15903 0.000277778 0.000277778 4.00455e-06 0.000277807 0.000277807 1.29573e+07 1.29573e+07 False 0.640807 0.72508 1740 Multiple parquet files can be opened at once using the open_many() method: import glob import vaex files = glob . glob ( \"pipeline-runs/images/*/measurements.parquet\" ) measurements = vaex . open_many ( files ) Tip You can convert a vaex dataframe to pandas by using the to_pandas_df() method: import vaex sources = vaex . open ( 'pipeline-runs/new-test-data/sources.parquet' ) sources = sources . to_pandas_df () Linking the Results \u00b6 The table below shows what parameters act as keys to link data from the different results tables. Tip If loading the measurements via the .arrow file, then the measurements already have the source column in-place. Tip The images.parquet file contains the column measurements_path which can be used to get the filepaths for all the selavy parquet files. Data Column Linked to Column associations.parquet meas_id measurements.parquet , forced_measurements*.parquet id associations.parquet source_id sources.parquet id (index column) measurements.parquet , forced_measurements*.parquet image_id images.parquet id images.parquet band_id bands.parquet id images.parquet skyreg_id skyregions.parquet id measurement_pairs.parquet meas_id_a , meas_id_b measurements.parquet , forced_measurements*.parquet id measurement_pairs.parquet source_id sources.parquet id (index column) relations.parquet from_source_id , to_source_id sources.parquet id (index column) vast-tools \u00b6 Link to the vast-tools repository . VAST has developed a python library called vast-tools that makes the exploration of results from the pipeline simple and efficient, in addition to being designed to be used in a Jupyter Notebook environment. For full details, refer to the repository located on GitHub and be sure to take a look at the example notebooks .","title":"Using the Outputs"},{"location":"outputs/usingoutputs/#using-the-outputs","text":"This page gives details on how to open and use the pipeline output files. It is recommended to use pandas or vaex to read the pipeline results from the parquet files. See the sections below for more information on using each library. Note It is also possible to use Dask to read the parquets in an out-of-core context but the general performance can sometimes be poor with many parquet files. vaex is the preferred out-of-core method. Tip Be sure to look at vast-tools , a ready-made library for exploring pipeline results!","title":"Using the Outputs"},{"location":"outputs/usingoutputs/#reading-with-pandas","text":"pandas documentation . Warning pyarrow will be required to open parquets with pandas . We recommend using this instead of fastparquet . To open a parquet using pandas use the read_parquet method: import pandas as pd sources = pd . read_parquet ( 'pipeline-runs/new-test-data/sources.parquet' ) sources . head () n_meas_forced n_meas ... vs_abs_significant_max_int m_abs_significant_max_int id ... 1 0 3 ... 55.050146 0.191083 2 1 3 ... 29.367098 0.525999 3 0 3 ... 4.388447 0.199877 4 1 3 ... 20.000058 1.047998 5 0 3 ... 0.000000 0.000000 [ 5 rows x 31 columns ] To read multiple parquets at once using pandas a loop must be used: import glob import pandas as pd files = glob . glob ( \"pipeline-runs/images/*/measurements.parquet\" ) data = [ pd . read_parquet ( f ) for f in files ] measurements = pd . concat ( data , ignore_index = True ) Tip If you don't require all the columns you can specify which columes to read using the columns variable. sources = pd . read_parquet ( 'pipeline-runs/new-test-data/sources.parquet' , columns = [ 'id' , 'n_meas' ])","title":"Reading with pandas"},{"location":"outputs/usingoutputs/#reading-with-vaex","text":"vaex documentation . Warning vaex is a young project so bugs may be expected along with frequent updates. It has currently been tested with version 3.0.0 . Version 4.0.0 promises opening parquet files in an out-of-core context. Warning Some pipeline parquet format files do not open with vaex 3.0.0. arrow format files should open successfully. A parquet, or arrow file, can be opened using the open() method: import vaex measurements = vaex . open ( 'pipeline-runs/new-test-data/measurements.arrow' ) measurements . head () # source island_id component_id local_rms ra ra_err dec dec_err flux_peak flux_peak_err flux_int flux_int_err bmaj err_bmaj bmin err_bmin pa err_pa psf_bmaj psf_bmin psf_pa flag_c4 chi_squared_fit spectral_index spectral_index_from_TT has_siblings image_id time name snr compactness ew_sys_err ns_sys_err error_radius uncertainty_ew uncertainty_ns weight_ew weight_ns forced flux_int_isl_ratio flux_peak_isl_ratio id 0 730 SB00004_island_1 SB00004_component_1a 0.463596 321.902 4.42819e-06 - 4.20097 2.2637e-06 307.991 0.4742 425.175 1.04728 20.95 1.08838e-05 12.28 4.32359e-06 108.19 0.00160065 15.58 0 - 73.86 False 3516.59 - 99 True True 2 2019 - 08 - 27 13 : 38 : 38.810000000 VAST_2118 + 00 A_SB00004_component_1a 664.352 1.38048 0.000277778 0.000277778 5.05099e-06 0.000277824 0.000277824 1.29557e+07 1.29557e+07 False 0.651019 0.709182 204 1 730 SB00009_island_1 SB00009_component_1a 0.463422 321.902 3.52062e-06 - 4.20103 2.45968e-06 318.544 0.472982 349.471 0.87264 21.4 8.50698e-06 12.78 5.46908e-06 107.02 0.0020224 5.72 0 46.03 True 15427.1 - 99 True True 3 2019 - 08 - 27 18 : 52 : 00.556000000 VAST_2118 - 06 A_SB00009_component_1a 687.374 1.09709 0.000277778 0.000277778 4.26887e-06 0.000277811 0.000277811 1.29569e+07 1.29569e+07 False 0.721399 0.7483 352 2 730 SB00006_island_1 SB00006_component_1a 0.627357 321.901 4.58076e-06 - 4.20086 2.92861e-06 310.503 0.662503 421.137 1.4171 17.05 1.07873e-05 12.42 6.89559e-06 90.71 0.00438684 10.51 4.24 - 82.52 False 4483.74 - 99 True True 5 2019 - 10 - 29 10 : 28 : 07.911000000 VAST_2118 + 00 A_SB00006_component_1a 494.938 1.35631 0.000277778 0.000277778 5.46682e-06 0.000277832 0.000277832 1.2955e+07 1.2955e+07 False 0.677562 0.721427 670 3 730 SB00011_island_1 SB00011_component_1a 0.627496 321.901 3.92144e-06 - 4.20087 3.21715e-06 310.998 0.643049 350.901 1.21083 17.06 9.23494e-06 12.43 7.57501e-06 91.19 0.00481863 6.43 0 27.62 False 4405.81 - 99 True True 4 2019 - 10 - 29 13 : 39 : 33.996000000 VAST_2118 - 06 A_SB00011_component_1a 495.618 1.12831 0.000277778 0.000277778 5.05099e-06 0.000277824 0.000277824 1.29557e+07 1.29557e+07 False 0.677576 0.721816 511 4 730 SB00005_island_1 SB00005_component_1a 0.346147 321.901 1.83032e-06 - 4.20051 1.71797e-06 299.072 0.342783 288.462 0.579893 14.13 4.37963e-06 12.14 3.97013e-06 65.14 0.00546325 2.86 0 6.42 False 2661.49 - 99 True True 7 2019 - 10 - 30 09 : 10 : 04.340000000 VAST_2118 + 00 A_SB00005_component_1a 864.004 0.964524 0.000277778 0.000277778 2.69987e-06 0.000277791 0.000277791 1.29588e+07 1.29588e+07 False 0.659887 0.719556 976 5 730 SB00010_island_1 SB00010_component_1a 0.347692 321.901 1.97328e-06 - 4.20052 1.75466e-06 300.969 0.360198 353.643 0.695754 14.16 4.76862e-06 12.16 3.99059e-06 65.77 0.00546515 6.12 3.95 49.18 False 2412.18 - 99 True True 6 2019 - 10 - 30 10 : 11 : 56.913000000 VAST_2118 - 06 A_SB00010_component_1a 865.62 1.17502 0.000277778 0.000277778 2.56132e-06 0.00027779 0.00027779 1.29589e+07 1.29589e+07 False 0.664605 0.723765 816 6 730 SB00007_island_1 SB00007_component_1a 0.387605 321.901 2.03947e-06 - 4.20032 1.77854e-06 317.014 0.392106 332.662 0.701451 14.56 4.96382e-06 11.54 3.99573e-06 64.78 0.00375775 4.81 0 24.08 False 1486.99 - 99 True True 10 2020 - 01 - 11 05 : 27 : 24.605000000 VAST_2118 + 00 A_SB00007_component_1a 817.88 1.04936 0.000277778 0.000277778 2.83165e-06 0.000277792 0.000277792 1.29587e+07 1.29587e+07 False 0.666924 0.716339 1493 7 730 SB00012_island_1 SB00012_component_1a 0.391978 321.901 2.12442e-06 - 4.20032 1.81129e-06 318.042 0.404457 365.987 0.770202 14.57 5.18203e-06 11.53 4.0454e-06 65.33 0.00378202 5.75 3.63 46.53 False 1328.58 - 99 True True 9 2020 - 01 - 11 05 : 40 : 11.007000000 VAST_2118 - 06 A_SB00012_component_1a 811.376 1.15075 0.000277778 0.000277778 2.69987e-06 0.000277791 0.000277791 1.29588e+07 1.29588e+07 False 0.667317 0.717746 1330 8 730 SB00008_island_1 SB00008_component_1a 0.432726 321.901 2.98443e-06 - 4.20052 2.32201e-06 293.737 0.436863 309.072 0.784657 18.35 7.14713e-06 12.12 5.31097e-06 105.78 0.00261377 4.82 0 18.62 False 2448.82 - 99 True True 13 2020 - 01 - 12 05 : 23 : 07.478000000 VAST_2118 + 00 A_SB00008_component_1a 678.807 1.05221 0.000277778 0.000277778 3.81819e-06 0.000277804 0.000277804 1.29576e+07 1.29576e+07 False 0.63994 0.725001 1889 9 730 SB00013_island_1 SB00013_component_1a 0.437279 321.901 3.14407e-06 - 4.20052 2.36161e-06 294.141 0.451346 340.92 0.864347 18.38 7.55055e-06 12.12 5.36009e-06 106.18 0.00262701 6.01 4 51.55 False 2368.93 - 99 True True 12 2020 - 01 - 12 05 : 36 : 03.834000000 VAST_2118 - 06 A_SB00013_component_1a 672.663 1.15903 0.000277778 0.000277778 4.00455e-06 0.000277807 0.000277807 1.29573e+07 1.29573e+07 False 0.640807 0.72508 1740 Multiple parquet files can be opened at once using the open_many() method: import glob import vaex files = glob . glob ( \"pipeline-runs/images/*/measurements.parquet\" ) measurements = vaex . open_many ( files ) Tip You can convert a vaex dataframe to pandas by using the to_pandas_df() method: import vaex sources = vaex . open ( 'pipeline-runs/new-test-data/sources.parquet' ) sources = sources . to_pandas_df ()","title":"Reading with vaex"},{"location":"outputs/usingoutputs/#linking-the-results","text":"The table below shows what parameters act as keys to link data from the different results tables. Tip If loading the measurements via the .arrow file, then the measurements already have the source column in-place. Tip The images.parquet file contains the column measurements_path which can be used to get the filepaths for all the selavy parquet files. Data Column Linked to Column associations.parquet meas_id measurements.parquet , forced_measurements*.parquet id associations.parquet source_id sources.parquet id (index column) measurements.parquet , forced_measurements*.parquet image_id images.parquet id images.parquet band_id bands.parquet id images.parquet skyreg_id skyregions.parquet id measurement_pairs.parquet meas_id_a , meas_id_b measurements.parquet , forced_measurements*.parquet id measurement_pairs.parquet source_id sources.parquet id (index column) relations.parquet from_source_id , to_source_id sources.parquet id (index column)","title":"Linking the Results"},{"location":"outputs/usingoutputs/#vast-tools","text":"Link to the vast-tools repository . VAST has developed a python library called vast-tools that makes the exploration of results from the pipeline simple and efficient, in addition to being designed to be used in a Jupyter Notebook environment. For full details, refer to the repository located on GitHub and be sure to take a look at the example notebooks .","title":"vast-tools"},{"location":"quickstart/configuration/","text":"Configuration \u00b6 This section describe how to configure your VAST Pipeline installation. Pipeline Configuration \u00b6 The following instructions, will get you started in setting up the database and pipeline configuration. Note The commands given in this section, unless otherwise stated, assume that the current directory is the pipeline root and that your pipeline Python environment has been activated. Create a database for the pipeline. If you followed the installation process, you will have a PostgreSQL Docker container running on your system. Use the provided script init-tools/init-db.py script to create a new database for the pipeline. As a security precaution, this script will also create a new database user and set the pipeline database owner to this new user. The initialization script requires several input parameters. For usage information, run with the --help option: python init-tools/init-db.py --help usage: init-db.py [-h] host port admin-username admin-password username password database-name Initialize a PostgreSQL database for VAST Pipeline use. Creates a new superuser and creates a new database owned by the new superuser. positional arguments: host database host port database port admin-username database administrator username admin-password database administrator password username username for the new user/role to create for the VAST Pipeline password password for the new user/role to create for the VAST Pipeline database-name name of the new database to create for the VAST Pipeline optional arguments: -h, --help show this help message and exit Fill in the parameters as appropriate for your configuration. If you followed the installation instructions, these would be the details for your PostgreSQL Docker container. Following from the same example in the installation section: python init-tools/init-db.py localhost 55002 postgres <password> vast <vast-user-password> vastdb Info Where <password> is the superuser password that was passed to docker run , and <vast-user-password> is a new password of your choice for the new vast database user. You may change the values for the username and database-name, the above is just an example. If everything went well the output should be: Creating new user/role vast ... Creating new database vastdb ... Done! Copy the setting configuration file template and modify it with your desired settings (see defaults ). cp webinterface/.env.template webinterface/.env Set the database connection settings in the webinterface/.env file by modifying DATABASE_URL (for URL syntax see this link ). For example: DATABASE_URL = psql://vast:<vast-user-password>@localhost:55002/vastdb Note The connection details are the same that you setup during the installation . The database/user names must not contain any spaces or dashes, so use the underscore if you want, e.g. this_is_my_db_name . Create the pipeline database tables. The createcachetable command creates the cache tables required by DjangoQ. python manage.py migrate python manage.py createcachetable Create the pipeline data directories. The pipeline has several directories that can be configured in webinterface/.env : PIPELINE_WORKING_DIR : location to store various pipeline output files. SURVEYS_WORKING_DIR : location of reference survey catalogues, e.g. NVSS, SUMSS. RAW_IMAGE_DIR : default location that the pipeline will search for input images and catalogues to ingest during a pipeline run. Data inputs can also be defined as absolute paths in a pipeline run configuration file, so this setting only affects relative paths in the pipeline run configuration. HOME_DATA_DIR : additional location to search for input images and catalogues that is relative to the user's home directory. Intended for multi-user server deployments and unlikely to be useful for local installations. While the default values for these settings are relative to the pipeline codebase root (i.e. within the repo), we recommend creating these directories outside of the repo and updating the webinterface/.env file appropriately with absolute paths. For example, assuming you wish to create these directories in /data/vast-pipeline : mkdir -p /data/vast-pipeline mkdir /data/vast-pipeline/pipeline-runs mkdir /data/vast-pipeline/reference-surveys mkdir /data/vast-pipeline/raw-images mkdir /data/vast-pipeline/vast-pipeline-extra-data and update the webinterface/.env file with: PIPELINE_WORKING_DIR = /data/vast-pipeline/pipeline-runs SURVEYS_WORKING_DIR = /data/vast-pipeline/reference-surveys RAW_IMAGE_DIR = /data/vast-pipeline/raw-images HOME_DATA_DIR = /data/vast-pipeline/vast-pipeline-extra-data Authentication \u00b6 The pipeline supports two authentication methods: GitHub Teams, intended to multi-user server deployments; and local Django administrator. For a single-user local installation, we recommend creating a Django superuser account. Django superuser \u00b6 Create a Django superuser account with the following command and follow the interactive prompts. python manage.py createsuperuser This account can be used to log into the Django admin panel once the webserver is running (see Starting the Pipeline Web App ) by navigating to https://localhost:8000/pipe-admin/ . Once logged in, you will land on the Django admin page. Navigate back to the pipeline homepage http://localhost:8000/ and you should be authenticated. Data Exploration via Django Web Server \u00b6 You can start the web app/server via the instructions provided in Starting the Pipeline Web App .","title":"Configuration"},{"location":"quickstart/configuration/#configuration","text":"This section describe how to configure your VAST Pipeline installation.","title":"Configuration"},{"location":"quickstart/configuration/#pipeline-configuration","text":"The following instructions, will get you started in setting up the database and pipeline configuration. Note The commands given in this section, unless otherwise stated, assume that the current directory is the pipeline root and that your pipeline Python environment has been activated. Create a database for the pipeline. If you followed the installation process, you will have a PostgreSQL Docker container running on your system. Use the provided script init-tools/init-db.py script to create a new database for the pipeline. As a security precaution, this script will also create a new database user and set the pipeline database owner to this new user. The initialization script requires several input parameters. For usage information, run with the --help option: python init-tools/init-db.py --help usage: init-db.py [-h] host port admin-username admin-password username password database-name Initialize a PostgreSQL database for VAST Pipeline use. Creates a new superuser and creates a new database owned by the new superuser. positional arguments: host database host port database port admin-username database administrator username admin-password database administrator password username username for the new user/role to create for the VAST Pipeline password password for the new user/role to create for the VAST Pipeline database-name name of the new database to create for the VAST Pipeline optional arguments: -h, --help show this help message and exit Fill in the parameters as appropriate for your configuration. If you followed the installation instructions, these would be the details for your PostgreSQL Docker container. Following from the same example in the installation section: python init-tools/init-db.py localhost 55002 postgres <password> vast <vast-user-password> vastdb Info Where <password> is the superuser password that was passed to docker run , and <vast-user-password> is a new password of your choice for the new vast database user. You may change the values for the username and database-name, the above is just an example. If everything went well the output should be: Creating new user/role vast ... Creating new database vastdb ... Done! Copy the setting configuration file template and modify it with your desired settings (see defaults ). cp webinterface/.env.template webinterface/.env Set the database connection settings in the webinterface/.env file by modifying DATABASE_URL (for URL syntax see this link ). For example: DATABASE_URL = psql://vast:<vast-user-password>@localhost:55002/vastdb Note The connection details are the same that you setup during the installation . The database/user names must not contain any spaces or dashes, so use the underscore if you want, e.g. this_is_my_db_name . Create the pipeline database tables. The createcachetable command creates the cache tables required by DjangoQ. python manage.py migrate python manage.py createcachetable Create the pipeline data directories. The pipeline has several directories that can be configured in webinterface/.env : PIPELINE_WORKING_DIR : location to store various pipeline output files. SURVEYS_WORKING_DIR : location of reference survey catalogues, e.g. NVSS, SUMSS. RAW_IMAGE_DIR : default location that the pipeline will search for input images and catalogues to ingest during a pipeline run. Data inputs can also be defined as absolute paths in a pipeline run configuration file, so this setting only affects relative paths in the pipeline run configuration. HOME_DATA_DIR : additional location to search for input images and catalogues that is relative to the user's home directory. Intended for multi-user server deployments and unlikely to be useful for local installations. While the default values for these settings are relative to the pipeline codebase root (i.e. within the repo), we recommend creating these directories outside of the repo and updating the webinterface/.env file appropriately with absolute paths. For example, assuming you wish to create these directories in /data/vast-pipeline : mkdir -p /data/vast-pipeline mkdir /data/vast-pipeline/pipeline-runs mkdir /data/vast-pipeline/reference-surveys mkdir /data/vast-pipeline/raw-images mkdir /data/vast-pipeline/vast-pipeline-extra-data and update the webinterface/.env file with: PIPELINE_WORKING_DIR = /data/vast-pipeline/pipeline-runs SURVEYS_WORKING_DIR = /data/vast-pipeline/reference-surveys RAW_IMAGE_DIR = /data/vast-pipeline/raw-images HOME_DATA_DIR = /data/vast-pipeline/vast-pipeline-extra-data","title":"Pipeline Configuration"},{"location":"quickstart/configuration/#authentication","text":"The pipeline supports two authentication methods: GitHub Teams, intended to multi-user server deployments; and local Django administrator. For a single-user local installation, we recommend creating a Django superuser account.","title":"Authentication"},{"location":"quickstart/configuration/#django-superuser","text":"Create a Django superuser account with the following command and follow the interactive prompts. python manage.py createsuperuser This account can be used to log into the Django admin panel once the webserver is running (see Starting the Pipeline Web App ) by navigating to https://localhost:8000/pipe-admin/ . Once logged in, you will land on the Django admin page. Navigate back to the pipeline homepage http://localhost:8000/ and you should be authenticated.","title":"Django superuser"},{"location":"quickstart/configuration/#data-exploration-via-django-web-server","text":"You can start the web app/server via the instructions provided in Starting the Pipeline Web App .","title":"Data Exploration via Django Web Server"},{"location":"quickstart/deployment/","text":"Deployment \u00b6 Production System \u00b6 This section describes a simple deployment without using Docker containers, assuming the use of WhiteNoise to serve the static files. It is possible to serve the static files using other methods (e.g. Nginx). And in the future it is possible to upgrade the deployment stack using Docker container and Docker compose (we foresee 3 main containers: Django, Dask and Traefik/Nginx). We recommend in any case reading Django deployment documentation for general knowledge. Note We assume deployment to a UNIX server . The following steps describes how to set up the Django side of the production deployment, and can be of reference for a future Dockerization. They assumed you have SSH access to your remote server and have sudo priviledges. Web App Deployment \u00b6 Clone the repo in a suitable path, e.g. /opt/ . $ cd /opt && sudo git clone https://github.com/askap-vast/vast-pipeline Follow the Installation Instructions . We recommend installing the Python virtual environment under the pipeline folder. $ cd /opt/vast-pipeline && virtualenv -p python3 pipeline_env Configure your .env files with all the right settings. Check that your server is running fine by changing DEBUG = True in the .env file. Run Django deployment checklist command to see what are you missing. It is possible that some options are turned off, as implemented in the reverse proxy or load balancer of your server (e.g. SECURE_SSL_REDIRECT = False or not set, assumes your reverse proxy redirect HTTP to HTTPS). ( pipeline_env ) $ ./manage.py check --deploy Build up the static and fix url in JS9: ( pipeline_env ) $ cd /opt/vast-pipeline && npm ci && npm start \\ && npm run js9staticprod && ./manage.py collectstatic -c --noinput Set up a unit/systemd file as recommended in Gunicorn docs (feel free to use the socket or an IP and port). An example of command to write in the file is (assuming a virtual environment is installed in venv under the main pipeline folder): ExecStart = /opt/vast-pipeline/venv/bin/gunicorn -w 3 -k gevent \\ --worker-connections = 1000 --timeout 120 --limit-request-line 6500 \\ -b 127 .0.0.1:8000 webinterface.wsgi NOTE : (for future development) the --limit-request-line parameter needs to be adjusted for the actual request length as that might change if more parameters are added to the query. Finalise the installation of the unit file. Some good instructions on where to put, link and install the unit file are described in the Jupyter Hub docs Extra Service(s) Deployment \u00b6 In order to run a pipeline run from the Web App, the Django-Q process need to be started and managed as a service by the OS. In order to do so we recommend building a unit/systemd file to manage the Django-Q process, in a similar way of the gunicorn process (following the Jupyter Hub docs ): ... WorkingDirectory = /opt/vast-pipeline ExecStart = /opt/vast-pipeline/venv/bin/python manage.py qcluster ... Tip In the examples above I decided to install the Python virtual enviroment of the pipeline, in venv folder under the cloned repository Security \u00b6 By default the settings file has some security parameters that are set when you run the web app in production ( DEBUG = False ), but you can read more in the Django documentation or in this blog post in which they explain how to get an A+ rating for your web site.","title":"Deployment"},{"location":"quickstart/deployment/#deployment","text":"","title":"Deployment"},{"location":"quickstart/deployment/#production-system","text":"This section describes a simple deployment without using Docker containers, assuming the use of WhiteNoise to serve the static files. It is possible to serve the static files using other methods (e.g. Nginx). And in the future it is possible to upgrade the deployment stack using Docker container and Docker compose (we foresee 3 main containers: Django, Dask and Traefik/Nginx). We recommend in any case reading Django deployment documentation for general knowledge. Note We assume deployment to a UNIX server . The following steps describes how to set up the Django side of the production deployment, and can be of reference for a future Dockerization. They assumed you have SSH access to your remote server and have sudo priviledges.","title":"Production System"},{"location":"quickstart/deployment/#web-app-deployment","text":"Clone the repo in a suitable path, e.g. /opt/ . $ cd /opt && sudo git clone https://github.com/askap-vast/vast-pipeline Follow the Installation Instructions . We recommend installing the Python virtual environment under the pipeline folder. $ cd /opt/vast-pipeline && virtualenv -p python3 pipeline_env Configure your .env files with all the right settings. Check that your server is running fine by changing DEBUG = True in the .env file. Run Django deployment checklist command to see what are you missing. It is possible that some options are turned off, as implemented in the reverse proxy or load balancer of your server (e.g. SECURE_SSL_REDIRECT = False or not set, assumes your reverse proxy redirect HTTP to HTTPS). ( pipeline_env ) $ ./manage.py check --deploy Build up the static and fix url in JS9: ( pipeline_env ) $ cd /opt/vast-pipeline && npm ci && npm start \\ && npm run js9staticprod && ./manage.py collectstatic -c --noinput Set up a unit/systemd file as recommended in Gunicorn docs (feel free to use the socket or an IP and port). An example of command to write in the file is (assuming a virtual environment is installed in venv under the main pipeline folder): ExecStart = /opt/vast-pipeline/venv/bin/gunicorn -w 3 -k gevent \\ --worker-connections = 1000 --timeout 120 --limit-request-line 6500 \\ -b 127 .0.0.1:8000 webinterface.wsgi NOTE : (for future development) the --limit-request-line parameter needs to be adjusted for the actual request length as that might change if more parameters are added to the query. Finalise the installation of the unit file. Some good instructions on where to put, link and install the unit file are described in the Jupyter Hub docs","title":"Web App Deployment"},{"location":"quickstart/deployment/#extra-services-deployment","text":"In order to run a pipeline run from the Web App, the Django-Q process need to be started and managed as a service by the OS. In order to do so we recommend building a unit/systemd file to manage the Django-Q process, in a similar way of the gunicorn process (following the Jupyter Hub docs ): ... WorkingDirectory = /opt/vast-pipeline ExecStart = /opt/vast-pipeline/venv/bin/python manage.py qcluster ... Tip In the examples above I decided to install the Python virtual enviroment of the pipeline, in venv folder under the cloned repository","title":"Extra Service(s) Deployment"},{"location":"quickstart/deployment/#security","text":"By default the settings file has some security parameters that are set when you run the web app in production ( DEBUG = False ), but you can read more in the Django documentation or in this blog post in which they explain how to get an A+ rating for your web site.","title":"Security"},{"location":"quickstart/installation/","text":"Installation \u00b6 This document provides instructions on installing the VAST Pipeline for local use. The VAST Pipeline consists of 3 main components that require installation: a PostgreSQL database, a Django application, a front-end website. The instructions have been tested on Debian/Ubuntu and macOS. PostgreSQL \u00b6 We recommend using a Docker container for the database rather than installing the database system-wide. Steps: Install Docker. Refer to the official documentation , and for Ubuntu users to this . Remember to add your user account to the docker group official docs , by running: sudo groupadd docker sudo usermod -aG docker $USER Create a PostgreSQL container. The VAST Pipeline requires a PostgreSQL database with the Q3C plugin to enable special indexing on coordinates and fast cone-search queries. We have prepared a Docker image based on the latest PostgreSQL image that includes Q3C . Start a container using this image by running the command below, replacing <container-name> with a name of your choice (e.g. vast-pipeline-db) and <password> with a password of your choice which will be set for the default postgres database superuser account. docker run --name <container-name> --env POSTGRES_PASSWORD=<password> --publish-all --detach ghcr.io/marxide/postgres-q3c:latest The --publish-all option will make the PostgreSQL server port 5432 in the container accessible on a random available port on your system (the host). The --detach option instructs Docker to start the container in the background rather than taking over your current shell. Verify that the container is running and note the host port that 5432/tcp is published on by running docker ps , e.g. in the example below, the host port is 55002 . docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8ff553add2ed ghcr.io/marxide/postgres-q3c:latest \"docker-entrypoint.s\u2026\" 4 seconds ago Up 3 seconds 0.0.0.0:55002->5432/tcp vast-pipeline-db The database server should now be running in a container on your machine. Tip To stop the database server, simply stop the container with the following command docker stop <container-name or container-id> You can start an existing stopped container with the following command docker start <container-name or container-id> Note that docker run and docker start are not the same. docker run will create and start a container from an image; docker start will start an existing stopped container. If you have previously created a VAST Pipeline database container and you wish to reuse it, you want to use docker start . You will likely need to restart the container after a system reboot. Python Environment \u00b6 We strongly recommend installing the VAST Pipeline in an isolated virtual environment (e.g. using Miniconda , Virtualenv , or venv ). This will keep the rather complex set of dependencies separated from the system-wide Python installation. Create a new Python environment using your chosen virtual environment manager and activate it. For example, Miniconda users should run the following command, replacing <environment-name> with an appropriate name (e.g. pipeline-env): conda create --name <environment-name> python=3.8 conda activate <environment-name> Note All further installation instructions will assume you have activated your new virtual environment. Your environment manager will usually prepend the virtual environment name to the shell prompt, e.g. (pipeline-env) $ ... Clone the pipeline repository https://github.com/askap-vast/vast-pipeline and change into the repo directory. git clone https://github.com/askap-vast/vast-pipeline.git cd vast-pipeline Warning Do not change the the repo folder name, e.g. git clone https://github.com/askap-vast/vast-pipeline.git my-pipeline-local-dev (Optional) Checkout the version you want to install. Currently, the repo will have cloned the latest code from the master branch. If you require a specific version, checkout the appropriate version tag into a new branch e.g. for version 0.2.0 git checkout -b <new-branch-name> 0.2.0 Install non-Python dependencies. Some of the Python dependencies required by the pipeline depend on some non-Python libraries. These can also be installed by Miniconda, otherwise they are best installed using an appropriate package manager for your operating system e.g. apt for Debian/Ubuntu, dnf for RHEL 8/CentOS 8, Homebrew for macOS. The dependencies are: Miniconda libpq graphviz Both are available on the conda-forge channel. They are also specified in the environment file requirements/environment.yml which can be used to install the required packages into an activated conda environment with the following command conda env update -f requirements/environment.yml Debian/Ubuntu libpq-dev libgraphviz-dev RHEL/CentOS libpq-devel graphviz-devel CentOS users You may need to enable the PowerTools repository to install graphviz-devel . dnf install dnf-plugins-core dnf config-manager --set-enabled powertools Homebrew libpq graphviz Install the pipeline and it's Python dependencies. pip install . Warning Don't forget the . at the end of the above command. It instructs pip that the root directory of the package to install is the current directory. Tip If you are intending to deploy an instance of the pipeline onto a server, you may also want to install the recommended production extras with pip install .[prod] . However, note that these are recommendations only and there are other alternative packages that may work just as well. Tip If you intend to contribute to development of the pipeline, you will need the Python dependency management tool Poetry . See the development guidelines . Front-End Assets Quickstart \u00b6 In order to install and compile the front-end website assets (modules like js9 and bootstrap, as well as minification of JS and CSS files) you need a recent version of NodeJS installed. Installation of NodeJS \u00b6 If you are using Miniconda and installed the requirements/environment.yml file as shown above, then NodeJS is already installed. Otherwise, we recommend following the instructions on the NodeJS downloads page for your OS (there are many installation options). Setting up the front-end assets \u00b6 In order to set up the front end assets, run: npm ci && npm start Note Ensure you are still in the root of the repo before running the command above. The npm ci command (\"clean install\") will remove all previous node modules and install all the dependencies from scratch. The npm start command will run the default gulp \"task\" which, among other things, compiles Sass into CSS, minifies CSS and JS files, and copies these files into the static/vendor folder. For more details of compilation of frontend assets (e.g. single tasks), and front-end developement set up read the Front End Developing Guidelines . Bug When npm start or npm run start was run in a Ubuntu 20.04 LTS (containerised environment), for some unknown reasons, both commands failed with the following error. [12:48:19] 'js9Make' errored after 7.67 ms [12:48:19] Error: spawn make ENOENT at Process.ChildProcess._handle.onexit (internal/child_process.js:267:19) at onErrorNT (internal/child_process.js:469:16) at processTicksAndRejections (internal/process/task_queues.js:84:21) [12:48:19] 'default' errored after 2.63 s npm ERR! code ELIFECYCLE npm ERR! errno 1 npm ERR! vast-pipeline@99.99.99-dev start: `gulp default` npm ERR! Exit status 1 npm ERR! npm ERR! Failed at the vast-pipeline@99.99.99-dev start script. npm ERR! This is probably not a problem with npm. There is likely additional logging output above. npm ERR! A complete log of this run can be found in: npm ERR! /home/vast/.npm/_logs/2020-10-06T01_48_19_215Z-debug.log The way around for this issue is unorthodox. The following steps were followed to overcome the issue: cd node_modules/js9/ ./configure make make install cd ~/vast-pipeline/ ## (to comeback to the root folder of the project) npm install That somehow solved the issue mentioned above. Done! Now go to Vast Pipeline Configuration file to see how to initialize and run the pipeline. Otherwise if you intend on developing the repo open the Contributing and Developing Guidelines file for instructions on how to contribute to the repo.","title":"Installation"},{"location":"quickstart/installation/#installation","text":"This document provides instructions on installing the VAST Pipeline for local use. The VAST Pipeline consists of 3 main components that require installation: a PostgreSQL database, a Django application, a front-end website. The instructions have been tested on Debian/Ubuntu and macOS.","title":"Installation"},{"location":"quickstart/installation/#postgresql","text":"We recommend using a Docker container for the database rather than installing the database system-wide. Steps: Install Docker. Refer to the official documentation , and for Ubuntu users to this . Remember to add your user account to the docker group official docs , by running: sudo groupadd docker sudo usermod -aG docker $USER Create a PostgreSQL container. The VAST Pipeline requires a PostgreSQL database with the Q3C plugin to enable special indexing on coordinates and fast cone-search queries. We have prepared a Docker image based on the latest PostgreSQL image that includes Q3C . Start a container using this image by running the command below, replacing <container-name> with a name of your choice (e.g. vast-pipeline-db) and <password> with a password of your choice which will be set for the default postgres database superuser account. docker run --name <container-name> --env POSTGRES_PASSWORD=<password> --publish-all --detach ghcr.io/marxide/postgres-q3c:latest The --publish-all option will make the PostgreSQL server port 5432 in the container accessible on a random available port on your system (the host). The --detach option instructs Docker to start the container in the background rather than taking over your current shell. Verify that the container is running and note the host port that 5432/tcp is published on by running docker ps , e.g. in the example below, the host port is 55002 . docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8ff553add2ed ghcr.io/marxide/postgres-q3c:latest \"docker-entrypoint.s\u2026\" 4 seconds ago Up 3 seconds 0.0.0.0:55002->5432/tcp vast-pipeline-db The database server should now be running in a container on your machine. Tip To stop the database server, simply stop the container with the following command docker stop <container-name or container-id> You can start an existing stopped container with the following command docker start <container-name or container-id> Note that docker run and docker start are not the same. docker run will create and start a container from an image; docker start will start an existing stopped container. If you have previously created a VAST Pipeline database container and you wish to reuse it, you want to use docker start . You will likely need to restart the container after a system reboot.","title":"PostgreSQL"},{"location":"quickstart/installation/#python-environment","text":"We strongly recommend installing the VAST Pipeline in an isolated virtual environment (e.g. using Miniconda , Virtualenv , or venv ). This will keep the rather complex set of dependencies separated from the system-wide Python installation. Create a new Python environment using your chosen virtual environment manager and activate it. For example, Miniconda users should run the following command, replacing <environment-name> with an appropriate name (e.g. pipeline-env): conda create --name <environment-name> python=3.8 conda activate <environment-name> Note All further installation instructions will assume you have activated your new virtual environment. Your environment manager will usually prepend the virtual environment name to the shell prompt, e.g. (pipeline-env) $ ... Clone the pipeline repository https://github.com/askap-vast/vast-pipeline and change into the repo directory. git clone https://github.com/askap-vast/vast-pipeline.git cd vast-pipeline Warning Do not change the the repo folder name, e.g. git clone https://github.com/askap-vast/vast-pipeline.git my-pipeline-local-dev (Optional) Checkout the version you want to install. Currently, the repo will have cloned the latest code from the master branch. If you require a specific version, checkout the appropriate version tag into a new branch e.g. for version 0.2.0 git checkout -b <new-branch-name> 0.2.0 Install non-Python dependencies. Some of the Python dependencies required by the pipeline depend on some non-Python libraries. These can also be installed by Miniconda, otherwise they are best installed using an appropriate package manager for your operating system e.g. apt for Debian/Ubuntu, dnf for RHEL 8/CentOS 8, Homebrew for macOS. The dependencies are: Miniconda libpq graphviz Both are available on the conda-forge channel. They are also specified in the environment file requirements/environment.yml which can be used to install the required packages into an activated conda environment with the following command conda env update -f requirements/environment.yml Debian/Ubuntu libpq-dev libgraphviz-dev RHEL/CentOS libpq-devel graphviz-devel CentOS users You may need to enable the PowerTools repository to install graphviz-devel . dnf install dnf-plugins-core dnf config-manager --set-enabled powertools Homebrew libpq graphviz Install the pipeline and it's Python dependencies. pip install . Warning Don't forget the . at the end of the above command. It instructs pip that the root directory of the package to install is the current directory. Tip If you are intending to deploy an instance of the pipeline onto a server, you may also want to install the recommended production extras with pip install .[prod] . However, note that these are recommendations only and there are other alternative packages that may work just as well. Tip If you intend to contribute to development of the pipeline, you will need the Python dependency management tool Poetry . See the development guidelines .","title":"Python Environment"},{"location":"quickstart/installation/#front-end-assets-quickstart","text":"In order to install and compile the front-end website assets (modules like js9 and bootstrap, as well as minification of JS and CSS files) you need a recent version of NodeJS installed.","title":"Front-End Assets Quickstart"},{"location":"quickstart/installation/#installation-of-nodejs","text":"If you are using Miniconda and installed the requirements/environment.yml file as shown above, then NodeJS is already installed. Otherwise, we recommend following the instructions on the NodeJS downloads page for your OS (there are many installation options).","title":"Installation of NodeJS"},{"location":"quickstart/installation/#setting-up-the-front-end-assets","text":"In order to set up the front end assets, run: npm ci && npm start Note Ensure you are still in the root of the repo before running the command above. The npm ci command (\"clean install\") will remove all previous node modules and install all the dependencies from scratch. The npm start command will run the default gulp \"task\" which, among other things, compiles Sass into CSS, minifies CSS and JS files, and copies these files into the static/vendor folder. For more details of compilation of frontend assets (e.g. single tasks), and front-end developement set up read the Front End Developing Guidelines . Bug When npm start or npm run start was run in a Ubuntu 20.04 LTS (containerised environment), for some unknown reasons, both commands failed with the following error. [12:48:19] 'js9Make' errored after 7.67 ms [12:48:19] Error: spawn make ENOENT at Process.ChildProcess._handle.onexit (internal/child_process.js:267:19) at onErrorNT (internal/child_process.js:469:16) at processTicksAndRejections (internal/process/task_queues.js:84:21) [12:48:19] 'default' errored after 2.63 s npm ERR! code ELIFECYCLE npm ERR! errno 1 npm ERR! vast-pipeline@99.99.99-dev start: `gulp default` npm ERR! Exit status 1 npm ERR! npm ERR! Failed at the vast-pipeline@99.99.99-dev start script. npm ERR! This is probably not a problem with npm. There is likely additional logging output above. npm ERR! A complete log of this run can be found in: npm ERR! /home/vast/.npm/_logs/2020-10-06T01_48_19_215Z-debug.log The way around for this issue is unorthodox. The following steps were followed to overcome the issue: cd node_modules/js9/ ./configure make make install cd ~/vast-pipeline/ ## (to comeback to the root folder of the project) npm install That somehow solved the issue mentioned above. Done! Now go to Vast Pipeline Configuration file to see how to initialize and run the pipeline. Otherwise if you intend on developing the repo open the Contributing and Developing Guidelines file for instructions on how to contribute to the repo.","title":"Setting up the front-end assets"},{"location":"reference/admin/","text":"This module contains the admin classes that are registered with the Django Admin site. ImageAdmin \u00b6 The ImageAdmin class. MeasurementAdmin \u00b6 The MeasurementAdmin class. RunAdmin \u00b6 The RunAdmin class. SkyRegionAdmin \u00b6 The SkyRegionAdmin class. SourceAdmin \u00b6 The SourceAdmin class. SourceFavAdmin \u00b6 The SourceFavAdmin class.","title":"admin.py"},{"location":"reference/admin/#vast_pipeline.admin.ImageAdmin","text":"The ImageAdmin class.","title":"ImageAdmin"},{"location":"reference/admin/#vast_pipeline.admin.MeasurementAdmin","text":"The MeasurementAdmin class.","title":"MeasurementAdmin"},{"location":"reference/admin/#vast_pipeline.admin.RunAdmin","text":"The RunAdmin class.","title":"RunAdmin"},{"location":"reference/admin/#vast_pipeline.admin.SkyRegionAdmin","text":"The SkyRegionAdmin class.","title":"SkyRegionAdmin"},{"location":"reference/admin/#vast_pipeline.admin.SourceAdmin","text":"The SourceAdmin class.","title":"SourceAdmin"},{"location":"reference/admin/#vast_pipeline.admin.SourceFavAdmin","text":"The SourceFavAdmin class.","title":"SourceFavAdmin"},{"location":"reference/apps/","text":"PipelineConfig \u00b6 Class representing the configuration for the vast_pipeline app.","title":"apps.py"},{"location":"reference/apps/#vast_pipeline.apps.PipelineConfig","text":"Class representing the configuration for the vast_pipeline app.","title":"PipelineConfig"},{"location":"reference/context_processors/","text":"maintainance_banner ( request ) \u00b6 Generates the maintainance banner for the web server if a message has been set in the Django settings. Parameters: Name Type Description Default request HttpRequest The web server request. required Returns: Type Description Dict[str, str] Dictionary representing the JSON object with the maintainance message. Source code in vast_pipeline/context_processors.py def maintainance_banner ( request : HttpRequest ) -> Dict [ str , str ]: \"\"\" Generates the maintainance banner for the web server if a message has been set in the Django settings. Args: request (HttpRequest): The web server request. Returns: Dictionary representing the JSON object with the maintainance message. \"\"\" if settings . PIPELINE_MAINTAINANCE_MESSAGE : return { 'maintainance_message' : settings . PIPELINE_MAINTAINANCE_MESSAGE } return { 'maintainance_message' : None }","title":"context_processors.py"},{"location":"reference/context_processors/#vast_pipeline.context_processors.maintainance_banner","text":"Generates the maintainance banner for the web server if a message has been set in the Django settings. Parameters: Name Type Description Default request HttpRequest The web server request. required Returns: Type Description Dict[str, str] Dictionary representing the JSON object with the maintainance message. Source code in vast_pipeline/context_processors.py def maintainance_banner ( request : HttpRequest ) -> Dict [ str , str ]: \"\"\" Generates the maintainance banner for the web server if a message has been set in the Django settings. Args: request (HttpRequest): The web server request. Returns: Dictionary representing the JSON object with the maintainance message. \"\"\" if settings . PIPELINE_MAINTAINANCE_MESSAGE : return { 'maintainance_message' : settings . PIPELINE_MAINTAINANCE_MESSAGE } return { 'maintainance_message' : None }","title":"maintainance_banner()"},{"location":"reference/converters/","text":"AngleConverter \u00b6 Accept any valid input value for an astropy.coordinates.Angle and ensure the returned value is a float in decimal degrees. The unit should be included in the input value. to_python ( self , value ) \u00b6 Return the decimal degrees from the coordinate input as an Angle object. Parameters: Name Type Description Default value The value of the angle input. required Returns: Type Description float The angle returned as an Angle object. Source code in vast_pipeline/converters.py def to_python ( self , value ) -> float : \"\"\" Return the decimal degrees from the coordinate input as an Angle object. Args: value: The value of the angle input. Returns: The angle returned as an Angle object. \"\"\" return Angle ( value ) . deg to_url ( self , value ) \u00b6 Return the string format of an Angle object from the coordinate input. Parameters: Name Type Description Default value The value of the angle input. required Returns: Type Description str The string representation of the Angle object created from the input. Source code in vast_pipeline/converters.py def to_url ( self , value ) -> str : \"\"\" Return the string format of an Angle object from the coordinate input. Args: value: The value of the angle input. Returns: The string representation of the Angle object created from the input. \"\"\" return value . to_string () DeclinationConverter \u00b6 Accept both decimal and sexigesimal representations of Dec and ensure the returned value is a float in decimal degrees. The input units are always assumed to be degrees. to_python ( self , value ) \u00b6 Return the decimal degrees from the coordinate input as a python float object. Parameters: Name Type Description Default value str The value of the declination input. required Returns: Type Description float The decimal degrees value. Source code in vast_pipeline/converters.py def to_python ( self , value : str ) -> float : \"\"\" Return the decimal degrees from the coordinate input as a python float object. Args: value: The value of the declination input. Returns: The decimal degrees value. \"\"\" return Latitude ( value , unit = \"deg\" ) . deg to_url ( self , value ) \u00b6 Return the decimal degrees from the coordinate input in a URL format. Parameters: Name Type Description Default value str The value of the declination input. required Returns: Type Description str The decimal degrees value as a string. Source code in vast_pipeline/converters.py def to_url ( self , value : str ) -> str : \"\"\" Return the decimal degrees from the coordinate input in a URL format. Args: value: The value of the declination input. Returns: The decimal degrees value as a string. \"\"\" return value . to_string ( unit = \"deg\" , decimal = True ) RightAscensionConverter \u00b6 Accept both decimal and sexigesimal representations of RA and ensure the returned value is a float in decimal degrees. If the input is in sexigesimal format, assume it is in units of hourangle. to_python ( self , value ) \u00b6 Return the decimal degrees from the coordinate input as a python float object. Parameters: Name Type Description Default value str The value of the RA input. required Returns: Type Description float The decimal degrees value. Source code in vast_pipeline/converters.py def to_python ( self , value : str ) -> float : \"\"\" Return the decimal degrees from the coordinate input as a python float object. Args: value: The value of the RA input. Returns: The decimal degrees value. \"\"\" unit = \"hourangle\" if \":\" in value else \"deg\" return Longitude ( value , unit = unit ) . deg to_url ( self , value ) \u00b6 Return the decimal degrees from the coordinate input in a URL format. Parameters: Name Type Description Default value str The value of the RA input. required Returns: Type Description str The decimal degrees value as a string. Source code in vast_pipeline/converters.py def to_url ( self , value : str ) -> str : \"\"\" Return the decimal degrees from the coordinate input in a URL format. Args: value: The value of the RA input. Returns: The decimal degrees value as a string. \"\"\" return value . to_string ( unit = \"deg\" , decimal = True )","title":"converters.py"},{"location":"reference/converters/#vast_pipeline.converters.AngleConverter","text":"Accept any valid input value for an astropy.coordinates.Angle and ensure the returned value is a float in decimal degrees. The unit should be included in the input value.","title":"AngleConverter"},{"location":"reference/converters/#vast_pipeline.converters.AngleConverter.to_python","text":"Return the decimal degrees from the coordinate input as an Angle object. Parameters: Name Type Description Default value The value of the angle input. required Returns: Type Description float The angle returned as an Angle object. Source code in vast_pipeline/converters.py def to_python ( self , value ) -> float : \"\"\" Return the decimal degrees from the coordinate input as an Angle object. Args: value: The value of the angle input. Returns: The angle returned as an Angle object. \"\"\" return Angle ( value ) . deg","title":"to_python()"},{"location":"reference/converters/#vast_pipeline.converters.AngleConverter.to_url","text":"Return the string format of an Angle object from the coordinate input. Parameters: Name Type Description Default value The value of the angle input. required Returns: Type Description str The string representation of the Angle object created from the input. Source code in vast_pipeline/converters.py def to_url ( self , value ) -> str : \"\"\" Return the string format of an Angle object from the coordinate input. Args: value: The value of the angle input. Returns: The string representation of the Angle object created from the input. \"\"\" return value . to_string ()","title":"to_url()"},{"location":"reference/converters/#vast_pipeline.converters.DeclinationConverter","text":"Accept both decimal and sexigesimal representations of Dec and ensure the returned value is a float in decimal degrees. The input units are always assumed to be degrees.","title":"DeclinationConverter"},{"location":"reference/converters/#vast_pipeline.converters.DeclinationConverter.to_python","text":"Return the decimal degrees from the coordinate input as a python float object. Parameters: Name Type Description Default value str The value of the declination input. required Returns: Type Description float The decimal degrees value. Source code in vast_pipeline/converters.py def to_python ( self , value : str ) -> float : \"\"\" Return the decimal degrees from the coordinate input as a python float object. Args: value: The value of the declination input. Returns: The decimal degrees value. \"\"\" return Latitude ( value , unit = \"deg\" ) . deg","title":"to_python()"},{"location":"reference/converters/#vast_pipeline.converters.DeclinationConverter.to_url","text":"Return the decimal degrees from the coordinate input in a URL format. Parameters: Name Type Description Default value str The value of the declination input. required Returns: Type Description str The decimal degrees value as a string. Source code in vast_pipeline/converters.py def to_url ( self , value : str ) -> str : \"\"\" Return the decimal degrees from the coordinate input in a URL format. Args: value: The value of the declination input. Returns: The decimal degrees value as a string. \"\"\" return value . to_string ( unit = \"deg\" , decimal = True )","title":"to_url()"},{"location":"reference/converters/#vast_pipeline.converters.RightAscensionConverter","text":"Accept both decimal and sexigesimal representations of RA and ensure the returned value is a float in decimal degrees. If the input is in sexigesimal format, assume it is in units of hourangle.","title":"RightAscensionConverter"},{"location":"reference/converters/#vast_pipeline.converters.RightAscensionConverter.to_python","text":"Return the decimal degrees from the coordinate input as a python float object. Parameters: Name Type Description Default value str The value of the RA input. required Returns: Type Description float The decimal degrees value. Source code in vast_pipeline/converters.py def to_python ( self , value : str ) -> float : \"\"\" Return the decimal degrees from the coordinate input as a python float object. Args: value: The value of the RA input. Returns: The decimal degrees value. \"\"\" unit = \"hourangle\" if \":\" in value else \"deg\" return Longitude ( value , unit = unit ) . deg","title":"to_python()"},{"location":"reference/converters/#vast_pipeline.converters.RightAscensionConverter.to_url","text":"Return the decimal degrees from the coordinate input in a URL format. Parameters: Name Type Description Default value str The value of the RA input. required Returns: Type Description str The decimal degrees value as a string. Source code in vast_pipeline/converters.py def to_url ( self , value : str ) -> str : \"\"\" Return the decimal degrees from the coordinate input in a URL format. Args: value: The value of the RA input. Returns: The decimal degrees value as a string. \"\"\" return value . to_string ( unit = \"deg\" , decimal = True )","title":"to_url()"},{"location":"reference/forms/","text":"CommentForm \u00b6 The form used for users to leave comments on objects. Meta \u00b6 model \u00b6 The model object for a comment. get_avatar_url ( self ) \u00b6 Get the URL for the user's avatar from GitHub. If the user has no associated GitHub account (e.g. a Django superuser), return the URL to the default user avatar. Returns: Type Description str The avatar URL. Source code in vast_pipeline/forms.py def get_avatar_url ( self ) -> str : \"\"\"Get the URL for the user's avatar from GitHub. If the user has no associated GitHub account (e.g. a Django superuser), return the URL to the default user avatar. Returns: The avatar URL. \"\"\" social = UserSocialAuth . get_social_auth_for_user ( self . author ) . first () if social and \"avatar_url\" in social . extra_data : return social . extra_data [ \"avatar_url\" ] else : return static ( \"img/user-32.png\" ) __init__ ( self , * args , ** kwargs ) special \u00b6 Initialise a CommentForm. Returns: Type Description None None. Source code in vast_pipeline/forms.py def __init__ ( self , * args , ** kwargs ) -> None : \"\"\" Initialise a CommentForm. Returns: None. \"\"\" super () . __init__ ( * args , ** kwargs ) self . helper = FormHelper () self . helper . layout = Layout ( Field ( \"comment\" , rows = 2 ), ) self . helper . add_input ( Submit ( \"submit\" , \"Submit\" )) PipelineRunForm \u00b6 Class for the form used in the creation of a new pipeline run through the webserver. TagWithCommentsForm \u00b6 Class to combined tags with the CommentsForm. __init__ ( self , * args , ** kwargs ) special \u00b6 Initialise a TagWithCommentsForm. Returns: Type Description None None. Source code in vast_pipeline/forms.py def __init__ ( self , * args , ** kwargs ) -> None : \"\"\" Initialise a TagWithCommentsForm. Returns: None. \"\"\" super () . __init__ ( * args , ** kwargs ) self . helper = FormHelper () self . helper . layout = Layout ( Field ( \"tags\" ), Field ( \"comment\" , rows = 2 , placeholder = ( \"Optional. If changing the tags, you should provide justification here.\" ), ), Submit ( \"submit\" , \"Submit\" , css_class = \"btn-block\" ), )","title":"forms.py"},{"location":"reference/forms/#vast_pipeline.forms.CommentForm","text":"The form used for users to leave comments on objects.","title":"CommentForm"},{"location":"reference/forms/#vast_pipeline.forms.CommentForm.Meta","text":"","title":"Meta"},{"location":"reference/forms/#vast_pipeline.forms.CommentForm.Meta.model","text":"The model object for a comment.","title":"model"},{"location":"reference/forms/#vast_pipeline.forms.CommentForm.__init__","text":"Initialise a CommentForm. Returns: Type Description None None. Source code in vast_pipeline/forms.py def __init__ ( self , * args , ** kwargs ) -> None : \"\"\" Initialise a CommentForm. Returns: None. \"\"\" super () . __init__ ( * args , ** kwargs ) self . helper = FormHelper () self . helper . layout = Layout ( Field ( \"comment\" , rows = 2 ), ) self . helper . add_input ( Submit ( \"submit\" , \"Submit\" ))","title":"__init__()"},{"location":"reference/forms/#vast_pipeline.forms.PipelineRunForm","text":"Class for the form used in the creation of a new pipeline run through the webserver.","title":"PipelineRunForm"},{"location":"reference/forms/#vast_pipeline.forms.TagWithCommentsForm","text":"Class to combined tags with the CommentsForm.","title":"TagWithCommentsForm"},{"location":"reference/forms/#vast_pipeline.forms.TagWithCommentsForm.__init__","text":"Initialise a TagWithCommentsForm. Returns: Type Description None None. Source code in vast_pipeline/forms.py def __init__ ( self , * args , ** kwargs ) -> None : \"\"\" Initialise a TagWithCommentsForm. Returns: None. \"\"\" super () . __init__ ( * args , ** kwargs ) self . helper = FormHelper () self . helper . layout = Layout ( Field ( \"tags\" ), Field ( \"comment\" , rows = 2 , placeholder = ( \"Optional. If changing the tags, you should provide justification here.\" ), ), Submit ( \"submit\" , \"Submit\" , css_class = \"btn-block\" ), )","title":"__init__()"},{"location":"reference/models/","text":"Association \u00b6 model association between sources and measurements based on some parameters Band \u00b6 A band on the frequency spectrum used for imaging. Each image is associated with one band. Comment \u00b6 The model object for a comment. get_avatar_url ( self ) \u00b6 Get the URL for the user's avatar from GitHub. If the user has no associated GitHub account (e.g. a Django superuser), return the URL to the default user avatar. Returns: Type Description str The avatar URL. Source code in vast_pipeline/models.py def get_avatar_url ( self ) -> str : \"\"\"Get the URL for the user's avatar from GitHub. If the user has no associated GitHub account (e.g. a Django superuser), return the URL to the default user avatar. Returns: The avatar URL. \"\"\" social = UserSocialAuth . get_social_auth_for_user ( self . author ) . first () if social and \"avatar_url\" in social . extra_data : return social . extra_data [ \"avatar_url\" ] else : return static ( \"img/user-32.png\" ) CommentableModel \u00b6 A class to provide a commentable model. CrossMatch \u00b6 An association between a pipeline source and a survey catalogue source. Each pipeline source may be associated with many sources from each survey catalogue as multiple survey sources may fit inside the beam of the pipeline telescope esp. MWA. The source and survey source rows are referenced by name instead of ID so these records can be retained between pipeline reprocessing runs that produce different IDs. Image \u00b6 An image is a 2D radio image from a cube Measurement \u00b6 A Measurement is an object in the sky that has been detected at least once. Essentially a source single measurement in time. MeasurementPair \u00b6 Links two Measurement objects from the same Source and stores two variability metrics for peak and integrated fluxes: - vs_peak and vs_int is the variability t-statistic. e.g. if Vs is 4.3, then the source is considered variable to a 95% CI. - m_peak and m_int is the modulation index, related to fractional variability. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. MeasurementQuerySet \u00b6 cone_search ( self , ra , dec , radius_deg ) \u00b6 Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Parameters: Name Type Description Default ra float The right ascension value of the cone search central coordinate. required dec float The declination value of the cone search central coordinate. required radius_deg float The radius over which to perform the cone search. required Returns: Type Description QuerySet Measurements found withing the cone search area. Source code in vast_pipeline/models.py def cone_search ( self , ra : float , dec : float , radius_deg : float ) -> models . QuerySet : \"\"\" Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Args: ra: The right ascension value of the cone search central coordinate. dec: The declination value of the cone search central coordinate. radius_deg: The radius over which to perform the cone search. Returns: Measurements found withing the cone search area. \"\"\" return ( self . extra ( select = { \"distance\" : \"q3c_dist(ra, dec, %s , %s ) * 3600\" }, select_params = [ ra , dec ], where = [ \"q3c_radial_query(ra, dec, %s , %s , %s )\" ], params = [ ra , dec , radius_deg ], ) . order_by ( \"distance\" ) ) RelatedSource \u00b6 Association table for the many to many Source relationship with itself Django doc https://docs.djangoproject.com/en/3.1/ref/models/fields/#django.db.models.ManyToManyField.through Run \u00b6 A Run is essentially a pipeline run/processing istance over a set of images save ( self , * args , ** kwargs ) \u00b6 Save the current instance. Override this in a subclass if you want to control the saving process. The 'force_insert' and 'force_update' parameters can be used to insist that the \"save\" must be an SQL insert or update (or equivalent for non-SQL backends), respectively. Normally, they should not be set. Source code in vast_pipeline/models.py def save ( self , * args , ** kwargs ): # enforce the full model validation on save self . full_clean () super ( Run , self ) . save ( * args , ** kwargs ) RunQuerySet \u00b6 check_max_runs ( self , max_runs = 5 ) \u00b6 Check if number of running pipeline runs is above threshold. Parameters: Name Type Description Default max_runs int The maximum number of processing runs allowed. 5 Returns: Type Description int The count of the current pipeline runs with a status of RUN . Source code in vast_pipeline/models.py def check_max_runs ( self , max_runs : int = 5 ) -> int : \"\"\" Check if number of running pipeline runs is above threshold. Args: max_runs: The maximum number of processing runs allowed. Returns: The count of the current pipeline runs with a status of `RUN`. \"\"\" return self . filter ( status = 'RUN' ) . count () >= max_runs SkyRegion \u00b6 SkyRegion(id, centre_ra, centre_dec, width_ra, width_dec, xtr_radius, x, y, z) Source \u00b6 Source(id, run, name, new, wavg_ra, wavg_dec, wavg_uncertainty_ew, wavg_uncertainty_ns, avg_flux_int, avg_flux_peak, max_flux_peak, min_flux_peak, max_flux_int, min_flux_int, min_flux_int_isl_ratio, min_flux_peak_isl_ratio, avg_compactness, min_snr, max_snr, v_int, v_peak, eta_int, eta_peak, new_high_sigma, n_neighbour_dist, vs_abs_significant_max_int, m_abs_significant_max_int, vs_abs_significant_max_peak, m_abs_significant_max_peak, n_meas, n_meas_sel, n_meas_forced, n_rel, n_sibl) SourceFav \u00b6 SourceFav(id, user, source, comment) SourceQuerySet \u00b6 cone_search ( self , ra , dec , radius_deg ) \u00b6 Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Parameters: Name Type Description Default ra float The right ascension value of the cone search central coordinate. required dec float The declination value of the cone search central coordinate. required radius_deg float The radius over which to perform the cone search. required Returns: Type Description QuerySet Sources found withing the cone search area. Source code in vast_pipeline/models.py def cone_search ( self , ra : float , dec : float , radius_deg : float ) -> models . QuerySet : \"\"\" Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Args: ra: The right ascension value of the cone search central coordinate. dec: The declination value of the cone search central coordinate. radius_deg: The radius over which to perform the cone search. Returns: Sources found withing the cone search area. \"\"\" return ( self . extra ( select = { \"distance\" : \"q3c_dist(wavg_ra, wavg_dec, %s , %s ) * 3600\" }, select_params = [ ra , dec ], where = [ \"q3c_radial_query(wavg_ra, wavg_dec, %s , %s , %s )\" ], params = [ ra , dec , radius_deg ], ) . order_by ( \"distance\" ) ) Survey \u00b6 An external survey eg NVSS, SUMSS SurveySource \u00b6 A source from a survey catalogue eg NVSS, SUMSS SurveySourceQuerySet \u00b6 cone_search ( self , ra , dec , radius_deg ) \u00b6 Return all the survey sources withing radius_deg of (ra,dec). Returns a QuerySet of survey sources, ordered by distance from (ra,dec) ascending. Parameters: Name Type Description Default ra float The right ascension value of the cone search central coordinate. required dec float The declination value of the cone search central coordinate. required radius_deg float The radius over which to perform the cone search. required Returns: Type Description QuerySet Sources found withing the cone search area. Source code in vast_pipeline/models.py def cone_search ( self , ra : float , dec : float , radius_deg : float ) -> models . QuerySet : \"\"\" Return all the survey sources withing radius_deg of (ra,dec). Returns a QuerySet of survey sources, ordered by distance from (ra,dec) ascending. Args: ra: The right ascension value of the cone search central coordinate. dec: The declination value of the cone search central coordinate. radius_deg: The radius over which to perform the cone search. Returns: Sources found withing the cone search area. \"\"\" return ( self . extra ( select = { \"distance\" : \"q3c_dist(ra, dec, %s , %s ) * 3600\" }, select_params = [ ra , dec ], where = [ \"q3c_radial_query(ra, dec, %s , %s , %s )\" ], params = [ ra , dec , radius_deg ], ) . order_by ( \"distance\" ) )","title":"models.py"},{"location":"reference/models/#vast_pipeline.models.Association","text":"model association between sources and measurements based on some parameters","title":"Association"},{"location":"reference/models/#vast_pipeline.models.Band","text":"A band on the frequency spectrum used for imaging. Each image is associated with one band.","title":"Band"},{"location":"reference/models/#vast_pipeline.models.Comment","text":"The model object for a comment.","title":"Comment"},{"location":"reference/models/#vast_pipeline.models.Comment.get_avatar_url","text":"Get the URL for the user's avatar from GitHub. If the user has no associated GitHub account (e.g. a Django superuser), return the URL to the default user avatar. Returns: Type Description str The avatar URL. Source code in vast_pipeline/models.py def get_avatar_url ( self ) -> str : \"\"\"Get the URL for the user's avatar from GitHub. If the user has no associated GitHub account (e.g. a Django superuser), return the URL to the default user avatar. Returns: The avatar URL. \"\"\" social = UserSocialAuth . get_social_auth_for_user ( self . author ) . first () if social and \"avatar_url\" in social . extra_data : return social . extra_data [ \"avatar_url\" ] else : return static ( \"img/user-32.png\" )","title":"get_avatar_url()"},{"location":"reference/models/#vast_pipeline.models.CommentableModel","text":"A class to provide a commentable model.","title":"CommentableModel"},{"location":"reference/models/#vast_pipeline.models.CrossMatch","text":"An association between a pipeline source and a survey catalogue source. Each pipeline source may be associated with many sources from each survey catalogue as multiple survey sources may fit inside the beam of the pipeline telescope esp. MWA. The source and survey source rows are referenced by name instead of ID so these records can be retained between pipeline reprocessing runs that produce different IDs.","title":"CrossMatch"},{"location":"reference/models/#vast_pipeline.models.Image","text":"An image is a 2D radio image from a cube","title":"Image"},{"location":"reference/models/#vast_pipeline.models.Measurement","text":"A Measurement is an object in the sky that has been detected at least once. Essentially a source single measurement in time.","title":"Measurement"},{"location":"reference/models/#vast_pipeline.models.MeasurementPair","text":"Links two Measurement objects from the same Source and stores two variability metrics for peak and integrated fluxes: - vs_peak and vs_int is the variability t-statistic. e.g. if Vs is 4.3, then the source is considered variable to a 95% CI. - m_peak and m_int is the modulation index, related to fractional variability. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105.","title":"MeasurementPair"},{"location":"reference/models/#vast_pipeline.models.MeasurementQuerySet","text":"","title":"MeasurementQuerySet"},{"location":"reference/models/#vast_pipeline.models.MeasurementQuerySet.cone_search","text":"Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Parameters: Name Type Description Default ra float The right ascension value of the cone search central coordinate. required dec float The declination value of the cone search central coordinate. required radius_deg float The radius over which to perform the cone search. required Returns: Type Description QuerySet Measurements found withing the cone search area. Source code in vast_pipeline/models.py def cone_search ( self , ra : float , dec : float , radius_deg : float ) -> models . QuerySet : \"\"\" Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Args: ra: The right ascension value of the cone search central coordinate. dec: The declination value of the cone search central coordinate. radius_deg: The radius over which to perform the cone search. Returns: Measurements found withing the cone search area. \"\"\" return ( self . extra ( select = { \"distance\" : \"q3c_dist(ra, dec, %s , %s ) * 3600\" }, select_params = [ ra , dec ], where = [ \"q3c_radial_query(ra, dec, %s , %s , %s )\" ], params = [ ra , dec , radius_deg ], ) . order_by ( \"distance\" ) )","title":"cone_search()"},{"location":"reference/models/#vast_pipeline.models.RelatedSource","text":"Association table for the many to many Source relationship with itself Django doc https://docs.djangoproject.com/en/3.1/ref/models/fields/#django.db.models.ManyToManyField.through","title":"RelatedSource"},{"location":"reference/models/#vast_pipeline.models.Run","text":"A Run is essentially a pipeline run/processing istance over a set of images","title":"Run"},{"location":"reference/models/#vast_pipeline.models.Run.save","text":"Save the current instance. Override this in a subclass if you want to control the saving process. The 'force_insert' and 'force_update' parameters can be used to insist that the \"save\" must be an SQL insert or update (or equivalent for non-SQL backends), respectively. Normally, they should not be set. Source code in vast_pipeline/models.py def save ( self , * args , ** kwargs ): # enforce the full model validation on save self . full_clean () super ( Run , self ) . save ( * args , ** kwargs )","title":"save()"},{"location":"reference/models/#vast_pipeline.models.RunQuerySet","text":"","title":"RunQuerySet"},{"location":"reference/models/#vast_pipeline.models.RunQuerySet.check_max_runs","text":"Check if number of running pipeline runs is above threshold. Parameters: Name Type Description Default max_runs int The maximum number of processing runs allowed. 5 Returns: Type Description int The count of the current pipeline runs with a status of RUN . Source code in vast_pipeline/models.py def check_max_runs ( self , max_runs : int = 5 ) -> int : \"\"\" Check if number of running pipeline runs is above threshold. Args: max_runs: The maximum number of processing runs allowed. Returns: The count of the current pipeline runs with a status of `RUN`. \"\"\" return self . filter ( status = 'RUN' ) . count () >= max_runs","title":"check_max_runs()"},{"location":"reference/models/#vast_pipeline.models.SkyRegion","text":"SkyRegion(id, centre_ra, centre_dec, width_ra, width_dec, xtr_radius, x, y, z)","title":"SkyRegion"},{"location":"reference/models/#vast_pipeline.models.Source","text":"Source(id, run, name, new, wavg_ra, wavg_dec, wavg_uncertainty_ew, wavg_uncertainty_ns, avg_flux_int, avg_flux_peak, max_flux_peak, min_flux_peak, max_flux_int, min_flux_int, min_flux_int_isl_ratio, min_flux_peak_isl_ratio, avg_compactness, min_snr, max_snr, v_int, v_peak, eta_int, eta_peak, new_high_sigma, n_neighbour_dist, vs_abs_significant_max_int, m_abs_significant_max_int, vs_abs_significant_max_peak, m_abs_significant_max_peak, n_meas, n_meas_sel, n_meas_forced, n_rel, n_sibl)","title":"Source"},{"location":"reference/models/#vast_pipeline.models.SourceFav","text":"SourceFav(id, user, source, comment)","title":"SourceFav"},{"location":"reference/models/#vast_pipeline.models.SourceQuerySet","text":"","title":"SourceQuerySet"},{"location":"reference/models/#vast_pipeline.models.SourceQuerySet.cone_search","text":"Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Parameters: Name Type Description Default ra float The right ascension value of the cone search central coordinate. required dec float The declination value of the cone search central coordinate. required radius_deg float The radius over which to perform the cone search. required Returns: Type Description QuerySet Sources found withing the cone search area. Source code in vast_pipeline/models.py def cone_search ( self , ra : float , dec : float , radius_deg : float ) -> models . QuerySet : \"\"\" Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Args: ra: The right ascension value of the cone search central coordinate. dec: The declination value of the cone search central coordinate. radius_deg: The radius over which to perform the cone search. Returns: Sources found withing the cone search area. \"\"\" return ( self . extra ( select = { \"distance\" : \"q3c_dist(wavg_ra, wavg_dec, %s , %s ) * 3600\" }, select_params = [ ra , dec ], where = [ \"q3c_radial_query(wavg_ra, wavg_dec, %s , %s , %s )\" ], params = [ ra , dec , radius_deg ], ) . order_by ( \"distance\" ) )","title":"cone_search()"},{"location":"reference/models/#vast_pipeline.models.Survey","text":"An external survey eg NVSS, SUMSS","title":"Survey"},{"location":"reference/models/#vast_pipeline.models.SurveySource","text":"A source from a survey catalogue eg NVSS, SUMSS","title":"SurveySource"},{"location":"reference/models/#vast_pipeline.models.SurveySourceQuerySet","text":"","title":"SurveySourceQuerySet"},{"location":"reference/models/#vast_pipeline.models.SurveySourceQuerySet.cone_search","text":"Return all the survey sources withing radius_deg of (ra,dec). Returns a QuerySet of survey sources, ordered by distance from (ra,dec) ascending. Parameters: Name Type Description Default ra float The right ascension value of the cone search central coordinate. required dec float The declination value of the cone search central coordinate. required radius_deg float The radius over which to perform the cone search. required Returns: Type Description QuerySet Sources found withing the cone search area. Source code in vast_pipeline/models.py def cone_search ( self , ra : float , dec : float , radius_deg : float ) -> models . QuerySet : \"\"\" Return all the survey sources withing radius_deg of (ra,dec). Returns a QuerySet of survey sources, ordered by distance from (ra,dec) ascending. Args: ra: The right ascension value of the cone search central coordinate. dec: The declination value of the cone search central coordinate. radius_deg: The radius over which to perform the cone search. Returns: Sources found withing the cone search area. \"\"\" return ( self . extra ( select = { \"distance\" : \"q3c_dist(ra, dec, %s , %s ) * 3600\" }, select_params = [ ra , dec ], where = [ \"q3c_radial_query(ra, dec, %s , %s , %s )\" ], params = [ ra , dec , radius_deg ], ) . order_by ( \"distance\" ) )","title":"cone_search()"},{"location":"reference/plots/","text":"Contains plotting code used by the web server. plot_lightcurve ( source , vs_abs_min = 4.3 , m_abs_min = 0.26 , use_peak_flux = True ) \u00b6 Create the lightcurve and 2-epoch metric graph for a source with Bokeh. Parameters: Name Type Description Default source Source Source object. required vs_abs_min float MeasurementPair objects with an absolute vs metric greater than vs_abs_min and m metric greater than m_abs_min will be connected in the metric graph. Defaults to 4.3. 4.3 m_abs_min float See vs_abs_min . Defaults to 0.26. 0.26 use_peak_flux bool If True, use peak fluxes, otherwise use integrated fluxes. Defaults to True. True Returns: Type Description Row Row: Bokeh Row layout object containing the lightcurve and graph plots. Source code in vast_pipeline/plots.py def plot_lightcurve ( source : Source , vs_abs_min : float = 4.3 , m_abs_min : float = 0.26 , use_peak_flux : bool = True , ) -> Row : \"\"\"Create the lightcurve and 2-epoch metric graph for a source with Bokeh. Args: source (Source): Source object. vs_abs_min (float, optional): MeasurementPair objects with an absolute vs metric greater than `vs_abs_min` and m metric greater than `m_abs_min` will be connected in the metric graph. Defaults to 4.3. m_abs_min (float, optional): See `vs_abs_min`. Defaults to 0.26. use_peak_flux (bool, optional): If True, use peak fluxes, otherwise use integrated fluxes. Defaults to True. Returns: Row: Bokeh Row layout object containing the lightcurve and graph plots. \"\"\" PLOT_WIDTH = 800 PLOT_HEIGHT = 300 flux_column = \"flux_peak\" if use_peak_flux else \"flux_int\" metric_suffix = \"peak\" if use_peak_flux else \"int\" measurements_qs = ( Measurement . objects . filter ( source__id = source . id ) . annotate ( taustart_ts = F ( \"image__datetime\" ), flux = F ( flux_column ), flux_err_lower = F ( flux_column ) - F ( f \" { flux_column } _err\" ), flux_err_upper = F ( flux_column ) + F ( f \" { flux_column } _err\" ), ) . values ( \"id\" , \"pk\" , \"taustart_ts\" , \"flux\" , \"flux_err_upper\" , \"flux_err_lower\" , \"forced\" , ) . order_by ( \"taustart_ts\" ) ) candidate_measurement_pairs_qs = ( source . measurementpair_set . annotate ( m_abs = Abs ( f \"m_ { metric_suffix } \" ), vs_abs = Abs ( f \"vs_ { metric_suffix } \" ) ) . filter ( vs_abs__gte = vs_abs_min , m_abs__gte = m_abs_min ) . values ( \"measurement_a_id\" , \"measurement_b_id\" , \"vs_abs\" , \"m_abs\" ) ) candidate_measurement_pairs_df = pd . DataFrame ( candidate_measurement_pairs_qs ) # lightcurve required cols: taustart_ts, flux, flux_err_upper, flux_err_lower, forced lightcurve = pd . DataFrame ( measurements_qs ) # remap method values to labels to make a better legend lightcurve [ \"method\" ] = lightcurve . forced . map ({ True : \"Forced\" , False : \"Selavy\" }) source = ColumnDataSource ( lightcurve ) method_mapper = factor_cmap ( \"method\" , palette = \"Colorblind3\" , factors = [ \"Selavy\" , \"Forced\" ] ) min_y = min ( 0 , lightcurve . flux_err_lower . min ()) max_y = lightcurve . flux_err_upper . max () y_padding = ( max_y - min_y ) * 0.1 fig_lc = figure ( plot_width = PLOT_WIDTH , plot_height = PLOT_HEIGHT , sizing_mode = \"stretch_width\" , x_axis_type = \"datetime\" , y_range = DataRange1d ( start = min_y , end = max_y + y_padding ), ) # line source must be a COPY of the data for the scatter source for the hover and # selection to work properly, using the same ColumnDataSource will break it fig_lc . line ( \"taustart_ts\" , \"flux\" , source = lightcurve ) lc_scatter = fig_lc . scatter ( \"taustart_ts\" , \"flux\" , marker = \"circle\" , size = 6 , color = method_mapper , nonselection_color = method_mapper , selection_color = \"red\" , nonselection_alpha = 1.0 , hover_color = \"red\" , alpha = 1.0 , source = source , legend_group = \"method\" , ) fig_lc . add_layout ( Whisker ( base = \"taustart_ts\" , upper = \"flux_err_upper\" , lower = \"flux_err_lower\" , source = source , ) ) fig_lc . xaxis . axis_label = \"Datetime\" fig_lc . xaxis [ 0 ] . formatter = DatetimeTickFormatter ( months = \" %F \" ) fig_lc . yaxis . axis_label = ( \"Peak flux (mJy/beam)\" if use_peak_flux else \"Integrated flux (mJy)\" ) # determine legend location: either bottom_left or top_left legend_location = ( \"top_left\" if lightcurve . sort_values ( \"taustart_ts\" ) . iloc [ 0 ] . flux < ( max_y - min_y ) / 2 else \"bottom_left\" ) fig_lc . legend . location = legend_location # TODO add vs and m metrics to graph edges # create plot fig_graph = figure ( plot_width = PLOT_HEIGHT , plot_height = PLOT_HEIGHT , x_range = Range1d ( - 1.1 , 1.1 ), y_range = Range1d ( - 1.1 , 1.1 ), x_axis_type = None , y_axis_type = None , sizing_mode = \"fixed\" , ) if len ( candidate_measurement_pairs_df ) > 0 : g = nx . Graph () for _row in candidate_measurement_pairs_df . itertuples ( index = False ): g . add_edge ( _row . measurement_a_id , _row . measurement_b_id ) node_layout = nx . circular_layout ( g , scale = 1 , center = ( 0 , 0 )) # add node positions to dataframe for suffix in [ \"a\" , \"b\" ]: pos_df = pd . DataFrame ( candidate_measurement_pairs_df [ f \"measurement_ { suffix } _id\" ] . map ( node_layout ) . to_list (), columns = [ f \"measurement_ { suffix } _x\" , f \"measurement_ { suffix } _y\" ], ) candidate_measurement_pairs_df = candidate_measurement_pairs_df . join ( pos_df ) candidate_measurement_pairs_df [ \"measurement_x\" ] = list ( zip ( candidate_measurement_pairs_df . measurement_a_x . values , candidate_measurement_pairs_df . measurement_b_x . values , ) ) candidate_measurement_pairs_df [ \"measurement_y\" ] = list ( zip ( candidate_measurement_pairs_df . measurement_a_y . values , candidate_measurement_pairs_df . measurement_b_y . values , ) ) node_positions_df = pd . DataFrame . from_dict ( node_layout , orient = \"index\" , columns = [ \"x\" , \"y\" ] ) node_positions_df [ \"lc_index\" ] = node_positions_df . index . map ( { v : k for k , v in lightcurve . id . to_dict () . items ()} ) . values node_source = ColumnDataSource ( node_positions_df ) edge_source = ColumnDataSource ( candidate_measurement_pairs_df ) # add edges to plot edge_renderer = fig_graph . multi_line ( \"measurement_x\" , \"measurement_y\" , line_width = 5 , hover_color = \"red\" , source = edge_source , name = \"edges\" , ) # add nodes to plot node_renderer = fig_graph . circle ( \"x\" , \"y\" , size = 20 , hover_color = \"red\" , selection_color = \"red\" , nonselection_alpha = 1.0 , source = node_source , name = \"nodes\" , ) # create hover tool for node edges edge_callback_code = \"\"\" // get edge index let indices_a = cb_data.index.indices.map(i => edge_data.data.measurement_a_id[i]); let indices_b = cb_data.index.indices.map(i => edge_data.data.measurement_b_id[i]); let indices = indices_a.concat(indices_b); let lightcurve_indices = indices.map(i => lightcurve_data.data.id.indexOf(i)); lightcurve_data.selected.indices = lightcurve_indices; \"\"\" hover_tool_edges = HoverTool ( tooltips = None , renderers = [ edge_renderer ], callback = CustomJS ( args = { \"lightcurve_data\" : lc_scatter . data_source , \"edge_data\" : edge_renderer . data_source , }, code = edge_callback_code , ), ) fig_graph . add_tools ( hover_tool_edges ) # create labels for nodes graph_source = ColumnDataSource ( node_positions_df ) labels = LabelSet ( x = \"x\" , y = \"y\" , text = \"lc_index\" , source = graph_source , text_align = \"center\" , text_baseline = \"middle\" , text_font_size = \"1em\" , text_color = \"white\" , ) fig_graph . renderers . append ( labels ) # create hover tool for lightcurve hover_tool_lc = HoverTool ( tooltips = [ ( \"Index\" , \"@index\" ), ( \"Date\" , \"@taustart_ts{ %F }\" ), ( f \"Flux { metric_suffix } \" , \"@flux mJy\" ), ], formatters = { \"@taustart_ts\" : \"datetime\" ,}, mode = \"vline\" , callback = CustomJS ( args = { \"node_data\" : node_renderer . data_source , \"lightcurve_data\" : lc_scatter . data_source , }, code = \"\"\" let ids = cb_data.index.indices.map(i => lightcurve_data.data.id[i]); let node_indices = ids.map(i => node_data.data.index.indexOf(i)); node_data.selected.indices = node_indices; \"\"\" , ), ) fig_lc . add_tools ( hover_tool_lc ) plot_row = row ( fig_lc , fig_graph , sizing_mode = \"stretch_width\" ) plot_row . css_classes . append ( \"mx-auto\" ) return plot_row","title":"plots.py"},{"location":"reference/plots/#vast_pipeline.plots.plot_lightcurve","text":"Create the lightcurve and 2-epoch metric graph for a source with Bokeh. Parameters: Name Type Description Default source Source Source object. required vs_abs_min float MeasurementPair objects with an absolute vs metric greater than vs_abs_min and m metric greater than m_abs_min will be connected in the metric graph. Defaults to 4.3. 4.3 m_abs_min float See vs_abs_min . Defaults to 0.26. 0.26 use_peak_flux bool If True, use peak fluxes, otherwise use integrated fluxes. Defaults to True. True Returns: Type Description Row Row: Bokeh Row layout object containing the lightcurve and graph plots. Source code in vast_pipeline/plots.py def plot_lightcurve ( source : Source , vs_abs_min : float = 4.3 , m_abs_min : float = 0.26 , use_peak_flux : bool = True , ) -> Row : \"\"\"Create the lightcurve and 2-epoch metric graph for a source with Bokeh. Args: source (Source): Source object. vs_abs_min (float, optional): MeasurementPair objects with an absolute vs metric greater than `vs_abs_min` and m metric greater than `m_abs_min` will be connected in the metric graph. Defaults to 4.3. m_abs_min (float, optional): See `vs_abs_min`. Defaults to 0.26. use_peak_flux (bool, optional): If True, use peak fluxes, otherwise use integrated fluxes. Defaults to True. Returns: Row: Bokeh Row layout object containing the lightcurve and graph plots. \"\"\" PLOT_WIDTH = 800 PLOT_HEIGHT = 300 flux_column = \"flux_peak\" if use_peak_flux else \"flux_int\" metric_suffix = \"peak\" if use_peak_flux else \"int\" measurements_qs = ( Measurement . objects . filter ( source__id = source . id ) . annotate ( taustart_ts = F ( \"image__datetime\" ), flux = F ( flux_column ), flux_err_lower = F ( flux_column ) - F ( f \" { flux_column } _err\" ), flux_err_upper = F ( flux_column ) + F ( f \" { flux_column } _err\" ), ) . values ( \"id\" , \"pk\" , \"taustart_ts\" , \"flux\" , \"flux_err_upper\" , \"flux_err_lower\" , \"forced\" , ) . order_by ( \"taustart_ts\" ) ) candidate_measurement_pairs_qs = ( source . measurementpair_set . annotate ( m_abs = Abs ( f \"m_ { metric_suffix } \" ), vs_abs = Abs ( f \"vs_ { metric_suffix } \" ) ) . filter ( vs_abs__gte = vs_abs_min , m_abs__gte = m_abs_min ) . values ( \"measurement_a_id\" , \"measurement_b_id\" , \"vs_abs\" , \"m_abs\" ) ) candidate_measurement_pairs_df = pd . DataFrame ( candidate_measurement_pairs_qs ) # lightcurve required cols: taustart_ts, flux, flux_err_upper, flux_err_lower, forced lightcurve = pd . DataFrame ( measurements_qs ) # remap method values to labels to make a better legend lightcurve [ \"method\" ] = lightcurve . forced . map ({ True : \"Forced\" , False : \"Selavy\" }) source = ColumnDataSource ( lightcurve ) method_mapper = factor_cmap ( \"method\" , palette = \"Colorblind3\" , factors = [ \"Selavy\" , \"Forced\" ] ) min_y = min ( 0 , lightcurve . flux_err_lower . min ()) max_y = lightcurve . flux_err_upper . max () y_padding = ( max_y - min_y ) * 0.1 fig_lc = figure ( plot_width = PLOT_WIDTH , plot_height = PLOT_HEIGHT , sizing_mode = \"stretch_width\" , x_axis_type = \"datetime\" , y_range = DataRange1d ( start = min_y , end = max_y + y_padding ), ) # line source must be a COPY of the data for the scatter source for the hover and # selection to work properly, using the same ColumnDataSource will break it fig_lc . line ( \"taustart_ts\" , \"flux\" , source = lightcurve ) lc_scatter = fig_lc . scatter ( \"taustart_ts\" , \"flux\" , marker = \"circle\" , size = 6 , color = method_mapper , nonselection_color = method_mapper , selection_color = \"red\" , nonselection_alpha = 1.0 , hover_color = \"red\" , alpha = 1.0 , source = source , legend_group = \"method\" , ) fig_lc . add_layout ( Whisker ( base = \"taustart_ts\" , upper = \"flux_err_upper\" , lower = \"flux_err_lower\" , source = source , ) ) fig_lc . xaxis . axis_label = \"Datetime\" fig_lc . xaxis [ 0 ] . formatter = DatetimeTickFormatter ( months = \" %F \" ) fig_lc . yaxis . axis_label = ( \"Peak flux (mJy/beam)\" if use_peak_flux else \"Integrated flux (mJy)\" ) # determine legend location: either bottom_left or top_left legend_location = ( \"top_left\" if lightcurve . sort_values ( \"taustart_ts\" ) . iloc [ 0 ] . flux < ( max_y - min_y ) / 2 else \"bottom_left\" ) fig_lc . legend . location = legend_location # TODO add vs and m metrics to graph edges # create plot fig_graph = figure ( plot_width = PLOT_HEIGHT , plot_height = PLOT_HEIGHT , x_range = Range1d ( - 1.1 , 1.1 ), y_range = Range1d ( - 1.1 , 1.1 ), x_axis_type = None , y_axis_type = None , sizing_mode = \"fixed\" , ) if len ( candidate_measurement_pairs_df ) > 0 : g = nx . Graph () for _row in candidate_measurement_pairs_df . itertuples ( index = False ): g . add_edge ( _row . measurement_a_id , _row . measurement_b_id ) node_layout = nx . circular_layout ( g , scale = 1 , center = ( 0 , 0 )) # add node positions to dataframe for suffix in [ \"a\" , \"b\" ]: pos_df = pd . DataFrame ( candidate_measurement_pairs_df [ f \"measurement_ { suffix } _id\" ] . map ( node_layout ) . to_list (), columns = [ f \"measurement_ { suffix } _x\" , f \"measurement_ { suffix } _y\" ], ) candidate_measurement_pairs_df = candidate_measurement_pairs_df . join ( pos_df ) candidate_measurement_pairs_df [ \"measurement_x\" ] = list ( zip ( candidate_measurement_pairs_df . measurement_a_x . values , candidate_measurement_pairs_df . measurement_b_x . values , ) ) candidate_measurement_pairs_df [ \"measurement_y\" ] = list ( zip ( candidate_measurement_pairs_df . measurement_a_y . values , candidate_measurement_pairs_df . measurement_b_y . values , ) ) node_positions_df = pd . DataFrame . from_dict ( node_layout , orient = \"index\" , columns = [ \"x\" , \"y\" ] ) node_positions_df [ \"lc_index\" ] = node_positions_df . index . map ( { v : k for k , v in lightcurve . id . to_dict () . items ()} ) . values node_source = ColumnDataSource ( node_positions_df ) edge_source = ColumnDataSource ( candidate_measurement_pairs_df ) # add edges to plot edge_renderer = fig_graph . multi_line ( \"measurement_x\" , \"measurement_y\" , line_width = 5 , hover_color = \"red\" , source = edge_source , name = \"edges\" , ) # add nodes to plot node_renderer = fig_graph . circle ( \"x\" , \"y\" , size = 20 , hover_color = \"red\" , selection_color = \"red\" , nonselection_alpha = 1.0 , source = node_source , name = \"nodes\" , ) # create hover tool for node edges edge_callback_code = \"\"\" // get edge index let indices_a = cb_data.index.indices.map(i => edge_data.data.measurement_a_id[i]); let indices_b = cb_data.index.indices.map(i => edge_data.data.measurement_b_id[i]); let indices = indices_a.concat(indices_b); let lightcurve_indices = indices.map(i => lightcurve_data.data.id.indexOf(i)); lightcurve_data.selected.indices = lightcurve_indices; \"\"\" hover_tool_edges = HoverTool ( tooltips = None , renderers = [ edge_renderer ], callback = CustomJS ( args = { \"lightcurve_data\" : lc_scatter . data_source , \"edge_data\" : edge_renderer . data_source , }, code = edge_callback_code , ), ) fig_graph . add_tools ( hover_tool_edges ) # create labels for nodes graph_source = ColumnDataSource ( node_positions_df ) labels = LabelSet ( x = \"x\" , y = \"y\" , text = \"lc_index\" , source = graph_source , text_align = \"center\" , text_baseline = \"middle\" , text_font_size = \"1em\" , text_color = \"white\" , ) fig_graph . renderers . append ( labels ) # create hover tool for lightcurve hover_tool_lc = HoverTool ( tooltips = [ ( \"Index\" , \"@index\" ), ( \"Date\" , \"@taustart_ts{ %F }\" ), ( f \"Flux { metric_suffix } \" , \"@flux mJy\" ), ], formatters = { \"@taustart_ts\" : \"datetime\" ,}, mode = \"vline\" , callback = CustomJS ( args = { \"node_data\" : node_renderer . data_source , \"lightcurve_data\" : lc_scatter . data_source , }, code = \"\"\" let ids = cb_data.index.indices.map(i => lightcurve_data.data.id[i]); let node_indices = ids.map(i => node_data.data.index.indexOf(i)); node_data.selected.indices = node_indices; \"\"\" , ), ) fig_lc . add_tools ( hover_tool_lc ) plot_row = row ( fig_lc , fig_graph , sizing_mode = \"stretch_width\" ) plot_row . css_classes . append ( \"mx-auto\" ) return plot_row","title":"plot_lightcurve()"},{"location":"reference/urls/","text":"This module contains the urls used by the Django web server.","title":"urls.py"},{"location":"reference/image/main/","text":"This module contains the relevant classes for the image ingestion. FitsImage \u00b6 FitsImage class to model FITS files Attributes: Name Type Description beam_bmaj float Major axis size of the restoring beam (degrees). beam_bmin float Minor axis size of the restoring beam (degrees). beam_bpa float Position angle of the restoring beam (degrees). datetime pd.Timestamp Date of the observation. duration float Duration of the observation in seconds. Is set to 0 if duration is not in header. fov_bmaj float Estimate of the field of view in the north-south direction (degrees). fov_bmin float Estimate of the field of view in the east-west direction (degrees). ra float Right ascension coordinate of the image centre (degrees). dec float Declination coordinate of the image centre (degrees). polarisation str The polarisation of the image. __init__ ( self , path , hdu_index = 0 ) special \u00b6 Initialise a FitsImage object. Parameters: Name Type Description Default path str The system path of the FITS image. required hdu_index int The index to use on the hdu to fetch the FITS header. 0 Returns: Type Description None None. Source code in vast_pipeline/image/main.py def __init__ ( self , path : str , hdu_index : int = 0 ) -> None : \"\"\" Initialise a FitsImage object. Args: path: The system path of the FITS image. hdu_index: The index to use on the hdu to fetch the FITS header. Returns: None. \"\"\" # inherit from parent super () . __init__ ( path ) # set other attributes header = self . __get_header ( hdu_index ) # set the rest of the attributes self . __set_img_attr_for_telescope ( header ) # get the frequency self . __get_frequency ( header ) Image \u00b6 Generic abstract class for an image. Attributes: Name Type Description name str The image name taken from the file name. path str The system path to the image. __init__ ( self , path ) special \u00b6 Initiliase an image object. Parameters: Name Type Description Default path str The system path to the FITS image. The name of the image is taken from the filename in the given path. required Returns: Type Description None None. Source code in vast_pipeline/image/main.py def __init__ ( self , path : str ) -> None : \"\"\" Initiliase an image object. Args: path: The system path to the FITS image. The name of the image is taken from the filename in the given path. Returns: None. \"\"\" self . name = os . path . basename ( path ) self . path = path __repr__ ( self ) special \u00b6 Defines the printable representation. Returns: Type Description str Printable representation which is the pipeline run name. Source code in vast_pipeline/image/main.py def __repr__ ( self ) -> str : \"\"\" Defines the printable representation. Returns: Printable representation which is the pipeline run name. \"\"\" return self . name SelavyImage \u00b6 Fits images that have a selavy catalogue. Attributes: Name Type Description selavy_path str The system path to the Selavy file. noise_path str The system path to the noise image associated with the image. background_path str The system path to the background image associated with the image. config config The pipeline configuration settings. __init__ ( self , path , paths , hdu_index = 0 , config = None ) special \u00b6 Initialise the SelavyImage. Parameters: Name Type Description Default path str The system path to the FITS image. required paths Dict[str, str] Dictionary containing the system paths to the associated image products and selavy catalogue. The keys are 'selavy', 'noise', 'background'. required hdu_index int The index number to use to access the header from the hdu object. 0 config config Configuration settings for the pipeline. None Returns: Type Description None None. Source code in vast_pipeline/image/main.py def __init__ ( self , path : str , paths : Dict [ str , str ], hdu_index : int = 0 , config = None ) -> None : \"\"\" Initialise the SelavyImage. Args: path: The system path to the FITS image. paths: Dictionary containing the system paths to the associated image products and selavy catalogue. The keys are 'selavy', 'noise', 'background'. hdu_index: The index number to use to access the header from the hdu object. config (config): Configuration settings for the pipeline. Returns: None. \"\"\" # inherit from parent self . selavy_path = paths [ 'selavy' ][ path ] self . noise_path = paths [ 'noise' ] . get ( path , '' ) self . background_path = paths [ 'background' ] . get ( path , '' ) self . config = config super () . __init__ ( path , hdu_index ) read_selavy ( self , dj_image ) \u00b6 Read the sources from the selavy catalogue, select wanted columns and remap them to correct names, followed by filtering and Condon error calculations. Parameters: Name Type Description Default dj_image Image The image model object. required Returns: Type Description DataFrame Dataframe containing the cleaned and processed Selavy components. Source code in vast_pipeline/image/main.py def read_selavy ( self , dj_image : Image ) -> pd . DataFrame : \"\"\" Read the sources from the selavy catalogue, select wanted columns and remap them to correct names, followed by filtering and Condon error calculations. Args: dj_image: The image model object. Returns: Dataframe containing the cleaned and processed Selavy components. \"\"\" # TODO: improve with loading only the cols we need and set datatype df = pd . read_fwf ( self . selavy_path , skiprows = [ 1 ]) # drop first line with unit of measure, select only wanted # columns and rename them df = df . loc [:, tr_selavy . keys ()] . rename ( columns = { x : tr_selavy [ x ][ \"name\" ] for x in tr_selavy } ) # fix dtype of columns for ky in tr_selavy : key = tr_selavy [ ky ] if df [ key [ 'name' ]] . dtype != key [ 'dtype' ]: df [ key [ 'name' ]] = df [ key [ 'name' ]] . astype ( key [ 'dtype' ]) # do checks and fill in missing field for uploading sources # in DB (see fields in models.py -> Source model) if df [ 'component_id' ] . duplicated () . any (): raise Exception ( 'Found duplicated names in sources' ) # drop unrealistic sources cols_to_check = [ 'bmaj' , 'bmin' , 'flux_peak' , 'flux_int' , ] bad_sources = df [( df [ cols_to_check ] == 0 ) . any ( axis = 1 )] if bad_sources . shape [ 0 ] > 0 : logger . debug ( \"Dropping %i bad sources.\" , bad_sources . shape [ 0 ]) df = df . drop ( bad_sources . index ) # dropping tiny sources nr_sources_old = df . shape [ 0 ] df = df . loc [ ( df [ 'bmaj' ] > dj_image . beam_bmaj * 500 ) & ( df [ 'bmin' ] > dj_image . beam_bmin * 500 ) ] if df . shape [ 0 ] != nr_sources_old : logger . info ( 'Dropped %i tiny sources.' , nr_sources_old - df . shape [ 0 ] ) # add fields from image and fix name column df [ 'image_id' ] = dj_image . id df [ 'time' ] = dj_image . datetime # append img prefix to source name img_prefix = dj_image . name . split ( '.i.' , 1 )[ - 1 ] . split ( '.' , 1 )[ 0 ] + '_' df [ 'name' ] = img_prefix + df [ 'component_id' ] # # fix error fluxes for col in [ 'flux_int_err' , 'flux_peak_err' ]: sel = df [ col ] < settings . FLUX_DEFAULT_MIN_ERROR if sel . any (): df . loc [ sel , col ] = settings . FLUX_DEFAULT_MIN_ERROR # # fix error ra dec for col in [ 'ra_err' , 'dec_err' ]: sel = df [ col ] < settings . POS_DEFAULT_MIN_ERROR if sel . any (): df . loc [ sel , col ] = settings . POS_DEFAULT_MIN_ERROR df [ col ] = df [ col ] / 3600. # replace 0 local_rms values using user config value df . loc [ df [ 'local_rms' ] == 0. , 'local_rms' ] = self . config . SELAVY_LOCAL_RMS_ZERO_FILL_VALUE df [ 'snr' ] = df [ 'flux_peak' ] . values / df [ 'local_rms' ] . values df [ 'compactness' ] = df [ 'flux_int' ] . values / df [ 'flux_peak' ] . values if self . config . USE_CONDON_ERRORS : logger . debug ( \"Calculating Condon '97 errors...\" ) theta_B = dj_image . beam_bmaj theta_b = dj_image . beam_bmin df [[ 'flux_peak_err' , 'flux_int_err' , 'err_bmaj' , 'err_bmin' , 'err_pa' , 'ra_err' , 'dec_err' , ]] = df [[ 'flux_peak' , 'flux_int' , 'bmaj' , 'bmin' , 'pa' , 'snr' , 'local_rms' , ]] . apply ( calc_condon_flux_errors , args = ( theta_B , theta_b ), axis = 1 , result_type = 'expand' ) logger . debug ( \"Condon errors done.\" ) logger . debug ( \"Calculating positional errors...\" ) # TODO: avoid extra column given that it is a single value df [ 'ew_sys_err' ] = self . config . ASTROMETRIC_UNCERTAINTY_RA / 3600. df [ 'ns_sys_err' ] = self . config . ASTROMETRIC_UNCERTAINTY_DEC / 3600. df [ 'error_radius' ] = calc_error_radius ( df [ 'ra' ] . values , df [ 'ra_err' ] . values , df [ 'dec' ] . values , df [ 'dec_err' ] . values , ) df [ 'uncertainty_ew' ] = np . hypot ( df [ 'ew_sys_err' ] . values , df [ 'error_radius' ] . values ) df [ 'uncertainty_ns' ] = np . hypot ( df [ 'ns_sys_err' ] . values , df [ 'error_radius' ] . values ) # weight calculations to use later df [ 'weight_ew' ] = 1. / df [ 'uncertainty_ew' ] . values ** 2 df [ 'weight_ns' ] = 1. / df [ 'uncertainty_ns' ] . values ** 2 logger . debug ( 'Positional errors done.' ) # Initialise the forced column as False df [ 'forced' ] = False # Calculate island flux fractions island_flux_totals = ( df [[ 'island_id' , 'flux_int' , 'flux_peak' ]] . groupby ( 'island_id' ) . agg ( 'sum' ) ) df [ 'flux_int_isl_ratio' ] = ( df [ 'flux_int' ] . values / island_flux_totals . loc [ df [ 'island_id' ]][ 'flux_int' ] . values ) df [ 'flux_peak_isl_ratio' ] = ( df [ 'flux_peak' ] . values / island_flux_totals . loc [ df [ 'island_id' ]][ 'flux_peak' ] . values ) return df","title":"main.py"},{"location":"reference/image/main/#vast_pipeline.image.main.FitsImage","text":"FitsImage class to model FITS files Attributes: Name Type Description beam_bmaj float Major axis size of the restoring beam (degrees). beam_bmin float Minor axis size of the restoring beam (degrees). beam_bpa float Position angle of the restoring beam (degrees). datetime pd.Timestamp Date of the observation. duration float Duration of the observation in seconds. Is set to 0 if duration is not in header. fov_bmaj float Estimate of the field of view in the north-south direction (degrees). fov_bmin float Estimate of the field of view in the east-west direction (degrees). ra float Right ascension coordinate of the image centre (degrees). dec float Declination coordinate of the image centre (degrees). polarisation str The polarisation of the image.","title":"FitsImage"},{"location":"reference/image/main/#vast_pipeline.image.main.FitsImage.__init__","text":"Initialise a FitsImage object. Parameters: Name Type Description Default path str The system path of the FITS image. required hdu_index int The index to use on the hdu to fetch the FITS header. 0 Returns: Type Description None None. Source code in vast_pipeline/image/main.py def __init__ ( self , path : str , hdu_index : int = 0 ) -> None : \"\"\" Initialise a FitsImage object. Args: path: The system path of the FITS image. hdu_index: The index to use on the hdu to fetch the FITS header. Returns: None. \"\"\" # inherit from parent super () . __init__ ( path ) # set other attributes header = self . __get_header ( hdu_index ) # set the rest of the attributes self . __set_img_attr_for_telescope ( header ) # get the frequency self . __get_frequency ( header )","title":"__init__()"},{"location":"reference/image/main/#vast_pipeline.image.main.Image","text":"Generic abstract class for an image. Attributes: Name Type Description name str The image name taken from the file name. path str The system path to the image.","title":"Image"},{"location":"reference/image/main/#vast_pipeline.image.main.Image.__init__","text":"Initiliase an image object. Parameters: Name Type Description Default path str The system path to the FITS image. The name of the image is taken from the filename in the given path. required Returns: Type Description None None. Source code in vast_pipeline/image/main.py def __init__ ( self , path : str ) -> None : \"\"\" Initiliase an image object. Args: path: The system path to the FITS image. The name of the image is taken from the filename in the given path. Returns: None. \"\"\" self . name = os . path . basename ( path ) self . path = path","title":"__init__()"},{"location":"reference/image/main/#vast_pipeline.image.main.Image.__repr__","text":"Defines the printable representation. Returns: Type Description str Printable representation which is the pipeline run name. Source code in vast_pipeline/image/main.py def __repr__ ( self ) -> str : \"\"\" Defines the printable representation. Returns: Printable representation which is the pipeline run name. \"\"\" return self . name","title":"__repr__()"},{"location":"reference/image/main/#vast_pipeline.image.main.SelavyImage","text":"Fits images that have a selavy catalogue. Attributes: Name Type Description selavy_path str The system path to the Selavy file. noise_path str The system path to the noise image associated with the image. background_path str The system path to the background image associated with the image. config config The pipeline configuration settings.","title":"SelavyImage"},{"location":"reference/image/main/#vast_pipeline.image.main.SelavyImage.__init__","text":"Initialise the SelavyImage. Parameters: Name Type Description Default path str The system path to the FITS image. required paths Dict[str, str] Dictionary containing the system paths to the associated image products and selavy catalogue. The keys are 'selavy', 'noise', 'background'. required hdu_index int The index number to use to access the header from the hdu object. 0 config config Configuration settings for the pipeline. None Returns: Type Description None None. Source code in vast_pipeline/image/main.py def __init__ ( self , path : str , paths : Dict [ str , str ], hdu_index : int = 0 , config = None ) -> None : \"\"\" Initialise the SelavyImage. Args: path: The system path to the FITS image. paths: Dictionary containing the system paths to the associated image products and selavy catalogue. The keys are 'selavy', 'noise', 'background'. hdu_index: The index number to use to access the header from the hdu object. config (config): Configuration settings for the pipeline. Returns: None. \"\"\" # inherit from parent self . selavy_path = paths [ 'selavy' ][ path ] self . noise_path = paths [ 'noise' ] . get ( path , '' ) self . background_path = paths [ 'background' ] . get ( path , '' ) self . config = config super () . __init__ ( path , hdu_index )","title":"__init__()"},{"location":"reference/image/main/#vast_pipeline.image.main.SelavyImage.read_selavy","text":"Read the sources from the selavy catalogue, select wanted columns and remap them to correct names, followed by filtering and Condon error calculations. Parameters: Name Type Description Default dj_image Image The image model object. required Returns: Type Description DataFrame Dataframe containing the cleaned and processed Selavy components. Source code in vast_pipeline/image/main.py def read_selavy ( self , dj_image : Image ) -> pd . DataFrame : \"\"\" Read the sources from the selavy catalogue, select wanted columns and remap them to correct names, followed by filtering and Condon error calculations. Args: dj_image: The image model object. Returns: Dataframe containing the cleaned and processed Selavy components. \"\"\" # TODO: improve with loading only the cols we need and set datatype df = pd . read_fwf ( self . selavy_path , skiprows = [ 1 ]) # drop first line with unit of measure, select only wanted # columns and rename them df = df . loc [:, tr_selavy . keys ()] . rename ( columns = { x : tr_selavy [ x ][ \"name\" ] for x in tr_selavy } ) # fix dtype of columns for ky in tr_selavy : key = tr_selavy [ ky ] if df [ key [ 'name' ]] . dtype != key [ 'dtype' ]: df [ key [ 'name' ]] = df [ key [ 'name' ]] . astype ( key [ 'dtype' ]) # do checks and fill in missing field for uploading sources # in DB (see fields in models.py -> Source model) if df [ 'component_id' ] . duplicated () . any (): raise Exception ( 'Found duplicated names in sources' ) # drop unrealistic sources cols_to_check = [ 'bmaj' , 'bmin' , 'flux_peak' , 'flux_int' , ] bad_sources = df [( df [ cols_to_check ] == 0 ) . any ( axis = 1 )] if bad_sources . shape [ 0 ] > 0 : logger . debug ( \"Dropping %i bad sources.\" , bad_sources . shape [ 0 ]) df = df . drop ( bad_sources . index ) # dropping tiny sources nr_sources_old = df . shape [ 0 ] df = df . loc [ ( df [ 'bmaj' ] > dj_image . beam_bmaj * 500 ) & ( df [ 'bmin' ] > dj_image . beam_bmin * 500 ) ] if df . shape [ 0 ] != nr_sources_old : logger . info ( 'Dropped %i tiny sources.' , nr_sources_old - df . shape [ 0 ] ) # add fields from image and fix name column df [ 'image_id' ] = dj_image . id df [ 'time' ] = dj_image . datetime # append img prefix to source name img_prefix = dj_image . name . split ( '.i.' , 1 )[ - 1 ] . split ( '.' , 1 )[ 0 ] + '_' df [ 'name' ] = img_prefix + df [ 'component_id' ] # # fix error fluxes for col in [ 'flux_int_err' , 'flux_peak_err' ]: sel = df [ col ] < settings . FLUX_DEFAULT_MIN_ERROR if sel . any (): df . loc [ sel , col ] = settings . FLUX_DEFAULT_MIN_ERROR # # fix error ra dec for col in [ 'ra_err' , 'dec_err' ]: sel = df [ col ] < settings . POS_DEFAULT_MIN_ERROR if sel . any (): df . loc [ sel , col ] = settings . POS_DEFAULT_MIN_ERROR df [ col ] = df [ col ] / 3600. # replace 0 local_rms values using user config value df . loc [ df [ 'local_rms' ] == 0. , 'local_rms' ] = self . config . SELAVY_LOCAL_RMS_ZERO_FILL_VALUE df [ 'snr' ] = df [ 'flux_peak' ] . values / df [ 'local_rms' ] . values df [ 'compactness' ] = df [ 'flux_int' ] . values / df [ 'flux_peak' ] . values if self . config . USE_CONDON_ERRORS : logger . debug ( \"Calculating Condon '97 errors...\" ) theta_B = dj_image . beam_bmaj theta_b = dj_image . beam_bmin df [[ 'flux_peak_err' , 'flux_int_err' , 'err_bmaj' , 'err_bmin' , 'err_pa' , 'ra_err' , 'dec_err' , ]] = df [[ 'flux_peak' , 'flux_int' , 'bmaj' , 'bmin' , 'pa' , 'snr' , 'local_rms' , ]] . apply ( calc_condon_flux_errors , args = ( theta_B , theta_b ), axis = 1 , result_type = 'expand' ) logger . debug ( \"Condon errors done.\" ) logger . debug ( \"Calculating positional errors...\" ) # TODO: avoid extra column given that it is a single value df [ 'ew_sys_err' ] = self . config . ASTROMETRIC_UNCERTAINTY_RA / 3600. df [ 'ns_sys_err' ] = self . config . ASTROMETRIC_UNCERTAINTY_DEC / 3600. df [ 'error_radius' ] = calc_error_radius ( df [ 'ra' ] . values , df [ 'ra_err' ] . values , df [ 'dec' ] . values , df [ 'dec_err' ] . values , ) df [ 'uncertainty_ew' ] = np . hypot ( df [ 'ew_sys_err' ] . values , df [ 'error_radius' ] . values ) df [ 'uncertainty_ns' ] = np . hypot ( df [ 'ns_sys_err' ] . values , df [ 'error_radius' ] . values ) # weight calculations to use later df [ 'weight_ew' ] = 1. / df [ 'uncertainty_ew' ] . values ** 2 df [ 'weight_ns' ] = 1. / df [ 'uncertainty_ns' ] . values ** 2 logger . debug ( 'Positional errors done.' ) # Initialise the forced column as False df [ 'forced' ] = False # Calculate island flux fractions island_flux_totals = ( df [[ 'island_id' , 'flux_int' , 'flux_peak' ]] . groupby ( 'island_id' ) . agg ( 'sum' ) ) df [ 'flux_int_isl_ratio' ] = ( df [ 'flux_int' ] . values / island_flux_totals . loc [ df [ 'island_id' ]][ 'flux_int' ] . values ) df [ 'flux_peak_isl_ratio' ] = ( df [ 'flux_peak' ] . values / island_flux_totals . loc [ df [ 'island_id' ]][ 'flux_peak' ] . values ) return df","title":"read_selavy()"},{"location":"reference/image/utils/","text":"This module contains utility functions used by the image ingestion section of the pipeline. calc_condon_flux_errors ( row , theta_B , theta_b , alpha_maj1 = 2.5 , alpha_min1 = 0.5 , alpha_maj2 = 0.5 , alpha_min2 = 2.5 , alpha_maj3 = 1.5 , alpha_min3 = 1.5 , clean_bias = 0.0 , clean_bias_error = 0.0 , frac_flux_cal_error = 0.0 ) \u00b6 The following code for this function taken from the TraP with a few modifications. Returns the errors on parameters from Gaussian fits according to the Condon (PASP 109, 166 (1997)) formulae. These formulae are not perfect, but we'll use them for the time being. (See Refregier and Brown (astro-ph/9803279v1) for a more rigorous approach.) It also returns the corrected peak. The peak can be corrected for the overestimate due to the local noise gradient, but this is currently not used in the function. Parameters: Name Type Description Default row Series The row containing the componenet information from the Selavy component catalogue. required theta_B float The major axis size of the restoring beam of the image (degrees). required theta_b float The minor axis size of the restoring beam of the image (degrees). required alpha_maj1 float The alpha_M exponent value for x_0. 2.5 alpha_min1 float The alpha_m exponent value for x_0. 0.5 alpha_maj2 float The alpha_M exponent value for y_0. 0.5 alpha_min2 float The alpha_m exponent value for y_0. 2.5 alpha_maj3 float The alpha_M exponent value for the amplitude error. 1.5 alpha_min3 float The alpha_m exponent value for the amplitude error. 1.5 clean_bias float Clean bias value used in the peak flux correction (not currently used). 0.0 clean_bias_error float The error of the clean bias value used in the peak flux correction (not currently used). 0.0 frac_flux_cal_error float Flux calibration error value. (Unsure of exact meaning, refer to TraP). 0.0 Returns: Type Description Tuple[float, float, float, float, float, float, float] Tuple containing the following calculated values: peak flux error, integrated flux error, major axis error, minor axis error, position angle error, right ascension error and the declination error. Source code in vast_pipeline/image/utils.py def calc_condon_flux_errors ( row : pd . Series , theta_B : float , theta_b : float , alpha_maj1 : float = 2.5 , alpha_min1 : float = 0.5 , alpha_maj2 : float = 0.5 , alpha_min2 : float = 2.5 , alpha_maj3 : float = 1.5 , alpha_min3 : float = 1.5 , clean_bias : float = 0.0 , clean_bias_error : float = 0.0 , frac_flux_cal_error : float = 0.0 , ) -> Tuple [ float , float , float , float , float , float , float ]: \"\"\" The following code for this function taken from the TraP with a few modifications. Returns the errors on parameters from Gaussian fits according to the Condon (PASP 109, 166 (1997)) formulae. These formulae are not perfect, but we'll use them for the time being. (See Refregier and Brown (astro-ph/9803279v1) for a more rigorous approach.) It also returns the corrected peak. The peak can be corrected for the overestimate due to the local noise gradient, but this is currently not used in the function. Args: row (pd.Series): The row containing the componenet information from the Selavy component catalogue. theta_B (float): The major axis size of the restoring beam of the image (degrees). theta_b (float): The minor axis size of the restoring beam of the image (degrees). alpha_maj1 (float): The alpha_M exponent value for x_0. alpha_min1 (float): The alpha_m exponent value for x_0. alpha_maj2 (float): The alpha_M exponent value for y_0. alpha_min2 (float): The alpha_m exponent value for y_0. alpha_maj3 (float): The alpha_M exponent value for the amplitude error. alpha_min3 (float): The alpha_m exponent value for the amplitude error. clean_bias (float): Clean bias value used in the peak flux correction (not currently used). clean_bias_error (float): The error of the clean bias value used in the peak flux correction (not currently used). frac_flux_cal_error (float): Flux calibration error value. (Unsure of exact meaning, refer to TraP). Returns: Tuple containing the following calculated values: peak flux error, integrated flux error, major axis error, minor axis error, position angle error, right ascension error and the declination error. \"\"\" major = row . bmaj / 3600. # degrees minor = row . bmin / 3600. # degrees theta = np . deg2rad ( row . pa ) flux_peak = row [ 'flux_peak' ] flux_int = row [ 'flux_int' ] snr = row [ 'snr' ] noise = row [ 'local_rms' ] variables = [ theta_B , theta_b , major , minor , flux_peak , flux_int , snr , noise ] # return 0 if the source is unrealistic. Should be rare # given that these sources are also filtered out before hand. if 0.0 in variables : logger . debug ( variables ) return 0. , 0. , 0. , 0. , 0. , 0. , 0. try : rho_sq1 = (( major * minor / ( 4. * theta_B * theta_b )) * ( 1. + ( theta_B / major ) ** 2 ) ** alpha_maj1 * ( 1. + ( theta_b / minor ) ** 2 ) ** alpha_min1 * snr ** 2 ) rho_sq2 = (( major * minor / ( 4. * theta_B * theta_b )) * ( 1. + ( theta_B / major ) ** 2 ) ** alpha_maj2 * ( 1. + ( theta_b / minor ) ** 2 ) ** alpha_min2 * snr ** 2 ) rho_sq3 = (( major * minor / ( 4. * theta_B * theta_b )) * ( 1. + ( theta_B / major ) ** 2 ) ** alpha_maj3 * ( 1. + ( theta_b / minor ) ** 2 ) ** alpha_min3 * snr ** 2 ) rho1 = np . sqrt ( rho_sq1 ) rho2 = np . sqrt ( rho_sq2 ) rho3 = np . sqrt ( rho_sq3 ) # here we change the TraP code slightly and base it # purely on Condon 97 and not the NVSS paper. denom1 = np . sqrt ( 4. * np . log ( 2. )) * rho1 denom2 = np . sqrt ( 4. * np . log ( 2. )) * rho2 # these are the 'xo' and 'y0' errors from Condon error_par_major = major / denom1 error_par_minor = minor / denom2 # ra and dec errors errorra = np . sqrt (( error_par_major * np . sin ( theta )) ** 2 + ( error_par_minor * np . cos ( theta )) ** 2 ) errordec = np . sqrt (( error_par_major * np . cos ( theta )) ** 2 + ( error_par_minor * np . sin ( theta )) ** 2 ) errormajor = np . sqrt ( 2 ) * major / rho1 errorminor = np . sqrt ( 2 ) * minor / rho2 if major > minor : errortheta = 2.0 * ( major * minor / ( major ** 2 - minor ** 2 )) / rho2 else : errortheta = np . pi if errortheta > np . pi : errortheta = np . pi # correction to flux peak not currently used # but might be in the future. # Do not remove! # flux_peak += -noise**2 / flux_peak + clean_bias errorpeaksq = (( frac_flux_cal_error * flux_peak ) ** 2 + clean_bias_error ** 2 + 2. * flux_peak ** 2 / rho_sq3 ) errorpeak = np . sqrt ( errorpeaksq ) help1 = ( errormajor / major ) ** 2 help2 = ( errorminor / minor ) ** 2 help3 = theta_B * theta_b / ( major * minor ) errorflux = np . abs ( flux_int ) * np . sqrt ( errorpeaksq / flux_peak ** 2 + help3 * ( help1 + help2 )) # need to return flux_peak if used. return errorpeak , errorflux , errormajor , errorminor , errortheta , errorra , errordec except Exception as e : logger . debug ( \"Error in the calculation of Condon errors for a source\" , exc_info = True ) return 0. , 0. , 0. , 0. , 0. , 0. , 0. calc_error_radius ( ra , ra_err , dec , dec_err ) \u00b6 Using the fitted errors from selavy, this function estimates the largest on sky angular size of the uncertainty. The four different combinations of the errors are analysed and the maximum is returned. Logic is taken from the TraP, where this is also used. Function has been vectorised for pandas. All inputs are in degrees. Parameters: Name Type Description Default ra float The right ascension of the coordinate (degrees). required ra_err float The error associated with the ra value (degrees). required dec float The declination of the coordinate (degrees). required dec_err float The error associated with the declination value (degrees). required Returns: Type Description float The calculated error radius (degrees). Source code in vast_pipeline/image/utils.py def calc_error_radius ( ra , ra_err , dec , dec_err ) -> float : \"\"\" Using the fitted errors from selavy, this function estimates the largest on sky angular size of the uncertainty. The four different combinations of the errors are analysed and the maximum is returned. Logic is taken from the TraP, where this is also used. Function has been vectorised for pandas. All inputs are in degrees. Args: ra (float): The right ascension of the coordinate (degrees). ra_err (float): The error associated with the ra value (degrees). dec (float): The declination of the coordinate (degrees). dec_err (float): The error associated with the declination value (degrees). Returns: The calculated error radius (degrees). \"\"\" ra_1 = np . deg2rad ( ra ) dec_1 = np . deg2rad ( dec ) ra_offsets = [ ( ra + ra_err ), ( ra + ra_err ), ( ra - ra_err ), ( ra - ra_err ) ] dec_offsets = [ ( dec + dec_err ), ( dec - dec_err ), ( dec + dec_err ), ( dec - dec_err ) ] seps = [ np . rad2deg ( on_sky_sep ( ra_1 , np . deg2rad ( i ), dec_1 , np . deg2rad ( j ) )) for i , j in zip ( ra_offsets , dec_offsets ) ] seps = np . column_stack ( seps ) return np . amax ( seps , 1 ) on_sky_sep ( ra_1 , ra_2 , dec_1 , dec_2 ) \u00b6 Simple on sky distance between two RA and Dec coordinates. Needed for fast calculation on dataframes as astropy is slow. All units are radians. Parameters: Name Type Description Default ra_1 float The right ascension of coodinate 1 (radians). required ra_2 float The right ascension of coodinate 2 (radians). required dec_1 float The declination of coodinate 1 (radians). required dec_2 float The declination of coodinate 2 (radians). required Returns: Type Description float The on-sky separation distance between the two coodinates (radians). Source code in vast_pipeline/image/utils.py def on_sky_sep ( ra_1 , ra_2 , dec_1 , dec_2 ) -> float : \"\"\" Simple on sky distance between two RA and Dec coordinates. Needed for fast calculation on dataframes as astropy is slow. All units are radians. Args: ra_1 (float): The right ascension of coodinate 1 (radians). ra_2 (float): The right ascension of coodinate 2 (radians). dec_1 (float): The declination of coodinate 1 (radians). dec_2 (float): The declination of coodinate 2 (radians). Returns: The on-sky separation distance between the two coodinates (radians). \"\"\" separation = ( np . sin ( dec_1 ) * np . sin ( dec_2 ) + np . cos ( dec_1 ) * np . cos ( dec_2 ) * np . cos ( ra_1 - ra_2 ) ) # fix errors on separation values over 1 separation [ separation > 1. ] = 1. return np . arccos ( separation )","title":"utils.py"},{"location":"reference/image/utils/#vast_pipeline.image.utils.calc_condon_flux_errors","text":"The following code for this function taken from the TraP with a few modifications. Returns the errors on parameters from Gaussian fits according to the Condon (PASP 109, 166 (1997)) formulae. These formulae are not perfect, but we'll use them for the time being. (See Refregier and Brown (astro-ph/9803279v1) for a more rigorous approach.) It also returns the corrected peak. The peak can be corrected for the overestimate due to the local noise gradient, but this is currently not used in the function. Parameters: Name Type Description Default row Series The row containing the componenet information from the Selavy component catalogue. required theta_B float The major axis size of the restoring beam of the image (degrees). required theta_b float The minor axis size of the restoring beam of the image (degrees). required alpha_maj1 float The alpha_M exponent value for x_0. 2.5 alpha_min1 float The alpha_m exponent value for x_0. 0.5 alpha_maj2 float The alpha_M exponent value for y_0. 0.5 alpha_min2 float The alpha_m exponent value for y_0. 2.5 alpha_maj3 float The alpha_M exponent value for the amplitude error. 1.5 alpha_min3 float The alpha_m exponent value for the amplitude error. 1.5 clean_bias float Clean bias value used in the peak flux correction (not currently used). 0.0 clean_bias_error float The error of the clean bias value used in the peak flux correction (not currently used). 0.0 frac_flux_cal_error float Flux calibration error value. (Unsure of exact meaning, refer to TraP). 0.0 Returns: Type Description Tuple[float, float, float, float, float, float, float] Tuple containing the following calculated values: peak flux error, integrated flux error, major axis error, minor axis error, position angle error, right ascension error and the declination error. Source code in vast_pipeline/image/utils.py def calc_condon_flux_errors ( row : pd . Series , theta_B : float , theta_b : float , alpha_maj1 : float = 2.5 , alpha_min1 : float = 0.5 , alpha_maj2 : float = 0.5 , alpha_min2 : float = 2.5 , alpha_maj3 : float = 1.5 , alpha_min3 : float = 1.5 , clean_bias : float = 0.0 , clean_bias_error : float = 0.0 , frac_flux_cal_error : float = 0.0 , ) -> Tuple [ float , float , float , float , float , float , float ]: \"\"\" The following code for this function taken from the TraP with a few modifications. Returns the errors on parameters from Gaussian fits according to the Condon (PASP 109, 166 (1997)) formulae. These formulae are not perfect, but we'll use them for the time being. (See Refregier and Brown (astro-ph/9803279v1) for a more rigorous approach.) It also returns the corrected peak. The peak can be corrected for the overestimate due to the local noise gradient, but this is currently not used in the function. Args: row (pd.Series): The row containing the componenet information from the Selavy component catalogue. theta_B (float): The major axis size of the restoring beam of the image (degrees). theta_b (float): The minor axis size of the restoring beam of the image (degrees). alpha_maj1 (float): The alpha_M exponent value for x_0. alpha_min1 (float): The alpha_m exponent value for x_0. alpha_maj2 (float): The alpha_M exponent value for y_0. alpha_min2 (float): The alpha_m exponent value for y_0. alpha_maj3 (float): The alpha_M exponent value for the amplitude error. alpha_min3 (float): The alpha_m exponent value for the amplitude error. clean_bias (float): Clean bias value used in the peak flux correction (not currently used). clean_bias_error (float): The error of the clean bias value used in the peak flux correction (not currently used). frac_flux_cal_error (float): Flux calibration error value. (Unsure of exact meaning, refer to TraP). Returns: Tuple containing the following calculated values: peak flux error, integrated flux error, major axis error, minor axis error, position angle error, right ascension error and the declination error. \"\"\" major = row . bmaj / 3600. # degrees minor = row . bmin / 3600. # degrees theta = np . deg2rad ( row . pa ) flux_peak = row [ 'flux_peak' ] flux_int = row [ 'flux_int' ] snr = row [ 'snr' ] noise = row [ 'local_rms' ] variables = [ theta_B , theta_b , major , minor , flux_peak , flux_int , snr , noise ] # return 0 if the source is unrealistic. Should be rare # given that these sources are also filtered out before hand. if 0.0 in variables : logger . debug ( variables ) return 0. , 0. , 0. , 0. , 0. , 0. , 0. try : rho_sq1 = (( major * minor / ( 4. * theta_B * theta_b )) * ( 1. + ( theta_B / major ) ** 2 ) ** alpha_maj1 * ( 1. + ( theta_b / minor ) ** 2 ) ** alpha_min1 * snr ** 2 ) rho_sq2 = (( major * minor / ( 4. * theta_B * theta_b )) * ( 1. + ( theta_B / major ) ** 2 ) ** alpha_maj2 * ( 1. + ( theta_b / minor ) ** 2 ) ** alpha_min2 * snr ** 2 ) rho_sq3 = (( major * minor / ( 4. * theta_B * theta_b )) * ( 1. + ( theta_B / major ) ** 2 ) ** alpha_maj3 * ( 1. + ( theta_b / minor ) ** 2 ) ** alpha_min3 * snr ** 2 ) rho1 = np . sqrt ( rho_sq1 ) rho2 = np . sqrt ( rho_sq2 ) rho3 = np . sqrt ( rho_sq3 ) # here we change the TraP code slightly and base it # purely on Condon 97 and not the NVSS paper. denom1 = np . sqrt ( 4. * np . log ( 2. )) * rho1 denom2 = np . sqrt ( 4. * np . log ( 2. )) * rho2 # these are the 'xo' and 'y0' errors from Condon error_par_major = major / denom1 error_par_minor = minor / denom2 # ra and dec errors errorra = np . sqrt (( error_par_major * np . sin ( theta )) ** 2 + ( error_par_minor * np . cos ( theta )) ** 2 ) errordec = np . sqrt (( error_par_major * np . cos ( theta )) ** 2 + ( error_par_minor * np . sin ( theta )) ** 2 ) errormajor = np . sqrt ( 2 ) * major / rho1 errorminor = np . sqrt ( 2 ) * minor / rho2 if major > minor : errortheta = 2.0 * ( major * minor / ( major ** 2 - minor ** 2 )) / rho2 else : errortheta = np . pi if errortheta > np . pi : errortheta = np . pi # correction to flux peak not currently used # but might be in the future. # Do not remove! # flux_peak += -noise**2 / flux_peak + clean_bias errorpeaksq = (( frac_flux_cal_error * flux_peak ) ** 2 + clean_bias_error ** 2 + 2. * flux_peak ** 2 / rho_sq3 ) errorpeak = np . sqrt ( errorpeaksq ) help1 = ( errormajor / major ) ** 2 help2 = ( errorminor / minor ) ** 2 help3 = theta_B * theta_b / ( major * minor ) errorflux = np . abs ( flux_int ) * np . sqrt ( errorpeaksq / flux_peak ** 2 + help3 * ( help1 + help2 )) # need to return flux_peak if used. return errorpeak , errorflux , errormajor , errorminor , errortheta , errorra , errordec except Exception as e : logger . debug ( \"Error in the calculation of Condon errors for a source\" , exc_info = True ) return 0. , 0. , 0. , 0. , 0. , 0. , 0.","title":"calc_condon_flux_errors()"},{"location":"reference/image/utils/#vast_pipeline.image.utils.calc_error_radius","text":"Using the fitted errors from selavy, this function estimates the largest on sky angular size of the uncertainty. The four different combinations of the errors are analysed and the maximum is returned. Logic is taken from the TraP, where this is also used. Function has been vectorised for pandas. All inputs are in degrees. Parameters: Name Type Description Default ra float The right ascension of the coordinate (degrees). required ra_err float The error associated with the ra value (degrees). required dec float The declination of the coordinate (degrees). required dec_err float The error associated with the declination value (degrees). required Returns: Type Description float The calculated error radius (degrees). Source code in vast_pipeline/image/utils.py def calc_error_radius ( ra , ra_err , dec , dec_err ) -> float : \"\"\" Using the fitted errors from selavy, this function estimates the largest on sky angular size of the uncertainty. The four different combinations of the errors are analysed and the maximum is returned. Logic is taken from the TraP, where this is also used. Function has been vectorised for pandas. All inputs are in degrees. Args: ra (float): The right ascension of the coordinate (degrees). ra_err (float): The error associated with the ra value (degrees). dec (float): The declination of the coordinate (degrees). dec_err (float): The error associated with the declination value (degrees). Returns: The calculated error radius (degrees). \"\"\" ra_1 = np . deg2rad ( ra ) dec_1 = np . deg2rad ( dec ) ra_offsets = [ ( ra + ra_err ), ( ra + ra_err ), ( ra - ra_err ), ( ra - ra_err ) ] dec_offsets = [ ( dec + dec_err ), ( dec - dec_err ), ( dec + dec_err ), ( dec - dec_err ) ] seps = [ np . rad2deg ( on_sky_sep ( ra_1 , np . deg2rad ( i ), dec_1 , np . deg2rad ( j ) )) for i , j in zip ( ra_offsets , dec_offsets ) ] seps = np . column_stack ( seps ) return np . amax ( seps , 1 )","title":"calc_error_radius()"},{"location":"reference/image/utils/#vast_pipeline.image.utils.on_sky_sep","text":"Simple on sky distance between two RA and Dec coordinates. Needed for fast calculation on dataframes as astropy is slow. All units are radians. Parameters: Name Type Description Default ra_1 float The right ascension of coodinate 1 (radians). required ra_2 float The right ascension of coodinate 2 (radians). required dec_1 float The declination of coodinate 1 (radians). required dec_2 float The declination of coodinate 2 (radians). required Returns: Type Description float The on-sky separation distance between the two coodinates (radians). Source code in vast_pipeline/image/utils.py def on_sky_sep ( ra_1 , ra_2 , dec_1 , dec_2 ) -> float : \"\"\" Simple on sky distance between two RA and Dec coordinates. Needed for fast calculation on dataframes as astropy is slow. All units are radians. Args: ra_1 (float): The right ascension of coodinate 1 (radians). ra_2 (float): The right ascension of coodinate 2 (radians). dec_1 (float): The declination of coodinate 1 (radians). dec_2 (float): The declination of coodinate 2 (radians). Returns: The on-sky separation distance between the two coodinates (radians). \"\"\" separation = ( np . sin ( dec_1 ) * np . sin ( dec_2 ) + np . cos ( dec_1 ) * np . cos ( dec_2 ) * np . cos ( ra_1 - ra_2 ) ) # fix errors on separation values over 1 separation [ separation > 1. ] = 1. return np . arccos ( separation )","title":"on_sky_sep()"},{"location":"reference/management/helpers/","text":"Helper functions for the commands. get_p_run_name ( name , return_folder = False ) \u00b6 Determines the name of the pipeline run. Can also return the output folder if selected. Parameters: Name Type Description Default name str The user entered name of the pipeline run. required return_folder bool When True the pipeline directory is also returned. False Returns: Type Description Tuple[str, str] The name of the pipeline run. If return_folder is True then both the name and directory are returned. Source code in vast_pipeline/management/helpers.py def get_p_run_name ( name : str , return_folder : bool = False ) -> Tuple [ str , str ]: \"\"\" Determines the name of the pipeline run. Can also return the output folder if selected. Args: name: The user entered name of the pipeline run. return_folder: When `True` the pipeline directory is also returned. Returns: The name of the pipeline run. If return_folder is `True` then both the name and directory are returned. \"\"\" if '/' in name : folder = os . path . realpath ( name ) run_name = os . path . basename ( folder ) return ( run_name , folder ) if return_folder else run_name folder = os . path . join ( os . path . realpath ( sett . PIPELINE_WORKING_DIR ), name ) return ( name , folder ) if return_folder else name","title":"helpers.py"},{"location":"reference/management/helpers/#vast_pipeline.management.helpers.get_p_run_name","text":"Determines the name of the pipeline run. Can also return the output folder if selected. Parameters: Name Type Description Default name str The user entered name of the pipeline run. required return_folder bool When True the pipeline directory is also returned. False Returns: Type Description Tuple[str, str] The name of the pipeline run. If return_folder is True then both the name and directory are returned. Source code in vast_pipeline/management/helpers.py def get_p_run_name ( name : str , return_folder : bool = False ) -> Tuple [ str , str ]: \"\"\" Determines the name of the pipeline run. Can also return the output folder if selected. Args: name: The user entered name of the pipeline run. return_folder: When `True` the pipeline directory is also returned. Returns: The name of the pipeline run. If return_folder is `True` then both the name and directory are returned. \"\"\" if '/' in name : folder = os . path . realpath ( name ) run_name = os . path . basename ( folder ) return ( run_name , folder ) if return_folder else run_name folder = os . path . join ( os . path . realpath ( sett . PIPELINE_WORKING_DIR ), name ) return ( name , folder ) if return_folder else name","title":"get_p_run_name()"},{"location":"reference/management/commands/clearpiperun/","text":"This module defines the command for clearing a run from the database. Command \u00b6 This script is used to clean the data for pipeline run(s). Use --help for usage. add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/clearpiperun.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments (required) parser . add_argument ( 'piperuns' , nargs = '+' , type = str , default = None , help = ( 'Name or path of pipeline run(s) to delete. Pass \"clearall\" to' ' delete all the runs.' ) ) # keyword arguments (optional) parser . add_argument ( '--keep-parquet' , required = False , default = False , action = 'store_true' , help = ( 'Flag to keep the pipeline run(s) parquet files. ' 'Will also apply to arrow files if present.' ) ) parser . add_argument ( '--remove-all' , required = False , default = False , action = 'store_true' , help = 'Flag to remove all the content of the pipeline run(s) folder.' ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/clearpiperun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True if options [ 'keep_parquet' ] and options [ 'remove_all' ]: raise CommandError ( '\"--keep-parquets\" flag is incompatible with \"--remove-all\" flag' ) piperuns = options [ 'piperuns' ] flag_all_runs = True if 'clearall' in piperuns else False if flag_all_runs : logger . info ( 'clearing all pipeline run in the database' ) piperuns = list ( Run . objects . values_list ( 'name' , flat = True )) for piperun in piperuns : p_run_name = get_p_run_name ( piperun ) try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) logger . info ( \"Deleting pipeline ' %s ' from database\" , p_run_name ) p_run . delete () # remove forced measurements in db if presents forced_parquets = remove_forced_meas ( p_run . path ) # Delete parquet or folder eventually if not options [ 'keep_parquet' ] and not options [ 'remove_all' ]: logger . info ( 'Deleting pipeline \" %s \" parquets' , p_run_name ) parquets = ( glob ( os . path . join ( p_run . path , '*.parquet' )) + glob ( os . path . join ( p_run . path , '*.arrow' )) ) for parquet in parquets : try : os . remove ( parquet ) except OSError as e : self . stdout . write ( self . style . WARNING ( f 'Parquet file \" { os . path . basename ( parquet ) } \" not existent' )) pass if options [ 'remove_all' ]: logger . info ( 'Deleting pipeline folder' ) try : shutil . rmtree ( p_run . path ) except Exception as e : self . stdout . write ( self . style . WARNING ( f 'Issues in removing run folder: { e } ' )) pass","title":"clearpiperun.py"},{"location":"reference/management/commands/clearpiperun/#vast_pipeline.management.commands.clearpiperun.Command","text":"This script is used to clean the data for pipeline run(s). Use --help for usage.","title":"Command"},{"location":"reference/management/commands/clearpiperun/#vast_pipeline.management.commands.clearpiperun.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/clearpiperun.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments (required) parser . add_argument ( 'piperuns' , nargs = '+' , type = str , default = None , help = ( 'Name or path of pipeline run(s) to delete. Pass \"clearall\" to' ' delete all the runs.' ) ) # keyword arguments (optional) parser . add_argument ( '--keep-parquet' , required = False , default = False , action = 'store_true' , help = ( 'Flag to keep the pipeline run(s) parquet files. ' 'Will also apply to arrow files if present.' ) ) parser . add_argument ( '--remove-all' , required = False , default = False , action = 'store_true' , help = 'Flag to remove all the content of the pipeline run(s) folder.' )","title":"add_arguments()"},{"location":"reference/management/commands/clearpiperun/#vast_pipeline.management.commands.clearpiperun.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/clearpiperun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True if options [ 'keep_parquet' ] and options [ 'remove_all' ]: raise CommandError ( '\"--keep-parquets\" flag is incompatible with \"--remove-all\" flag' ) piperuns = options [ 'piperuns' ] flag_all_runs = True if 'clearall' in piperuns else False if flag_all_runs : logger . info ( 'clearing all pipeline run in the database' ) piperuns = list ( Run . objects . values_list ( 'name' , flat = True )) for piperun in piperuns : p_run_name = get_p_run_name ( piperun ) try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) logger . info ( \"Deleting pipeline ' %s ' from database\" , p_run_name ) p_run . delete () # remove forced measurements in db if presents forced_parquets = remove_forced_meas ( p_run . path ) # Delete parquet or folder eventually if not options [ 'keep_parquet' ] and not options [ 'remove_all' ]: logger . info ( 'Deleting pipeline \" %s \" parquets' , p_run_name ) parquets = ( glob ( os . path . join ( p_run . path , '*.parquet' )) + glob ( os . path . join ( p_run . path , '*.arrow' )) ) for parquet in parquets : try : os . remove ( parquet ) except OSError as e : self . stdout . write ( self . style . WARNING ( f 'Parquet file \" { os . path . basename ( parquet ) } \" not existent' )) pass if options [ 'remove_all' ]: logger . info ( 'Deleting pipeline folder' ) try : shutil . rmtree ( p_run . path ) except Exception as e : self . stdout . write ( self . style . WARNING ( f 'Issues in removing run folder: { e } ' )) pass","title":"handle()"},{"location":"reference/management/commands/createmeasarrow/","text":"This module defines the command for creating an arrow output file for a previously completed pipeline run. Command \u00b6 This command creates measurements and measurement_pairs arrow files for a completed pipeline run. add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/createmeasarrow.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments parser . add_argument ( 'piperun' , type = str , help = 'Path or name of the pipeline run.' ) parser . add_argument ( '--overwrite' , action = 'store_true' , required = False , default = False , help = \"Overwrite previous 'measurements.arrow' file.\" , ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/createmeasarrow.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True piperun = options [ 'piperun' ] p_run_name , run_folder = get_p_run_name ( piperun , return_folder = True ) try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) if p_run . status != 'END' : raise CommandError ( f 'Pipeline run { p_run_name } has not completed.' ) measurements_arrow = os . path . join ( run_folder , 'measurements.arrow' ) measurement_pairs_arrow = os . path . join ( run_folder , 'measurement_pairs.arrow' ) if os . path . isfile ( measurements_arrow ): if options [ 'overwrite' ]: logger . info ( \"Removing previous 'measurements.arrow' file.\" ) os . remove ( measurements_arrow ) else : raise CommandError ( f 'Measurements arrow file already exists for { p_run_name } ' ' and `--overwrite` has not been selected.' ) if os . path . isfile ( measurement_pairs_arrow ): if options [ 'overwrite' ]: logger . info ( \"Removing previous 'measurement_pairs.arrow' file.\" ) os . remove ( measurement_pairs_arrow ) else : raise CommandError ( 'Measurement pairs arrow file already exists for' f ' { p_run_name } and `--overwrite` has not been selected.' ) logger . info ( \"Creating measurements arrow file for ' %s '.\" , p_run_name ) create_measurements_arrow_file ( p_run ) logger . info ( \"Creating measurement pairs arrow file for ' %s '.\" , p_run_name ) create_measurement_pairs_arrow_file ( p_run )","title":"createmeasarrow.py"},{"location":"reference/management/commands/createmeasarrow/#vast_pipeline.management.commands.createmeasarrow.Command","text":"This command creates measurements and measurement_pairs arrow files for a completed pipeline run.","title":"Command"},{"location":"reference/management/commands/createmeasarrow/#vast_pipeline.management.commands.createmeasarrow.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/createmeasarrow.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments parser . add_argument ( 'piperun' , type = str , help = 'Path or name of the pipeline run.' ) parser . add_argument ( '--overwrite' , action = 'store_true' , required = False , default = False , help = \"Overwrite previous 'measurements.arrow' file.\" , )","title":"add_arguments()"},{"location":"reference/management/commands/createmeasarrow/#vast_pipeline.management.commands.createmeasarrow.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/createmeasarrow.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True piperun = options [ 'piperun' ] p_run_name , run_folder = get_p_run_name ( piperun , return_folder = True ) try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) if p_run . status != 'END' : raise CommandError ( f 'Pipeline run { p_run_name } has not completed.' ) measurements_arrow = os . path . join ( run_folder , 'measurements.arrow' ) measurement_pairs_arrow = os . path . join ( run_folder , 'measurement_pairs.arrow' ) if os . path . isfile ( measurements_arrow ): if options [ 'overwrite' ]: logger . info ( \"Removing previous 'measurements.arrow' file.\" ) os . remove ( measurements_arrow ) else : raise CommandError ( f 'Measurements arrow file already exists for { p_run_name } ' ' and `--overwrite` has not been selected.' ) if os . path . isfile ( measurement_pairs_arrow ): if options [ 'overwrite' ]: logger . info ( \"Removing previous 'measurement_pairs.arrow' file.\" ) os . remove ( measurement_pairs_arrow ) else : raise CommandError ( 'Measurement pairs arrow file already exists for' f ' { p_run_name } and `--overwrite` has not been selected.' ) logger . info ( \"Creating measurements arrow file for ' %s '.\" , p_run_name ) create_measurements_arrow_file ( p_run ) logger . info ( \"Creating measurement pairs arrow file for ' %s '.\" , p_run_name ) create_measurement_pairs_arrow_file ( p_run )","title":"handle()"},{"location":"reference/management/commands/debugrun/","text":"This module defines the command for debugging a pipeline run, which prints out statistics and logging. Command \u00b6 This script is used to debug data on specific pipeline run(s) or all. Use --help for usage. add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/debugrun.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments (required) parser . add_argument ( 'piperuns' , nargs = '+' , type = str , help = ( 'Name or path of pipeline run(s) to debug.Pass \"all\" to' ' print summary data of all the runs.' ) ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/debugrun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" piperuns = options [ 'piperuns' ] flag_all_runs = True if 'all' in piperuns else False if flag_all_runs : piperuns = list ( Run . objects . values_list ( 'name' , flat = True )) print ( ' ' . join ( 40 * [ '*' ])) for piperun in piperuns : p_run_name = get_p_run_name ( piperun ) try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) print ( f 'Printing summary data of pipeline run \" { p_run . name } \"' ) images = list ( p_run . image_set . values_list ( 'name' , flat = True )) print ( f 'Nr of images: { len ( images ) } ' , ) print ( 'Nr of measurements:' , Measurement . objects . filter ( image__name__in = images ) . count () ) print ( 'Nr of forced measurements:' , ( Measurement . objects . filter ( image__name__in = images , forced = True ) . count () ) ) sources = ( Source . objects . filter ( run__name = p_run . name ) . values_list ( 'id' , flat = True ) ) print ( 'Nr of sources:' , len ( sources )) print ( 'Nr of association:' , Association . objects . filter ( source_id__in = sources ) . count () ) print ( ' ' . join ( 40 * [ '*' ]))","title":"debugrun.py"},{"location":"reference/management/commands/debugrun/#vast_pipeline.management.commands.debugrun.Command","text":"This script is used to debug data on specific pipeline run(s) or all. Use --help for usage.","title":"Command"},{"location":"reference/management/commands/debugrun/#vast_pipeline.management.commands.debugrun.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/debugrun.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments (required) parser . add_argument ( 'piperuns' , nargs = '+' , type = str , help = ( 'Name or path of pipeline run(s) to debug.Pass \"all\" to' ' print summary data of all the runs.' ) )","title":"add_arguments()"},{"location":"reference/management/commands/debugrun/#vast_pipeline.management.commands.debugrun.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/debugrun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" piperuns = options [ 'piperuns' ] flag_all_runs = True if 'all' in piperuns else False if flag_all_runs : piperuns = list ( Run . objects . values_list ( 'name' , flat = True )) print ( ' ' . join ( 40 * [ '*' ])) for piperun in piperuns : p_run_name = get_p_run_name ( piperun ) try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) print ( f 'Printing summary data of pipeline run \" { p_run . name } \"' ) images = list ( p_run . image_set . values_list ( 'name' , flat = True )) print ( f 'Nr of images: { len ( images ) } ' , ) print ( 'Nr of measurements:' , Measurement . objects . filter ( image__name__in = images ) . count () ) print ( 'Nr of forced measurements:' , ( Measurement . objects . filter ( image__name__in = images , forced = True ) . count () ) ) sources = ( Source . objects . filter ( run__name = p_run . name ) . values_list ( 'id' , flat = True ) ) print ( 'Nr of sources:' , len ( sources )) print ( 'Nr of association:' , Association . objects . filter ( source_id__in = sources ) . count () ) print ( ' ' . join ( 40 * [ '*' ]))","title":"handle()"},{"location":"reference/management/commands/importsurvey/","text":"Import a survey catalogue into the VAST pipeline database (SurveySource table). Usage: ./manage.py importsurvey survey_name file Eg : ./manage.py importsurvey NVSS /data/nvss.fits Command \u00b6 This script is used to import data for a survey Use --help for usage, and refer README. add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/importsurvey.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments parser . add_argument ( 'survey_name' , nargs = 1 , type = str , help = 'the name of the survey' ) parser . add_argument ( 'path_to_survey_file' , nargs = 1 , type = str , help = 'the path to the survey file' ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/importsurvey.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # check/create survey folder if not os . path . exists ( settings . SURVEYS_WORKING_DIR ): os . mkdir ( settings . SURVEYS_WORKING_DIR ) watch = StopWatch () survey_name = options . get ( 'survey_name' )[ 0 ] tr = translators [ 'DEFAULT' ] if survey_name . upper () in translators . keys (): tr = translators [ survey_name . upper ()] logger . info ( \"Using translator %s \" , survey_name . upper ()) else : logger . info ( \"Using translator DEFAULT\" ) # check if the survey exists and eventually delete it survey = Survey . objects . filter ( name__exact = survey_name ) if survey : logger . info ( 'found previous survey with same name, deleting them' ) watch . reset () survey . delete () logger . info ( 'total time survey deletion %f s' , watch . reset ()) # create the survey survey = Survey ( name = survey_name , frequency = tr [ 'freq' ]) survey . save () # get the sources path = os . path . realpath ( options . get ( 'path_to_survey_file' )[ 0 ]) if not os . path . exists ( path ): raise CommandError ( 'passed survey file does not exist!' ) sources = get_survey ( path , survey . name , survey . id , tr ) # dump sources to parquet watch . reset () parq_folder = os . path . join ( settings . SURVEYS_WORKING_DIR , survey_name ) if not os . path . exists ( parq_folder ): os . mkdir ( parq_folder ) f_parquet = os . path . join ( parq_folder , 'sources.parquet' ) sources . to_parquet ( f_parquet , index = False ) logger . info ( 'dumping sources to parquet, time: %f s' , watch . reset ()) # upload the sources config = settings . DATABASES [ 'default' ] eng = create_engine ( ( f \" { config [ 'ENGINE' ] . split ( '.' )[ - 1 ] } ://\" f \" { config [ 'USER' ] } : { config [ 'PASSWORD' ] } @\" f \" { config [ 'HOST' ] } : { config [ 'PORT' ] } / { config [ 'NAME' ] } \" ) ) logger . info ( 'Inserting # %i records in db' , sources . shape [ 0 ]) sources . to_sql ( SurveySource . _meta . db_table , eng , if_exists = 'append' , index = False , chunksize = 100_000 , method = 'multi' , ) logger . info ( 'Inserted # %i records' , SurveySource . objects . filter ( survey_id = survey . id ) . count ()) logger . info ( 'Upload successful! Duration: %s ' , watch . reset_init ())","title":"importsurvey.py"},{"location":"reference/management/commands/importsurvey/#vast_pipeline.management.commands.importsurvey.Command","text":"This script is used to import data for a survey Use --help for usage, and refer README.","title":"Command"},{"location":"reference/management/commands/importsurvey/#vast_pipeline.management.commands.importsurvey.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/importsurvey.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments parser . add_argument ( 'survey_name' , nargs = 1 , type = str , help = 'the name of the survey' ) parser . add_argument ( 'path_to_survey_file' , nargs = 1 , type = str , help = 'the path to the survey file' )","title":"add_arguments()"},{"location":"reference/management/commands/importsurvey/#vast_pipeline.management.commands.importsurvey.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/importsurvey.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # check/create survey folder if not os . path . exists ( settings . SURVEYS_WORKING_DIR ): os . mkdir ( settings . SURVEYS_WORKING_DIR ) watch = StopWatch () survey_name = options . get ( 'survey_name' )[ 0 ] tr = translators [ 'DEFAULT' ] if survey_name . upper () in translators . keys (): tr = translators [ survey_name . upper ()] logger . info ( \"Using translator %s \" , survey_name . upper ()) else : logger . info ( \"Using translator DEFAULT\" ) # check if the survey exists and eventually delete it survey = Survey . objects . filter ( name__exact = survey_name ) if survey : logger . info ( 'found previous survey with same name, deleting them' ) watch . reset () survey . delete () logger . info ( 'total time survey deletion %f s' , watch . reset ()) # create the survey survey = Survey ( name = survey_name , frequency = tr [ 'freq' ]) survey . save () # get the sources path = os . path . realpath ( options . get ( 'path_to_survey_file' )[ 0 ]) if not os . path . exists ( path ): raise CommandError ( 'passed survey file does not exist!' ) sources = get_survey ( path , survey . name , survey . id , tr ) # dump sources to parquet watch . reset () parq_folder = os . path . join ( settings . SURVEYS_WORKING_DIR , survey_name ) if not os . path . exists ( parq_folder ): os . mkdir ( parq_folder ) f_parquet = os . path . join ( parq_folder , 'sources.parquet' ) sources . to_parquet ( f_parquet , index = False ) logger . info ( 'dumping sources to parquet, time: %f s' , watch . reset ()) # upload the sources config = settings . DATABASES [ 'default' ] eng = create_engine ( ( f \" { config [ 'ENGINE' ] . split ( '.' )[ - 1 ] } ://\" f \" { config [ 'USER' ] } : { config [ 'PASSWORD' ] } @\" f \" { config [ 'HOST' ] } : { config [ 'PORT' ] } / { config [ 'NAME' ] } \" ) ) logger . info ( 'Inserting # %i records in db' , sources . shape [ 0 ]) sources . to_sql ( SurveySource . _meta . db_table , eng , if_exists = 'append' , index = False , chunksize = 100_000 , method = 'multi' , ) logger . info ( 'Inserted # %i records' , SurveySource . objects . filter ( survey_id = survey . id ) . count ()) logger . info ( 'Upload successful! Duration: %s ' , watch . reset_init ())","title":"handle()"},{"location":"reference/management/commands/initpiperun/","text":"Initialises a pipeline run and creates the relevant directories. Usage: ./manage.py initpiperun pipeline_run_name Command \u00b6 This script initialise the Pipeline Run folder and related config for the pipeline. add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/initpiperun.py def add_arguments ( self , parser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments parser . add_argument ( 'runname' , type = str , help = 'Name of the pipeline run.' ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/initpiperun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True try : p_run = initialise_run ( options [ 'runname' ]) except Exception as e : raise CommandError ( e ) logger . info (( 'pipeline run initialisation successful! Please modify the ' '\"config.py\"' )) initialise_run ( run_name , run_description = None , user = None , config = None ) \u00b6 Initialises a pipeline run object. Parameters: Name Type Description Default run_name str The string name of the run. required run_description str The description of the run supplied by the user. None user User The Django user that is performing the run (None if from CLI). None config The pipeline configuration settings. None Returns: Type Description Run The pipeline run Django model object. Source code in vast_pipeline/management/commands/initpiperun.py def initialise_run ( run_name : str , run_description : str = None , user : User = None , config = None ) -> Run : \"\"\" Initialises a pipeline run object. Args: run_name: The string name of the run. run_description: The description of the run supplied by the user. user: The Django user that is performing the run (None if from CLI). config: The pipeline configuration settings. Returns: The pipeline run Django model object. \"\"\" # check for duplicated run name p_run = Run . objects . filter ( name__exact = run_name ) if p_run : msg = 'Pipeline run name already used. Change name' raise PipelineInitError ( msg ) # create the pipeline run folder run_path = os . path . join ( sett . PIPELINE_WORKING_DIR , run_name ) if os . path . exists ( run_path ): msg = 'pipeline run path already present!' raise PipelineInitError ( msg ) else : logger . info ( 'creating pipeline run folder' ) os . mkdir ( run_path ) # copy default config into the pipeline run folder logger . info ( 'copying default config in pipeline run folder' ) template_f = os . path . join ( sett . BASE_DIR , 'vast_pipeline' , 'config_template.py.j2' ) with open ( template_f , 'r' ) as fp : template_str = fp . read () tm = Template ( template_str ) with open ( os . path . join ( run_path , 'config.py' ), 'w' ) as fp : if config : fp . write ( tm . render ( ** config )) else : fp . write ( tm . render ( ** sett . PIPE_RUN_CONFIG_DEFAULTS )) # create entry in db p_run , _ = get_create_p_run ( run_name , run_path , run_description , user ) return p_run","title":"initpiperun.py"},{"location":"reference/management/commands/initpiperun/#vast_pipeline.management.commands.initpiperun.Command","text":"This script initialise the Pipeline Run folder and related config for the pipeline.","title":"Command"},{"location":"reference/management/commands/initpiperun/#vast_pipeline.management.commands.initpiperun.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/initpiperun.py def add_arguments ( self , parser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments parser . add_argument ( 'runname' , type = str , help = 'Name of the pipeline run.' )","title":"add_arguments()"},{"location":"reference/management/commands/initpiperun/#vast_pipeline.management.commands.initpiperun.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/initpiperun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True try : p_run = initialise_run ( options [ 'runname' ]) except Exception as e : raise CommandError ( e ) logger . info (( 'pipeline run initialisation successful! Please modify the ' '\"config.py\"' ))","title":"handle()"},{"location":"reference/management/commands/initpiperun/#vast_pipeline.management.commands.initpiperun.initialise_run","text":"Initialises a pipeline run object. Parameters: Name Type Description Default run_name str The string name of the run. required run_description str The description of the run supplied by the user. None user User The Django user that is performing the run (None if from CLI). None config The pipeline configuration settings. None Returns: Type Description Run The pipeline run Django model object. Source code in vast_pipeline/management/commands/initpiperun.py def initialise_run ( run_name : str , run_description : str = None , user : User = None , config = None ) -> Run : \"\"\" Initialises a pipeline run object. Args: run_name: The string name of the run. run_description: The description of the run supplied by the user. user: The Django user that is performing the run (None if from CLI). config: The pipeline configuration settings. Returns: The pipeline run Django model object. \"\"\" # check for duplicated run name p_run = Run . objects . filter ( name__exact = run_name ) if p_run : msg = 'Pipeline run name already used. Change name' raise PipelineInitError ( msg ) # create the pipeline run folder run_path = os . path . join ( sett . PIPELINE_WORKING_DIR , run_name ) if os . path . exists ( run_path ): msg = 'pipeline run path already present!' raise PipelineInitError ( msg ) else : logger . info ( 'creating pipeline run folder' ) os . mkdir ( run_path ) # copy default config into the pipeline run folder logger . info ( 'copying default config in pipeline run folder' ) template_f = os . path . join ( sett . BASE_DIR , 'vast_pipeline' , 'config_template.py.j2' ) with open ( template_f , 'r' ) as fp : template_str = fp . read () tm = Template ( template_str ) with open ( os . path . join ( run_path , 'config.py' ), 'w' ) as fp : if config : fp . write ( tm . render ( ** config )) else : fp . write ( tm . render ( ** sett . PIPE_RUN_CONFIG_DEFAULTS )) # create entry in db p_run , _ = get_create_p_run ( run_name , run_path , run_description , user ) return p_run","title":"initialise_run()"},{"location":"reference/management/commands/restorepiperun/","text":"Command \u00b6 This command is used to restore a pipeline run to the previous verion after add mode has been used. Use --help for usage. add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/restorepiperun.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments (required) parser . add_argument ( 'piperuns' , nargs = '+' , type = str , default = None , help = 'Name or path of pipeline run(s) to restore.' ) # keyword arguments (optional) parser . add_argument ( '--no-confirm' , required = False , default = False , action = 'store_true' , help = ( 'Flag to skip the confirmation stage and proceed to restore' ' the pipeline run.' ) ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/restorepiperun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True piperuns = options [ 'piperuns' ] for piperun in piperuns : p_run_name = get_p_run_name ( piperun ) try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) if p_run . status not in [ 'END' , 'ERR' ]: raise CommandError ( f \"Run { p_run_name } does not have an 'END' or 'ERR' status.\" \" Unable to run restore.\" ) path = p_run . path pipeline = Pipeline ( name = p_run_name , config_path = os . path . join ( path , 'config.py' ) ) try : # update pipeline run status to restoring prev_status = p_run . status pipeline . set_status ( 'RES' ) prev_config_file = os . path . join ( p_run . path , 'config.py.bak' ) if os . path . isfile ( prev_config_file ): shutil . copy ( prev_config_file , prev_config_file . replace ( '.py.bak' , '.bak.py' ) ) prev_config_file = prev_config_file . replace ( '.py.bak' , '.bak.py' ) prev_config = Pipeline . load_cfg ( prev_config_file ) os . remove ( prev_config_file ) else : raise CommandError ( f 'Previous config file does not exist.' ' Cannot restore pipeline run.' ) bak_files = {} for i in [ 'associations' , 'bands' , 'images' , 'measurement_pairs' , 'relations' , 'skyregions' , 'sources' , 'config' ]: if i == 'config' : f_name = os . path . join ( p_run . path , f ' { i } .py.bak' ) else : f_name = os . path . join ( p_run . path , f ' { i } .parquet.bak' ) if os . path . isfile ( f_name ): bak_files [ i ] = f_name else : raise CommandError ( f 'File { f_name } does not exist.' ' Cannot restore pipeline run.' ) logger_msg = \"Will restore the run to the following config:\" keys = settings . PIPE_RUN_CONFIG_DEFAULTS . keys () for i in keys : setting_val = getattr ( prev_config , i . upper ()) logger_msg += f \" \\n { i . upper () : .<50s }{ setting_val } \" logger . info ( logger_msg ) user_continue = True if options [ 'no_confirm' ] else yesno ( \"Would you like to restore the run\" ) if user_continue : restore_pipe ( p_run , bak_files , prev_config ) pipeline . set_status ( 'END' ) logger . info ( 'Restore complete.' ) else : pipeline . set_status ( prev_status ) logger . info ( 'No actions performed.' ) except Exception as e : logger . error ( 'Restoring failed!' ) logger . error ( e ) pipeline . set_status ( 'ERR' ) restore_pipe ( p_run , bak_files , prev_config ) \u00b6 Restores the pipeline to the backup files version. TODO: Update prev_config type hint. Parameters: Name Type Description Default p_run Run The run model object. required bak_files Dict[str, str] Dictionary containing the paths to the .bak files. required prev_config config Module object that represents the back up run configuration. required Returns: Type Description None None Source code in vast_pipeline/management/commands/restorepiperun.py def restore_pipe ( p_run : Run , bak_files : Dict [ str , str ], prev_config ) -> None : \"\"\" Restores the pipeline to the backup files version. TODO: Update prev_config type hint. Args: p_run (Run): The run model object. bak_files (Dict[str, str]): Dictionary containing the paths to the .bak files. prev_config (config): Module object that represents the back up run configuration. Returns: None \"\"\" # check images match img_f_list = getattr ( prev_config , 'IMAGE_FILES' ) if isinstance ( img_f_list , dict ): img_f_list = [ item for sublist in img_f_list . values () for item in sublist ] img_f_list = [ os . path . basename ( i ) for i in img_f_list ] prev_images = pd . read_parquet ( bak_files [ 'images' ], columns = [ 'id' , 'name' , 'measurements_path' ] ) if sorted ( prev_images [ 'name' ] . tolist ()) != sorted ( img_f_list ): raise CommandError ( 'Images in previous config file does not' ' match those found in the previous images.parquet.bak.' ' Cannot restore pipeline run.' ) # check forced measurements monitor = getattr ( prev_config , 'MONITOR' ) if monitor : forced_parquets = glob ( os . path . join ( p_run . path , 'forced_*.parquet.bak' )) if not forced_parquets : raise CommandError ( 'Monitor is \\' True \\' in the previous configuration but' ' no .bak forced parquet files have been found.' ' Cannot restore pipeline run.' ) else : # load old associations bak_meas_id = pd . read_parquet ( bak_files [ 'associations' ], columns = [ 'meas_id' ] )[ 'meas_id' ] . unique () # load backup forced measurements forced_meas = pd . concat ( [ pd . read_parquet ( i , columns = [ 'id' ]) for i in forced_parquets ] ) # load image meas meas = pd . concat ( [ pd . read_parquet ( i , columns = [ 'id' ] ) for i in prev_images [ 'measurements_path' ]] ) # Get forced ids from the associations forced_meas_id = bak_meas_id [ np . isin ( bak_meas_id , meas [ 'id' ] . to_numpy (), invert = True ) ] if not np . array_equal ( np . sort ( forced_meas_id ), np . sort ( forced_meas [ 'id' ] . to_numpy ()) ): raise CommandError ( 'The forced measurements .bak files do not match the' ' previous run.' ' Cannot restore pipeline run.' ) del meas logger . info ( \"Restoring ' %s ' from backup parquet files.\" , p_run . name ) # Delete any new sources bak_sources = pd . read_parquet ( bak_files [ 'sources' ]) sources_to_delete = ( Source . objects . filter ( run = p_run ) . exclude ( id__in = bak_sources . index . to_numpy ()) ) if sources_to_delete . exists (): with transaction . atomic (): n_del , detail_del = sources_to_delete . delete () logger . info ( ( 'Deleting new sources and associated objects to restore run' ' Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) if monitor : current_forced_parquets = glob ( os . path . join ( p_run . path , 'forced_*.parquet' )) current_forced_meas = pd . concat ( [ pd . read_parquet ( i , columns = [ 'id' ] ) for i in current_forced_parquets ] ) ids_to_delete = current_forced_meas . loc [ ~ current_forced_meas [ 'id' ] . isin ( forced_meas [ 'id' ] . to_numpy ()), 'id' ] meas_to_delete = Measurement . objects . filter ( id__in = ids_to_delete ) del ids_to_delete if meas_to_delete . exists (): with transaction . atomic (): n_del , detail_del = meas_to_delete . delete () logger . info ( ( 'Deleting forced measurement and associated' ' objects to restore run. Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) # restore source metrics logger . info ( f 'Restoring metrics for { bak_sources . shape [ 0 ] } sources.' ) bak_sources = update_sources ( bak_sources ) # remove images from run images_to_remove = ( Image . objects . filter ( run = p_run ) . exclude ( id__in = prev_images [ 'id' ] . to_numpy ()) ) logger . info ( f 'Removing { len ( images_to_remove ) } images from the run.' ) if images_to_remove . exists (): with transaction . atomic (): p_run . image_set . remove ( * images_to_remove ) # load image meas meas = pd . concat ( [ pd . read_parquet ( i , columns = [ 'id' ] ) for i in prev_images [ 'measurements_path' ]] ) association_criteria_1 = Q ( source_id__in = bak_sources [ 'id' ] . to_numpy ()) association_criteria_2 = ~ Q ( meas_id__in = meas [ 'id' ] . to_numpy ()) associations_to_delete = Association . objects . filter ( association_criteria_1 and association_criteria_2 ) if associations_to_delete . exists (): with transaction . atomic (): n_del , detail_del = associations_to_delete . delete () logger . info ( ( 'Deleting associations to restore run.' ' Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) pair_criteria_1 = Q ( source_id__in = bak_sources [ 'id' ] . to_numpy ()) pair_criteria_2 = ~ Q ( measurement_a__in = meas [ 'id' ] . to_numpy ()) pair_criteria_3 = ~ Q ( measurement_b__in = meas [ 'id' ] . to_numpy ()) pairs_to_delete = MeasurementPair . objects . filter ( pair_criteria_1 and ( pair_criteria_2 | pair_criteria_3 ) ) if pairs_to_delete . exists (): with transaction . atomic (): n_del , detail_del = pairs_to_delete . delete () logger . info ( ( 'Deleting measurement pairs to restore run.' ' Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) logger . info ( f 'Restoring run metrics.' ) p_run . n_images = prev_images . shape [ 0 ] p_run . n_sources = bak_sources . shape [ 0 ] p_run . n_selavy_measurements = meas . shape [ 0 ] if monitor : p_run . n_forced_measurements = forced_meas . shape [ 0 ] with transaction . atomic (): p_run . save () # switch files and delete backups logger . info ( f 'Restoring parquet files and removing .bak files.' ) for i in bak_files : bak_file = bak_files [ i ] if i == 'config' : actual_file = bak_file . replace ( '.py.bak' , '_prev.py' ) else : actual_file = bak_file . replace ( '.bak' , '' ) shutil . copy ( bak_file , actual_file ) os . remove ( bak_file ) if monitor : for i in current_forced_parquets : os . remove ( i ) for i in forced_parquets : new_file = i . replace ( '.bak' , '' ) shutil . copy ( i , new_file ) os . remove ( i ) yesno ( question ) \u00b6 Simple Yes/No Function. Parameters: Name Type Description Default question str The question to show to the user for a y/n response. required Returns: Type Description bool True if user enters 'y', False if 'n'. Source code in vast_pipeline/management/commands/restorepiperun.py def yesno ( question : str ) -> bool : \"\"\" Simple Yes/No Function. Args: question (str): The question to show to the user for a y/n response. Returns: True if user enters 'y', False if 'n'. \"\"\" prompt = f ' { question } ? (y/n): ' ans = input ( prompt ) . strip () . lower () if ans not in [ 'y' , 'n' ]: print ( f ' { ans } is invalid, please try again...' ) return yesno ( question ) if ans == 'y' : return True return False","title":"restorepiperun.py"},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.Command","text":"This command is used to restore a pipeline run to the previous verion after add mode has been used. Use --help for usage.","title":"Command"},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/restorepiperun.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments (required) parser . add_argument ( 'piperuns' , nargs = '+' , type = str , default = None , help = 'Name or path of pipeline run(s) to restore.' ) # keyword arguments (optional) parser . add_argument ( '--no-confirm' , required = False , default = False , action = 'store_true' , help = ( 'Flag to skip the confirmation stage and proceed to restore' ' the pipeline run.' ) )","title":"add_arguments()"},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/restorepiperun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True piperuns = options [ 'piperuns' ] for piperun in piperuns : p_run_name = get_p_run_name ( piperun ) try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) if p_run . status not in [ 'END' , 'ERR' ]: raise CommandError ( f \"Run { p_run_name } does not have an 'END' or 'ERR' status.\" \" Unable to run restore.\" ) path = p_run . path pipeline = Pipeline ( name = p_run_name , config_path = os . path . join ( path , 'config.py' ) ) try : # update pipeline run status to restoring prev_status = p_run . status pipeline . set_status ( 'RES' ) prev_config_file = os . path . join ( p_run . path , 'config.py.bak' ) if os . path . isfile ( prev_config_file ): shutil . copy ( prev_config_file , prev_config_file . replace ( '.py.bak' , '.bak.py' ) ) prev_config_file = prev_config_file . replace ( '.py.bak' , '.bak.py' ) prev_config = Pipeline . load_cfg ( prev_config_file ) os . remove ( prev_config_file ) else : raise CommandError ( f 'Previous config file does not exist.' ' Cannot restore pipeline run.' ) bak_files = {} for i in [ 'associations' , 'bands' , 'images' , 'measurement_pairs' , 'relations' , 'skyregions' , 'sources' , 'config' ]: if i == 'config' : f_name = os . path . join ( p_run . path , f ' { i } .py.bak' ) else : f_name = os . path . join ( p_run . path , f ' { i } .parquet.bak' ) if os . path . isfile ( f_name ): bak_files [ i ] = f_name else : raise CommandError ( f 'File { f_name } does not exist.' ' Cannot restore pipeline run.' ) logger_msg = \"Will restore the run to the following config:\" keys = settings . PIPE_RUN_CONFIG_DEFAULTS . keys () for i in keys : setting_val = getattr ( prev_config , i . upper ()) logger_msg += f \" \\n { i . upper () : .<50s }{ setting_val } \" logger . info ( logger_msg ) user_continue = True if options [ 'no_confirm' ] else yesno ( \"Would you like to restore the run\" ) if user_continue : restore_pipe ( p_run , bak_files , prev_config ) pipeline . set_status ( 'END' ) logger . info ( 'Restore complete.' ) else : pipeline . set_status ( prev_status ) logger . info ( 'No actions performed.' ) except Exception as e : logger . error ( 'Restoring failed!' ) logger . error ( e ) pipeline . set_status ( 'ERR' )","title":"handle()"},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.restore_pipe","text":"Restores the pipeline to the backup files version. TODO: Update prev_config type hint. Parameters: Name Type Description Default p_run Run The run model object. required bak_files Dict[str, str] Dictionary containing the paths to the .bak files. required prev_config config Module object that represents the back up run configuration. required Returns: Type Description None None Source code in vast_pipeline/management/commands/restorepiperun.py def restore_pipe ( p_run : Run , bak_files : Dict [ str , str ], prev_config ) -> None : \"\"\" Restores the pipeline to the backup files version. TODO: Update prev_config type hint. Args: p_run (Run): The run model object. bak_files (Dict[str, str]): Dictionary containing the paths to the .bak files. prev_config (config): Module object that represents the back up run configuration. Returns: None \"\"\" # check images match img_f_list = getattr ( prev_config , 'IMAGE_FILES' ) if isinstance ( img_f_list , dict ): img_f_list = [ item for sublist in img_f_list . values () for item in sublist ] img_f_list = [ os . path . basename ( i ) for i in img_f_list ] prev_images = pd . read_parquet ( bak_files [ 'images' ], columns = [ 'id' , 'name' , 'measurements_path' ] ) if sorted ( prev_images [ 'name' ] . tolist ()) != sorted ( img_f_list ): raise CommandError ( 'Images in previous config file does not' ' match those found in the previous images.parquet.bak.' ' Cannot restore pipeline run.' ) # check forced measurements monitor = getattr ( prev_config , 'MONITOR' ) if monitor : forced_parquets = glob ( os . path . join ( p_run . path , 'forced_*.parquet.bak' )) if not forced_parquets : raise CommandError ( 'Monitor is \\' True \\' in the previous configuration but' ' no .bak forced parquet files have been found.' ' Cannot restore pipeline run.' ) else : # load old associations bak_meas_id = pd . read_parquet ( bak_files [ 'associations' ], columns = [ 'meas_id' ] )[ 'meas_id' ] . unique () # load backup forced measurements forced_meas = pd . concat ( [ pd . read_parquet ( i , columns = [ 'id' ]) for i in forced_parquets ] ) # load image meas meas = pd . concat ( [ pd . read_parquet ( i , columns = [ 'id' ] ) for i in prev_images [ 'measurements_path' ]] ) # Get forced ids from the associations forced_meas_id = bak_meas_id [ np . isin ( bak_meas_id , meas [ 'id' ] . to_numpy (), invert = True ) ] if not np . array_equal ( np . sort ( forced_meas_id ), np . sort ( forced_meas [ 'id' ] . to_numpy ()) ): raise CommandError ( 'The forced measurements .bak files do not match the' ' previous run.' ' Cannot restore pipeline run.' ) del meas logger . info ( \"Restoring ' %s ' from backup parquet files.\" , p_run . name ) # Delete any new sources bak_sources = pd . read_parquet ( bak_files [ 'sources' ]) sources_to_delete = ( Source . objects . filter ( run = p_run ) . exclude ( id__in = bak_sources . index . to_numpy ()) ) if sources_to_delete . exists (): with transaction . atomic (): n_del , detail_del = sources_to_delete . delete () logger . info ( ( 'Deleting new sources and associated objects to restore run' ' Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) if monitor : current_forced_parquets = glob ( os . path . join ( p_run . path , 'forced_*.parquet' )) current_forced_meas = pd . concat ( [ pd . read_parquet ( i , columns = [ 'id' ] ) for i in current_forced_parquets ] ) ids_to_delete = current_forced_meas . loc [ ~ current_forced_meas [ 'id' ] . isin ( forced_meas [ 'id' ] . to_numpy ()), 'id' ] meas_to_delete = Measurement . objects . filter ( id__in = ids_to_delete ) del ids_to_delete if meas_to_delete . exists (): with transaction . atomic (): n_del , detail_del = meas_to_delete . delete () logger . info ( ( 'Deleting forced measurement and associated' ' objects to restore run. Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) # restore source metrics logger . info ( f 'Restoring metrics for { bak_sources . shape [ 0 ] } sources.' ) bak_sources = update_sources ( bak_sources ) # remove images from run images_to_remove = ( Image . objects . filter ( run = p_run ) . exclude ( id__in = prev_images [ 'id' ] . to_numpy ()) ) logger . info ( f 'Removing { len ( images_to_remove ) } images from the run.' ) if images_to_remove . exists (): with transaction . atomic (): p_run . image_set . remove ( * images_to_remove ) # load image meas meas = pd . concat ( [ pd . read_parquet ( i , columns = [ 'id' ] ) for i in prev_images [ 'measurements_path' ]] ) association_criteria_1 = Q ( source_id__in = bak_sources [ 'id' ] . to_numpy ()) association_criteria_2 = ~ Q ( meas_id__in = meas [ 'id' ] . to_numpy ()) associations_to_delete = Association . objects . filter ( association_criteria_1 and association_criteria_2 ) if associations_to_delete . exists (): with transaction . atomic (): n_del , detail_del = associations_to_delete . delete () logger . info ( ( 'Deleting associations to restore run.' ' Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) pair_criteria_1 = Q ( source_id__in = bak_sources [ 'id' ] . to_numpy ()) pair_criteria_2 = ~ Q ( measurement_a__in = meas [ 'id' ] . to_numpy ()) pair_criteria_3 = ~ Q ( measurement_b__in = meas [ 'id' ] . to_numpy ()) pairs_to_delete = MeasurementPair . objects . filter ( pair_criteria_1 and ( pair_criteria_2 | pair_criteria_3 ) ) if pairs_to_delete . exists (): with transaction . atomic (): n_del , detail_del = pairs_to_delete . delete () logger . info ( ( 'Deleting measurement pairs to restore run.' ' Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) logger . info ( f 'Restoring run metrics.' ) p_run . n_images = prev_images . shape [ 0 ] p_run . n_sources = bak_sources . shape [ 0 ] p_run . n_selavy_measurements = meas . shape [ 0 ] if monitor : p_run . n_forced_measurements = forced_meas . shape [ 0 ] with transaction . atomic (): p_run . save () # switch files and delete backups logger . info ( f 'Restoring parquet files and removing .bak files.' ) for i in bak_files : bak_file = bak_files [ i ] if i == 'config' : actual_file = bak_file . replace ( '.py.bak' , '_prev.py' ) else : actual_file = bak_file . replace ( '.bak' , '' ) shutil . copy ( bak_file , actual_file ) os . remove ( bak_file ) if monitor : for i in current_forced_parquets : os . remove ( i ) for i in forced_parquets : new_file = i . replace ( '.bak' , '' ) shutil . copy ( i , new_file ) os . remove ( i )","title":"restore_pipe()"},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.yesno","text":"Simple Yes/No Function. Parameters: Name Type Description Default question str The question to show to the user for a y/n response. required Returns: Type Description bool True if user enters 'y', False if 'n'. Source code in vast_pipeline/management/commands/restorepiperun.py def yesno ( question : str ) -> bool : \"\"\" Simple Yes/No Function. Args: question (str): The question to show to the user for a y/n response. Returns: True if user enters 'y', False if 'n'. \"\"\" prompt = f ' { question } ? (y/n): ' ans = input ( prompt ) . strip () . lower () if ans not in [ 'y' , 'n' ]: print ( f ' { ans } is invalid, please try again...' ) return yesno ( question ) if ans == 'y' : return True return False","title":"yesno()"},{"location":"reference/management/commands/runpipeline/","text":"The main command to launch the processing of a pipeline run. Usage: ./manage.py runpipeline pipeline_run_name Command \u00b6 This script is used to process images with the ASKAP transient pipeline. Use --help for usage, and refer README. add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/runpipeline.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments parser . add_argument ( 'piperun' , type = str , help = 'Path or name of the pipeline run.' ) parser . add_argument ( '--full-rerun' , required = False , default = False , action = 'store_true' , help = ( 'Flag to signify that a full re-run is requested.' ' Old data is completely removed and replaced.' ) ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/runpipeline.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" p_run_name , run_folder = get_p_run_name ( options [ 'piperun' ], return_folder = True ) # configure logging root_logger = logging . getLogger ( '' ) f_handler = logging . FileHandler ( os . path . join ( run_folder , 'log.txt' ), mode = 'w' ) f_handler . setFormatter ( root_logger . handlers [ 0 ] . formatter ) root_logger . addHandler ( f_handler ) if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True # p_run_name = p_run_path # remove ending / if present if p_run_name [ - 1 ] == '/' : p_run_name = p_run_name [: - 1 ] # grab only the name from the path p_run_name = p_run_name . split ( os . path . sep )[ - 1 ] debug_flag = True if options [ 'verbosity' ] > 1 else False done = run_pipe ( p_run_name , path_name = run_folder , debug = debug_flag , full_rerun = options [ 'full_rerun' ]) self . stdout . write ( self . style . SUCCESS ( 'Finished' )) run_pipe ( name , path_name = None , run_dj_obj = None , cli = True , debug = False , user = None , full_rerun = False , prev_ui_status = 'END' ) \u00b6 Main function to run the pipeline. Parameters: Name Type Description Default name str The name of the pipeline run (p_run.name). required path_name Optional[str] The path of the directory of the pipeline run (p_run.path), defaults to None. None run_dj_obj Optional[vast_pipeline.models.Run] The Run object of the pipeline run, defaults to None. None cli bool Flag to signify whether the pipeline run has been run via the UI (False), or the command line (True). Defaults to True. True debug bool Flag to signify whether to enable debug verbosity to the logging output. Defaults to False. False user Optional[django.contrib.auth.models.User] The User of the request if made through the UI. Defaults to None. None full_rerun bool If the run already exists, a complete rerun will be performed which will remove and replace all the previous results. False prev_ui_status str The previous status through the UI. Defaults to 'END'. 'END' Returns: Type Description bool Boolean equal to True on a successful completion, or in cases of failures a CommandError is returned. Source code in vast_pipeline/management/commands/runpipeline.py def run_pipe ( name : str , path_name : Optional [ str ] = None , run_dj_obj : Optional [ Run ] = None , cli : bool = True , debug : bool = False , user : Optional [ User ] = None , full_rerun : bool = False , prev_ui_status : str = 'END' ) -> bool : ''' Main function to run the pipeline. Args: name: The name of the pipeline run (p_run.name). path_name: The path of the directory of the pipeline run (p_run.path), defaults to None. run_dj_obj: The Run object of the pipeline run, defaults to None. cli: Flag to signify whether the pipeline run has been run via the UI (False), or the command line (True). Defaults to True. debug: Flag to signify whether to enable debug verbosity to the logging output. Defaults to False. user: The User of the request if made through the UI. Defaults to None. full_rerun: If the run already exists, a complete rerun will be performed which will remove and replace all the previous results. prev_ui_status: The previous status through the UI. Defaults to 'END'. Returns: Boolean equal to `True` on a successful completion, or in cases of failures a CommandError is returned. ''' path = run_dj_obj . path if run_dj_obj else path_name pipeline = Pipeline ( name = run_dj_obj . name if run_dj_obj else name , config_path = os . path . join ( path , 'config.py' ) ) # set up logging for running pipeline from UI if not cli : # set up the logger for the UI job root_logger = logging . getLogger ( '' ) if debug : root_logger . setLevel ( logging . DEBUG ) f_handler = logging . FileHandler ( os . path . join ( path , 'log.txt' ), mode = 'w' ) f_handler . setFormatter ( root_logger . handlers [ 0 ] . formatter ) root_logger . addHandler ( f_handler ) # Create the pipeline run in DB p_run , flag_exist = get_create_p_run ( pipeline . name , pipeline . config . PIPE_RUN_PATH ) # copy across config file at the start logger . debug ( \"Copying temp config file.\" ) shutil . copyfile ( os . path . join ( p_run . path , 'config.py' ), os . path . join ( p_run . path , 'config_temp.py' ) ) # load and validate run configs try : pipeline . validate_cfg ( user = user ) except Exception as e : if debug : traceback . print_exc () logger . exception ( 'Config error: \\n %s ' , e ) msg = f 'Config error: \\n { e } ' # If the run is already created (e.g. through UI) then set status to # error pipeline . set_status ( p_run , 'ERR' ) raise CommandError ( msg ) if cli else PipelineConfigError ( msg ) # clean up pipeline images and forced measurements for re-runs # Scenarios: # A. Complete Re-run: If the job is marked as successful then backup # old parquets and proceed to remove parquets along with forced # extractions from the database. # B. Additional Run on successful run: Backup parquets, remove current # parquets and proceed. # C. Additional Run on errored run: Do not backup parquets, just delete # current. # Flag on the pipeline object on whether the addition mode is on or off. pipeline . add_mode = False pipeline . previous_parquets = {} if not flag_exist : # check for and remove any present .parquet (and .arrow) files parquets = ( glob . glob ( os . path . join ( p_run . path , \"*.parquet\" )) # TODO Remove arrow when vaex support is dropped. + glob . glob ( os . path . join ( p_run . path , \"*.arrow\" )) + glob . glob ( os . path . join ( p_run . path , \"*.bak\" )) ) for parquet in parquets : os . remove ( parquet ) else : # Check if the status is already running or queued. Exit if this is the # case. if p_run . status in [ 'RUN' , 'RES' ]: logger . error ( \"The pipeline run requested to process already has a running\" \" or restoring status! Performing no actions. Exiting.\" ) return True # Check for an error status and whether any previous config file # exists - if it doesn't exist it means the run has failed during # the first run. In this case we want to clear anything that has gone # on before so to do that `complete-rerun` mode is activated. if p_run . status == 'ERR' and not os . path . isfile ( os . path . join ( p_run . path , 'config_prev.py' )): full_rerun = True # Backup the previous run config if os . path . isfile ( os . path . join ( p_run . path , 'config_prev.py' ) ): shutil . copy ( os . path . join ( p_run . path , 'config_prev.py' ), os . path . join ( p_run . path , 'config.py.bak' ) ) # Check if the run has only been initialised, if so we don't want to do # any previous run checks or cleaning. if p_run . status == 'INI' : initial_run = True # check if coming from UI elif cli is False and prev_ui_status == 'INI' : initial_run = True else : initial_run = False if initial_run is False : parquets = ( glob . glob ( os . path . join ( p_run . path , \"*.parquet\" )) # TODO Remove arrow when vaex support is dropped. + glob . glob ( os . path . join ( p_run . path , \"*.arrow\" )) ) if full_rerun : if p_run . status == 'END' : backup_parquets ( p_run . path ) logger . info ( 'Cleaning up pipeline run before re-process data' ) p_run . image_set . clear () logger . info ( 'Cleaning up forced measurements before re-process data' ) remove_forced_meas ( p_run . path ) for parquet in parquets : os . remove ( parquet ) # remove bak files bak_files = glob . glob ( os . path . join ( p_run . path , \"*.bak\" )) if bak_files : for bf in bak_files : os . remove ( bf ) # remove previous config if it exists if os . path . isfile ( os . path . join ( p_run . path , 'config_prev.py' )): os . remove ( os . path . join ( p_run . path , 'config_prev.py' )) # reset epoch_based flag with transaction . atomic (): p_run . epoch_based = False p_run . save () else : # Before parquets are started to be copied and backed up, a # check is run to see if anything has actually changed in # the config config_diff = pipeline . check_prev_config_diff ( p_run . path ) if config_diff : logger . info ( \"The config file has either not changed since the\" \" previous run or other settings have changed such\" \" that a new or complete re-run should be performed\" \" instead. Performing no actions. Exiting.\" ) os . remove ( os . path . join ( p_run . path , 'config_temp.py' )) pipeline . set_status ( p_run , 'END' ) return True if pipeline . epoch_based != p_run . epoch_based : logger . info ( \"The 'epoch based' setting has changed since the\" \" previous run. A complete re-run is required if\" \" changing to epoch based mode or vice versa.\" ) os . remove ( os . path . join ( p_run . path , 'config_temp.py' )) pipeline . set_status ( p_run , 'END' ) return True if cli and p_run . status == 'END' : backup_parquets ( p_run . path ) elif not cli and prev_ui_status == 'END' : backup_parquets ( p_run . path ) pipeline . add_mode = True for i in [ 'images' , 'associations' , 'sources' , 'relations' , 'measurement_pairs' ]: pipeline . previous_parquets [ i ] = os . path . join ( p_run . path , f ' { i } .parquet.bak' ) if pipeline . config . CREATE_MEASUREMENTS_ARROW_FILES and cli is False : logger . warning ( 'The creation of arrow files is currently unavailable when running' ' through the UI. Please ask an admin to complete this step for' ' you upon a successful completion.' ) logger . warning ( \"Setting 'CREATE_MEASUREMENTS_ARROW_FILES' to 'False'.\" ) pipeline . config . CREATE_MEASUREMENTS_ARROW_FILES = False if pipeline . config . SUPPRESS_ASTROPY_WARNINGS : warnings . simplefilter ( \"ignore\" , category = AstropyWarning ) logger . info ( \"Source finder: %s \" , pipeline . config . SOURCE_FINDER ) logger . info ( \"Using pipeline run ' %s '\" , pipeline . name ) logger . info ( \"Source monitoring: %s \" , pipeline . config . MONITOR ) stopwatch = StopWatch () # run the pipeline operations try : # check if max runs number is reached pipeline . check_current_runs () # run the pipeline pipeline . set_status ( p_run , 'RUN' ) pipeline . process_pipeline ( p_run ) # Create arrow file after success if selected. if pipeline . config . CREATE_MEASUREMENTS_ARROW_FILES : create_measurements_arrow_file ( p_run ) create_measurement_pairs_arrow_file ( p_run ) except Exception as e : # set the pipeline status as error pipeline . set_status ( p_run , 'ERR' ) logger . exception ( 'Processing error: \\n %s ' , e ) raise CommandError ( f 'Processing error: \\n { e } ' ) # copy across config file now that it is successful logger . debug ( \"Copying and cleaning temp config file.\" ) shutil . copyfile ( os . path . join ( p_run . path , 'config_temp.py' ), os . path . join ( p_run . path , 'config_prev.py' )) os . remove ( os . path . join ( p_run . path , 'config_temp.py' )) # set the pipeline status as completed pipeline . set_status ( p_run , 'END' ) logger . info ( 'Total pipeline processing time %.2f sec' , stopwatch . reset () ) return True","title":"runpipeline.py"},{"location":"reference/management/commands/runpipeline/#vast_pipeline.management.commands.runpipeline.Command","text":"This script is used to process images with the ASKAP transient pipeline. Use --help for usage, and refer README.","title":"Command"},{"location":"reference/management/commands/runpipeline/#vast_pipeline.management.commands.runpipeline.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/runpipeline.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments parser . add_argument ( 'piperun' , type = str , help = 'Path or name of the pipeline run.' ) parser . add_argument ( '--full-rerun' , required = False , default = False , action = 'store_true' , help = ( 'Flag to signify that a full re-run is requested.' ' Old data is completely removed and replaced.' ) )","title":"add_arguments()"},{"location":"reference/management/commands/runpipeline/#vast_pipeline.management.commands.runpipeline.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/runpipeline.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" p_run_name , run_folder = get_p_run_name ( options [ 'piperun' ], return_folder = True ) # configure logging root_logger = logging . getLogger ( '' ) f_handler = logging . FileHandler ( os . path . join ( run_folder , 'log.txt' ), mode = 'w' ) f_handler . setFormatter ( root_logger . handlers [ 0 ] . formatter ) root_logger . addHandler ( f_handler ) if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True # p_run_name = p_run_path # remove ending / if present if p_run_name [ - 1 ] == '/' : p_run_name = p_run_name [: - 1 ] # grab only the name from the path p_run_name = p_run_name . split ( os . path . sep )[ - 1 ] debug_flag = True if options [ 'verbosity' ] > 1 else False done = run_pipe ( p_run_name , path_name = run_folder , debug = debug_flag , full_rerun = options [ 'full_rerun' ]) self . stdout . write ( self . style . SUCCESS ( 'Finished' ))","title":"handle()"},{"location":"reference/management/commands/runpipeline/#vast_pipeline.management.commands.runpipeline.run_pipe","text":"Main function to run the pipeline. Parameters: Name Type Description Default name str The name of the pipeline run (p_run.name). required path_name Optional[str] The path of the directory of the pipeline run (p_run.path), defaults to None. None run_dj_obj Optional[vast_pipeline.models.Run] The Run object of the pipeline run, defaults to None. None cli bool Flag to signify whether the pipeline run has been run via the UI (False), or the command line (True). Defaults to True. True debug bool Flag to signify whether to enable debug verbosity to the logging output. Defaults to False. False user Optional[django.contrib.auth.models.User] The User of the request if made through the UI. Defaults to None. None full_rerun bool If the run already exists, a complete rerun will be performed which will remove and replace all the previous results. False prev_ui_status str The previous status through the UI. Defaults to 'END'. 'END' Returns: Type Description bool Boolean equal to True on a successful completion, or in cases of failures a CommandError is returned. Source code in vast_pipeline/management/commands/runpipeline.py def run_pipe ( name : str , path_name : Optional [ str ] = None , run_dj_obj : Optional [ Run ] = None , cli : bool = True , debug : bool = False , user : Optional [ User ] = None , full_rerun : bool = False , prev_ui_status : str = 'END' ) -> bool : ''' Main function to run the pipeline. Args: name: The name of the pipeline run (p_run.name). path_name: The path of the directory of the pipeline run (p_run.path), defaults to None. run_dj_obj: The Run object of the pipeline run, defaults to None. cli: Flag to signify whether the pipeline run has been run via the UI (False), or the command line (True). Defaults to True. debug: Flag to signify whether to enable debug verbosity to the logging output. Defaults to False. user: The User of the request if made through the UI. Defaults to None. full_rerun: If the run already exists, a complete rerun will be performed which will remove and replace all the previous results. prev_ui_status: The previous status through the UI. Defaults to 'END'. Returns: Boolean equal to `True` on a successful completion, or in cases of failures a CommandError is returned. ''' path = run_dj_obj . path if run_dj_obj else path_name pipeline = Pipeline ( name = run_dj_obj . name if run_dj_obj else name , config_path = os . path . join ( path , 'config.py' ) ) # set up logging for running pipeline from UI if not cli : # set up the logger for the UI job root_logger = logging . getLogger ( '' ) if debug : root_logger . setLevel ( logging . DEBUG ) f_handler = logging . FileHandler ( os . path . join ( path , 'log.txt' ), mode = 'w' ) f_handler . setFormatter ( root_logger . handlers [ 0 ] . formatter ) root_logger . addHandler ( f_handler ) # Create the pipeline run in DB p_run , flag_exist = get_create_p_run ( pipeline . name , pipeline . config . PIPE_RUN_PATH ) # copy across config file at the start logger . debug ( \"Copying temp config file.\" ) shutil . copyfile ( os . path . join ( p_run . path , 'config.py' ), os . path . join ( p_run . path , 'config_temp.py' ) ) # load and validate run configs try : pipeline . validate_cfg ( user = user ) except Exception as e : if debug : traceback . print_exc () logger . exception ( 'Config error: \\n %s ' , e ) msg = f 'Config error: \\n { e } ' # If the run is already created (e.g. through UI) then set status to # error pipeline . set_status ( p_run , 'ERR' ) raise CommandError ( msg ) if cli else PipelineConfigError ( msg ) # clean up pipeline images and forced measurements for re-runs # Scenarios: # A. Complete Re-run: If the job is marked as successful then backup # old parquets and proceed to remove parquets along with forced # extractions from the database. # B. Additional Run on successful run: Backup parquets, remove current # parquets and proceed. # C. Additional Run on errored run: Do not backup parquets, just delete # current. # Flag on the pipeline object on whether the addition mode is on or off. pipeline . add_mode = False pipeline . previous_parquets = {} if not flag_exist : # check for and remove any present .parquet (and .arrow) files parquets = ( glob . glob ( os . path . join ( p_run . path , \"*.parquet\" )) # TODO Remove arrow when vaex support is dropped. + glob . glob ( os . path . join ( p_run . path , \"*.arrow\" )) + glob . glob ( os . path . join ( p_run . path , \"*.bak\" )) ) for parquet in parquets : os . remove ( parquet ) else : # Check if the status is already running or queued. Exit if this is the # case. if p_run . status in [ 'RUN' , 'RES' ]: logger . error ( \"The pipeline run requested to process already has a running\" \" or restoring status! Performing no actions. Exiting.\" ) return True # Check for an error status and whether any previous config file # exists - if it doesn't exist it means the run has failed during # the first run. In this case we want to clear anything that has gone # on before so to do that `complete-rerun` mode is activated. if p_run . status == 'ERR' and not os . path . isfile ( os . path . join ( p_run . path , 'config_prev.py' )): full_rerun = True # Backup the previous run config if os . path . isfile ( os . path . join ( p_run . path , 'config_prev.py' ) ): shutil . copy ( os . path . join ( p_run . path , 'config_prev.py' ), os . path . join ( p_run . path , 'config.py.bak' ) ) # Check if the run has only been initialised, if so we don't want to do # any previous run checks or cleaning. if p_run . status == 'INI' : initial_run = True # check if coming from UI elif cli is False and prev_ui_status == 'INI' : initial_run = True else : initial_run = False if initial_run is False : parquets = ( glob . glob ( os . path . join ( p_run . path , \"*.parquet\" )) # TODO Remove arrow when vaex support is dropped. + glob . glob ( os . path . join ( p_run . path , \"*.arrow\" )) ) if full_rerun : if p_run . status == 'END' : backup_parquets ( p_run . path ) logger . info ( 'Cleaning up pipeline run before re-process data' ) p_run . image_set . clear () logger . info ( 'Cleaning up forced measurements before re-process data' ) remove_forced_meas ( p_run . path ) for parquet in parquets : os . remove ( parquet ) # remove bak files bak_files = glob . glob ( os . path . join ( p_run . path , \"*.bak\" )) if bak_files : for bf in bak_files : os . remove ( bf ) # remove previous config if it exists if os . path . isfile ( os . path . join ( p_run . path , 'config_prev.py' )): os . remove ( os . path . join ( p_run . path , 'config_prev.py' )) # reset epoch_based flag with transaction . atomic (): p_run . epoch_based = False p_run . save () else : # Before parquets are started to be copied and backed up, a # check is run to see if anything has actually changed in # the config config_diff = pipeline . check_prev_config_diff ( p_run . path ) if config_diff : logger . info ( \"The config file has either not changed since the\" \" previous run or other settings have changed such\" \" that a new or complete re-run should be performed\" \" instead. Performing no actions. Exiting.\" ) os . remove ( os . path . join ( p_run . path , 'config_temp.py' )) pipeline . set_status ( p_run , 'END' ) return True if pipeline . epoch_based != p_run . epoch_based : logger . info ( \"The 'epoch based' setting has changed since the\" \" previous run. A complete re-run is required if\" \" changing to epoch based mode or vice versa.\" ) os . remove ( os . path . join ( p_run . path , 'config_temp.py' )) pipeline . set_status ( p_run , 'END' ) return True if cli and p_run . status == 'END' : backup_parquets ( p_run . path ) elif not cli and prev_ui_status == 'END' : backup_parquets ( p_run . path ) pipeline . add_mode = True for i in [ 'images' , 'associations' , 'sources' , 'relations' , 'measurement_pairs' ]: pipeline . previous_parquets [ i ] = os . path . join ( p_run . path , f ' { i } .parquet.bak' ) if pipeline . config . CREATE_MEASUREMENTS_ARROW_FILES and cli is False : logger . warning ( 'The creation of arrow files is currently unavailable when running' ' through the UI. Please ask an admin to complete this step for' ' you upon a successful completion.' ) logger . warning ( \"Setting 'CREATE_MEASUREMENTS_ARROW_FILES' to 'False'.\" ) pipeline . config . CREATE_MEASUREMENTS_ARROW_FILES = False if pipeline . config . SUPPRESS_ASTROPY_WARNINGS : warnings . simplefilter ( \"ignore\" , category = AstropyWarning ) logger . info ( \"Source finder: %s \" , pipeline . config . SOURCE_FINDER ) logger . info ( \"Using pipeline run ' %s '\" , pipeline . name ) logger . info ( \"Source monitoring: %s \" , pipeline . config . MONITOR ) stopwatch = StopWatch () # run the pipeline operations try : # check if max runs number is reached pipeline . check_current_runs () # run the pipeline pipeline . set_status ( p_run , 'RUN' ) pipeline . process_pipeline ( p_run ) # Create arrow file after success if selected. if pipeline . config . CREATE_MEASUREMENTS_ARROW_FILES : create_measurements_arrow_file ( p_run ) create_measurement_pairs_arrow_file ( p_run ) except Exception as e : # set the pipeline status as error pipeline . set_status ( p_run , 'ERR' ) logger . exception ( 'Processing error: \\n %s ' , e ) raise CommandError ( f 'Processing error: \\n { e } ' ) # copy across config file now that it is successful logger . debug ( \"Copying and cleaning temp config file.\" ) shutil . copyfile ( os . path . join ( p_run . path , 'config_temp.py' ), os . path . join ( p_run . path , 'config_prev.py' )) os . remove ( os . path . join ( p_run . path , 'config_temp.py' )) # set the pipeline status as completed pipeline . set_status ( p_run , 'END' ) logger . info ( 'Total pipeline processing time %.2f sec' , stopwatch . reset () ) return True","title":"run_pipe()"},{"location":"reference/pipeline/association/","text":"This module contains all the functions required to perform source association. advanced_association ( method , sources_df , skyc1_srcs , skyc1 , skyc2_srcs , skyc2 , dr_limit , bw_max , id_incr_par_assoc = 0 ) \u00b6 The loop for advanced source association that uses the astropy 'search_around_sky' function (i.e. all matching sources are found). The BMAJ of the image * the user supplied beamwidth limit is the base distance for association. This is followed by calculating the 'de Ruiter' radius. Parameters: Name Type Description Default method str The advanced association method 'advanced' or 'deruiter'. required sources_df DataFrame The dataframe containing all current measurements along with their association source and relations. required skyc1_srcs DataFrame The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. required skyc1 SkyCoord A SkyCoord object with the weighted average sky positions from skyc1_srcs. required skyc2_srcs DataFrame The same structure as sources_df containing the measurements to be associated. required skyc2 SkyCoord A SkyCoord object with the sky positions from skyc2_srcs. required dr_limit float The de Ruiter radius limit to use (applies to de ruiter only). required bw_max float The beamwidth limit to use (applies to de ruiter only). required id_incr_par_assoc int An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. Source code in vast_pipeline/pipeline/association.py def advanced_association ( method : str , sources_df : pd . DataFrame , skyc1_srcs : pd . DataFrame , skyc1 : SkyCoord , skyc2_srcs : pd . DataFrame , skyc2 : SkyCoord , dr_limit : float , bw_max : float , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: ''' The loop for advanced source association that uses the astropy 'search_around_sky' function (i.e. all matching sources are found). The BMAJ of the image * the user supplied beamwidth limit is the base distance for association. This is followed by calculating the 'de Ruiter' radius. Args: method: The advanced association method 'advanced' or 'deruiter'. sources_df: The dataframe containing all current measurements along with their association source and relations. skyc1_srcs: The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. skyc1: A SkyCoord object with the weighted average sky positions from skyc1_srcs. skyc2_srcs: The same structure as sources_df containing the measurements to be associated. skyc2: A SkyCoord object with the sky positions from skyc2_srcs. dr_limit: The de Ruiter radius limit to use (applies to de ruiter only). bw_max: The beamwidth limit to use (applies to de ruiter only). id_incr_par_assoc: An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. Returns: The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. ''' # read the needed sources fields # Step 1: get matches within semimajor axis of image. idx_skyc1 , idx_skyc2 , d2d , d3d = skyc2 . search_around_sky ( skyc1 , bw_max ) # Step 2: merge the candidates so the de ruiter can be calculated temp_skyc1_srcs = ( skyc1_srcs . loc [ idx_skyc1 ] . reset_index () . rename ( columns = { 'index' : 'index_old' }) ) temp_skyc2_srcs = ( skyc2_srcs . loc [ idx_skyc2 ] . reset_index () . rename ( columns = { 'index' : 'index_old' }) ) temp_skyc2_srcs [ 'd2d' ] = d2d . arcsec temp_srcs = temp_skyc1_srcs . merge ( temp_skyc2_srcs , left_index = True , right_index = True , suffixes = ( '_skyc1' , '_skyc2' ) ) del temp_skyc1_srcs , temp_skyc2_srcs # Step 3: Apply the beamwidth limit temp_srcs = temp_srcs [ d2d <= bw_max ] . copy () # Step 4: Calculate and perform De Ruiter radius cut if method == 'deruiter' : temp_srcs [ 'dr' ] = calc_de_ruiter ( temp_srcs ) temp_srcs = temp_srcs [ temp_srcs [ 'dr' ] <= dr_limit ] else : temp_srcs [ 'dr' ] = 0. # Now have the 'good' matches # Step 5: Check for one-to-many, many-to-one and many-to-many # associations. First the many-to-many temp_srcs = many_to_many_advanced ( temp_srcs , method ) # Next one-to-many # Get the sources which are doubled temp_srcs , sources_df = one_to_many_advanced ( temp_srcs , sources_df , method , id_incr_par_assoc ) # Finally many-to-one associations, the opposite of above but we # don't have to create new ids for these so it's much simpler in fact # we don't need to do anything but lets get the number for debugging. temp_srcs = many_to_one_advanced ( temp_srcs ) # Now everything in place to append # First the skyc2 sources with a match. # This is created from the temp_srcs df. # This will take care of the extra skyc2 sources needed. skyc2_srcs_toappend = skyc2_srcs . loc [ temp_srcs [ 'index_old_skyc2' ] . values ] . reset_index ( drop = True ) skyc2_srcs_toappend [ 'source' ] = temp_srcs [ 'source_skyc1' ] . values skyc2_srcs_toappend [ 'related' ] = temp_srcs [ 'related_skyc1' ] . values skyc2_srcs_toappend [ 'dr' ] = temp_srcs [ 'dr' ] . values # and get the skyc2 sources with no match logger . info ( 'Updating sources catalogue with new sources...' ) new_sources = skyc2_srcs . loc [ skyc2_srcs . index . difference ( temp_srcs [ 'index_old_skyc2' ] . values ) ] . reset_index ( drop = True ) # update the src numbers for those sources in skyc2 with no match # using the max current src as the start and incrementing by one start_elem = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc new_sources [ 'source' ] = np . arange ( start_elem , start_elem + new_sources . shape [ 0 ], dtype = int ) skyc2_srcs_toappend = skyc2_srcs_toappend . append ( new_sources , ignore_index = True ) # and skyc2 is now ready to be appended to source_df sources_df = sources_df . append ( skyc2_srcs_toappend , ignore_index = True ) . reset_index ( drop = True ) # update skyc1 and df for next association iteration # calculate average angles for skyc1 skyc1_srcs = ( skyc1_srcs . append ( new_sources , ignore_index = True ) . reset_index ( drop = True ) ) # also need to append any related sources that created a new # source, we can use the skyc2_srcs_toappend to get these skyc1_srcs = skyc1_srcs . append ( skyc2_srcs_toappend . loc [ ~ skyc2_srcs_toappend . source . isin ( skyc1_srcs . source ) ] ) return sources_df , skyc1_srcs association ( images_df , limit , dr_limit , bw_limit , duplicate_limit , config , add_mode , previous_parquets , done_images_df , id_incr_par_assoc = 0 , parallel = False ) \u00b6 The main association function that does the common tasks between basic and advanced modes. Parameters: Name Type Description Default images_df DataFrame The input images to be associated. required limit Angle The association limit to use (applies to basic and advanced only). required dr_limit float The de Ruiter radius limit to use (applies to de ruiter only). required bw_limit float The beamwidth limit to use (applies to de ruiter only). required duplicate_limit Angle The limit of separation for which a measurement is considered to be a duplicate (epoch based association). required config The pipeline configuration object. required add_mode bool Whether the pipeline is currently being run in add image mode. required previous_parquets Dict[str, str] Dictionary containing the paths of the previous successful run parquet files (used in add image mode). required done_images_df DataFrame Datafraame containing the images of the previous successful run (used in add image mode). required id_incr_par_assoc int An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. 0 parallel bool Whether parallel association is being used. False Returns: Type Description DataFrame The output sources_df containing all input measurements along with the association and relation information. Exceptions: Type Description Exception Raised if association method is not valid. Source code in vast_pipeline/pipeline/association.py def association ( images_df : pd . DataFrame , limit : Angle , dr_limit : float , bw_limit : float , duplicate_limit : Angle , config , add_mode : bool , previous_parquets : Dict [ str , str ], done_images_df : pd . DataFrame , id_incr_par_assoc : int = 0 , parallel : bool = False ) -> pd . DataFrame : ''' The main association function that does the common tasks between basic and advanced modes. Args: images_df: The input images to be associated. limit: The association limit to use (applies to basic and advanced only). dr_limit: The de Ruiter radius limit to use (applies to de ruiter only). bw_limit: The beamwidth limit to use (applies to de ruiter only). duplicate_limit: The limit of separation for which a measurement is considered to be a duplicate (epoch based association). config: The pipeline configuration object. add_mode: Whether the pipeline is currently being run in add image mode. previous_parquets: Dictionary containing the paths of the previous successful run parquet files (used in add image mode). done_images_df: Datafraame containing the images of the previous successful run (used in add image mode). id_incr_par_assoc: An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. parallel: Whether parallel association is being used. Returns: The output sources_df containing all input measurements along with the association and relation information. Raises: Exception: Raised if association method is not valid. ''' timer = StopWatch () if parallel : images_df = ( images_df . sort_values ( by = 'image_datetime' ) . drop ( 'image_datetime' , axis = 1 ) ) if 'skyreg_group' in images_df . columns : skyreg_group = images_df [ 'skyreg_group' ] . iloc [ 0 ] skyreg_tag = \" (sky region group %s )\" % skyreg_group else : skyreg_group = - 1 skyreg_tag = \"\" method = config . ASSOCIATION_METHOD logger . info ( 'Starting association %s .' , skyreg_tag ) logger . info ( 'Association mode selected: %s .' , method ) unique_epochs = np . sort ( images_df [ 'epoch' ] . unique ()) if add_mode : # Here the skyc1_srcs and sources_df are recreated and the done images # are filtered out. image_mask = images_df [ 'image_name' ] . isin ( done_images_df [ 'name' ]) images_df_done = images_df [ image_mask ] . copy () sources_df , skyc1_srcs = reconstruct_associtaion_dfs ( images_df_done , previous_parquets ) images_df = images_df . loc [ ~ image_mask ] if images_df . empty : logger . info ( 'No new images found, stopping association %s .' , skyreg_tag ) sources_df [ 'interim_ew' ] = ( sources_df [ 'ra_source' ] . values * sources_df [ 'weight_ew' ] . values ) sources_df [ 'interim_ns' ] = ( sources_df [ 'dec_source' ] . values * sources_df [ 'weight_ns' ] . values ) return ( sources_df . drop ([ 'ra' , 'dec' ], axis = 1 ) . rename ( columns = { 'ra_source' : 'ra' , 'dec_source' : 'dec' }) ) logger . info ( f 'Found { images_df . shape [ 0 ] } images to add to the run { skyreg_tag } .' ) # re-get the unique epochs unique_epochs = np . sort ( images_df [ 'epoch' ] . unique ()) start_epoch = 0 else : # Do full set up for a new run. first_images = ( images_df . loc [ images_df [ 'epoch' ] == unique_epochs [ 0 ], 'image_dj' ] . to_list () ) # initialise sky source dataframe skyc1_srcs = prep_skysrc_df ( first_images , config . FLUX_PERC_ERROR , duplicate_limit , ini_df = True ) skyc1_srcs [ 'epoch' ] = unique_epochs [ 0 ] # create base catalogue # initialise the sources dataframe using first image as base sources_df = skyc1_srcs . copy () start_epoch = 1 if unique_epochs . shape [ 0 ] == 1 and not add_mode : # This means only one image is present - or one group of images (epoch # mode) - so the same approach as above in add mode where there are no # images to be added, the interim needs to be calculated and skyc1_srcs # can just be returned as sources_df. ra_source and dec_source can just # be dropped as the ra and dec are already the average values. logger . warning ( 'No images to associate with! %s .' , skyreg_tag ) logger . info ( 'Returning base sources only %s .' , skyreg_tag ) # reorder the columns to match Dask expectations (parallel) skyc1_srcs = skyc1_srcs [[ 'id' , 'uncertainty_ew' , 'weight_ew' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image' , 'datetime' , 'source' , 'ra' , 'dec' , 'ra_source' , 'dec_source' , 'd2d' , 'dr' , 'related' , 'epoch' , ]] skyc1_srcs [ 'interim_ew' ] = ( skyc1_srcs [ 'ra' ] . values * skyc1_srcs [ 'weight_ew' ] . values ) skyc1_srcs [ 'interim_ns' ] = ( skyc1_srcs [ 'dec' ] . values * skyc1_srcs [ 'weight_ns' ] . values ) return skyc1_srcs . drop ([ 'ra_source' , 'dec_source' ], axis = 1 ) skyc1 = SkyCoord ( skyc1_srcs [ 'ra' ] . values , skyc1_srcs [ 'dec' ] . values , unit = ( u . deg , u . deg ) ) for it , epoch in enumerate ( unique_epochs [ start_epoch :]): logger . info ( 'Association iteration: # %i%s ' , it + 1 , skyreg_tag ) # load skyc2 source measurements and create SkyCoord images = ( images_df . loc [ images_df [ 'epoch' ] == epoch , 'image_dj' ] . to_list () ) max_beam_maj = ( images_df . loc [ images_df [ 'epoch' ] == epoch , 'image_dj' ] . apply ( lambda x : x . beam_bmaj ) . max () ) skyc2_srcs = prep_skysrc_df ( images , config . FLUX_PERC_ERROR , duplicate_limit ) skyc2_srcs [ 'epoch' ] = epoch skyc2 = SkyCoord ( skyc2_srcs [ 'ra' ] . values , skyc2_srcs [ 'dec' ] . values , unit = ( u . deg , u . deg ) ) if method == 'basic' : sources_df , skyc1_srcs = basic_association ( sources_df , skyc1_srcs , skyc1 , skyc2_srcs , skyc2 , limit , id_incr_par_assoc ) elif method in [ 'advanced' , 'deruiter' ]: if method == 'deruiter' : bw_max = Angle ( bw_limit * ( max_beam_maj * 3600. / 2. ) * u . arcsec ) else : bw_max = limit sources_df , skyc1_srcs = advanced_association ( method , sources_df , skyc1_srcs , skyc1 , skyc2_srcs , skyc2 , dr_limit , bw_max , id_incr_par_assoc ) else : raise Exception ( 'association method not implemented!' ) logger . info ( 'Calculating weighted average RA and Dec for sources %s ...' , skyreg_tag ) # account for RA wrapping ra_wrap_mask = sources_df . ra <= 0.1 sources_df [ 'ra_wrap' ] = sources_df . ra . values sources_df . at [ ra_wrap_mask , 'ra_wrap' ] = sources_df [ ra_wrap_mask ] . ra . values + 360. sources_df [ 'interim_ew' ] = ( sources_df [ 'ra_wrap' ] . values * sources_df [ 'weight_ew' ] . values ) sources_df [ 'interim_ns' ] = ( sources_df [ 'dec' ] . values * sources_df [ 'weight_ns' ] . values ) sources_df = sources_df . drop ([ 'ra_wrap' ], axis = 1 ) tmp_srcs_df = ( sources_df . loc [ ( sources_df [ 'source' ] != - 1 ) & ( sources_df [ 'forced' ] == False ), [ 'ra' , 'dec' , 'uncertainty_ew' , 'uncertainty_ns' , 'source' , 'interim_ew' , 'interim_ns' , 'weight_ew' , 'weight_ns' ] ] . groupby ( 'source' ) ) stats = StopWatch () wm_ra = tmp_srcs_df [ 'interim_ew' ] . sum () / tmp_srcs_df [ 'weight_ew' ] . sum () wm_uncertainty_ew = 1. / np . sqrt ( tmp_srcs_df [ 'weight_ew' ] . sum ()) wm_dec = tmp_srcs_df [ 'interim_ns' ] . sum () / tmp_srcs_df [ 'weight_ns' ] . sum () wm_uncertainty_ns = 1. / np . sqrt ( tmp_srcs_df [ 'weight_ns' ] . sum ()) weighted_df = ( pd . concat ( [ wm_ra , wm_uncertainty_ew , wm_dec , wm_uncertainty_ns ], axis = 1 , sort = False ) . reset_index () . rename ( columns = { 0 : 'ra' , 'weight_ew' : 'uncertainty_ew' , 1 : 'dec' , 'weight_ns' : 'uncertainty_ns' }) ) # correct the RA wrapping ra_wrap_mask = weighted_df . ra >= 360. weighted_df . at [ ra_wrap_mask , 'ra' ] = weighted_df [ ra_wrap_mask ] . ra . values - 360. logger . debug ( 'Groupby concat time %f ' , stats . reset ()) logger . info ( 'Finalising base sources catalogue ready for next iteration %s ...' , skyreg_tag ) # merge the weighted ra and dec and replace the values skyc1_srcs = skyc1_srcs . merge ( weighted_df , on = 'source' , how = 'left' , suffixes = ( '' , '_skyc2' ) ) del tmp_srcs_df , weighted_df skyc1_srcs [ 'ra' ] = skyc1_srcs [ 'ra_skyc2' ] skyc1_srcs [ 'dec' ] = skyc1_srcs [ 'dec_skyc2' ] skyc1_srcs [ 'uncertainty_ew' ] = skyc1_srcs [ 'uncertainty_ew_skyc2' ] skyc1_srcs [ 'uncertainty_ns' ] = skyc1_srcs [ 'uncertainty_ns_skyc2' ] skyc1_srcs = skyc1_srcs . drop ( [ 'ra_skyc2' , 'dec_skyc2' , 'uncertainty_ew_skyc2' , 'uncertainty_ns_skyc2' ], axis = 1 ) # generate new sky coord ready for next iteration skyc1 = SkyCoord ( skyc1_srcs [ 'ra' ] . values , skyc1_srcs [ 'dec' ] . values , unit = ( u . deg , u . deg ) ) # and update relations in skyc1 skyc1_srcs = skyc1_srcs . drop ( 'related' , axis = 1 ) relations_unique = pd . DataFrame ( sources_df [ sources_df [ 'related' ] . notna ()] . explode ( 'related' ) . groupby ( 'source' )[ 'related' ] . apply ( lambda x : x . unique () . tolist ()) ) skyc1_srcs = skyc1_srcs . merge ( relations_unique , how = 'left' , left_on = 'source' , right_index = True ) logger . info ( 'Association iteration # %i complete %s .' , it + 1 , skyreg_tag ) # End of iteration over images, ra and dec columns are actually the # average over each iteration so remove ave ra and ave dec used for # calculation and use ra_source and dec_source columns sources_df = ( sources_df . drop ([ 'ra' , 'dec' ], axis = 1 ) . rename ( columns = { 'ra_source' : 'ra' , 'dec_source' : 'dec' }) ) del skyc1_srcs , skyc2_srcs logger . info ( 'Total association time: %.2f seconds %s .' , timer . reset_init (), skyreg_tag ) return sources_df basic_association ( sources_df , skyc1_srcs , skyc1 , skyc2_srcs , skyc2 , limit , id_incr_par_assoc = 0 ) \u00b6 The loop for basic source association that uses the astropy 'match_to_catalog_sky' function (i.e. only the nearest match between the catalogs). A direct on sky separation is used to define the association. Parameters: Name Type Description Default sources_df DataFrame The dataframe containing all current measurements along with their association source and relations. required skyc1_srcs DataFrame The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. required skyc1 SkyCoord A SkyCoord object with the weighted average sky positions from skyc1_srcs. required skyc2_srcs DataFrame The same structure as sources_df containing the measurements to be associated. required skyc2 SkyCoord A SkyCoord object with the sky positions from skyc2_srcs. required limit Angle The association limit to use (applies to basic and advanced only). required id_incr_par_assoc int An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. Source code in vast_pipeline/pipeline/association.py def basic_association ( sources_df : pd . DataFrame , skyc1_srcs : pd . DataFrame , skyc1 : SkyCoord , skyc2_srcs : pd . DataFrame , skyc2 : SkyCoord , limit : Angle , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: ''' The loop for basic source association that uses the astropy 'match_to_catalog_sky' function (i.e. only the nearest match between the catalogs). A direct on sky separation is used to define the association. Args: sources_df: The dataframe containing all current measurements along with their association source and relations. skyc1_srcs: The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. skyc1: A SkyCoord object with the weighted average sky positions from skyc1_srcs. skyc2_srcs: The same structure as sources_df containing the measurements to be associated. skyc2: A SkyCoord object with the sky positions from skyc2_srcs. limit: The association limit to use (applies to basic and advanced only). id_incr_par_assoc: An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. Returns: The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. ''' # match the new sources to the base # idx gives the index of the closest match in the base for skyc2 idx , d2d , d3d = skyc2 . match_to_catalog_sky ( skyc1 ) # acceptable selection sel = d2d <= limit # The good matches can be assinged the src id from base skyc2_srcs . loc [ sel , 'source' ] = skyc1_srcs . loc [ idx [ sel ], 'source' ] . values # Need the d2d to make analysing doubles easier. skyc2_srcs . loc [ sel , 'd2d' ] = d2d [ sel ] . arcsec # must check for double matches in the acceptable matches just made # this would mean that multiple sources in skyc2 have been matched # to the same base source we want to keep closest match and move # the other match(es) back to having a -1 src id skyc2_srcs , sources_df = one_to_many_basic ( skyc2_srcs , sources_df , id_incr_par_assoc ) logger . info ( 'Updating sources catalogue with new sources...' ) # update the src numbers for those sources in skyc2 with no match # using the max current src as the start and incrementing by one start_elem = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc nan_sel = ( skyc2_srcs [ 'source' ] == - 1 ) . values skyc2_srcs . loc [ nan_sel , 'source' ] = ( np . arange ( start_elem , start_elem + skyc2_srcs . loc [ nan_sel ] . shape [ 0 ], dtype = int ) ) # and skyc2 is now ready to be appended to new sources sources_df = sources_df . append ( skyc2_srcs , ignore_index = True ) . reset_index ( drop = True ) # and update skyc1 with the sources that were created from the one # to many relations and any new sources. skyc1_srcs = skyc1_srcs . append ( skyc2_srcs [ ~ skyc2_srcs [ 'source' ] . isin ( skyc1_srcs [ 'source' ]) ], ignore_index = True ) . reset_index ( drop = True ) return sources_df , skyc1_srcs calc_de_ruiter ( df ) \u00b6 Calculates the unitless 'de Ruiter' radius of the association. Works on the 'temp_df' dataframe of the advanced association, where the two sources associated with each other have been merged into one row. Parameters: Name Type Description Default df DataFrame The 'temp_df' from advanced association. It must contain the columns ra_skyc1 , 'ra_skyc2', 'uncertainty_ew_skyc1', 'uncertainty_ew_skyc2', 'dec_skyc1', 'dec_skyc2', 'uncertainty_ns_skyc1' and 'uncertainty_ns_skyc2'. required Returns: Type Description ndarray Array containing the de Ruiter radius for all rows in the df. Source code in vast_pipeline/pipeline/association.py def calc_de_ruiter ( df : pd . DataFrame ) -> np . ndarray : \"\"\" Calculates the unitless 'de Ruiter' radius of the association. Works on the 'temp_df' dataframe of the advanced association, where the two sources associated with each other have been merged into one row. Args: df: The 'temp_df' from advanced association. It must contain the columns `ra_skyc1`, 'ra_skyc2', 'uncertainty_ew_skyc1', 'uncertainty_ew_skyc2', 'dec_skyc1', 'dec_skyc2', 'uncertainty_ns_skyc1' and 'uncertainty_ns_skyc2'. Returns: Array containing the de Ruiter radius for all rows in the df. \"\"\" ra_1 = df [ 'ra_skyc1' ] . values ra_2 = df [ 'ra_skyc2' ] . values # avoid wrapping issues ra_1 [ ra_1 > 270. ] -= 180. ra_2 [ ra_2 > 270. ] -= 180. ra_1 [ ra_1 < 90. ] += 180. ra_2 [ ra_2 < 90. ] += 180. ra_1 = np . deg2rad ( ra_1 ) ra_2 = np . deg2rad ( ra_2 ) ra_1_err = np . deg2rad ( df [ 'uncertainty_ew_skyc1' ] . values ) ra_2_err = np . deg2rad ( df [ 'uncertainty_ew_skyc2' ] . values ) dec_1 = np . deg2rad ( df [ 'dec_skyc1' ] . values ) dec_2 = np . deg2rad ( df [ 'dec_skyc2' ] . values ) dec_1_err = np . deg2rad ( df [ 'uncertainty_ns_skyc1' ] . values ) dec_2_err = np . deg2rad ( df [ 'uncertainty_ns_skyc2' ] . values ) dr1 = ( ra_1 - ra_2 ) * ( ra_1 - ra_2 ) dr1_1 = np . cos (( dec_1 + dec_2 ) / 2. ) dr1 *= dr1_1 * dr1_1 dr1 /= ra_1_err * ra_1_err + ra_2_err * ra_2_err dr2 = ( dec_1 - dec_2 ) * ( dec_1 - dec_2 ) dr2 /= dec_1_err * dec_1_err + dec_2_err * dec_2_err dr = np . sqrt ( dr1 + dr2 ) return dr many_to_many_advanced ( temp_srcs , method ) \u00b6 Finds and processes the many-to-many associations in the advanced association. We do not want to build many-to-many associations as this will make the database get very large (see TraP documentation). The skyc2 sources which are listed more than once are found, and of these, those which have a skyc1 source association which is also listed twice in the associations are selected. The closest (by limit or de Ruiter radius, depending on the method) is kept where as the other associations are dropped. This follows the same logic used by the TraP (see TraP documentation). Parameters: Name Type Description Default temp_srcs DataFrame The temporary associtation dataframe used through the advanced association process. required method str Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. required Returns: Type Description DataFrame Updated temp_srcs with the many_to_many relations dropped. Source code in vast_pipeline/pipeline/association.py def many_to_many_advanced ( temp_srcs : pd . DataFrame , method : str ) -> pd . DataFrame : ''' Finds and processes the many-to-many associations in the advanced association. We do not want to build many-to-many associations as this will make the database get very large (see TraP documentation). The skyc2 sources which are listed more than once are found, and of these, those which have a skyc1 source association which is also listed twice in the associations are selected. The closest (by limit or de Ruiter radius, depending on the method) is kept where as the other associations are dropped. This follows the same logic used by the TraP (see TraP documentation). Args: temp_srcs: The temporary associtation dataframe used through the advanced association process. method: Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. Returns: Updated temp_srcs with the many_to_many relations dropped. ''' # Select those where the extracted source is listed more than once # (e.g. index_old_skyc2 duplicated values) and of these get those that # have a source id that is listed more than once (e.g. source_skyc1 # duplicated values) in the temps_srcs df m_to_m = temp_srcs [( temp_srcs [ 'index_old_skyc2' ] . duplicated ( keep = False ) & temp_srcs [ 'source_skyc1' ] . duplicated ( keep = False ) )] . copy () if m_to_m . empty : logger . debug ( 'No many-to-many assocations.' ) return temp_srcs logger . debug ( 'Detected # %i many-to-many assocations, cleaning...' , m_to_m . shape [ 0 ] ) dist_col = 'd2d_skyc2' if method == 'advanced' else 'dr' min_col = 'min_' + dist_col # get the minimum de ruiter value for each extracted source m_to_m [ min_col ] = ( m_to_m . groupby ( 'index_old_skyc2' )[ dist_col ] . transform ( 'min' ) ) # get the ids of those crossmatches that are larger than the minimum m_to_m_to_drop = m_to_m [ m_to_m [ dist_col ] != m_to_m [ min_col ]] . index . values # and drop these from the temp_srcs temp_srcs = temp_srcs . drop ( m_to_m_to_drop ) return temp_srcs many_to_one_advanced ( temp_srcs ) \u00b6 Finds and processes the many-to-one associations in the advanced association. In this case in the related column of the 'many' sources we need to append the ids of all the other 'many' (expect for itself). Parameters: Name Type Description Default temp_srcs DataFrame The temporary associtation dataframe used through the advanced association process. required Returns: Type Description DataFrame Updated temp_srcs with all many_to_one relation information added. Source code in vast_pipeline/pipeline/association.py def many_to_one_advanced ( temp_srcs : pd . DataFrame ) -> pd . DataFrame : ''' Finds and processes the many-to-one associations in the advanced association. In this case in the related column of the 'many' sources we need to append the ids of all the other 'many' (expect for itself). Args: temp_srcs: The temporary associtation dataframe used through the advanced association process. Returns: Updated temp_srcs with all many_to_one relation information added. ''' # use only these columns for easy debugging of the dataframe cols = [ 'index_old_skyc1' , 'id_skyc1' , 'source_skyc1' , 'd2d_skyc1' , 'related_skyc1' , 'index_old_skyc2' , 'id_skyc2' , 'source_skyc2' , 'd2d_skyc2' , 'dr' ] # select those sources which have been matched to the same measurement # in the sky catalogue 2. duplicated_skyc2 = temp_srcs . loc [ temp_srcs [ 'index_old_skyc2' ] . duplicated ( keep = False ), cols ] # duplicated_skyc2 # +-----+-------------------+------------+----------------+------------- # | | index_old_skyc1 | id_skyc1 | source_skyc1 | d2d_skyc1 # |-----+-------------------+------------+----------------+------------- # | 447 | 477 | 478 | 478 | 0 # | 448 | 478 | 479 | 479 | 0 # | 477 | 507 | 508 | 508 | 0 # | 478 | 508 | 509 | 509 | 0 # | 695 | 738 | 739 | 739 | 0 # +-----+-------------------+------------+----------------+------------- # +-----------------+-------------------+------------+----------------+ # | related_skyc1 | index_old_skyc2 | id_skyc2 | source_skyc2 | # +-----------------+-------------------+------------+----------------+ # | | 305 | 5847 | -1 | # | | 305 | 5847 | -1 | # | | 648 | 6190 | -1 | # | | 648 | 6190 | -1 | # | | 561 | 6103 | -1 | # +-----------------+-------------------+------------+----------------+ # -------------+------+ # d2d_skyc2 | dr | # -------------+------| # 8.63598 | 0 | # 8.63598 | 0 | # 6.5777 | 0 | # 6.5777 | 0 | # 7.76527 | 0 | # -------------+------+ # if there are none no action is required. if duplicated_skyc2 . empty : logger . debug ( 'No many-to-one associations.' ) return temp_srcs logger . debug ( 'Detected # %i many-to-one associations' , duplicated_skyc2 . shape [ 0 ] ) # The new relations become that for each 'many' source we need to append # the ids of the other 'many' sources that have been associationed with the # 'one'. Below for each 'one' group we gather all the ids of the many # sources. new_relations = pd . DataFrame ( duplicated_skyc2 . groupby ( 'index_old_skyc2' ) . apply ( lambda grp : grp [ 'source_skyc1' ] . tolist ()) ) . rename ( columns = { 0 : 'new_relations' }) # new_relations # +-------------------+-----------------+ # | index_old_skyc2 | new_relations | # |-------------------+-----------------| # | 305 | [478, 479] | # | 561 | [739, 740] | # | 648 | [508, 509] | # | 764 | [841, 842] | # | 816 | [1213, 1215] | # +-------------------+-----------------+ # these new relations are then added to the duplciated dataframe so # they can easily be used by the next function. duplicated_skyc2 = duplicated_skyc2 . merge ( new_relations , left_on = 'index_old_skyc2' , right_index = True , how = 'left' ) # Remove the 'self' relations. The 'x['source_skyc1']' is an integer so it # is placed within a list notation, [], to be able to be easily subtracted # from the new_relations. duplicated_skyc2 [ 'new_relations' ] = ( duplicated_skyc2 . apply ( lambda x : list ( set ( x [ 'new_relations' ]) - set ([ x [ 'source_skyc1' ]])), axis = 1 ) ) # Use the 'add_new_many_to_one_relations' method to add tthe new relatitons # to the actual `related_skyc1' column. duplicated_skyc2 [ 'related_skyc1' ] = ( duplicated_skyc2 . apply ( add_new_many_to_one_relations , axis = 1 ) ) # Transfer the new relations from the duplicated df to the temp_srcs. The # index is explicitly declared to avoid any mixups. temp_srcs . loc [ duplicated_skyc2 . index . values , 'related_skyc1' ] = duplicated_skyc2 . loc [ duplicated_skyc2 . index . values , 'related_skyc1' ] . values return temp_srcs one_to_many_advanced ( temp_srcs , sources_df , method , id_incr_par_assoc = 0 ) \u00b6 Finds and processes the one-to-many associations in the advanced association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the basic version as the data products between the two are different. Parameters: Name Type Description Default temp_srcs DataFrame The temporary associtation dataframe used through the advanced association process. required sources_df DataFrame The sources_df produced by each step of association holding the current 'sources'. required method str Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. required id_incr_par_assoc int An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] Updated temp_srcs and sources_df with all one_to_many relation information added. Source code in vast_pipeline/pipeline/association.py def one_to_many_advanced ( temp_srcs : pd . DataFrame , sources_df : pd . DataFrame , method : str , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: ''' Finds and processes the one-to-many associations in the advanced association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the basic version as the data products between the two are different. Args: temp_srcs: The temporary associtation dataframe used through the advanced association process. sources_df: The sources_df produced by each step of association holding the current 'sources'. method: Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. id_incr_par_assoc: An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association Returns: Updated temp_srcs and sources_df with all one_to_many relation information added. ''' # use only these columns for easy debugging of the dataframe cols = [ 'index_old_skyc1' , 'id_skyc1' , 'source_skyc1' , 'd2d_skyc1' , 'related_skyc1' , 'index_old_skyc2' , 'id_skyc2' , 'source_skyc2' , 'd2d_skyc2' , 'dr' ] duplicated_skyc1 = temp_srcs . loc [ temp_srcs [ 'source_skyc1' ] . duplicated ( keep = False ), cols ] . copy () # duplicated_skyc1 # +-----+-------------------+------------+----------------+-------------+ # | | index_old_skyc1 | id_skyc1 | source_skyc1 | d2d_skyc1 | # |-----+-------------------+------------+----------------+-------------+ # | 117 | 121 | 122 | 122 | 0 | # | 118 | 121 | 122 | 122 | 0 | # | 238 | 253 | 254 | 254 | 0 | # | 239 | 253 | 254 | 254 | 0 | # | 246 | 261 | 262 | 262 | 0 | # +-----+-------------------+------------+----------------+-------------+ # -----------------+-------------------+------------+----------------+ # related_skyc1 | index_old_skyc2 | id_skyc2 | source_skyc2 | # -----------------+-------------------+------------+----------------+ # | 526 | 6068 | -1 | # | 528 | 6070 | -1 | # | 264 | 5806 | -1 | # | 265 | 5807 | -1 | # | 327 | 5869 | -1 | # -----------------+-------------------+------------+----------------+ # -------------+------+ # d2d_skyc2 | dr | # -------------+------| # 3.07478 | 0 | # 6.41973 | 0 | # 2.04422 | 0 | # 6.16881 | 0 | # 3.20439 | 0 | # -------------+------+ # If no relations then no action is required if duplicated_skyc1 . empty : logger . debug ( 'No one-to-many associations.' ) return temp_srcs , sources_df logger . debug ( 'Detected # %i one-to-many assocations, cleaning...' , duplicated_skyc1 . shape [ 0 ] ) # Get the column to check for the minimum depending on the method # set the column names needed for filtering the 'to-many' # associations depending on the method (advanced or deruiter) dist_col = 'd2d_skyc2' if method == 'advanced' else 'dr' # go through the doubles and # 1. Keep the closest d2d or de ruiter as the primary id # 2. Increment a new source id for others # 3. Add a copy of the previously matched # source into sources. # multi_srcs = duplicated_skyc1['source_skyc1'].unique() # Get the duplicated, sort by the distance column duplicated_skyc1 = duplicated_skyc1 . sort_values ( by = [ 'source_skyc1' , dist_col ] ) # Get those that need to be given a new ID number (i.e. not the min dist_col) idx_to_change = duplicated_skyc1 . index . values [ duplicated_skyc1 . duplicated ( 'source_skyc1' ) ] # Create a new `new_source_id` column to store the 'correct' IDs duplicated_skyc1 [ 'new_source_id' ] = duplicated_skyc1 [ 'source_skyc1' ] # +-----------------+ # | new_source_id | # +-----------------| # | 122 | # | 122 | # | 254 | # | 254 | # | 262 | # +-----------------+ # Define the range of new source ids start_new_src_id = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc # Create an arange to use to change the ones that need to be changed. new_source_ids = np . arange ( start_new_src_id , start_new_src_id + idx_to_change . shape [ 0 ], dtype = int ) # Assign the new IDs to those that need to be changed. duplicated_skyc1 . loc [ idx_to_change , 'new_source_id' ] = new_source_ids # Now we need to sort out the related, essentially here the 'original' # and 'non original' need to be treated differently. # The original source need all the assoicated new ids appended to the # related column. # The not_original ones need just the original ID appended. not_original = duplicated_skyc1 . loc [ idx_to_change ] . copy () original = duplicated_skyc1 . drop_duplicates ( 'source_skyc1' ) . copy () # This gathers all the new ids that need to be appended # to the original related column. new_original_related = pd . DataFrame ( not_original [ [ 'source_skyc1' , 'new_source_id' ] ] . groupby ( 'source_skyc1' ) . apply ( lambda grp : grp [ 'new_source_id' ] . tolist () ) ) #new_original_related # +----------------+--------+ # | source_skyc1 | 0 | # |----------------+--------| # | 122 | [5542] | # | 254 | [5543] | # | 262 | [5544] | # | 405 | [5545] | # | 656 | [5546] | # +----------------+--------+ # Append the relations in each case, using the above 'new_original_related' # for the original ones. # The not original only require the appending of the original index. original [ 'related_skyc1' ] = ( original [[ 'related_skyc1' , 'source_skyc1' ]] . apply ( add_new_one_to_many_relations , args = ( True , new_original_related ), axis = 1 ) ) # what the column looks like after the above # +-----------------+ # | related_skyc1 | # +-----------------+ # | [5542] | # | [5543] | # | [5544] | # | [5545] | # | [5546] | # +-----------------+ not_original . loc [:, 'related_skyc1' ] = not_original . apply ( add_new_one_to_many_relations , args = ( True ,), axis = 1 ) # Merge them back together duplicated_skyc1 = original . append ( not_original ) del original , not_original # Apply the updates to the actual temp_srcs. temp_srcs . loc [ idx_to_change , 'source_skyc1' ] = new_source_ids temp_srcs . loc [ duplicated_skyc1 . index . values , 'related_skyc1' ] = duplicated_skyc1 . loc [ duplicated_skyc1 . index . values , 'related_skyc1' ] . values # Finally we need to create copies of the previous sources in the # sources_df to complete the new sources. # To do this we get only the non-original sources duplicated_skyc1 = duplicated_skyc1 . loc [ duplicated_skyc1 . duplicated ( 'source_skyc1' ) ] # Get all the indexes required for each original # `source_skyc1` value source_df_index_to_copy = pd . DataFrame ( duplicated_skyc1 . groupby ( 'source_skyc1' ) . apply ( lambda grp : sources_df [ sources_df [ 'source' ] == grp . name ] . index . values . tolist () ) ) # source_df_index_to_copy # +----------------+-------+ # | source_skyc1 | 0 | # |----------------+-------| # | 122 | [121] | # | 254 | [253] | # | 262 | [261] | # | 405 | [404] | # | 656 | [655] | # +----------------+-------+ # merge these so it's easy to explode and copy the index values. duplicated_skyc1 = ( duplicated_skyc1 . loc [:,[ 'source_skyc1' , 'new_source_id' ]] . merge ( source_df_index_to_copy , left_on = 'source_skyc1' , right_index = True , how = 'left' ) . rename ( columns = { 0 : 'source_index' }) . explode ( 'source_index' ) ) # duplicated_skyc1 # +-----+----------------+-----------------+----------------+ # | | source_skyc1 | new_source_id | source_index | # |-----+----------------+-----------------+----------------| # | 118 | 122 | 5542 | 121 | # | 239 | 254 | 5543 | 253 | # | 247 | 262 | 5544 | 261 | # | 380 | 405 | 5545 | 404 | # | 615 | 656 | 5546 | 655 | # +-----+----------------+-----------------+----------------+ # Get the sources sources_to_copy = sources_df . loc [ duplicated_skyc1 [ 'source_index' ] . values ] # Apply the new_source_id sources_to_copy [ 'source' ] = duplicated_skyc1 [ 'new_source_id' ] . values # and finally append. sources_df = sources_df . append ( sources_to_copy , ignore_index = True ) return temp_srcs , sources_df one_to_many_basic ( skyc2_srcs , sources_df , id_incr_par_assoc = 0 ) \u00b6 Finds and processes the one-to-many associations in the basic association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the advanced version as the data products between the two are different. Parameters: Name Type Description Default skyc2_srcs DataFrame The sky catalogue 2 sources (i.e. the sources being associated to the base) used during basic association. required sources_df DataFrame The sources_df produced by each step of association holding the current 'sources'. required id_incr_par_assoc int An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] Tuple containging the updated 'skyc2_srcs' and 'sources_df' with all one_to_many relation information added. Source code in vast_pipeline/pipeline/association.py def one_to_many_basic ( skyc2_srcs : pd . DataFrame , sources_df : pd . DataFrame , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Finds and processes the one-to-many associations in the basic association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the advanced version as the data products between the two are different. Args: skyc2_srcs: The sky catalogue 2 sources (i.e. the sources being associated to the base) used during basic association. sources_df: The sources_df produced by each step of association holding the current 'sources'. id_incr_par_assoc: An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association Returns: Tuple containging the updated 'skyc2_srcs' and 'sources_df' with all one_to_many relation information added. \"\"\" # select duplicated in 'source' field in skyc2_srcs, excluding -1 duplicated_skyc2 = skyc2_srcs . loc [ ( skyc2_srcs [ 'source' ] != - 1 ) & skyc2_srcs [ 'source' ] . duplicated ( keep = False ), [ 'source' , 'related' , 'd2d' ] ] # duplicated_skyc2 # +-----+----------+-----------+---------+ # | | source | related | d2d | # |-----+----------+-----------+---------| # | 264 | 254 | | 2.04422 | # | 265 | 254 | | 6.16881 | # | 327 | 262 | | 3.20439 | # | 328 | 262 | | 3.84425 | # | 526 | 122 | | 3.07478 | # +-----+----------+-----------+---------+ if duplicated_skyc2 . empty : logger . debug ( 'No one-to-many associations.' ) return skyc2_srcs , sources_df logger . info ( 'Detected # %i double matches, cleaning...' , duplicated_skyc2 . shape [ 0 ] ) # now we have the src values which are doubled. # make the nearest match have the \"original\" src id # give the other matched source a new src id # and make sure to copy the other previously # matched sources. # Get the duplicated, sort by the distance column duplicated_skyc2 = duplicated_skyc2 . sort_values ( by = [ 'source' , 'd2d' ]) # Get those that need to be given a new ID number (i.e. not the min dist_col) idx_to_change = duplicated_skyc2 . index . values [ duplicated_skyc2 . duplicated ( 'source' ) ] # Create a new `new_source_id` column to store the 'correct' IDs duplicated_skyc2 [ 'new_source_id' ] = duplicated_skyc2 [ 'source' ] # Define the range of new source ids start_new_src_id = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc new_source_ids = np . arange ( start_new_src_id , start_new_src_id + idx_to_change . shape [ 0 ], dtype = int ) # Assign the new IDs duplicated_skyc2 . loc [ idx_to_change , 'new_source_id' ] = new_source_ids # duplicated_skyc2 # +-----+----------+-----------+---------+-----------------+ # | | source | related | d2d | new_source_id | # |-----+----------+-----------+---------+-----------------| # | 526 | 122 | | 3.07478 | 122 | # | 528 | 122 | | 6.41973 | 5542 | # | 264 | 254 | | 2.04422 | 254 | # | 265 | 254 | | 6.16881 | 5543 | # | 327 | 262 | | 3.20439 | 262 | # +-----+----------+-----------+---------+-----------------+ # Now we need to sort out the related, essentially here the 'original' # and 'non original' need to be treated differently. # The original source need all the assoicated new ids appended to the # related column. # The not_original ones need just the original ID appended. # copy() is used here to avoid chained indexing (set with copy warnings) not_original = duplicated_skyc2 . loc [ idx_to_change ] . copy () original = duplicated_skyc2 . drop_duplicates ( 'source' ) . copy () new_original_related = pd . DataFrame ( not_original [ [ 'source' , 'new_source_id' ] ] . groupby ( 'source' ) . apply ( lambda grp : grp [ 'new_source_id' ] . tolist () ) ) # new_original_related # +----------+--------+ # | source | 0 | # |----------+--------| # | 122 | [5542] | # | 254 | [5543] | # | 262 | [5544] | # | 405 | [5545] | # | 656 | [5546] | # +----------+--------+ # Append the relations in each case, using the above 'new_original_related' # for the original ones. # The not original only require the appending of the original index. original [ 'related' ] = ( original [[ 'related' , 'source' ]] . apply ( add_new_one_to_many_relations , args = ( False , new_original_related ), axis = 1 ) ) not_original [ 'related' ] = not_original . apply ( add_new_one_to_many_relations , args = ( False ,), axis = 1 ) duplicated_skyc2 = original . append ( not_original ) # duplicated_skyc2 # +-----+----------+-----------+---------+-----------------+ # | | source | related | d2d | new_source_id | # |-----+----------+-----------+---------+-----------------| # | 526 | 122 | [5542] | 3.07478 | 122 | # | 264 | 254 | [5543] | 2.04422 | 254 | # | 327 | 262 | [5544] | 3.20439 | 262 | # | 848 | 405 | [5545] | 5.52865 | 405 | # | 695 | 656 | [5546] | 4.69094 | 656 | # +-----+----------+-----------+---------+-----------------+ del original , not_original # Apply the updates to the actual temp_srcs. skyc2_srcs . loc [ idx_to_change , 'source' ] = new_source_ids skyc2_srcs . loc [ duplicated_skyc2 . index . values , 'related' ] = duplicated_skyc2 . loc [ duplicated_skyc2 . index . values , 'related' ] . values # Finally we need to copy copies of the previous sources in the # sources_df to complete the new sources. # To do this we get only the non-original sources duplicated_skyc2 = duplicated_skyc2 . loc [ duplicated_skyc2 . duplicated ( 'source' ) ] # Get all the indexes required for each original # `source_skyc1` value source_df_index_to_copy = pd . DataFrame ( duplicated_skyc2 . groupby ( 'source' ) . apply ( lambda grp : sources_df [ sources_df [ 'source' ] == grp . name ] . index . values . tolist () ) ) # source_df_index_to_copy # +----------+-------+ # | source | 0 | # |----------+-------| # | 122 | [121] | # | 254 | [253] | # | 262 | [261] | # | 405 | [404] | # | 656 | [655] | # +----------+-------+ # merge these so it's easy to explode and copy the index values. duplicated_skyc2 = ( duplicated_skyc2 [[ 'source' , 'new_source_id' ]] . merge ( source_df_index_to_copy , left_on = 'source' , right_index = True , how = 'left' ) . rename ( columns = { 0 : 'source_index' }) . explode ( 'source_index' ) ) # Get the sources - all columns from the sources_df table sources_to_copy = sources_df . loc [ duplicated_skyc2 [ 'source_index' ] . values ] # Apply the new_source_id sources_to_copy [ 'source' ] = duplicated_skyc2 [ 'new_source_id' ] . values # and finally append. sources_df = sources_df . append ( sources_to_copy , ignore_index = True ) return skyc2_srcs , sources_df parallel_association ( images_df , limit , dr_limit , bw_limit , duplicate_limit , config , n_skyregion_groups , add_mode , previous_parquets , done_images_df , done_source_ids ) \u00b6 Launches association on different sky region groups in parallel using Dask. Parameters: Name Type Description Default images_df DataFrame Holds the images that are being processed. Also contains what sky region group the image belongs to. required limit Angle The association radius limit. required dr_limit float The de Ruiter radius limit. required bw_limit float The beamwidth limit. required duplicate_limit Angle The duplicate radius detection limit. required config module The pipeline config settings. required n_skyregion_groups int The number of sky region groups. required Returns: Type Description DataFrame pd.DataFrame: The combined association results of the parallel association with corrected source ids. Source code in vast_pipeline/pipeline/association.py def parallel_association ( images_df : pd . DataFrame , limit : Angle , dr_limit : float , bw_limit : float , duplicate_limit : Angle , # TODO update config typing. config , # a 'module` typing. n_skyregion_groups : int , add_mode : bool , previous_parquets : Dict [ str , str ], done_images_df : pd . DataFrame , done_source_ids : List [ int ] ) -> pd . DataFrame : \"\"\" Launches association on different sky region groups in parallel using Dask. Args: images_df: Holds the images that are being processed. Also contains what sky region group the image belongs to. limit: The association radius limit. dr_limit: The de Ruiter radius limit. bw_limit: The beamwidth limit. duplicate_limit: The duplicate radius detection limit. config (module): The pipeline config settings. n_skyregion_groups: The number of sky region groups. Returns: pd.DataFrame: The combined association results of the parallel association with corrected source ids. \"\"\" logger . info ( \"Running parallel association for %i sky region groups.\" , n_skyregion_groups ) timer = StopWatch () meta = { 'id' : 'i' , 'uncertainty_ew' : 'f' , 'weight_ew' : 'f' , 'uncertainty_ns' : 'f' , 'weight_ns' : 'f' , 'flux_int' : 'f' , 'flux_int_err' : 'f' , 'flux_int_isl_ratio' : 'f' , 'flux_peak' : 'f' , 'flux_peak_err' : 'f' , 'flux_peak_isl_ratio' : 'f' , 'forced' : '?' , 'compactness' : 'f' , 'has_siblings' : '?' , 'snr' : 'f' , 'image' : 'U' , 'datetime' : 'datetime64[ns]' , 'source' : 'i' , 'ra' : 'f' , 'dec' : 'f' , 'd2d' : 'f' , 'dr' : 'f' , 'related' : 'O' , 'epoch' : 'i' , 'interim_ew' : 'f' , 'interim_ns' : 'f' , } # Add an increment to any new source values when using add_mode to avoid # getting duplicates in the result laater id_incr_par_assoc = max ( done_source_ids ) if add_mode else 0 n_cpu = cpu_count () - 1 # pass each skyreg_group through the normal association process. results = ( dd . from_pandas ( images_df , n_cpu ) . groupby ( 'skyreg_group' ) . apply ( association , limit = limit , dr_limit = dr_limit , bw_limit = bw_limit , duplicate_limit = duplicate_limit , config = config , add_mode = add_mode , previous_parquets = previous_parquets , done_images_df = done_images_df , id_incr_par_assoc = id_incr_par_assoc , parallel = True , meta = meta ) . compute ( n_workers = n_cpu , scheduler = 'processes' ) ) # results are the normal dataframe of results with the columns: # 'id', 'uncertainty_ew', 'weight_ew', 'uncertainty_ns', 'weight_ns', # 'flux_int', 'flux_int_err', 'flux_peak', 'flux_peak_err', 'forced', # 'compactness', 'has_siblings', 'snr', 'image', 'datetime', 'source', # 'ra', 'dec', 'd2d', 'dr', 'related', 'epoch', 'interim_ew' and # 'interim_ns'. # The index however is now a multi index with the skyregion group and # a general result index. Hence the general result index is repeated for # each skyreg_group along with the source_ids. This needs to be collapsed # and the source id's corrected. # Index example: # id # skyreg_group # -------------------------- # 2 0 15640 # 1 15641 # 2 15642 # 3 15643 # 4 15644 # ... ... # 1 46975 53992 # 46976 54062 # 46977 54150 # 46978 54161 # 46979 54164 # Get the indexes (skyreg_groups) to loop over for source id correction indexes = results . index . levels [ 0 ] . values if add_mode : # Need to correct all skyreg_groups. # First get the starting id for new sources. new_id = max ( done_source_ids ) + 1 for i in indexes : corr_df , new_id = _correct_parallel_source_ids_add_mode ( results . loc [ i , [ 'source' , 'related' ]], done_source_ids , new_id ) results . loc [ ( i , slice ( None )), [ 'source' , 'related' ] ] = corr_df . values else : # The first index acts as the base, so the others are looped over and # corrected. for i , val in enumerate ( indexes ): # skip first one, makes the enumerate easier to deal with if i == 0 : continue # Get the maximum source ID from the previous group. max_id = results . loc [ indexes [ i - 1 ]] . source . max () # Run through the correction function, only the 'source' and # 'related' # columns are passed and returned (corrected). corr_df = _correct_parallel_source_ids ( results . loc [ val , [ 'source' , 'related' ]], max_id ) # replace the values in the results with the corrected source and # related values results . loc [ ( val , slice ( None )), [ 'source' , 'related' ] ] = corr_df . values del corr_df # reset the indeex of the final corrected and collapsed result results = results . reset_index ( drop = True ) logger . info ( 'Total parallel association time: %.2f seconds' , timer . reset_init () ) return results","title":"association.py"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.advanced_association","text":"The loop for advanced source association that uses the astropy 'search_around_sky' function (i.e. all matching sources are found). The BMAJ of the image * the user supplied beamwidth limit is the base distance for association. This is followed by calculating the 'de Ruiter' radius. Parameters: Name Type Description Default method str The advanced association method 'advanced' or 'deruiter'. required sources_df DataFrame The dataframe containing all current measurements along with their association source and relations. required skyc1_srcs DataFrame The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. required skyc1 SkyCoord A SkyCoord object with the weighted average sky positions from skyc1_srcs. required skyc2_srcs DataFrame The same structure as sources_df containing the measurements to be associated. required skyc2 SkyCoord A SkyCoord object with the sky positions from skyc2_srcs. required dr_limit float The de Ruiter radius limit to use (applies to de ruiter only). required bw_max float The beamwidth limit to use (applies to de ruiter only). required id_incr_par_assoc int An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. Source code in vast_pipeline/pipeline/association.py def advanced_association ( method : str , sources_df : pd . DataFrame , skyc1_srcs : pd . DataFrame , skyc1 : SkyCoord , skyc2_srcs : pd . DataFrame , skyc2 : SkyCoord , dr_limit : float , bw_max : float , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: ''' The loop for advanced source association that uses the astropy 'search_around_sky' function (i.e. all matching sources are found). The BMAJ of the image * the user supplied beamwidth limit is the base distance for association. This is followed by calculating the 'de Ruiter' radius. Args: method: The advanced association method 'advanced' or 'deruiter'. sources_df: The dataframe containing all current measurements along with their association source and relations. skyc1_srcs: The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. skyc1: A SkyCoord object with the weighted average sky positions from skyc1_srcs. skyc2_srcs: The same structure as sources_df containing the measurements to be associated. skyc2: A SkyCoord object with the sky positions from skyc2_srcs. dr_limit: The de Ruiter radius limit to use (applies to de ruiter only). bw_max: The beamwidth limit to use (applies to de ruiter only). id_incr_par_assoc: An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. Returns: The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. ''' # read the needed sources fields # Step 1: get matches within semimajor axis of image. idx_skyc1 , idx_skyc2 , d2d , d3d = skyc2 . search_around_sky ( skyc1 , bw_max ) # Step 2: merge the candidates so the de ruiter can be calculated temp_skyc1_srcs = ( skyc1_srcs . loc [ idx_skyc1 ] . reset_index () . rename ( columns = { 'index' : 'index_old' }) ) temp_skyc2_srcs = ( skyc2_srcs . loc [ idx_skyc2 ] . reset_index () . rename ( columns = { 'index' : 'index_old' }) ) temp_skyc2_srcs [ 'd2d' ] = d2d . arcsec temp_srcs = temp_skyc1_srcs . merge ( temp_skyc2_srcs , left_index = True , right_index = True , suffixes = ( '_skyc1' , '_skyc2' ) ) del temp_skyc1_srcs , temp_skyc2_srcs # Step 3: Apply the beamwidth limit temp_srcs = temp_srcs [ d2d <= bw_max ] . copy () # Step 4: Calculate and perform De Ruiter radius cut if method == 'deruiter' : temp_srcs [ 'dr' ] = calc_de_ruiter ( temp_srcs ) temp_srcs = temp_srcs [ temp_srcs [ 'dr' ] <= dr_limit ] else : temp_srcs [ 'dr' ] = 0. # Now have the 'good' matches # Step 5: Check for one-to-many, many-to-one and many-to-many # associations. First the many-to-many temp_srcs = many_to_many_advanced ( temp_srcs , method ) # Next one-to-many # Get the sources which are doubled temp_srcs , sources_df = one_to_many_advanced ( temp_srcs , sources_df , method , id_incr_par_assoc ) # Finally many-to-one associations, the opposite of above but we # don't have to create new ids for these so it's much simpler in fact # we don't need to do anything but lets get the number for debugging. temp_srcs = many_to_one_advanced ( temp_srcs ) # Now everything in place to append # First the skyc2 sources with a match. # This is created from the temp_srcs df. # This will take care of the extra skyc2 sources needed. skyc2_srcs_toappend = skyc2_srcs . loc [ temp_srcs [ 'index_old_skyc2' ] . values ] . reset_index ( drop = True ) skyc2_srcs_toappend [ 'source' ] = temp_srcs [ 'source_skyc1' ] . values skyc2_srcs_toappend [ 'related' ] = temp_srcs [ 'related_skyc1' ] . values skyc2_srcs_toappend [ 'dr' ] = temp_srcs [ 'dr' ] . values # and get the skyc2 sources with no match logger . info ( 'Updating sources catalogue with new sources...' ) new_sources = skyc2_srcs . loc [ skyc2_srcs . index . difference ( temp_srcs [ 'index_old_skyc2' ] . values ) ] . reset_index ( drop = True ) # update the src numbers for those sources in skyc2 with no match # using the max current src as the start and incrementing by one start_elem = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc new_sources [ 'source' ] = np . arange ( start_elem , start_elem + new_sources . shape [ 0 ], dtype = int ) skyc2_srcs_toappend = skyc2_srcs_toappend . append ( new_sources , ignore_index = True ) # and skyc2 is now ready to be appended to source_df sources_df = sources_df . append ( skyc2_srcs_toappend , ignore_index = True ) . reset_index ( drop = True ) # update skyc1 and df for next association iteration # calculate average angles for skyc1 skyc1_srcs = ( skyc1_srcs . append ( new_sources , ignore_index = True ) . reset_index ( drop = True ) ) # also need to append any related sources that created a new # source, we can use the skyc2_srcs_toappend to get these skyc1_srcs = skyc1_srcs . append ( skyc2_srcs_toappend . loc [ ~ skyc2_srcs_toappend . source . isin ( skyc1_srcs . source ) ] ) return sources_df , skyc1_srcs","title":"advanced_association()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.association","text":"The main association function that does the common tasks between basic and advanced modes. Parameters: Name Type Description Default images_df DataFrame The input images to be associated. required limit Angle The association limit to use (applies to basic and advanced only). required dr_limit float The de Ruiter radius limit to use (applies to de ruiter only). required bw_limit float The beamwidth limit to use (applies to de ruiter only). required duplicate_limit Angle The limit of separation for which a measurement is considered to be a duplicate (epoch based association). required config The pipeline configuration object. required add_mode bool Whether the pipeline is currently being run in add image mode. required previous_parquets Dict[str, str] Dictionary containing the paths of the previous successful run parquet files (used in add image mode). required done_images_df DataFrame Datafraame containing the images of the previous successful run (used in add image mode). required id_incr_par_assoc int An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. 0 parallel bool Whether parallel association is being used. False Returns: Type Description DataFrame The output sources_df containing all input measurements along with the association and relation information. Exceptions: Type Description Exception Raised if association method is not valid. Source code in vast_pipeline/pipeline/association.py def association ( images_df : pd . DataFrame , limit : Angle , dr_limit : float , bw_limit : float , duplicate_limit : Angle , config , add_mode : bool , previous_parquets : Dict [ str , str ], done_images_df : pd . DataFrame , id_incr_par_assoc : int = 0 , parallel : bool = False ) -> pd . DataFrame : ''' The main association function that does the common tasks between basic and advanced modes. Args: images_df: The input images to be associated. limit: The association limit to use (applies to basic and advanced only). dr_limit: The de Ruiter radius limit to use (applies to de ruiter only). bw_limit: The beamwidth limit to use (applies to de ruiter only). duplicate_limit: The limit of separation for which a measurement is considered to be a duplicate (epoch based association). config: The pipeline configuration object. add_mode: Whether the pipeline is currently being run in add image mode. previous_parquets: Dictionary containing the paths of the previous successful run parquet files (used in add image mode). done_images_df: Datafraame containing the images of the previous successful run (used in add image mode). id_incr_par_assoc: An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. parallel: Whether parallel association is being used. Returns: The output sources_df containing all input measurements along with the association and relation information. Raises: Exception: Raised if association method is not valid. ''' timer = StopWatch () if parallel : images_df = ( images_df . sort_values ( by = 'image_datetime' ) . drop ( 'image_datetime' , axis = 1 ) ) if 'skyreg_group' in images_df . columns : skyreg_group = images_df [ 'skyreg_group' ] . iloc [ 0 ] skyreg_tag = \" (sky region group %s )\" % skyreg_group else : skyreg_group = - 1 skyreg_tag = \"\" method = config . ASSOCIATION_METHOD logger . info ( 'Starting association %s .' , skyreg_tag ) logger . info ( 'Association mode selected: %s .' , method ) unique_epochs = np . sort ( images_df [ 'epoch' ] . unique ()) if add_mode : # Here the skyc1_srcs and sources_df are recreated and the done images # are filtered out. image_mask = images_df [ 'image_name' ] . isin ( done_images_df [ 'name' ]) images_df_done = images_df [ image_mask ] . copy () sources_df , skyc1_srcs = reconstruct_associtaion_dfs ( images_df_done , previous_parquets ) images_df = images_df . loc [ ~ image_mask ] if images_df . empty : logger . info ( 'No new images found, stopping association %s .' , skyreg_tag ) sources_df [ 'interim_ew' ] = ( sources_df [ 'ra_source' ] . values * sources_df [ 'weight_ew' ] . values ) sources_df [ 'interim_ns' ] = ( sources_df [ 'dec_source' ] . values * sources_df [ 'weight_ns' ] . values ) return ( sources_df . drop ([ 'ra' , 'dec' ], axis = 1 ) . rename ( columns = { 'ra_source' : 'ra' , 'dec_source' : 'dec' }) ) logger . info ( f 'Found { images_df . shape [ 0 ] } images to add to the run { skyreg_tag } .' ) # re-get the unique epochs unique_epochs = np . sort ( images_df [ 'epoch' ] . unique ()) start_epoch = 0 else : # Do full set up for a new run. first_images = ( images_df . loc [ images_df [ 'epoch' ] == unique_epochs [ 0 ], 'image_dj' ] . to_list () ) # initialise sky source dataframe skyc1_srcs = prep_skysrc_df ( first_images , config . FLUX_PERC_ERROR , duplicate_limit , ini_df = True ) skyc1_srcs [ 'epoch' ] = unique_epochs [ 0 ] # create base catalogue # initialise the sources dataframe using first image as base sources_df = skyc1_srcs . copy () start_epoch = 1 if unique_epochs . shape [ 0 ] == 1 and not add_mode : # This means only one image is present - or one group of images (epoch # mode) - so the same approach as above in add mode where there are no # images to be added, the interim needs to be calculated and skyc1_srcs # can just be returned as sources_df. ra_source and dec_source can just # be dropped as the ra and dec are already the average values. logger . warning ( 'No images to associate with! %s .' , skyreg_tag ) logger . info ( 'Returning base sources only %s .' , skyreg_tag ) # reorder the columns to match Dask expectations (parallel) skyc1_srcs = skyc1_srcs [[ 'id' , 'uncertainty_ew' , 'weight_ew' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image' , 'datetime' , 'source' , 'ra' , 'dec' , 'ra_source' , 'dec_source' , 'd2d' , 'dr' , 'related' , 'epoch' , ]] skyc1_srcs [ 'interim_ew' ] = ( skyc1_srcs [ 'ra' ] . values * skyc1_srcs [ 'weight_ew' ] . values ) skyc1_srcs [ 'interim_ns' ] = ( skyc1_srcs [ 'dec' ] . values * skyc1_srcs [ 'weight_ns' ] . values ) return skyc1_srcs . drop ([ 'ra_source' , 'dec_source' ], axis = 1 ) skyc1 = SkyCoord ( skyc1_srcs [ 'ra' ] . values , skyc1_srcs [ 'dec' ] . values , unit = ( u . deg , u . deg ) ) for it , epoch in enumerate ( unique_epochs [ start_epoch :]): logger . info ( 'Association iteration: # %i%s ' , it + 1 , skyreg_tag ) # load skyc2 source measurements and create SkyCoord images = ( images_df . loc [ images_df [ 'epoch' ] == epoch , 'image_dj' ] . to_list () ) max_beam_maj = ( images_df . loc [ images_df [ 'epoch' ] == epoch , 'image_dj' ] . apply ( lambda x : x . beam_bmaj ) . max () ) skyc2_srcs = prep_skysrc_df ( images , config . FLUX_PERC_ERROR , duplicate_limit ) skyc2_srcs [ 'epoch' ] = epoch skyc2 = SkyCoord ( skyc2_srcs [ 'ra' ] . values , skyc2_srcs [ 'dec' ] . values , unit = ( u . deg , u . deg ) ) if method == 'basic' : sources_df , skyc1_srcs = basic_association ( sources_df , skyc1_srcs , skyc1 , skyc2_srcs , skyc2 , limit , id_incr_par_assoc ) elif method in [ 'advanced' , 'deruiter' ]: if method == 'deruiter' : bw_max = Angle ( bw_limit * ( max_beam_maj * 3600. / 2. ) * u . arcsec ) else : bw_max = limit sources_df , skyc1_srcs = advanced_association ( method , sources_df , skyc1_srcs , skyc1 , skyc2_srcs , skyc2 , dr_limit , bw_max , id_incr_par_assoc ) else : raise Exception ( 'association method not implemented!' ) logger . info ( 'Calculating weighted average RA and Dec for sources %s ...' , skyreg_tag ) # account for RA wrapping ra_wrap_mask = sources_df . ra <= 0.1 sources_df [ 'ra_wrap' ] = sources_df . ra . values sources_df . at [ ra_wrap_mask , 'ra_wrap' ] = sources_df [ ra_wrap_mask ] . ra . values + 360. sources_df [ 'interim_ew' ] = ( sources_df [ 'ra_wrap' ] . values * sources_df [ 'weight_ew' ] . values ) sources_df [ 'interim_ns' ] = ( sources_df [ 'dec' ] . values * sources_df [ 'weight_ns' ] . values ) sources_df = sources_df . drop ([ 'ra_wrap' ], axis = 1 ) tmp_srcs_df = ( sources_df . loc [ ( sources_df [ 'source' ] != - 1 ) & ( sources_df [ 'forced' ] == False ), [ 'ra' , 'dec' , 'uncertainty_ew' , 'uncertainty_ns' , 'source' , 'interim_ew' , 'interim_ns' , 'weight_ew' , 'weight_ns' ] ] . groupby ( 'source' ) ) stats = StopWatch () wm_ra = tmp_srcs_df [ 'interim_ew' ] . sum () / tmp_srcs_df [ 'weight_ew' ] . sum () wm_uncertainty_ew = 1. / np . sqrt ( tmp_srcs_df [ 'weight_ew' ] . sum ()) wm_dec = tmp_srcs_df [ 'interim_ns' ] . sum () / tmp_srcs_df [ 'weight_ns' ] . sum () wm_uncertainty_ns = 1. / np . sqrt ( tmp_srcs_df [ 'weight_ns' ] . sum ()) weighted_df = ( pd . concat ( [ wm_ra , wm_uncertainty_ew , wm_dec , wm_uncertainty_ns ], axis = 1 , sort = False ) . reset_index () . rename ( columns = { 0 : 'ra' , 'weight_ew' : 'uncertainty_ew' , 1 : 'dec' , 'weight_ns' : 'uncertainty_ns' }) ) # correct the RA wrapping ra_wrap_mask = weighted_df . ra >= 360. weighted_df . at [ ra_wrap_mask , 'ra' ] = weighted_df [ ra_wrap_mask ] . ra . values - 360. logger . debug ( 'Groupby concat time %f ' , stats . reset ()) logger . info ( 'Finalising base sources catalogue ready for next iteration %s ...' , skyreg_tag ) # merge the weighted ra and dec and replace the values skyc1_srcs = skyc1_srcs . merge ( weighted_df , on = 'source' , how = 'left' , suffixes = ( '' , '_skyc2' ) ) del tmp_srcs_df , weighted_df skyc1_srcs [ 'ra' ] = skyc1_srcs [ 'ra_skyc2' ] skyc1_srcs [ 'dec' ] = skyc1_srcs [ 'dec_skyc2' ] skyc1_srcs [ 'uncertainty_ew' ] = skyc1_srcs [ 'uncertainty_ew_skyc2' ] skyc1_srcs [ 'uncertainty_ns' ] = skyc1_srcs [ 'uncertainty_ns_skyc2' ] skyc1_srcs = skyc1_srcs . drop ( [ 'ra_skyc2' , 'dec_skyc2' , 'uncertainty_ew_skyc2' , 'uncertainty_ns_skyc2' ], axis = 1 ) # generate new sky coord ready for next iteration skyc1 = SkyCoord ( skyc1_srcs [ 'ra' ] . values , skyc1_srcs [ 'dec' ] . values , unit = ( u . deg , u . deg ) ) # and update relations in skyc1 skyc1_srcs = skyc1_srcs . drop ( 'related' , axis = 1 ) relations_unique = pd . DataFrame ( sources_df [ sources_df [ 'related' ] . notna ()] . explode ( 'related' ) . groupby ( 'source' )[ 'related' ] . apply ( lambda x : x . unique () . tolist ()) ) skyc1_srcs = skyc1_srcs . merge ( relations_unique , how = 'left' , left_on = 'source' , right_index = True ) logger . info ( 'Association iteration # %i complete %s .' , it + 1 , skyreg_tag ) # End of iteration over images, ra and dec columns are actually the # average over each iteration so remove ave ra and ave dec used for # calculation and use ra_source and dec_source columns sources_df = ( sources_df . drop ([ 'ra' , 'dec' ], axis = 1 ) . rename ( columns = { 'ra_source' : 'ra' , 'dec_source' : 'dec' }) ) del skyc1_srcs , skyc2_srcs logger . info ( 'Total association time: %.2f seconds %s .' , timer . reset_init (), skyreg_tag ) return sources_df","title":"association()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.basic_association","text":"The loop for basic source association that uses the astropy 'match_to_catalog_sky' function (i.e. only the nearest match between the catalogs). A direct on sky separation is used to define the association. Parameters: Name Type Description Default sources_df DataFrame The dataframe containing all current measurements along with their association source and relations. required skyc1_srcs DataFrame The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. required skyc1 SkyCoord A SkyCoord object with the weighted average sky positions from skyc1_srcs. required skyc2_srcs DataFrame The same structure as sources_df containing the measurements to be associated. required skyc2 SkyCoord A SkyCoord object with the sky positions from skyc2_srcs. required limit Angle The association limit to use (applies to basic and advanced only). required id_incr_par_assoc int An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. Source code in vast_pipeline/pipeline/association.py def basic_association ( sources_df : pd . DataFrame , skyc1_srcs : pd . DataFrame , skyc1 : SkyCoord , skyc2_srcs : pd . DataFrame , skyc2 : SkyCoord , limit : Angle , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: ''' The loop for basic source association that uses the astropy 'match_to_catalog_sky' function (i.e. only the nearest match between the catalogs). A direct on sky separation is used to define the association. Args: sources_df: The dataframe containing all current measurements along with their association source and relations. skyc1_srcs: The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. skyc1: A SkyCoord object with the weighted average sky positions from skyc1_srcs. skyc2_srcs: The same structure as sources_df containing the measurements to be associated. skyc2: A SkyCoord object with the sky positions from skyc2_srcs. limit: The association limit to use (applies to basic and advanced only). id_incr_par_assoc: An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. Returns: The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. ''' # match the new sources to the base # idx gives the index of the closest match in the base for skyc2 idx , d2d , d3d = skyc2 . match_to_catalog_sky ( skyc1 ) # acceptable selection sel = d2d <= limit # The good matches can be assinged the src id from base skyc2_srcs . loc [ sel , 'source' ] = skyc1_srcs . loc [ idx [ sel ], 'source' ] . values # Need the d2d to make analysing doubles easier. skyc2_srcs . loc [ sel , 'd2d' ] = d2d [ sel ] . arcsec # must check for double matches in the acceptable matches just made # this would mean that multiple sources in skyc2 have been matched # to the same base source we want to keep closest match and move # the other match(es) back to having a -1 src id skyc2_srcs , sources_df = one_to_many_basic ( skyc2_srcs , sources_df , id_incr_par_assoc ) logger . info ( 'Updating sources catalogue with new sources...' ) # update the src numbers for those sources in skyc2 with no match # using the max current src as the start and incrementing by one start_elem = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc nan_sel = ( skyc2_srcs [ 'source' ] == - 1 ) . values skyc2_srcs . loc [ nan_sel , 'source' ] = ( np . arange ( start_elem , start_elem + skyc2_srcs . loc [ nan_sel ] . shape [ 0 ], dtype = int ) ) # and skyc2 is now ready to be appended to new sources sources_df = sources_df . append ( skyc2_srcs , ignore_index = True ) . reset_index ( drop = True ) # and update skyc1 with the sources that were created from the one # to many relations and any new sources. skyc1_srcs = skyc1_srcs . append ( skyc2_srcs [ ~ skyc2_srcs [ 'source' ] . isin ( skyc1_srcs [ 'source' ]) ], ignore_index = True ) . reset_index ( drop = True ) return sources_df , skyc1_srcs","title":"basic_association()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.calc_de_ruiter","text":"Calculates the unitless 'de Ruiter' radius of the association. Works on the 'temp_df' dataframe of the advanced association, where the two sources associated with each other have been merged into one row. Parameters: Name Type Description Default df DataFrame The 'temp_df' from advanced association. It must contain the columns ra_skyc1 , 'ra_skyc2', 'uncertainty_ew_skyc1', 'uncertainty_ew_skyc2', 'dec_skyc1', 'dec_skyc2', 'uncertainty_ns_skyc1' and 'uncertainty_ns_skyc2'. required Returns: Type Description ndarray Array containing the de Ruiter radius for all rows in the df. Source code in vast_pipeline/pipeline/association.py def calc_de_ruiter ( df : pd . DataFrame ) -> np . ndarray : \"\"\" Calculates the unitless 'de Ruiter' radius of the association. Works on the 'temp_df' dataframe of the advanced association, where the two sources associated with each other have been merged into one row. Args: df: The 'temp_df' from advanced association. It must contain the columns `ra_skyc1`, 'ra_skyc2', 'uncertainty_ew_skyc1', 'uncertainty_ew_skyc2', 'dec_skyc1', 'dec_skyc2', 'uncertainty_ns_skyc1' and 'uncertainty_ns_skyc2'. Returns: Array containing the de Ruiter radius for all rows in the df. \"\"\" ra_1 = df [ 'ra_skyc1' ] . values ra_2 = df [ 'ra_skyc2' ] . values # avoid wrapping issues ra_1 [ ra_1 > 270. ] -= 180. ra_2 [ ra_2 > 270. ] -= 180. ra_1 [ ra_1 < 90. ] += 180. ra_2 [ ra_2 < 90. ] += 180. ra_1 = np . deg2rad ( ra_1 ) ra_2 = np . deg2rad ( ra_2 ) ra_1_err = np . deg2rad ( df [ 'uncertainty_ew_skyc1' ] . values ) ra_2_err = np . deg2rad ( df [ 'uncertainty_ew_skyc2' ] . values ) dec_1 = np . deg2rad ( df [ 'dec_skyc1' ] . values ) dec_2 = np . deg2rad ( df [ 'dec_skyc2' ] . values ) dec_1_err = np . deg2rad ( df [ 'uncertainty_ns_skyc1' ] . values ) dec_2_err = np . deg2rad ( df [ 'uncertainty_ns_skyc2' ] . values ) dr1 = ( ra_1 - ra_2 ) * ( ra_1 - ra_2 ) dr1_1 = np . cos (( dec_1 + dec_2 ) / 2. ) dr1 *= dr1_1 * dr1_1 dr1 /= ra_1_err * ra_1_err + ra_2_err * ra_2_err dr2 = ( dec_1 - dec_2 ) * ( dec_1 - dec_2 ) dr2 /= dec_1_err * dec_1_err + dec_2_err * dec_2_err dr = np . sqrt ( dr1 + dr2 ) return dr","title":"calc_de_ruiter()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.many_to_many_advanced","text":"Finds and processes the many-to-many associations in the advanced association. We do not want to build many-to-many associations as this will make the database get very large (see TraP documentation). The skyc2 sources which are listed more than once are found, and of these, those which have a skyc1 source association which is also listed twice in the associations are selected. The closest (by limit or de Ruiter radius, depending on the method) is kept where as the other associations are dropped. This follows the same logic used by the TraP (see TraP documentation). Parameters: Name Type Description Default temp_srcs DataFrame The temporary associtation dataframe used through the advanced association process. required method str Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. required Returns: Type Description DataFrame Updated temp_srcs with the many_to_many relations dropped. Source code in vast_pipeline/pipeline/association.py def many_to_many_advanced ( temp_srcs : pd . DataFrame , method : str ) -> pd . DataFrame : ''' Finds and processes the many-to-many associations in the advanced association. We do not want to build many-to-many associations as this will make the database get very large (see TraP documentation). The skyc2 sources which are listed more than once are found, and of these, those which have a skyc1 source association which is also listed twice in the associations are selected. The closest (by limit or de Ruiter radius, depending on the method) is kept where as the other associations are dropped. This follows the same logic used by the TraP (see TraP documentation). Args: temp_srcs: The temporary associtation dataframe used through the advanced association process. method: Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. Returns: Updated temp_srcs with the many_to_many relations dropped. ''' # Select those where the extracted source is listed more than once # (e.g. index_old_skyc2 duplicated values) and of these get those that # have a source id that is listed more than once (e.g. source_skyc1 # duplicated values) in the temps_srcs df m_to_m = temp_srcs [( temp_srcs [ 'index_old_skyc2' ] . duplicated ( keep = False ) & temp_srcs [ 'source_skyc1' ] . duplicated ( keep = False ) )] . copy () if m_to_m . empty : logger . debug ( 'No many-to-many assocations.' ) return temp_srcs logger . debug ( 'Detected # %i many-to-many assocations, cleaning...' , m_to_m . shape [ 0 ] ) dist_col = 'd2d_skyc2' if method == 'advanced' else 'dr' min_col = 'min_' + dist_col # get the minimum de ruiter value for each extracted source m_to_m [ min_col ] = ( m_to_m . groupby ( 'index_old_skyc2' )[ dist_col ] . transform ( 'min' ) ) # get the ids of those crossmatches that are larger than the minimum m_to_m_to_drop = m_to_m [ m_to_m [ dist_col ] != m_to_m [ min_col ]] . index . values # and drop these from the temp_srcs temp_srcs = temp_srcs . drop ( m_to_m_to_drop ) return temp_srcs","title":"many_to_many_advanced()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.many_to_one_advanced","text":"Finds and processes the many-to-one associations in the advanced association. In this case in the related column of the 'many' sources we need to append the ids of all the other 'many' (expect for itself). Parameters: Name Type Description Default temp_srcs DataFrame The temporary associtation dataframe used through the advanced association process. required Returns: Type Description DataFrame Updated temp_srcs with all many_to_one relation information added. Source code in vast_pipeline/pipeline/association.py def many_to_one_advanced ( temp_srcs : pd . DataFrame ) -> pd . DataFrame : ''' Finds and processes the many-to-one associations in the advanced association. In this case in the related column of the 'many' sources we need to append the ids of all the other 'many' (expect for itself). Args: temp_srcs: The temporary associtation dataframe used through the advanced association process. Returns: Updated temp_srcs with all many_to_one relation information added. ''' # use only these columns for easy debugging of the dataframe cols = [ 'index_old_skyc1' , 'id_skyc1' , 'source_skyc1' , 'd2d_skyc1' , 'related_skyc1' , 'index_old_skyc2' , 'id_skyc2' , 'source_skyc2' , 'd2d_skyc2' , 'dr' ] # select those sources which have been matched to the same measurement # in the sky catalogue 2. duplicated_skyc2 = temp_srcs . loc [ temp_srcs [ 'index_old_skyc2' ] . duplicated ( keep = False ), cols ] # duplicated_skyc2 # +-----+-------------------+------------+----------------+------------- # | | index_old_skyc1 | id_skyc1 | source_skyc1 | d2d_skyc1 # |-----+-------------------+------------+----------------+------------- # | 447 | 477 | 478 | 478 | 0 # | 448 | 478 | 479 | 479 | 0 # | 477 | 507 | 508 | 508 | 0 # | 478 | 508 | 509 | 509 | 0 # | 695 | 738 | 739 | 739 | 0 # +-----+-------------------+------------+----------------+------------- # +-----------------+-------------------+------------+----------------+ # | related_skyc1 | index_old_skyc2 | id_skyc2 | source_skyc2 | # +-----------------+-------------------+------------+----------------+ # | | 305 | 5847 | -1 | # | | 305 | 5847 | -1 | # | | 648 | 6190 | -1 | # | | 648 | 6190 | -1 | # | | 561 | 6103 | -1 | # +-----------------+-------------------+------------+----------------+ # -------------+------+ # d2d_skyc2 | dr | # -------------+------| # 8.63598 | 0 | # 8.63598 | 0 | # 6.5777 | 0 | # 6.5777 | 0 | # 7.76527 | 0 | # -------------+------+ # if there are none no action is required. if duplicated_skyc2 . empty : logger . debug ( 'No many-to-one associations.' ) return temp_srcs logger . debug ( 'Detected # %i many-to-one associations' , duplicated_skyc2 . shape [ 0 ] ) # The new relations become that for each 'many' source we need to append # the ids of the other 'many' sources that have been associationed with the # 'one'. Below for each 'one' group we gather all the ids of the many # sources. new_relations = pd . DataFrame ( duplicated_skyc2 . groupby ( 'index_old_skyc2' ) . apply ( lambda grp : grp [ 'source_skyc1' ] . tolist ()) ) . rename ( columns = { 0 : 'new_relations' }) # new_relations # +-------------------+-----------------+ # | index_old_skyc2 | new_relations | # |-------------------+-----------------| # | 305 | [478, 479] | # | 561 | [739, 740] | # | 648 | [508, 509] | # | 764 | [841, 842] | # | 816 | [1213, 1215] | # +-------------------+-----------------+ # these new relations are then added to the duplciated dataframe so # they can easily be used by the next function. duplicated_skyc2 = duplicated_skyc2 . merge ( new_relations , left_on = 'index_old_skyc2' , right_index = True , how = 'left' ) # Remove the 'self' relations. The 'x['source_skyc1']' is an integer so it # is placed within a list notation, [], to be able to be easily subtracted # from the new_relations. duplicated_skyc2 [ 'new_relations' ] = ( duplicated_skyc2 . apply ( lambda x : list ( set ( x [ 'new_relations' ]) - set ([ x [ 'source_skyc1' ]])), axis = 1 ) ) # Use the 'add_new_many_to_one_relations' method to add tthe new relatitons # to the actual `related_skyc1' column. duplicated_skyc2 [ 'related_skyc1' ] = ( duplicated_skyc2 . apply ( add_new_many_to_one_relations , axis = 1 ) ) # Transfer the new relations from the duplicated df to the temp_srcs. The # index is explicitly declared to avoid any mixups. temp_srcs . loc [ duplicated_skyc2 . index . values , 'related_skyc1' ] = duplicated_skyc2 . loc [ duplicated_skyc2 . index . values , 'related_skyc1' ] . values return temp_srcs","title":"many_to_one_advanced()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.one_to_many_advanced","text":"Finds and processes the one-to-many associations in the advanced association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the basic version as the data products between the two are different. Parameters: Name Type Description Default temp_srcs DataFrame The temporary associtation dataframe used through the advanced association process. required sources_df DataFrame The sources_df produced by each step of association holding the current 'sources'. required method str Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. required id_incr_par_assoc int An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] Updated temp_srcs and sources_df with all one_to_many relation information added. Source code in vast_pipeline/pipeline/association.py def one_to_many_advanced ( temp_srcs : pd . DataFrame , sources_df : pd . DataFrame , method : str , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: ''' Finds and processes the one-to-many associations in the advanced association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the basic version as the data products between the two are different. Args: temp_srcs: The temporary associtation dataframe used through the advanced association process. sources_df: The sources_df produced by each step of association holding the current 'sources'. method: Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. id_incr_par_assoc: An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association Returns: Updated temp_srcs and sources_df with all one_to_many relation information added. ''' # use only these columns for easy debugging of the dataframe cols = [ 'index_old_skyc1' , 'id_skyc1' , 'source_skyc1' , 'd2d_skyc1' , 'related_skyc1' , 'index_old_skyc2' , 'id_skyc2' , 'source_skyc2' , 'd2d_skyc2' , 'dr' ] duplicated_skyc1 = temp_srcs . loc [ temp_srcs [ 'source_skyc1' ] . duplicated ( keep = False ), cols ] . copy () # duplicated_skyc1 # +-----+-------------------+------------+----------------+-------------+ # | | index_old_skyc1 | id_skyc1 | source_skyc1 | d2d_skyc1 | # |-----+-------------------+------------+----------------+-------------+ # | 117 | 121 | 122 | 122 | 0 | # | 118 | 121 | 122 | 122 | 0 | # | 238 | 253 | 254 | 254 | 0 | # | 239 | 253 | 254 | 254 | 0 | # | 246 | 261 | 262 | 262 | 0 | # +-----+-------------------+------------+----------------+-------------+ # -----------------+-------------------+------------+----------------+ # related_skyc1 | index_old_skyc2 | id_skyc2 | source_skyc2 | # -----------------+-------------------+------------+----------------+ # | 526 | 6068 | -1 | # | 528 | 6070 | -1 | # | 264 | 5806 | -1 | # | 265 | 5807 | -1 | # | 327 | 5869 | -1 | # -----------------+-------------------+------------+----------------+ # -------------+------+ # d2d_skyc2 | dr | # -------------+------| # 3.07478 | 0 | # 6.41973 | 0 | # 2.04422 | 0 | # 6.16881 | 0 | # 3.20439 | 0 | # -------------+------+ # If no relations then no action is required if duplicated_skyc1 . empty : logger . debug ( 'No one-to-many associations.' ) return temp_srcs , sources_df logger . debug ( 'Detected # %i one-to-many assocations, cleaning...' , duplicated_skyc1 . shape [ 0 ] ) # Get the column to check for the minimum depending on the method # set the column names needed for filtering the 'to-many' # associations depending on the method (advanced or deruiter) dist_col = 'd2d_skyc2' if method == 'advanced' else 'dr' # go through the doubles and # 1. Keep the closest d2d or de ruiter as the primary id # 2. Increment a new source id for others # 3. Add a copy of the previously matched # source into sources. # multi_srcs = duplicated_skyc1['source_skyc1'].unique() # Get the duplicated, sort by the distance column duplicated_skyc1 = duplicated_skyc1 . sort_values ( by = [ 'source_skyc1' , dist_col ] ) # Get those that need to be given a new ID number (i.e. not the min dist_col) idx_to_change = duplicated_skyc1 . index . values [ duplicated_skyc1 . duplicated ( 'source_skyc1' ) ] # Create a new `new_source_id` column to store the 'correct' IDs duplicated_skyc1 [ 'new_source_id' ] = duplicated_skyc1 [ 'source_skyc1' ] # +-----------------+ # | new_source_id | # +-----------------| # | 122 | # | 122 | # | 254 | # | 254 | # | 262 | # +-----------------+ # Define the range of new source ids start_new_src_id = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc # Create an arange to use to change the ones that need to be changed. new_source_ids = np . arange ( start_new_src_id , start_new_src_id + idx_to_change . shape [ 0 ], dtype = int ) # Assign the new IDs to those that need to be changed. duplicated_skyc1 . loc [ idx_to_change , 'new_source_id' ] = new_source_ids # Now we need to sort out the related, essentially here the 'original' # and 'non original' need to be treated differently. # The original source need all the assoicated new ids appended to the # related column. # The not_original ones need just the original ID appended. not_original = duplicated_skyc1 . loc [ idx_to_change ] . copy () original = duplicated_skyc1 . drop_duplicates ( 'source_skyc1' ) . copy () # This gathers all the new ids that need to be appended # to the original related column. new_original_related = pd . DataFrame ( not_original [ [ 'source_skyc1' , 'new_source_id' ] ] . groupby ( 'source_skyc1' ) . apply ( lambda grp : grp [ 'new_source_id' ] . tolist () ) ) #new_original_related # +----------------+--------+ # | source_skyc1 | 0 | # |----------------+--------| # | 122 | [5542] | # | 254 | [5543] | # | 262 | [5544] | # | 405 | [5545] | # | 656 | [5546] | # +----------------+--------+ # Append the relations in each case, using the above 'new_original_related' # for the original ones. # The not original only require the appending of the original index. original [ 'related_skyc1' ] = ( original [[ 'related_skyc1' , 'source_skyc1' ]] . apply ( add_new_one_to_many_relations , args = ( True , new_original_related ), axis = 1 ) ) # what the column looks like after the above # +-----------------+ # | related_skyc1 | # +-----------------+ # | [5542] | # | [5543] | # | [5544] | # | [5545] | # | [5546] | # +-----------------+ not_original . loc [:, 'related_skyc1' ] = not_original . apply ( add_new_one_to_many_relations , args = ( True ,), axis = 1 ) # Merge them back together duplicated_skyc1 = original . append ( not_original ) del original , not_original # Apply the updates to the actual temp_srcs. temp_srcs . loc [ idx_to_change , 'source_skyc1' ] = new_source_ids temp_srcs . loc [ duplicated_skyc1 . index . values , 'related_skyc1' ] = duplicated_skyc1 . loc [ duplicated_skyc1 . index . values , 'related_skyc1' ] . values # Finally we need to create copies of the previous sources in the # sources_df to complete the new sources. # To do this we get only the non-original sources duplicated_skyc1 = duplicated_skyc1 . loc [ duplicated_skyc1 . duplicated ( 'source_skyc1' ) ] # Get all the indexes required for each original # `source_skyc1` value source_df_index_to_copy = pd . DataFrame ( duplicated_skyc1 . groupby ( 'source_skyc1' ) . apply ( lambda grp : sources_df [ sources_df [ 'source' ] == grp . name ] . index . values . tolist () ) ) # source_df_index_to_copy # +----------------+-------+ # | source_skyc1 | 0 | # |----------------+-------| # | 122 | [121] | # | 254 | [253] | # | 262 | [261] | # | 405 | [404] | # | 656 | [655] | # +----------------+-------+ # merge these so it's easy to explode and copy the index values. duplicated_skyc1 = ( duplicated_skyc1 . loc [:,[ 'source_skyc1' , 'new_source_id' ]] . merge ( source_df_index_to_copy , left_on = 'source_skyc1' , right_index = True , how = 'left' ) . rename ( columns = { 0 : 'source_index' }) . explode ( 'source_index' ) ) # duplicated_skyc1 # +-----+----------------+-----------------+----------------+ # | | source_skyc1 | new_source_id | source_index | # |-----+----------------+-----------------+----------------| # | 118 | 122 | 5542 | 121 | # | 239 | 254 | 5543 | 253 | # | 247 | 262 | 5544 | 261 | # | 380 | 405 | 5545 | 404 | # | 615 | 656 | 5546 | 655 | # +-----+----------------+-----------------+----------------+ # Get the sources sources_to_copy = sources_df . loc [ duplicated_skyc1 [ 'source_index' ] . values ] # Apply the new_source_id sources_to_copy [ 'source' ] = duplicated_skyc1 [ 'new_source_id' ] . values # and finally append. sources_df = sources_df . append ( sources_to_copy , ignore_index = True ) return temp_srcs , sources_df","title":"one_to_many_advanced()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.one_to_many_basic","text":"Finds and processes the one-to-many associations in the basic association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the advanced version as the data products between the two are different. Parameters: Name Type Description Default skyc2_srcs DataFrame The sky catalogue 2 sources (i.e. the sources being associated to the base) used during basic association. required sources_df DataFrame The sources_df produced by each step of association holding the current 'sources'. required id_incr_par_assoc int An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] Tuple containging the updated 'skyc2_srcs' and 'sources_df' with all one_to_many relation information added. Source code in vast_pipeline/pipeline/association.py def one_to_many_basic ( skyc2_srcs : pd . DataFrame , sources_df : pd . DataFrame , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Finds and processes the one-to-many associations in the basic association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the advanced version as the data products between the two are different. Args: skyc2_srcs: The sky catalogue 2 sources (i.e. the sources being associated to the base) used during basic association. sources_df: The sources_df produced by each step of association holding the current 'sources'. id_incr_par_assoc: An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association Returns: Tuple containging the updated 'skyc2_srcs' and 'sources_df' with all one_to_many relation information added. \"\"\" # select duplicated in 'source' field in skyc2_srcs, excluding -1 duplicated_skyc2 = skyc2_srcs . loc [ ( skyc2_srcs [ 'source' ] != - 1 ) & skyc2_srcs [ 'source' ] . duplicated ( keep = False ), [ 'source' , 'related' , 'd2d' ] ] # duplicated_skyc2 # +-----+----------+-----------+---------+ # | | source | related | d2d | # |-----+----------+-----------+---------| # | 264 | 254 | | 2.04422 | # | 265 | 254 | | 6.16881 | # | 327 | 262 | | 3.20439 | # | 328 | 262 | | 3.84425 | # | 526 | 122 | | 3.07478 | # +-----+----------+-----------+---------+ if duplicated_skyc2 . empty : logger . debug ( 'No one-to-many associations.' ) return skyc2_srcs , sources_df logger . info ( 'Detected # %i double matches, cleaning...' , duplicated_skyc2 . shape [ 0 ] ) # now we have the src values which are doubled. # make the nearest match have the \"original\" src id # give the other matched source a new src id # and make sure to copy the other previously # matched sources. # Get the duplicated, sort by the distance column duplicated_skyc2 = duplicated_skyc2 . sort_values ( by = [ 'source' , 'd2d' ]) # Get those that need to be given a new ID number (i.e. not the min dist_col) idx_to_change = duplicated_skyc2 . index . values [ duplicated_skyc2 . duplicated ( 'source' ) ] # Create a new `new_source_id` column to store the 'correct' IDs duplicated_skyc2 [ 'new_source_id' ] = duplicated_skyc2 [ 'source' ] # Define the range of new source ids start_new_src_id = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc new_source_ids = np . arange ( start_new_src_id , start_new_src_id + idx_to_change . shape [ 0 ], dtype = int ) # Assign the new IDs duplicated_skyc2 . loc [ idx_to_change , 'new_source_id' ] = new_source_ids # duplicated_skyc2 # +-----+----------+-----------+---------+-----------------+ # | | source | related | d2d | new_source_id | # |-----+----------+-----------+---------+-----------------| # | 526 | 122 | | 3.07478 | 122 | # | 528 | 122 | | 6.41973 | 5542 | # | 264 | 254 | | 2.04422 | 254 | # | 265 | 254 | | 6.16881 | 5543 | # | 327 | 262 | | 3.20439 | 262 | # +-----+----------+-----------+---------+-----------------+ # Now we need to sort out the related, essentially here the 'original' # and 'non original' need to be treated differently. # The original source need all the assoicated new ids appended to the # related column. # The not_original ones need just the original ID appended. # copy() is used here to avoid chained indexing (set with copy warnings) not_original = duplicated_skyc2 . loc [ idx_to_change ] . copy () original = duplicated_skyc2 . drop_duplicates ( 'source' ) . copy () new_original_related = pd . DataFrame ( not_original [ [ 'source' , 'new_source_id' ] ] . groupby ( 'source' ) . apply ( lambda grp : grp [ 'new_source_id' ] . tolist () ) ) # new_original_related # +----------+--------+ # | source | 0 | # |----------+--------| # | 122 | [5542] | # | 254 | [5543] | # | 262 | [5544] | # | 405 | [5545] | # | 656 | [5546] | # +----------+--------+ # Append the relations in each case, using the above 'new_original_related' # for the original ones. # The not original only require the appending of the original index. original [ 'related' ] = ( original [[ 'related' , 'source' ]] . apply ( add_new_one_to_many_relations , args = ( False , new_original_related ), axis = 1 ) ) not_original [ 'related' ] = not_original . apply ( add_new_one_to_many_relations , args = ( False ,), axis = 1 ) duplicated_skyc2 = original . append ( not_original ) # duplicated_skyc2 # +-----+----------+-----------+---------+-----------------+ # | | source | related | d2d | new_source_id | # |-----+----------+-----------+---------+-----------------| # | 526 | 122 | [5542] | 3.07478 | 122 | # | 264 | 254 | [5543] | 2.04422 | 254 | # | 327 | 262 | [5544] | 3.20439 | 262 | # | 848 | 405 | [5545] | 5.52865 | 405 | # | 695 | 656 | [5546] | 4.69094 | 656 | # +-----+----------+-----------+---------+-----------------+ del original , not_original # Apply the updates to the actual temp_srcs. skyc2_srcs . loc [ idx_to_change , 'source' ] = new_source_ids skyc2_srcs . loc [ duplicated_skyc2 . index . values , 'related' ] = duplicated_skyc2 . loc [ duplicated_skyc2 . index . values , 'related' ] . values # Finally we need to copy copies of the previous sources in the # sources_df to complete the new sources. # To do this we get only the non-original sources duplicated_skyc2 = duplicated_skyc2 . loc [ duplicated_skyc2 . duplicated ( 'source' ) ] # Get all the indexes required for each original # `source_skyc1` value source_df_index_to_copy = pd . DataFrame ( duplicated_skyc2 . groupby ( 'source' ) . apply ( lambda grp : sources_df [ sources_df [ 'source' ] == grp . name ] . index . values . tolist () ) ) # source_df_index_to_copy # +----------+-------+ # | source | 0 | # |----------+-------| # | 122 | [121] | # | 254 | [253] | # | 262 | [261] | # | 405 | [404] | # | 656 | [655] | # +----------+-------+ # merge these so it's easy to explode and copy the index values. duplicated_skyc2 = ( duplicated_skyc2 [[ 'source' , 'new_source_id' ]] . merge ( source_df_index_to_copy , left_on = 'source' , right_index = True , how = 'left' ) . rename ( columns = { 0 : 'source_index' }) . explode ( 'source_index' ) ) # Get the sources - all columns from the sources_df table sources_to_copy = sources_df . loc [ duplicated_skyc2 [ 'source_index' ] . values ] # Apply the new_source_id sources_to_copy [ 'source' ] = duplicated_skyc2 [ 'new_source_id' ] . values # and finally append. sources_df = sources_df . append ( sources_to_copy , ignore_index = True ) return skyc2_srcs , sources_df","title":"one_to_many_basic()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.parallel_association","text":"Launches association on different sky region groups in parallel using Dask. Parameters: Name Type Description Default images_df DataFrame Holds the images that are being processed. Also contains what sky region group the image belongs to. required limit Angle The association radius limit. required dr_limit float The de Ruiter radius limit. required bw_limit float The beamwidth limit. required duplicate_limit Angle The duplicate radius detection limit. required config module The pipeline config settings. required n_skyregion_groups int The number of sky region groups. required Returns: Type Description DataFrame pd.DataFrame: The combined association results of the parallel association with corrected source ids. Source code in vast_pipeline/pipeline/association.py def parallel_association ( images_df : pd . DataFrame , limit : Angle , dr_limit : float , bw_limit : float , duplicate_limit : Angle , # TODO update config typing. config , # a 'module` typing. n_skyregion_groups : int , add_mode : bool , previous_parquets : Dict [ str , str ], done_images_df : pd . DataFrame , done_source_ids : List [ int ] ) -> pd . DataFrame : \"\"\" Launches association on different sky region groups in parallel using Dask. Args: images_df: Holds the images that are being processed. Also contains what sky region group the image belongs to. limit: The association radius limit. dr_limit: The de Ruiter radius limit. bw_limit: The beamwidth limit. duplicate_limit: The duplicate radius detection limit. config (module): The pipeline config settings. n_skyregion_groups: The number of sky region groups. Returns: pd.DataFrame: The combined association results of the parallel association with corrected source ids. \"\"\" logger . info ( \"Running parallel association for %i sky region groups.\" , n_skyregion_groups ) timer = StopWatch () meta = { 'id' : 'i' , 'uncertainty_ew' : 'f' , 'weight_ew' : 'f' , 'uncertainty_ns' : 'f' , 'weight_ns' : 'f' , 'flux_int' : 'f' , 'flux_int_err' : 'f' , 'flux_int_isl_ratio' : 'f' , 'flux_peak' : 'f' , 'flux_peak_err' : 'f' , 'flux_peak_isl_ratio' : 'f' , 'forced' : '?' , 'compactness' : 'f' , 'has_siblings' : '?' , 'snr' : 'f' , 'image' : 'U' , 'datetime' : 'datetime64[ns]' , 'source' : 'i' , 'ra' : 'f' , 'dec' : 'f' , 'd2d' : 'f' , 'dr' : 'f' , 'related' : 'O' , 'epoch' : 'i' , 'interim_ew' : 'f' , 'interim_ns' : 'f' , } # Add an increment to any new source values when using add_mode to avoid # getting duplicates in the result laater id_incr_par_assoc = max ( done_source_ids ) if add_mode else 0 n_cpu = cpu_count () - 1 # pass each skyreg_group through the normal association process. results = ( dd . from_pandas ( images_df , n_cpu ) . groupby ( 'skyreg_group' ) . apply ( association , limit = limit , dr_limit = dr_limit , bw_limit = bw_limit , duplicate_limit = duplicate_limit , config = config , add_mode = add_mode , previous_parquets = previous_parquets , done_images_df = done_images_df , id_incr_par_assoc = id_incr_par_assoc , parallel = True , meta = meta ) . compute ( n_workers = n_cpu , scheduler = 'processes' ) ) # results are the normal dataframe of results with the columns: # 'id', 'uncertainty_ew', 'weight_ew', 'uncertainty_ns', 'weight_ns', # 'flux_int', 'flux_int_err', 'flux_peak', 'flux_peak_err', 'forced', # 'compactness', 'has_siblings', 'snr', 'image', 'datetime', 'source', # 'ra', 'dec', 'd2d', 'dr', 'related', 'epoch', 'interim_ew' and # 'interim_ns'. # The index however is now a multi index with the skyregion group and # a general result index. Hence the general result index is repeated for # each skyreg_group along with the source_ids. This needs to be collapsed # and the source id's corrected. # Index example: # id # skyreg_group # -------------------------- # 2 0 15640 # 1 15641 # 2 15642 # 3 15643 # 4 15644 # ... ... # 1 46975 53992 # 46976 54062 # 46977 54150 # 46978 54161 # 46979 54164 # Get the indexes (skyreg_groups) to loop over for source id correction indexes = results . index . levels [ 0 ] . values if add_mode : # Need to correct all skyreg_groups. # First get the starting id for new sources. new_id = max ( done_source_ids ) + 1 for i in indexes : corr_df , new_id = _correct_parallel_source_ids_add_mode ( results . loc [ i , [ 'source' , 'related' ]], done_source_ids , new_id ) results . loc [ ( i , slice ( None )), [ 'source' , 'related' ] ] = corr_df . values else : # The first index acts as the base, so the others are looped over and # corrected. for i , val in enumerate ( indexes ): # skip first one, makes the enumerate easier to deal with if i == 0 : continue # Get the maximum source ID from the previous group. max_id = results . loc [ indexes [ i - 1 ]] . source . max () # Run through the correction function, only the 'source' and # 'related' # columns are passed and returned (corrected). corr_df = _correct_parallel_source_ids ( results . loc [ val , [ 'source' , 'related' ]], max_id ) # replace the values in the results with the corrected source and # related values results . loc [ ( val , slice ( None )), [ 'source' , 'related' ] ] = corr_df . values del corr_df # reset the indeex of the final corrected and collapsed result results = results . reset_index ( drop = True ) logger . info ( 'Total parallel association time: %.2f seconds' , timer . reset_init () ) return results","title":"parallel_association()"},{"location":"reference/pipeline/errors/","text":"Defines errors for the pipeline to return. MaxPipelineRunsError \u00b6 Error for reporting the number of concurrent jobs is maxed out. PipelineConfigError \u00b6 Error for issue in the pipeline configuration __init__ ( self , msg = None ) special \u00b6 Initialises the config error. Parameters: Name Type Description Default msg The error message returned by the pipeline. None Source code in vast_pipeline/pipeline/errors.py def __init__ ( self , msg = None ): \"\"\" Initialises the config error. Args: msg: The error message returned by the pipeline. \"\"\" super ( PipelineConfigError , self ) . __init__ ( msg ) PipelineError \u00b6 Generic pipeline error Attributes: Name Type Description msg str The full error string to return. __init__ ( self , msg = None ) special \u00b6 Initialises the error. Parameters: Name Type Description Default msg str The error message returned by the pipeline. None Source code in vast_pipeline/pipeline/errors.py def __init__ ( self , msg : str = None ) -> None : \"\"\" Initialises the error. Args: msg: The error message returned by the pipeline. \"\"\" self . msg = ( 'Pipeline error: {0} .' . format ( msg ) if msg else 'Undefined Pipeline error.' ) __str__ ( self ) special \u00b6 Returns the string representation. Returns: Type Description str The string representation of the error. Source code in vast_pipeline/pipeline/errors.py def __str__ ( self ) -> str : \"\"\" Returns the string representation. Returns: The string representation of the error. \"\"\" return self . msg PipelineInitError \u00b6 Error for issue in the pipeline initialisation __init__ ( self , msg = None ) special \u00b6 Initialises the init error. Parameters: Name Type Description Default msg The error message returned by the pipeline. None Source code in vast_pipeline/pipeline/errors.py def __init__ ( self , msg = None ): \"\"\" Initialises the init error. Args: msg: The error message returned by the pipeline. \"\"\" super ( PipelineInitError , self ) . __init__ ( msg )","title":"errors.py"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.MaxPipelineRunsError","text":"Error for reporting the number of concurrent jobs is maxed out.","title":"MaxPipelineRunsError"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineConfigError","text":"Error for issue in the pipeline configuration","title":"PipelineConfigError"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineConfigError.__init__","text":"Initialises the config error. Parameters: Name Type Description Default msg The error message returned by the pipeline. None Source code in vast_pipeline/pipeline/errors.py def __init__ ( self , msg = None ): \"\"\" Initialises the config error. Args: msg: The error message returned by the pipeline. \"\"\" super ( PipelineConfigError , self ) . __init__ ( msg )","title":"__init__()"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineError","text":"Generic pipeline error Attributes: Name Type Description msg str The full error string to return.","title":"PipelineError"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineError.__init__","text":"Initialises the error. Parameters: Name Type Description Default msg str The error message returned by the pipeline. None Source code in vast_pipeline/pipeline/errors.py def __init__ ( self , msg : str = None ) -> None : \"\"\" Initialises the error. Args: msg: The error message returned by the pipeline. \"\"\" self . msg = ( 'Pipeline error: {0} .' . format ( msg ) if msg else 'Undefined Pipeline error.' )","title":"__init__()"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineError.__str__","text":"Returns the string representation. Returns: Type Description str The string representation of the error. Source code in vast_pipeline/pipeline/errors.py def __str__ ( self ) -> str : \"\"\" Returns the string representation. Returns: The string representation of the error. \"\"\" return self . msg","title":"__str__()"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineInitError","text":"Error for issue in the pipeline initialisation","title":"PipelineInitError"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineInitError.__init__","text":"Initialises the init error. Parameters: Name Type Description Default msg The error message returned by the pipeline. None Source code in vast_pipeline/pipeline/errors.py def __init__ ( self , msg = None ): \"\"\" Initialises the init error. Args: msg: The error message returned by the pipeline. \"\"\" super ( PipelineInitError , self ) . __init__ ( msg )","title":"__init__()"},{"location":"reference/pipeline/finalise/","text":"calculate_measurement_pair_aggregate_metrics ( measurement_pairs_df , min_vs , flux_type = 'peak' ) \u00b6 Calculate the aggregate maximum measurement pair variability metrics to be stored in Source objects. Only measurement pairs with abs(Vs metric) >= min_vs are considered. The measurement pairs are filtered on abs(Vs metric) >= min_vs , grouped by the source ID column source , then the row index of the maximum abs(m) metric is found. The absolute Vs and m metric values from this row are returned for each source. Parameters: Name Type Description Default measurement_pairs_df DataFrame The measurement pairs and their variability metrics. Must at least contain the columns: source, vs_{flux_type}, m_{flux_type}. required min_vs float The minimum value of the Vs metric (i.e. column vs_{flux_type} ) the measurement pair must have to be included in the aggregate metric determination. required flux_type str The flux type on which to perform the aggregation, either \"peak\" or \"int\". Default is \"peak\". 'peak' Returns: Type Description DataFrame Measurement pair aggregate metrics indexed by the source ID, source . The metric columns are named: vs_abs_significant_max_{flux_type} and m_abs_significant_max_{flux_type} . Source code in vast_pipeline/pipeline/finalise.py def calculate_measurement_pair_aggregate_metrics ( measurement_pairs_df : pd . DataFrame , min_vs : float , flux_type : str = \"peak\" , ) -> pd . DataFrame : \"\"\" Calculate the aggregate maximum measurement pair variability metrics to be stored in `Source` objects. Only measurement pairs with abs(Vs metric) >= `min_vs` are considered. The measurement pairs are filtered on abs(Vs metric) >= `min_vs`, grouped by the source ID column `source`, then the row index of the maximum abs(m) metric is found. The absolute Vs and m metric values from this row are returned for each source. Args: measurement_pairs_df: The measurement pairs and their variability metrics. Must at least contain the columns: source, vs_{flux_type}, m_{flux_type}. min_vs: The minimum value of the Vs metric (i.e. column `vs_{flux_type}`) the measurement pair must have to be included in the aggregate metric determination. flux_type: The flux type on which to perform the aggregation, either \"peak\" or \"int\". Default is \"peak\". Returns: Measurement pair aggregate metrics indexed by the source ID, `source`. The metric columns are named: `vs_abs_significant_max_{flux_type}` and `m_abs_significant_max_{flux_type}`. \"\"\" pair_agg_metrics = measurement_pairs_df . set_index ( \"source\" ) . iloc [ measurement_pairs_df . query ( f \"abs(vs_ { flux_type } ) >= @min_vs\" ) . groupby ( \"source\" ) . agg ( m_abs_max_idx = ( f \"m_ { flux_type } \" , lambda x : x . abs () . idxmax ()),) . astype ( np . int32 )[ \"m_abs_max_idx\" ] # cast row indices to int and select them . reset_index ( drop = True ) # keep only the row indices ][[ f \"vs_ { flux_type } \" , f \"m_ { flux_type } \" ]] pair_agg_metrics = pair_agg_metrics . abs () . rename ( columns = { f \"vs_ { flux_type } \" : f \"vs_abs_significant_max_ { flux_type } \" , f \"m_ { flux_type } \" : f \"m_abs_significant_max_ { flux_type } \" , }) return pair_agg_metrics final_operations ( sources_df , p_run , new_sources_df , source_aggregate_pair_metrics_min_abs_vs , add_mode , done_source_ids , previous_parquets ) \u00b6 Performs the final operations of the pipeline: - Calculates the statistics for the final sources. - Uploads sources and writes parquet. - Uploads related sources and writes parquet. - Uploads associations and writes parquet. Parameters: Name Type Description Default sources_df DataFrame The main sources_df dataframe produced from the pipeline. Contains all measurements and the association information. The id column is the Measurement object primary key that has already been saved to the database. required p_run Run The pipeline Run object of which the sources are associated with. required new_sources_df DataFrame The new sources dataframe, only contains the 'new_source_high_sigma' column (source_id is the index). required source_aggregate_pair_metrics_min_abs_vs float Only measurement pairs where the Vs metric exceeds this value are selected for the aggregate pair metrics that are stored in Source objects. required add_mode bool Whether the pipeline is running in add mode. required done_source_ids List[int] A list containing the source ids that have already been uploaded in the previous run in add mode. required Returns: Type Description int The number of sources contained in the pipeline (used in the next steps of main.py). Source code in vast_pipeline/pipeline/finalise.py def final_operations ( sources_df : pd . DataFrame , p_run : Run , new_sources_df : pd . DataFrame , source_aggregate_pair_metrics_min_abs_vs : float , add_mode : bool , done_source_ids : List [ int ], previous_parquets : Dict [ str , str ] ) -> int : \"\"\" Performs the final operations of the pipeline: - Calculates the statistics for the final sources. - Uploads sources and writes parquet. - Uploads related sources and writes parquet. - Uploads associations and writes parquet. Args: sources_df: The main sources_df dataframe produced from the pipeline. Contains all measurements and the association information. The `id` column is the Measurement object primary key that has already been saved to the database. p_run: The pipeline Run object of which the sources are associated with. new_sources_df: The new sources dataframe, only contains the 'new_source_high_sigma' column (source_id is the index). source_aggregate_pair_metrics_min_abs_vs: Only measurement pairs where the Vs metric exceeds this value are selected for the aggregate pair metrics that are stored in `Source` objects. add_mode: Whether the pipeline is running in add mode. done_source_ids: A list containing the source ids that have already been uploaded in the previous run in add mode. Returns: The number of sources contained in the pipeline (used in the next steps of main.py). \"\"\" timer = StopWatch () # calculate source fields logger . info ( 'Calculating statistics for %i sources...' , sources_df . source . unique () . shape [ 0 ] ) srcs_df = parallel_groupby ( sources_df ) logger . info ( 'Groupby-apply time: %.2f seconds' , timer . reset ()) # add new sources srcs_df [ \"new\" ] = srcs_df . index . isin ( new_sources_df . index ) srcs_df = pd . merge ( srcs_df , new_sources_df [ \"new_high_sigma\" ], left_on = \"source\" , right_index = True , how = \"left\" , ) srcs_df [ \"new_high_sigma\" ] = srcs_df [ \"new_high_sigma\" ] . fillna ( 0.0 ) # calculate nearest neighbour srcs_skycoord = SkyCoord ( srcs_df [ 'wavg_ra' ] . values , srcs_df [ 'wavg_dec' ] . values , unit = ( u . deg , u . deg ) ) idx , d2d , _ = srcs_skycoord . match_to_catalog_sky ( srcs_skycoord , nthneighbor = 2 ) # add the separation distance in degrees srcs_df [ 'n_neighbour_dist' ] = d2d . deg # create measurement pairs, aka 2-epoch metrics timer . reset () measurement_pairs_df = calculate_measurement_pair_metrics ( sources_df ) logger . info ( 'Measurement pair metrics time: %.2f seconds' , timer . reset ()) # calculate measurement pair metric aggregates for sources by finding the row indices # of the aggregate max of the abs(m) metric for each flux type. pair_agg_metrics = pd . merge ( calculate_measurement_pair_aggregate_metrics ( measurement_pairs_df , source_aggregate_pair_metrics_min_abs_vs , flux_type = \"peak\" , ), calculate_measurement_pair_aggregate_metrics ( measurement_pairs_df , source_aggregate_pair_metrics_min_abs_vs , flux_type = \"int\" , ), how = \"outer\" , left_index = True , right_index = True , ) # join with sources and replace agg metrics NaNs with 0 as the DataTables API JSON # serialization doesn't like them srcs_df = srcs_df . join ( pair_agg_metrics ) . fillna ( value = { \"vs_abs_significant_max_peak\" : 0.0 , \"m_abs_significant_max_peak\" : 0.0 , \"vs_abs_significant_max_int\" : 0.0 , \"m_abs_significant_max_int\" : 0.0 , }) logger . info ( \"Measurement pair aggregate metrics time: %.2f seconds\" , timer . reset ()) # upload sources to DB, column 'id' with DB id is contained in return if add_mode : # if add mode is being used some sources need to updated where as some # need to be newly uploaded. # upload new ones first (new id's are fetched) src_done_mask = srcs_df . index . isin ( done_source_ids ) srcs_df_upload = srcs_df . loc [ ~ src_done_mask ] . copy () srcs_df_upload = make_upload_sources ( srcs_df_upload , p_run , add_mode ) # And now update srcs_df_update = srcs_df . loc [ src_done_mask ] . copy () logger . info ( f \"Updating { srcs_df_update . shape [ 0 ] } sources with new metrics.\" ) srcs_df = update_sources ( srcs_df_update , batch_size = 1000 ) # Add back together if not srcs_df_upload . empty : srcs_df = srcs_df . append ( srcs_df_upload ) else : srcs_df = make_upload_sources ( srcs_df , p_run , add_mode ) # gather the related df, upload to db and save to parquet file # the df will look like # # from_source_id to_source_id # source # 714 60 14396 # 1211 94 12961 # # the index ('source') has the initial id generated by the pipeline to # identify unique sources, the 'from_source_id' column has the django # model id (in db), the 'to_source_id' has the pipeline index related_df = ( srcs_df . loc [ srcs_df [ \"related_list\" ] != - 1 , [ \"id\" , \"related_list\" ]] . explode ( \"related_list\" ) . rename ( columns = { \"id\" : \"from_source_id\" , \"related_list\" : \"to_source_id\" }) ) # for the column 'from_source_id', replace relation source ids with db id related_df [ \"to_source_id\" ] = related_df [ \"to_source_id\" ] . map ( srcs_df [ \"id\" ] . to_dict ()) # drop relationships with the same source related_df = related_df [ related_df [ \"from_source_id\" ] != related_df [ \"to_source_id\" ]] # write symmetrical relations to parquet related_df . to_parquet ( os . path . join ( p_run . path , 'relations.parquet' ), index = False ) # upload the relations to DB # check for add_mode first if add_mode : # Load old relations so the already uploaded ones can be removed old_relations = ( pd . read_parquet ( previous_parquets [ 'relations' ]) ) related_df = ( related_df . append ( old_relations , ignore_index = True ) . drop_duplicates ( keep = False ) ) logger . debug ( f 'Add mode: # { related_df . shape [ 0 ] } relations to upload.' ) make_upload_related_sources ( related_df ) del related_df # write sources to parquet file srcs_df = srcs_df . drop ([ \"related_list\" , \"img_list\" ], axis = 1 ) ( srcs_df . set_index ( 'id' ) # set the index to db ids, dropping the source idx . to_parquet ( os . path . join ( p_run . path , 'sources.parquet' )) ) # update measurments with sources to get associations sources_df = ( sources_df . drop ( 'related' , axis = 1 ) . merge ( srcs_df . rename ( columns = { 'id' : 'source_id' }), on = 'source' ) ) if add_mode : # Load old associations so the already uploaded ones can be removed old_assoications = ( pd . read_parquet ( previous_parquets [ 'associations' ]) . rename ( columns = { 'meas_id' : 'id' }) ) sources_df_upload = sources_df . append ( old_assoications , ignore_index = True ) sources_df_upload = sources_df_upload . drop_duplicates ( [ 'source_id' , 'id' , 'd2d' , 'dr' ], keep = False ) logger . debug ( f 'Add mode: # { sources_df_upload . shape [ 0 ] } associations to upload.' ) else : sources_df_upload = sources_df # upload associations into DB make_upload_associations ( sources_df_upload ) # write associations to parquet file sources_df . rename ( columns = { 'id' : 'meas_id' })[ [ 'source_id' , 'meas_id' , 'd2d' , 'dr' ] ] . to_parquet ( os . path . join ( p_run . path , 'associations.parquet' )) # get the Source object primary keys for the measurement pairs measurement_pairs_df = measurement_pairs_df . join ( srcs_df . id . rename ( \"source_id\" ), on = \"source\" ) if add_mode : # Load old associations so the already uploaded ones can be removed old_measurement_pairs = ( pd . read_parquet ( previous_parquets [ 'measurement_pairs' ]) ) . rename ( columns = { 'meas_id_a' : 'id_a' , 'meas_id_b' : 'id_b' }) measurement_pairs_df_upload = measurement_pairs_df . append ( old_measurement_pairs , ignore_index = True ) measurement_pairs_df_upload = ( measurement_pairs_df_upload . drop_duplicates ( [ 'id_a' , 'id_b' , 'source_id' ], keep = False ) ) logger . debug ( f 'Add mode: # { measurement_pairs_df_upload . shape [ 0 ] } ' ' measurement pairs to upload.' ) else : measurement_pairs_df_upload = measurement_pairs_df # create the measurement pair objects and upload to DB measurement_pairs_df = make_upload_measurement_pairs ( measurement_pairs_df_upload ) if add_mode : measurement_pairs_df = old_measurement_pairs . append ( measurement_pairs_df ) # optimize measurement pair DataFrame and save to parquet file measurement_pairs_df = optimize_ints ( optimize_floats ( measurement_pairs_df . drop ( columns = [ \"source\" ]) . rename ( columns = { \"id_a\" : \"meas_id_a\" , \"id_b\" : \"meas_id_b\" } ) ) ) measurement_pairs_df . to_parquet ( os . path . join ( p_run . path , \"measurement_pairs.parquet\" ), index = False ) logger . info ( \"Total final operations time: %.2f seconds\" , timer . reset_init ()) # calculate and return total number of extracted sources return srcs_df [ \"id\" ] . count ()","title":"finalise.py"},{"location":"reference/pipeline/finalise/#vast_pipeline.pipeline.finalise.calculate_measurement_pair_aggregate_metrics","text":"Calculate the aggregate maximum measurement pair variability metrics to be stored in Source objects. Only measurement pairs with abs(Vs metric) >= min_vs are considered. The measurement pairs are filtered on abs(Vs metric) >= min_vs , grouped by the source ID column source , then the row index of the maximum abs(m) metric is found. The absolute Vs and m metric values from this row are returned for each source. Parameters: Name Type Description Default measurement_pairs_df DataFrame The measurement pairs and their variability metrics. Must at least contain the columns: source, vs_{flux_type}, m_{flux_type}. required min_vs float The minimum value of the Vs metric (i.e. column vs_{flux_type} ) the measurement pair must have to be included in the aggregate metric determination. required flux_type str The flux type on which to perform the aggregation, either \"peak\" or \"int\". Default is \"peak\". 'peak' Returns: Type Description DataFrame Measurement pair aggregate metrics indexed by the source ID, source . The metric columns are named: vs_abs_significant_max_{flux_type} and m_abs_significant_max_{flux_type} . Source code in vast_pipeline/pipeline/finalise.py def calculate_measurement_pair_aggregate_metrics ( measurement_pairs_df : pd . DataFrame , min_vs : float , flux_type : str = \"peak\" , ) -> pd . DataFrame : \"\"\" Calculate the aggregate maximum measurement pair variability metrics to be stored in `Source` objects. Only measurement pairs with abs(Vs metric) >= `min_vs` are considered. The measurement pairs are filtered on abs(Vs metric) >= `min_vs`, grouped by the source ID column `source`, then the row index of the maximum abs(m) metric is found. The absolute Vs and m metric values from this row are returned for each source. Args: measurement_pairs_df: The measurement pairs and their variability metrics. Must at least contain the columns: source, vs_{flux_type}, m_{flux_type}. min_vs: The minimum value of the Vs metric (i.e. column `vs_{flux_type}`) the measurement pair must have to be included in the aggregate metric determination. flux_type: The flux type on which to perform the aggregation, either \"peak\" or \"int\". Default is \"peak\". Returns: Measurement pair aggregate metrics indexed by the source ID, `source`. The metric columns are named: `vs_abs_significant_max_{flux_type}` and `m_abs_significant_max_{flux_type}`. \"\"\" pair_agg_metrics = measurement_pairs_df . set_index ( \"source\" ) . iloc [ measurement_pairs_df . query ( f \"abs(vs_ { flux_type } ) >= @min_vs\" ) . groupby ( \"source\" ) . agg ( m_abs_max_idx = ( f \"m_ { flux_type } \" , lambda x : x . abs () . idxmax ()),) . astype ( np . int32 )[ \"m_abs_max_idx\" ] # cast row indices to int and select them . reset_index ( drop = True ) # keep only the row indices ][[ f \"vs_ { flux_type } \" , f \"m_ { flux_type } \" ]] pair_agg_metrics = pair_agg_metrics . abs () . rename ( columns = { f \"vs_ { flux_type } \" : f \"vs_abs_significant_max_ { flux_type } \" , f \"m_ { flux_type } \" : f \"m_abs_significant_max_ { flux_type } \" , }) return pair_agg_metrics","title":"calculate_measurement_pair_aggregate_metrics()"},{"location":"reference/pipeline/finalise/#vast_pipeline.pipeline.finalise.final_operations","text":"Performs the final operations of the pipeline: - Calculates the statistics for the final sources. - Uploads sources and writes parquet. - Uploads related sources and writes parquet. - Uploads associations and writes parquet. Parameters: Name Type Description Default sources_df DataFrame The main sources_df dataframe produced from the pipeline. Contains all measurements and the association information. The id column is the Measurement object primary key that has already been saved to the database. required p_run Run The pipeline Run object of which the sources are associated with. required new_sources_df DataFrame The new sources dataframe, only contains the 'new_source_high_sigma' column (source_id is the index). required source_aggregate_pair_metrics_min_abs_vs float Only measurement pairs where the Vs metric exceeds this value are selected for the aggregate pair metrics that are stored in Source objects. required add_mode bool Whether the pipeline is running in add mode. required done_source_ids List[int] A list containing the source ids that have already been uploaded in the previous run in add mode. required Returns: Type Description int The number of sources contained in the pipeline (used in the next steps of main.py). Source code in vast_pipeline/pipeline/finalise.py def final_operations ( sources_df : pd . DataFrame , p_run : Run , new_sources_df : pd . DataFrame , source_aggregate_pair_metrics_min_abs_vs : float , add_mode : bool , done_source_ids : List [ int ], previous_parquets : Dict [ str , str ] ) -> int : \"\"\" Performs the final operations of the pipeline: - Calculates the statistics for the final sources. - Uploads sources and writes parquet. - Uploads related sources and writes parquet. - Uploads associations and writes parquet. Args: sources_df: The main sources_df dataframe produced from the pipeline. Contains all measurements and the association information. The `id` column is the Measurement object primary key that has already been saved to the database. p_run: The pipeline Run object of which the sources are associated with. new_sources_df: The new sources dataframe, only contains the 'new_source_high_sigma' column (source_id is the index). source_aggregate_pair_metrics_min_abs_vs: Only measurement pairs where the Vs metric exceeds this value are selected for the aggregate pair metrics that are stored in `Source` objects. add_mode: Whether the pipeline is running in add mode. done_source_ids: A list containing the source ids that have already been uploaded in the previous run in add mode. Returns: The number of sources contained in the pipeline (used in the next steps of main.py). \"\"\" timer = StopWatch () # calculate source fields logger . info ( 'Calculating statistics for %i sources...' , sources_df . source . unique () . shape [ 0 ] ) srcs_df = parallel_groupby ( sources_df ) logger . info ( 'Groupby-apply time: %.2f seconds' , timer . reset ()) # add new sources srcs_df [ \"new\" ] = srcs_df . index . isin ( new_sources_df . index ) srcs_df = pd . merge ( srcs_df , new_sources_df [ \"new_high_sigma\" ], left_on = \"source\" , right_index = True , how = \"left\" , ) srcs_df [ \"new_high_sigma\" ] = srcs_df [ \"new_high_sigma\" ] . fillna ( 0.0 ) # calculate nearest neighbour srcs_skycoord = SkyCoord ( srcs_df [ 'wavg_ra' ] . values , srcs_df [ 'wavg_dec' ] . values , unit = ( u . deg , u . deg ) ) idx , d2d , _ = srcs_skycoord . match_to_catalog_sky ( srcs_skycoord , nthneighbor = 2 ) # add the separation distance in degrees srcs_df [ 'n_neighbour_dist' ] = d2d . deg # create measurement pairs, aka 2-epoch metrics timer . reset () measurement_pairs_df = calculate_measurement_pair_metrics ( sources_df ) logger . info ( 'Measurement pair metrics time: %.2f seconds' , timer . reset ()) # calculate measurement pair metric aggregates for sources by finding the row indices # of the aggregate max of the abs(m) metric for each flux type. pair_agg_metrics = pd . merge ( calculate_measurement_pair_aggregate_metrics ( measurement_pairs_df , source_aggregate_pair_metrics_min_abs_vs , flux_type = \"peak\" , ), calculate_measurement_pair_aggregate_metrics ( measurement_pairs_df , source_aggregate_pair_metrics_min_abs_vs , flux_type = \"int\" , ), how = \"outer\" , left_index = True , right_index = True , ) # join with sources and replace agg metrics NaNs with 0 as the DataTables API JSON # serialization doesn't like them srcs_df = srcs_df . join ( pair_agg_metrics ) . fillna ( value = { \"vs_abs_significant_max_peak\" : 0.0 , \"m_abs_significant_max_peak\" : 0.0 , \"vs_abs_significant_max_int\" : 0.0 , \"m_abs_significant_max_int\" : 0.0 , }) logger . info ( \"Measurement pair aggregate metrics time: %.2f seconds\" , timer . reset ()) # upload sources to DB, column 'id' with DB id is contained in return if add_mode : # if add mode is being used some sources need to updated where as some # need to be newly uploaded. # upload new ones first (new id's are fetched) src_done_mask = srcs_df . index . isin ( done_source_ids ) srcs_df_upload = srcs_df . loc [ ~ src_done_mask ] . copy () srcs_df_upload = make_upload_sources ( srcs_df_upload , p_run , add_mode ) # And now update srcs_df_update = srcs_df . loc [ src_done_mask ] . copy () logger . info ( f \"Updating { srcs_df_update . shape [ 0 ] } sources with new metrics.\" ) srcs_df = update_sources ( srcs_df_update , batch_size = 1000 ) # Add back together if not srcs_df_upload . empty : srcs_df = srcs_df . append ( srcs_df_upload ) else : srcs_df = make_upload_sources ( srcs_df , p_run , add_mode ) # gather the related df, upload to db and save to parquet file # the df will look like # # from_source_id to_source_id # source # 714 60 14396 # 1211 94 12961 # # the index ('source') has the initial id generated by the pipeline to # identify unique sources, the 'from_source_id' column has the django # model id (in db), the 'to_source_id' has the pipeline index related_df = ( srcs_df . loc [ srcs_df [ \"related_list\" ] != - 1 , [ \"id\" , \"related_list\" ]] . explode ( \"related_list\" ) . rename ( columns = { \"id\" : \"from_source_id\" , \"related_list\" : \"to_source_id\" }) ) # for the column 'from_source_id', replace relation source ids with db id related_df [ \"to_source_id\" ] = related_df [ \"to_source_id\" ] . map ( srcs_df [ \"id\" ] . to_dict ()) # drop relationships with the same source related_df = related_df [ related_df [ \"from_source_id\" ] != related_df [ \"to_source_id\" ]] # write symmetrical relations to parquet related_df . to_parquet ( os . path . join ( p_run . path , 'relations.parquet' ), index = False ) # upload the relations to DB # check for add_mode first if add_mode : # Load old relations so the already uploaded ones can be removed old_relations = ( pd . read_parquet ( previous_parquets [ 'relations' ]) ) related_df = ( related_df . append ( old_relations , ignore_index = True ) . drop_duplicates ( keep = False ) ) logger . debug ( f 'Add mode: # { related_df . shape [ 0 ] } relations to upload.' ) make_upload_related_sources ( related_df ) del related_df # write sources to parquet file srcs_df = srcs_df . drop ([ \"related_list\" , \"img_list\" ], axis = 1 ) ( srcs_df . set_index ( 'id' ) # set the index to db ids, dropping the source idx . to_parquet ( os . path . join ( p_run . path , 'sources.parquet' )) ) # update measurments with sources to get associations sources_df = ( sources_df . drop ( 'related' , axis = 1 ) . merge ( srcs_df . rename ( columns = { 'id' : 'source_id' }), on = 'source' ) ) if add_mode : # Load old associations so the already uploaded ones can be removed old_assoications = ( pd . read_parquet ( previous_parquets [ 'associations' ]) . rename ( columns = { 'meas_id' : 'id' }) ) sources_df_upload = sources_df . append ( old_assoications , ignore_index = True ) sources_df_upload = sources_df_upload . drop_duplicates ( [ 'source_id' , 'id' , 'd2d' , 'dr' ], keep = False ) logger . debug ( f 'Add mode: # { sources_df_upload . shape [ 0 ] } associations to upload.' ) else : sources_df_upload = sources_df # upload associations into DB make_upload_associations ( sources_df_upload ) # write associations to parquet file sources_df . rename ( columns = { 'id' : 'meas_id' })[ [ 'source_id' , 'meas_id' , 'd2d' , 'dr' ] ] . to_parquet ( os . path . join ( p_run . path , 'associations.parquet' )) # get the Source object primary keys for the measurement pairs measurement_pairs_df = measurement_pairs_df . join ( srcs_df . id . rename ( \"source_id\" ), on = \"source\" ) if add_mode : # Load old associations so the already uploaded ones can be removed old_measurement_pairs = ( pd . read_parquet ( previous_parquets [ 'measurement_pairs' ]) ) . rename ( columns = { 'meas_id_a' : 'id_a' , 'meas_id_b' : 'id_b' }) measurement_pairs_df_upload = measurement_pairs_df . append ( old_measurement_pairs , ignore_index = True ) measurement_pairs_df_upload = ( measurement_pairs_df_upload . drop_duplicates ( [ 'id_a' , 'id_b' , 'source_id' ], keep = False ) ) logger . debug ( f 'Add mode: # { measurement_pairs_df_upload . shape [ 0 ] } ' ' measurement pairs to upload.' ) else : measurement_pairs_df_upload = measurement_pairs_df # create the measurement pair objects and upload to DB measurement_pairs_df = make_upload_measurement_pairs ( measurement_pairs_df_upload ) if add_mode : measurement_pairs_df = old_measurement_pairs . append ( measurement_pairs_df ) # optimize measurement pair DataFrame and save to parquet file measurement_pairs_df = optimize_ints ( optimize_floats ( measurement_pairs_df . drop ( columns = [ \"source\" ]) . rename ( columns = { \"id_a\" : \"meas_id_a\" , \"id_b\" : \"meas_id_b\" } ) ) ) measurement_pairs_df . to_parquet ( os . path . join ( p_run . path , \"measurement_pairs.parquet\" ), index = False ) logger . info ( \"Total final operations time: %.2f seconds\" , timer . reset_init ()) # calculate and return total number of extracted sources return srcs_df [ \"id\" ] . count ()","title":"final_operations()"},{"location":"reference/pipeline/forced_extraction/","text":"extract_from_image ( df , image , background , noise , edge_buffer , cluster_threshold , allow_nan ) \u00b6 Extract the flux, its erros and chi squared data from the image files (image FIT, background and noise files) and return a dictionary with the dataframe and image name Parameters: Name Type Description Default df DataFrame input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak] required image str a string with the path of the image FIT file required background str a string with the path of the image background file required noise str a string with the path of the image noise file required edge_buffer float flag to pass to ForcedPhot.measure method required cluster_threshold float flag to pass to ForcedPhot.measure method required allow_nan bool flag to pass to ForcedPhot.measure method required Returns: Type Description Dict Dictionary with input dataframe with added columns (flux_int, flux_int_err, chi_squared_fit) and image name. Source code in vast_pipeline/pipeline/forced_extraction.py def extract_from_image ( df : pd . DataFrame , image : str , background : str , noise : str , edge_buffer : float , cluster_threshold : float , allow_nan : bool ) -> Dict : \"\"\" Extract the flux, its erros and chi squared data from the image files (image FIT, background and noise files) and return a dictionary with the dataframe and image name Args: df: input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak] image: a string with the path of the image FIT file background: a string with the path of the image background file noise: a string with the path of the image noise file edge_buffer: flag to pass to ForcedPhot.measure method cluster_threshold: flag to pass to ForcedPhot.measure method allow_nan: flag to pass to ForcedPhot.measure method Returns: Dictionary with input dataframe with added columns (flux_int, flux_int_err, chi_squared_fit) and image name. \"\"\" # create the skycoord obj to pass to the forced extraction # see usage https://github.com/dlakaplan/forced_phot P_islands = SkyCoord ( df [ 'wavg_ra' ] . values , df [ 'wavg_dec' ] . values , unit = ( u . deg , u . deg ) ) FP = ForcedPhot ( image , background , noise ) flux , flux_err , chisq , DOF , cluster_id = FP . measure ( P_islands , cluster_threshold = cluster_threshold , allow_nan = allow_nan , edge_buffer = edge_buffer ) df [ 'flux_int' ] = flux * 1.e3 df [ 'flux_int_err' ] = flux_err * 1.e3 df [ 'chi_squared_fit' ] = chisq return { 'df' : df , 'image' : df [ 'image_name' ] . iloc [ 0 ]} finalise_forced_dfs ( df , prefix , max_id , beam_bmaj , beam_bmin , beam_bpa , id , datetime , image ) \u00b6 Compute populate leftover columns for the dataframe with forced photometry data given the input parameters Parameters: Name Type Description Default df DataFrame input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak, flux_int, flux_int_err, chi_squared_fit] required prefix str string to use to generate the 'island_id' column required max_id int integer to use to generate the 'island_id' column required beam_bmaj float image beam major axis required beam_bmin float image beam minor axis required beam_bpa float image beam position angle required id int image id in database required datetime datetime timestamp of the image file (from header) required image str string with the image name required Returns: Type Description DataFrame Input dataframe with added columns island_id, component_id, name, bmaj, bmin, pa, image_id, time. Source code in vast_pipeline/pipeline/forced_extraction.py def finalise_forced_dfs ( df : pd . DataFrame , prefix : str , max_id : int , beam_bmaj : float , beam_bmin : float , beam_bpa : float , id : int , datetime : datetime . datetime , image : str ) -> pd . DataFrame : \"\"\" Compute populate leftover columns for the dataframe with forced photometry data given the input parameters Args: df: input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak, flux_int, flux_int_err, chi_squared_fit] prefix: string to use to generate the 'island_id' column max_id: integer to use to generate the 'island_id' column beam_bmaj: image beam major axis beam_bmin: image beam minor axis beam_bpa: image beam position angle id: image id in database datetime: timestamp of the image file (from header) image: string with the image name Returns: Input dataframe with added columns island_id, component_id, name, bmaj, bmin, pa, image_id, time. \"\"\" # make up the measurements name from the image island_id and component_id df [ 'island_id' ] = np . char . add ( prefix , np . arange ( max_id , max_id + df . shape [ 0 ]) . astype ( str ) ) df [ 'component_id' ] = df [ 'island_id' ] . str . replace ( 'island' , 'component' ) + 'a' img_prefix = image . split ( '.' )[ 0 ] + '_' df [ 'name' ] = img_prefix + df [ 'component_id' ] # assign all the other columns # convert fluxes to mJy # store source bmaj and bmin in arcsec df [ 'bmaj' ] = beam_bmaj * 3600. df [ 'bmin' ] = beam_bmin * 3600. df [ 'pa' ] = beam_bpa # add image id and time df [ 'image_id' ] = id df [ 'time' ] = datetime return df forced_extraction ( sources_df , cfg_err_ra , cfg_err_dec , p_run , extr_df , min_sigma , edge_buffer , cluster_threshold , allow_nan , add_mode , done_images_df , done_source_ids ) \u00b6 Check and extract expected measurements, and associated them with the related source(s). Parameters: Name Type Description Default sources_df DataFrame Dataframe containing all the extracted measurements and associations (product from association step). required cfg_err_ra float The minimum RA error from the config file (in degrees). required cfg_err_dec float The minimum declination error from the config file (in degrees). required p_run Run The pipeline run object. required extr_df DataFrame The dataframe containing the information on what sources are missing from which images (output from get_src_skyregion_merged_df in main.py). required min_sigma float Minimum sigma value to drop forced extracted measurements. required edge_buffer float Flag to pass to ForcedPhot.measure method. required cluster_threshold float Flag to pass to ForcedPhot.measure method. required allow_nan bool Flag to pass to ForcedPhot.measure method. required add_mode bool True when the pipeline is running in add image mode. required done_images_df DataFrame Dataframe containing the images that thave already been processed in a previous run (used in add image mode). required done_source_ids List[int] List of the source ids that were already present in the previous run (used in add image mode). required Returns: Type Description Tuple[pandas.core.frame.DataFrame, int] The sources_df with the extracted sources added and n_forced is the total number of forced measurements present in the run. Source code in vast_pipeline/pipeline/forced_extraction.py def forced_extraction ( sources_df : pd . DataFrame , cfg_err_ra : float , cfg_err_dec : float , p_run : Run , extr_df : pd . DataFrame , min_sigma : float , edge_buffer : float , cluster_threshold : float , allow_nan : bool , add_mode : bool , done_images_df : pd . DataFrame , done_source_ids : List [ int ] ) -> Tuple [ pd . DataFrame , int ]: \"\"\" Check and extract expected measurements, and associated them with the related source(s). Args: sources_df: Dataframe containing all the extracted measurements and associations (product from association step). cfg_err_ra: The minimum RA error from the config file (in degrees). cfg_err_dec: The minimum declination error from the config file (in degrees). p_run: The pipeline run object. extr_df: The dataframe containing the information on what sources are missing from which images (output from get_src_skyregion_merged_df in main.py). min_sigma: Minimum sigma value to drop forced extracted measurements. edge_buffer: Flag to pass to ForcedPhot.measure method. cluster_threshold: Flag to pass to ForcedPhot.measure method. allow_nan: Flag to pass to ForcedPhot.measure method. add_mode: True when the pipeline is running in add image mode. done_images_df: Dataframe containing the images that thave already been processed in a previous run (used in add image mode). done_source_ids: List of the source ids that were already present in the previous run (used in add image mode). Returns: The sources_df with the extracted sources added and n_forced is the total number of forced measurements present in the run. \"\"\" logger . info ( 'Starting force extraction step.' ) timer = StopWatch () # get all the skyregions and related images cols = [ 'id' , 'name' , 'measurements_path' , 'path' , 'noise_path' , 'beam_bmaj' , 'beam_bmin' , 'beam_bpa' , 'background_path' , 'rms_min' , 'datetime' , 'skyreg__centre_ra' , 'skyreg__centre_dec' , 'skyreg__xtr_radius' ] images_df = pd . DataFrame ( list ( Image . objects . filter ( run = p_run ) . select_related ( 'skyreg' ) . order_by ( 'datetime' ) . values ( * tuple ( cols )) )) . set_index ( 'name' ) # | name | id | measurements_path | path | noise_path | # |:------------------------------|-----:|:--------------------|:-------------|:-------------| # | VAST_2118-06A.EPOCH01.I.fits | 1 | path/to/file | path/to/file | path/to/file | # | VAST_2118-06A.EPOCH03x.I.fits | 3 | path/to/file | path/to/file | path/to/file | # | VAST_2118-06A.EPOCH02.I.fits | 2 | path/to/file | path/to/file | path/to/file | # | name | beam_bmaj | beam_bmin | beam_bpa | background_path | # |:------------------------------|------------:|------------:|-----------:|:------------------| # | VAST_2118-06A.EPOCH01.I.fits | 0.00589921 | 0.00326088 | -70.4032 | path/to/file | # | VAST_2118-06A.EPOCH03x.I.fits | 0.00470991 | 0.00300502 | -83.1128 | path/to/file | # | VAST_2118-06A.EPOCH02.I.fits | 0.00351331 | 0.00308565 | 77.2395 | path/to/file | # | name | rms_min | datetime | skyreg__centre_ra | skyreg__centre_dec | skyreg__xtr_radius | # |:------------------------------|----------:|:---------------------------------|--------------------:|---------------------:|---------------------:| # | VAST_2118-06A.EPOCH01.I.fits | 0.173946 | 2019-08-27 18:12:16.700000+00:00 | 319.652 | -6.2989 | 6.7401 | # | VAST_2118-06A.EPOCH03x.I.fits | 0.165395 | 2019-10-29 10:01:20.500000+00:00 | 319.652 | -6.2989 | 6.7401 | # | VAST_2118-06A.EPOCH02.I.fits | 0.16323 | 2019-10-30 08:31:20.200000+00:00 | 319.652 | -6.2989 | 6.7401 | # Explode out the img_diff column. extr_df = extr_df . explode ( 'img_diff' ) . reset_index () total_to_extract = extr_df . shape [ 0 ] if add_mode : # If we are adding images to the run we assume that monitoring was # also performed before (enforced by the pre-run checks) so now we # only want to force extract in three situations: # 1. Any force extraction in a new image. # 2. The forced extraction is attached to a new source from the new # images. # 3. A new relation has been created and they need the forced # measuremnts filled in (actually covered by 2.) extr_df = ( extr_df [ ~ extr_df [ 'img_diff' ] . isin ( done_images_df [ 'name' ])] . append ( extr_df [ ( ~ extr_df [ 'source' ] . isin ( done_source_ids )) & ( extr_df [ 'img_diff' ] . isin ( done_images_df . name )) ]) . sort_index () ) logger . info ( f \" { extr_df . shape [ 0 ] } new measurements to force extract\" f \" (from { total_to_extract } total)\" ) timer . reset () extr_df = parallel_extraction ( extr_df , images_df , sources_df [[ 'source' , 'image' , 'flux_peak' ]], min_sigma , edge_buffer , cluster_threshold , allow_nan , add_mode , p_run . path ) logger . info ( 'Force extraction step time: %.2f seconds' , timer . reset () ) # make measurement names unique for db constraint extr_df [ 'name' ] = extr_df [ 'name' ] + f '_f_run { p_run . id : 06d } ' # select sensible flux values and set the columns with fix values values = { 'flux_int' : 0 , 'flux_int_err' : 0 } extr_df = extr_df . fillna ( value = values ) extr_df = extr_df [ ( extr_df [ 'flux_int' ] != 0 ) & ( extr_df [ 'flux_int_err' ] != 0 ) & ( extr_df [ 'chi_squared_fit' ] != np . inf ) & ( extr_df [ 'chi_squared_fit' ] != np . nan ) ] default_pos_err = settings . POS_DEFAULT_MIN_ERROR / 3600. extr_df [ 'ra_err' ] = default_pos_err extr_df [ 'dec_err' ] = default_pos_err extr_df [ 'err_bmaj' ] = 0. extr_df [ 'err_bmin' ] = 0. extr_df [ 'err_pa' ] = 0. extr_df [ 'ew_sys_err' ] = cfg_err_ra extr_df [ 'ns_sys_err' ] = cfg_err_dec extr_df [ 'error_radius' ] = 0. extr_df [ 'uncertainty_ew' ] = np . hypot ( cfg_err_ra , default_pos_err ) extr_df [ 'weight_ew' ] = 1. / extr_df [ 'uncertainty_ew' ] . values ** 2 extr_df [ 'uncertainty_ns' ] = np . hypot ( cfg_err_dec , default_pos_err ) extr_df [ 'weight_ns' ] = 1. / extr_df [ 'uncertainty_ns' ] . values ** 2 extr_df [ 'flux_peak' ] = extr_df [ 'flux_int' ] extr_df [ 'flux_peak_err' ] = extr_df [ 'flux_int_err' ] extr_df [ 'local_rms' ] = extr_df [ 'flux_int_err' ] extr_df [ 'snr' ] = ( extr_df [ 'flux_peak' ] . values / extr_df [ 'local_rms' ] . values ) extr_df [ 'spectral_index' ] = 0. extr_df [ 'dr' ] = 0. extr_df [ 'd2d' ] = 0. extr_df [ 'forced' ] = True extr_df [ 'compactness' ] = 1. extr_df [ 'psf_bmaj' ] = extr_df [ 'bmaj' ] extr_df [ 'psf_bmin' ] = extr_df [ 'bmin' ] extr_df [ 'psf_pa' ] = extr_df [ 'pa' ] extr_df [ 'flag_c4' ] = False extr_df [ 'spectral_index_from_TT' ] = False extr_df [ 'has_siblings' ] = False extr_df [ 'flux_int_isl_ratio' ] = 1.0 extr_df [ 'flux_peak_isl_ratio' ] = 1.0 col_order = read_schema ( images_df . iloc [ 0 ][ 'measurements_path' ] ) . names col_order . remove ( 'id' ) remaining = list ( set ( extr_df . columns ) - set ( col_order )) extr_df = extr_df [ col_order + remaining ] # upload the measurements, a column 'id' is returned with the DB id extr_df = make_upload_measurements ( extr_df ) extr_df = extr_df . rename ( columns = { 'source_tmp_id' : 'source' }) # write forced measurements to specific parquet logger . info ( 'Saving forced measurements to specific parquet file...' ) parallel_write_parquet ( extr_df , p_run . path , add_mode ) # Required to rename this column for the image add mode. extr_df = extr_df . rename ( columns = { 'time' : 'datetime' }) # append new meas into main df and proceed with source groupby etc sources_df = sources_df . append ( extr_df . loc [:, extr_df . columns . isin ( sources_df . columns )], ignore_index = True ) # get the number of forced extractions for the run forced_parquets = glob ( os . path . join ( p_run . path , \"forced_measurements*.parquet\" )) if forced_parquets : n_forced = ( dd . read_parquet ( forced_parquets , columns = [ 'id' ]) . count () . compute () . values [ 0 ] ) else : n_forced = 0 logger . info ( 'Total forced extraction time: %.2f seconds' , timer . reset_init () ) return sources_df , n_forced get_data_from_parquet ( file , p_run_path , add_mode = False ) \u00b6 Get the prefix, max id and image id from the measurements parquets Parameters: Name Type Description Default file str a string with the path of the measurements parquet file required p_run_path str Pipeline run path to get forced parquet in case of add mode. required add_mode bool Whether image add mode is being used where the forced parquet needs to be used instead. False Returns: Type Description Dict Dictionary with prefix string, an interger max_id and a string with the id of the image Source code in vast_pipeline/pipeline/forced_extraction.py def get_data_from_parquet ( file : str , p_run_path : str , add_mode : bool = False ,) -> Dict : ''' Get the prefix, max id and image id from the measurements parquets Args: file: a string with the path of the measurements parquet file p_run_path: Pipeline run path to get forced parquet in case of add mode. add_mode: Whether image add mode is being used where the forced parquet needs to be used instead. Returns: Dictionary with prefix string, an interger max_id and a string with the id of the image ''' if add_mode : image_name = file . split ( \"/\" )[ - 2 ] forced_parquet = os . path . join ( p_run_path , f \"forced_measurements_ { image_name } .parquet\" ) if os . path . isfile ( forced_parquet ): file = forced_parquet # get max component id from parquet file df = pd . read_parquet ( file , columns = [ 'island_id' , 'image_id' ]) prefix = df [ 'island_id' ] . iloc [ 0 ] . rsplit ( '_' , maxsplit = 1 )[ 0 ] + '_' max_id = ( df [ 'island_id' ] . str . rsplit ( '_' , n = 1 ) . str . get ( - 1 ) . astype ( int ) . values . max () + 1 ) return { 'prefix' : prefix , 'max_id' : max_id , 'id' : df [ 'image_id' ] . iloc [ 0 ]} parallel_extraction ( df , df_images , df_sources , min_sigma , edge_buffer , cluster_threshold , allow_nan , add_mode , p_run_path ) \u00b6 Parallelize forced extraction with Dask Parameters: Name Type Description Default df DataFrame dataframe with columns 'wavg_ra', 'wavg_dec', 'img_diff', 'detection' required df_images DataFrame dataframe with the images data and columns 'id', 'measurements_path', 'path', 'noise_path', 'beam_bmaj', 'beam_bmin', 'beam_bpa', 'background_path', 'rms_min', 'datetime', 'skyreg__centre_ra', 'skyreg__centre_dec', 'skyreg__xtr_radius' and 'name' as the index. required df_sources DataFrame dataframe derived from the measurement data with columns 'source', 'image', 'flux_peak'. required min_sigma float minimum sigma value to drop forced extracted measurements. required edge_buffer float flag to pass to ForcedPhot.measure method. required cluster_threshold float flag to pass to ForcedPhot.measure method. required allow_nan bool flag to pass to ForcedPhot.measure method. required add_mode bool True when the pipeline is running in add image mode. required p_run_path str The system path of the pipeline run output. required Returns: Type Description DataFrame Dataframe with forced extracted measurements data, columns are 'source_tmp_id', 'ra', 'dec', 'image', 'flux_peak', 'island_id', 'component_id', 'name', 'flux_int', 'flux_int_err' Source code in vast_pipeline/pipeline/forced_extraction.py def parallel_extraction ( df : pd . DataFrame , df_images : pd . DataFrame , df_sources : pd . DataFrame , min_sigma : float , edge_buffer : float , cluster_threshold : float , allow_nan : bool , add_mode : bool , p_run_path : str ) -> pd . DataFrame : \"\"\" Parallelize forced extraction with Dask Args: df: dataframe with columns 'wavg_ra', 'wavg_dec', 'img_diff', 'detection' df_images: dataframe with the images data and columns 'id', 'measurements_path', 'path', 'noise_path', 'beam_bmaj', 'beam_bmin', 'beam_bpa', 'background_path', 'rms_min', 'datetime', 'skyreg__centre_ra', 'skyreg__centre_dec', 'skyreg__xtr_radius' and 'name' as the index. df_sources: dataframe derived from the measurement data with columns 'source', 'image', 'flux_peak'. min_sigma: minimum sigma value to drop forced extracted measurements. edge_buffer: flag to pass to ForcedPhot.measure method. cluster_threshold: flag to pass to ForcedPhot.measure method. allow_nan: flag to pass to ForcedPhot.measure method. add_mode: True when the pipeline is running in add image mode. p_run_path: The system path of the pipeline run output. Returns: Dataframe with forced extracted measurements data, columns are 'source_tmp_id', 'ra', 'dec', 'image', 'flux_peak', 'island_id', 'component_id', 'name', 'flux_int', 'flux_int_err' \"\"\" # explode the lists in 'img_diff' column (this will make a copy of the df) out = ( df . rename ( columns = { 'img_diff' : 'image' , 'source' : 'source_tmp_id' }) # merge the rms_min column from df_images . merge ( df_images [[ 'rms_min' ]], left_on = 'image' , right_on = 'name' , how = 'left' ) . rename ( columns = { 'rms_min' : 'image_rms_min' }) # merge the measurements columns 'source', 'image', 'flux_peak' . merge ( df_sources , left_on = [ 'source_tmp_id' , 'detection' ], right_on = [ 'source' , 'image' ], how = 'left' ) . drop ( columns = [ 'image_y' , 'source' ]) . rename ( columns = { 'image_x' : 'image' }) ) # drop the source for which we would have no hope of detecting predrop_shape = out . shape [ 0 ] out [ 'max_snr' ] = out [ 'flux_peak' ] . values / out [ 'image_rms_min' ] . values out = out [ out [ 'max_snr' ] > min_sigma ] . reset_index ( drop = True ) logger . debug ( \"Min forced sigma dropped %i sources\" , predrop_shape - out . shape [ 0 ] ) # drop some columns that are no longer needed and the df should look like # out # | | source_tmp_id | wavg_ra | wavg_dec | image_name | flux_peak | # |--:|--------------:|--------:|---------:|:-----------------|----------:| # | 0 | 81 | 317.607 | -8.66952 | VAST_2118-06A... | 11.555 | # | 1 | 894 | 323.803 | -2.6899 | VAST_2118-06A... | 2.178 | # | 2 | 1076 | 316.147 | -3.11408 | VAST_2118-06A... | 6.815 | # | 3 | 1353 | 322.094 | -4.44977 | VAST_2118-06A... | 1.879 | # | 4 | 1387 | 321.734 | -6.82934 | VAST_2118-06A... | 1.61 | out = ( out . drop ([ 'max_snr' , 'image_rms_min' , 'detection' ], axis = 1 ) . rename ( columns = { 'image' : 'image_name' }) ) # get the unique images to extract from unique_images_to_extract = out [ 'image_name' ] . unique () . tolist () # create a list of dictionaries with image file paths and dataframes # with data related to each images image_data_func = lambda x : { 'image' : df_images . at [ x , 'path' ], 'background' : df_images . at [ x , 'background_path' ], 'noise' : df_images . at [ x , 'noise_path' ], 'df' : out [ out [ 'image_name' ] == x ] } list_to_map = list ( map ( image_data_func , unique_images_to_extract )) # create a list of all the measurements parquet files to extract data from, # such as prefix and max_id list_meas_parquets = list ( map ( lambda el : df_images . at [ el , 'measurements_path' ], unique_images_to_extract )) del out , unique_images_to_extract , image_data_func # get a map of the columns that have a fixed value mapping = ( db . from_sequence ( list_meas_parquets , npartitions = len ( list_meas_parquets ) ) . map ( get_data_from_parquet , p_run_path , add_mode ) . compute () ) mapping = pd . DataFrame ( mapping ) # remove not used columns from images_df and merge into mapping col_to_drop = list ( filter ( lambda x : ( 'path' in x ) or ( 'skyreg' in x ), df_images . columns . values . tolist () )) mapping = ( mapping . merge ( df_images . drop ( col_to_drop , axis = 1 ) . reset_index (), on = 'id' , how = 'left' ) . drop ( 'rms_min' , axis = 1 ) . set_index ( 'name' ) ) del col_to_drop n_cpu = cpu_count () - 1 bags = db . from_sequence ( list_to_map , npartitions = len ( list_to_map )) forced_dfs = ( bags . map ( lambda x : extract_from_image ( edge_buffer = edge_buffer , cluster_threshold = cluster_threshold , allow_nan = allow_nan , ** x )) . compute () ) del bags # create intermediates dfs combining the mapping data and the forced # extracted data from the images intermediate_df = list ( map ( lambda x : { ** ( mapping . loc [ x [ 'image' ], :] . to_dict ()), ** x }, forced_dfs )) # compute the rest of the columns intermediate_df = ( db . from_sequence ( intermediate_df ) . map ( lambda x : finalise_forced_dfs ( ** x )) . compute () ) df_out = ( pd . concat ( intermediate_df , axis = 0 , sort = False ) . rename ( columns = { 'wavg_ra' : 'ra' , 'wavg_dec' : 'dec' , 'image_name' : 'image' } ) ) return df_out parallel_write_parquet ( df , run_path , add_mode = False ) \u00b6 Parallelize writing parquet files for forced measurements. Parameters: Name Type Description Default df DataFrame Dataframe containing all the extracted measurements. required run_path str The run path of the pipeline run. required add_mode bool True when the pipeline is running in add image mode. False Returns: Type Description None None Source code in vast_pipeline/pipeline/forced_extraction.py def parallel_write_parquet ( df : pd . DataFrame , run_path : str , add_mode : bool = False ) -> None : ''' Parallelize writing parquet files for forced measurements. Args: df: Dataframe containing all the extracted measurements. run_path: The run path of the pipeline run. add_mode: True when the pipeline is running in add image mode. Returns: None ''' images = df [ 'image' ] . unique () . tolist () get_fname = lambda n : os . path . join ( run_path , 'forced_measurements_' + n . replace ( '.' , '_' ) + '.parquet' ) dfs = list ( map ( lambda x : ( df [ df [ 'image' ] == x ], get_fname ( x )), images )) n_cpu = cpu_count () - 1 # writing parquets using Dask bag bags = db . from_sequence ( dfs ) bags = bags . starmap ( lambda df , fname : write_group_to_parquet ( df , fname , add_mode )) bags . compute ( num_workers = n_cpu ) pass remove_forced_meas ( run_path ) \u00b6 Remove forced measurements from the database if forced parquet files are found. Parameters: Name Type Description Default run_path The run path of the pipeline run. required Returns: Type Description None None Source code in vast_pipeline/pipeline/forced_extraction.py def remove_forced_meas ( run_path ) -> None : ''' Remove forced measurements from the database if forced parquet files are found. Args: run_path: The run path of the pipeline run. Returns: None ''' path_glob = glob ( os . path . join ( run_path , 'forced_measurements_*.parquet' ) ) if path_glob : ids = ( dd . read_parquet ( path_glob , columns = 'id' ) . values . compute () . tolist () ) obj_to_delete = Measurement . objects . filter ( id__in = ids ) del ids if obj_to_delete . exists (): with transaction . atomic (): n_del , detail_del = obj_to_delete . delete () logger . info ( ( 'Deleting all previous forced measurement and association' ' objects for this run. Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) write_group_to_parquet ( df , fname , add_mode ) \u00b6 Write a dataframe correpondent to a single group/image to a parquet file. Parameters: Name Type Description Default df DataFrame Dataframe containing all the extracted measurements. required fname str The file name of the output parquet. required add_mode bool True when the pipeline is running in add image mode. required Returns: Type Description None None Source code in vast_pipeline/pipeline/forced_extraction.py def write_group_to_parquet ( df : pd . DataFrame , fname : str , add_mode : bool ) -> None : ''' Write a dataframe correpondent to a single group/image to a parquet file. Args: df: Dataframe containing all the extracted measurements. fname: The file name of the output parquet. add_mode: True when the pipeline is running in add image mode. Returns: None ''' out_df = df . drop ([ 'd2d' , 'dr' , 'source' , 'image' ], axis = 1 ) if os . path . isfile ( fname ) and add_mode : exist_df = pd . read_parquet ( fname ) out_df = exist_df . append ( out_df ) out_df . to_parquet ( fname , index = False ) pass","title":"forced_extraction.py"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.extract_from_image","text":"Extract the flux, its erros and chi squared data from the image files (image FIT, background and noise files) and return a dictionary with the dataframe and image name Parameters: Name Type Description Default df DataFrame input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak] required image str a string with the path of the image FIT file required background str a string with the path of the image background file required noise str a string with the path of the image noise file required edge_buffer float flag to pass to ForcedPhot.measure method required cluster_threshold float flag to pass to ForcedPhot.measure method required allow_nan bool flag to pass to ForcedPhot.measure method required Returns: Type Description Dict Dictionary with input dataframe with added columns (flux_int, flux_int_err, chi_squared_fit) and image name. Source code in vast_pipeline/pipeline/forced_extraction.py def extract_from_image ( df : pd . DataFrame , image : str , background : str , noise : str , edge_buffer : float , cluster_threshold : float , allow_nan : bool ) -> Dict : \"\"\" Extract the flux, its erros and chi squared data from the image files (image FIT, background and noise files) and return a dictionary with the dataframe and image name Args: df: input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak] image: a string with the path of the image FIT file background: a string with the path of the image background file noise: a string with the path of the image noise file edge_buffer: flag to pass to ForcedPhot.measure method cluster_threshold: flag to pass to ForcedPhot.measure method allow_nan: flag to pass to ForcedPhot.measure method Returns: Dictionary with input dataframe with added columns (flux_int, flux_int_err, chi_squared_fit) and image name. \"\"\" # create the skycoord obj to pass to the forced extraction # see usage https://github.com/dlakaplan/forced_phot P_islands = SkyCoord ( df [ 'wavg_ra' ] . values , df [ 'wavg_dec' ] . values , unit = ( u . deg , u . deg ) ) FP = ForcedPhot ( image , background , noise ) flux , flux_err , chisq , DOF , cluster_id = FP . measure ( P_islands , cluster_threshold = cluster_threshold , allow_nan = allow_nan , edge_buffer = edge_buffer ) df [ 'flux_int' ] = flux * 1.e3 df [ 'flux_int_err' ] = flux_err * 1.e3 df [ 'chi_squared_fit' ] = chisq return { 'df' : df , 'image' : df [ 'image_name' ] . iloc [ 0 ]}","title":"extract_from_image()"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.finalise_forced_dfs","text":"Compute populate leftover columns for the dataframe with forced photometry data given the input parameters Parameters: Name Type Description Default df DataFrame input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak, flux_int, flux_int_err, chi_squared_fit] required prefix str string to use to generate the 'island_id' column required max_id int integer to use to generate the 'island_id' column required beam_bmaj float image beam major axis required beam_bmin float image beam minor axis required beam_bpa float image beam position angle required id int image id in database required datetime datetime timestamp of the image file (from header) required image str string with the image name required Returns: Type Description DataFrame Input dataframe with added columns island_id, component_id, name, bmaj, bmin, pa, image_id, time. Source code in vast_pipeline/pipeline/forced_extraction.py def finalise_forced_dfs ( df : pd . DataFrame , prefix : str , max_id : int , beam_bmaj : float , beam_bmin : float , beam_bpa : float , id : int , datetime : datetime . datetime , image : str ) -> pd . DataFrame : \"\"\" Compute populate leftover columns for the dataframe with forced photometry data given the input parameters Args: df: input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak, flux_int, flux_int_err, chi_squared_fit] prefix: string to use to generate the 'island_id' column max_id: integer to use to generate the 'island_id' column beam_bmaj: image beam major axis beam_bmin: image beam minor axis beam_bpa: image beam position angle id: image id in database datetime: timestamp of the image file (from header) image: string with the image name Returns: Input dataframe with added columns island_id, component_id, name, bmaj, bmin, pa, image_id, time. \"\"\" # make up the measurements name from the image island_id and component_id df [ 'island_id' ] = np . char . add ( prefix , np . arange ( max_id , max_id + df . shape [ 0 ]) . astype ( str ) ) df [ 'component_id' ] = df [ 'island_id' ] . str . replace ( 'island' , 'component' ) + 'a' img_prefix = image . split ( '.' )[ 0 ] + '_' df [ 'name' ] = img_prefix + df [ 'component_id' ] # assign all the other columns # convert fluxes to mJy # store source bmaj and bmin in arcsec df [ 'bmaj' ] = beam_bmaj * 3600. df [ 'bmin' ] = beam_bmin * 3600. df [ 'pa' ] = beam_bpa # add image id and time df [ 'image_id' ] = id df [ 'time' ] = datetime return df","title":"finalise_forced_dfs()"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.forced_extraction","text":"Check and extract expected measurements, and associated them with the related source(s). Parameters: Name Type Description Default sources_df DataFrame Dataframe containing all the extracted measurements and associations (product from association step). required cfg_err_ra float The minimum RA error from the config file (in degrees). required cfg_err_dec float The minimum declination error from the config file (in degrees). required p_run Run The pipeline run object. required extr_df DataFrame The dataframe containing the information on what sources are missing from which images (output from get_src_skyregion_merged_df in main.py). required min_sigma float Minimum sigma value to drop forced extracted measurements. required edge_buffer float Flag to pass to ForcedPhot.measure method. required cluster_threshold float Flag to pass to ForcedPhot.measure method. required allow_nan bool Flag to pass to ForcedPhot.measure method. required add_mode bool True when the pipeline is running in add image mode. required done_images_df DataFrame Dataframe containing the images that thave already been processed in a previous run (used in add image mode). required done_source_ids List[int] List of the source ids that were already present in the previous run (used in add image mode). required Returns: Type Description Tuple[pandas.core.frame.DataFrame, int] The sources_df with the extracted sources added and n_forced is the total number of forced measurements present in the run. Source code in vast_pipeline/pipeline/forced_extraction.py def forced_extraction ( sources_df : pd . DataFrame , cfg_err_ra : float , cfg_err_dec : float , p_run : Run , extr_df : pd . DataFrame , min_sigma : float , edge_buffer : float , cluster_threshold : float , allow_nan : bool , add_mode : bool , done_images_df : pd . DataFrame , done_source_ids : List [ int ] ) -> Tuple [ pd . DataFrame , int ]: \"\"\" Check and extract expected measurements, and associated them with the related source(s). Args: sources_df: Dataframe containing all the extracted measurements and associations (product from association step). cfg_err_ra: The minimum RA error from the config file (in degrees). cfg_err_dec: The minimum declination error from the config file (in degrees). p_run: The pipeline run object. extr_df: The dataframe containing the information on what sources are missing from which images (output from get_src_skyregion_merged_df in main.py). min_sigma: Minimum sigma value to drop forced extracted measurements. edge_buffer: Flag to pass to ForcedPhot.measure method. cluster_threshold: Flag to pass to ForcedPhot.measure method. allow_nan: Flag to pass to ForcedPhot.measure method. add_mode: True when the pipeline is running in add image mode. done_images_df: Dataframe containing the images that thave already been processed in a previous run (used in add image mode). done_source_ids: List of the source ids that were already present in the previous run (used in add image mode). Returns: The sources_df with the extracted sources added and n_forced is the total number of forced measurements present in the run. \"\"\" logger . info ( 'Starting force extraction step.' ) timer = StopWatch () # get all the skyregions and related images cols = [ 'id' , 'name' , 'measurements_path' , 'path' , 'noise_path' , 'beam_bmaj' , 'beam_bmin' , 'beam_bpa' , 'background_path' , 'rms_min' , 'datetime' , 'skyreg__centre_ra' , 'skyreg__centre_dec' , 'skyreg__xtr_radius' ] images_df = pd . DataFrame ( list ( Image . objects . filter ( run = p_run ) . select_related ( 'skyreg' ) . order_by ( 'datetime' ) . values ( * tuple ( cols )) )) . set_index ( 'name' ) # | name | id | measurements_path | path | noise_path | # |:------------------------------|-----:|:--------------------|:-------------|:-------------| # | VAST_2118-06A.EPOCH01.I.fits | 1 | path/to/file | path/to/file | path/to/file | # | VAST_2118-06A.EPOCH03x.I.fits | 3 | path/to/file | path/to/file | path/to/file | # | VAST_2118-06A.EPOCH02.I.fits | 2 | path/to/file | path/to/file | path/to/file | # | name | beam_bmaj | beam_bmin | beam_bpa | background_path | # |:------------------------------|------------:|------------:|-----------:|:------------------| # | VAST_2118-06A.EPOCH01.I.fits | 0.00589921 | 0.00326088 | -70.4032 | path/to/file | # | VAST_2118-06A.EPOCH03x.I.fits | 0.00470991 | 0.00300502 | -83.1128 | path/to/file | # | VAST_2118-06A.EPOCH02.I.fits | 0.00351331 | 0.00308565 | 77.2395 | path/to/file | # | name | rms_min | datetime | skyreg__centre_ra | skyreg__centre_dec | skyreg__xtr_radius | # |:------------------------------|----------:|:---------------------------------|--------------------:|---------------------:|---------------------:| # | VAST_2118-06A.EPOCH01.I.fits | 0.173946 | 2019-08-27 18:12:16.700000+00:00 | 319.652 | -6.2989 | 6.7401 | # | VAST_2118-06A.EPOCH03x.I.fits | 0.165395 | 2019-10-29 10:01:20.500000+00:00 | 319.652 | -6.2989 | 6.7401 | # | VAST_2118-06A.EPOCH02.I.fits | 0.16323 | 2019-10-30 08:31:20.200000+00:00 | 319.652 | -6.2989 | 6.7401 | # Explode out the img_diff column. extr_df = extr_df . explode ( 'img_diff' ) . reset_index () total_to_extract = extr_df . shape [ 0 ] if add_mode : # If we are adding images to the run we assume that monitoring was # also performed before (enforced by the pre-run checks) so now we # only want to force extract in three situations: # 1. Any force extraction in a new image. # 2. The forced extraction is attached to a new source from the new # images. # 3. A new relation has been created and they need the forced # measuremnts filled in (actually covered by 2.) extr_df = ( extr_df [ ~ extr_df [ 'img_diff' ] . isin ( done_images_df [ 'name' ])] . append ( extr_df [ ( ~ extr_df [ 'source' ] . isin ( done_source_ids )) & ( extr_df [ 'img_diff' ] . isin ( done_images_df . name )) ]) . sort_index () ) logger . info ( f \" { extr_df . shape [ 0 ] } new measurements to force extract\" f \" (from { total_to_extract } total)\" ) timer . reset () extr_df = parallel_extraction ( extr_df , images_df , sources_df [[ 'source' , 'image' , 'flux_peak' ]], min_sigma , edge_buffer , cluster_threshold , allow_nan , add_mode , p_run . path ) logger . info ( 'Force extraction step time: %.2f seconds' , timer . reset () ) # make measurement names unique for db constraint extr_df [ 'name' ] = extr_df [ 'name' ] + f '_f_run { p_run . id : 06d } ' # select sensible flux values and set the columns with fix values values = { 'flux_int' : 0 , 'flux_int_err' : 0 } extr_df = extr_df . fillna ( value = values ) extr_df = extr_df [ ( extr_df [ 'flux_int' ] != 0 ) & ( extr_df [ 'flux_int_err' ] != 0 ) & ( extr_df [ 'chi_squared_fit' ] != np . inf ) & ( extr_df [ 'chi_squared_fit' ] != np . nan ) ] default_pos_err = settings . POS_DEFAULT_MIN_ERROR / 3600. extr_df [ 'ra_err' ] = default_pos_err extr_df [ 'dec_err' ] = default_pos_err extr_df [ 'err_bmaj' ] = 0. extr_df [ 'err_bmin' ] = 0. extr_df [ 'err_pa' ] = 0. extr_df [ 'ew_sys_err' ] = cfg_err_ra extr_df [ 'ns_sys_err' ] = cfg_err_dec extr_df [ 'error_radius' ] = 0. extr_df [ 'uncertainty_ew' ] = np . hypot ( cfg_err_ra , default_pos_err ) extr_df [ 'weight_ew' ] = 1. / extr_df [ 'uncertainty_ew' ] . values ** 2 extr_df [ 'uncertainty_ns' ] = np . hypot ( cfg_err_dec , default_pos_err ) extr_df [ 'weight_ns' ] = 1. / extr_df [ 'uncertainty_ns' ] . values ** 2 extr_df [ 'flux_peak' ] = extr_df [ 'flux_int' ] extr_df [ 'flux_peak_err' ] = extr_df [ 'flux_int_err' ] extr_df [ 'local_rms' ] = extr_df [ 'flux_int_err' ] extr_df [ 'snr' ] = ( extr_df [ 'flux_peak' ] . values / extr_df [ 'local_rms' ] . values ) extr_df [ 'spectral_index' ] = 0. extr_df [ 'dr' ] = 0. extr_df [ 'd2d' ] = 0. extr_df [ 'forced' ] = True extr_df [ 'compactness' ] = 1. extr_df [ 'psf_bmaj' ] = extr_df [ 'bmaj' ] extr_df [ 'psf_bmin' ] = extr_df [ 'bmin' ] extr_df [ 'psf_pa' ] = extr_df [ 'pa' ] extr_df [ 'flag_c4' ] = False extr_df [ 'spectral_index_from_TT' ] = False extr_df [ 'has_siblings' ] = False extr_df [ 'flux_int_isl_ratio' ] = 1.0 extr_df [ 'flux_peak_isl_ratio' ] = 1.0 col_order = read_schema ( images_df . iloc [ 0 ][ 'measurements_path' ] ) . names col_order . remove ( 'id' ) remaining = list ( set ( extr_df . columns ) - set ( col_order )) extr_df = extr_df [ col_order + remaining ] # upload the measurements, a column 'id' is returned with the DB id extr_df = make_upload_measurements ( extr_df ) extr_df = extr_df . rename ( columns = { 'source_tmp_id' : 'source' }) # write forced measurements to specific parquet logger . info ( 'Saving forced measurements to specific parquet file...' ) parallel_write_parquet ( extr_df , p_run . path , add_mode ) # Required to rename this column for the image add mode. extr_df = extr_df . rename ( columns = { 'time' : 'datetime' }) # append new meas into main df and proceed with source groupby etc sources_df = sources_df . append ( extr_df . loc [:, extr_df . columns . isin ( sources_df . columns )], ignore_index = True ) # get the number of forced extractions for the run forced_parquets = glob ( os . path . join ( p_run . path , \"forced_measurements*.parquet\" )) if forced_parquets : n_forced = ( dd . read_parquet ( forced_parquets , columns = [ 'id' ]) . count () . compute () . values [ 0 ] ) else : n_forced = 0 logger . info ( 'Total forced extraction time: %.2f seconds' , timer . reset_init () ) return sources_df , n_forced","title":"forced_extraction()"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.get_data_from_parquet","text":"Get the prefix, max id and image id from the measurements parquets Parameters: Name Type Description Default file str a string with the path of the measurements parquet file required p_run_path str Pipeline run path to get forced parquet in case of add mode. required add_mode bool Whether image add mode is being used where the forced parquet needs to be used instead. False Returns: Type Description Dict Dictionary with prefix string, an interger max_id and a string with the id of the image Source code in vast_pipeline/pipeline/forced_extraction.py def get_data_from_parquet ( file : str , p_run_path : str , add_mode : bool = False ,) -> Dict : ''' Get the prefix, max id and image id from the measurements parquets Args: file: a string with the path of the measurements parquet file p_run_path: Pipeline run path to get forced parquet in case of add mode. add_mode: Whether image add mode is being used where the forced parquet needs to be used instead. Returns: Dictionary with prefix string, an interger max_id and a string with the id of the image ''' if add_mode : image_name = file . split ( \"/\" )[ - 2 ] forced_parquet = os . path . join ( p_run_path , f \"forced_measurements_ { image_name } .parquet\" ) if os . path . isfile ( forced_parquet ): file = forced_parquet # get max component id from parquet file df = pd . read_parquet ( file , columns = [ 'island_id' , 'image_id' ]) prefix = df [ 'island_id' ] . iloc [ 0 ] . rsplit ( '_' , maxsplit = 1 )[ 0 ] + '_' max_id = ( df [ 'island_id' ] . str . rsplit ( '_' , n = 1 ) . str . get ( - 1 ) . astype ( int ) . values . max () + 1 ) return { 'prefix' : prefix , 'max_id' : max_id , 'id' : df [ 'image_id' ] . iloc [ 0 ]}","title":"get_data_from_parquet()"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.parallel_extraction","text":"Parallelize forced extraction with Dask Parameters: Name Type Description Default df DataFrame dataframe with columns 'wavg_ra', 'wavg_dec', 'img_diff', 'detection' required df_images DataFrame dataframe with the images data and columns 'id', 'measurements_path', 'path', 'noise_path', 'beam_bmaj', 'beam_bmin', 'beam_bpa', 'background_path', 'rms_min', 'datetime', 'skyreg__centre_ra', 'skyreg__centre_dec', 'skyreg__xtr_radius' and 'name' as the index. required df_sources DataFrame dataframe derived from the measurement data with columns 'source', 'image', 'flux_peak'. required min_sigma float minimum sigma value to drop forced extracted measurements. required edge_buffer float flag to pass to ForcedPhot.measure method. required cluster_threshold float flag to pass to ForcedPhot.measure method. required allow_nan bool flag to pass to ForcedPhot.measure method. required add_mode bool True when the pipeline is running in add image mode. required p_run_path str The system path of the pipeline run output. required Returns: Type Description DataFrame Dataframe with forced extracted measurements data, columns are 'source_tmp_id', 'ra', 'dec', 'image', 'flux_peak', 'island_id', 'component_id', 'name', 'flux_int', 'flux_int_err' Source code in vast_pipeline/pipeline/forced_extraction.py def parallel_extraction ( df : pd . DataFrame , df_images : pd . DataFrame , df_sources : pd . DataFrame , min_sigma : float , edge_buffer : float , cluster_threshold : float , allow_nan : bool , add_mode : bool , p_run_path : str ) -> pd . DataFrame : \"\"\" Parallelize forced extraction with Dask Args: df: dataframe with columns 'wavg_ra', 'wavg_dec', 'img_diff', 'detection' df_images: dataframe with the images data and columns 'id', 'measurements_path', 'path', 'noise_path', 'beam_bmaj', 'beam_bmin', 'beam_bpa', 'background_path', 'rms_min', 'datetime', 'skyreg__centre_ra', 'skyreg__centre_dec', 'skyreg__xtr_radius' and 'name' as the index. df_sources: dataframe derived from the measurement data with columns 'source', 'image', 'flux_peak'. min_sigma: minimum sigma value to drop forced extracted measurements. edge_buffer: flag to pass to ForcedPhot.measure method. cluster_threshold: flag to pass to ForcedPhot.measure method. allow_nan: flag to pass to ForcedPhot.measure method. add_mode: True when the pipeline is running in add image mode. p_run_path: The system path of the pipeline run output. Returns: Dataframe with forced extracted measurements data, columns are 'source_tmp_id', 'ra', 'dec', 'image', 'flux_peak', 'island_id', 'component_id', 'name', 'flux_int', 'flux_int_err' \"\"\" # explode the lists in 'img_diff' column (this will make a copy of the df) out = ( df . rename ( columns = { 'img_diff' : 'image' , 'source' : 'source_tmp_id' }) # merge the rms_min column from df_images . merge ( df_images [[ 'rms_min' ]], left_on = 'image' , right_on = 'name' , how = 'left' ) . rename ( columns = { 'rms_min' : 'image_rms_min' }) # merge the measurements columns 'source', 'image', 'flux_peak' . merge ( df_sources , left_on = [ 'source_tmp_id' , 'detection' ], right_on = [ 'source' , 'image' ], how = 'left' ) . drop ( columns = [ 'image_y' , 'source' ]) . rename ( columns = { 'image_x' : 'image' }) ) # drop the source for which we would have no hope of detecting predrop_shape = out . shape [ 0 ] out [ 'max_snr' ] = out [ 'flux_peak' ] . values / out [ 'image_rms_min' ] . values out = out [ out [ 'max_snr' ] > min_sigma ] . reset_index ( drop = True ) logger . debug ( \"Min forced sigma dropped %i sources\" , predrop_shape - out . shape [ 0 ] ) # drop some columns that are no longer needed and the df should look like # out # | | source_tmp_id | wavg_ra | wavg_dec | image_name | flux_peak | # |--:|--------------:|--------:|---------:|:-----------------|----------:| # | 0 | 81 | 317.607 | -8.66952 | VAST_2118-06A... | 11.555 | # | 1 | 894 | 323.803 | -2.6899 | VAST_2118-06A... | 2.178 | # | 2 | 1076 | 316.147 | -3.11408 | VAST_2118-06A... | 6.815 | # | 3 | 1353 | 322.094 | -4.44977 | VAST_2118-06A... | 1.879 | # | 4 | 1387 | 321.734 | -6.82934 | VAST_2118-06A... | 1.61 | out = ( out . drop ([ 'max_snr' , 'image_rms_min' , 'detection' ], axis = 1 ) . rename ( columns = { 'image' : 'image_name' }) ) # get the unique images to extract from unique_images_to_extract = out [ 'image_name' ] . unique () . tolist () # create a list of dictionaries with image file paths and dataframes # with data related to each images image_data_func = lambda x : { 'image' : df_images . at [ x , 'path' ], 'background' : df_images . at [ x , 'background_path' ], 'noise' : df_images . at [ x , 'noise_path' ], 'df' : out [ out [ 'image_name' ] == x ] } list_to_map = list ( map ( image_data_func , unique_images_to_extract )) # create a list of all the measurements parquet files to extract data from, # such as prefix and max_id list_meas_parquets = list ( map ( lambda el : df_images . at [ el , 'measurements_path' ], unique_images_to_extract )) del out , unique_images_to_extract , image_data_func # get a map of the columns that have a fixed value mapping = ( db . from_sequence ( list_meas_parquets , npartitions = len ( list_meas_parquets ) ) . map ( get_data_from_parquet , p_run_path , add_mode ) . compute () ) mapping = pd . DataFrame ( mapping ) # remove not used columns from images_df and merge into mapping col_to_drop = list ( filter ( lambda x : ( 'path' in x ) or ( 'skyreg' in x ), df_images . columns . values . tolist () )) mapping = ( mapping . merge ( df_images . drop ( col_to_drop , axis = 1 ) . reset_index (), on = 'id' , how = 'left' ) . drop ( 'rms_min' , axis = 1 ) . set_index ( 'name' ) ) del col_to_drop n_cpu = cpu_count () - 1 bags = db . from_sequence ( list_to_map , npartitions = len ( list_to_map )) forced_dfs = ( bags . map ( lambda x : extract_from_image ( edge_buffer = edge_buffer , cluster_threshold = cluster_threshold , allow_nan = allow_nan , ** x )) . compute () ) del bags # create intermediates dfs combining the mapping data and the forced # extracted data from the images intermediate_df = list ( map ( lambda x : { ** ( mapping . loc [ x [ 'image' ], :] . to_dict ()), ** x }, forced_dfs )) # compute the rest of the columns intermediate_df = ( db . from_sequence ( intermediate_df ) . map ( lambda x : finalise_forced_dfs ( ** x )) . compute () ) df_out = ( pd . concat ( intermediate_df , axis = 0 , sort = False ) . rename ( columns = { 'wavg_ra' : 'ra' , 'wavg_dec' : 'dec' , 'image_name' : 'image' } ) ) return df_out","title":"parallel_extraction()"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.parallel_write_parquet","text":"Parallelize writing parquet files for forced measurements. Parameters: Name Type Description Default df DataFrame Dataframe containing all the extracted measurements. required run_path str The run path of the pipeline run. required add_mode bool True when the pipeline is running in add image mode. False Returns: Type Description None None Source code in vast_pipeline/pipeline/forced_extraction.py def parallel_write_parquet ( df : pd . DataFrame , run_path : str , add_mode : bool = False ) -> None : ''' Parallelize writing parquet files for forced measurements. Args: df: Dataframe containing all the extracted measurements. run_path: The run path of the pipeline run. add_mode: True when the pipeline is running in add image mode. Returns: None ''' images = df [ 'image' ] . unique () . tolist () get_fname = lambda n : os . path . join ( run_path , 'forced_measurements_' + n . replace ( '.' , '_' ) + '.parquet' ) dfs = list ( map ( lambda x : ( df [ df [ 'image' ] == x ], get_fname ( x )), images )) n_cpu = cpu_count () - 1 # writing parquets using Dask bag bags = db . from_sequence ( dfs ) bags = bags . starmap ( lambda df , fname : write_group_to_parquet ( df , fname , add_mode )) bags . compute ( num_workers = n_cpu ) pass","title":"parallel_write_parquet()"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.remove_forced_meas","text":"Remove forced measurements from the database if forced parquet files are found. Parameters: Name Type Description Default run_path The run path of the pipeline run. required Returns: Type Description None None Source code in vast_pipeline/pipeline/forced_extraction.py def remove_forced_meas ( run_path ) -> None : ''' Remove forced measurements from the database if forced parquet files are found. Args: run_path: The run path of the pipeline run. Returns: None ''' path_glob = glob ( os . path . join ( run_path , 'forced_measurements_*.parquet' ) ) if path_glob : ids = ( dd . read_parquet ( path_glob , columns = 'id' ) . values . compute () . tolist () ) obj_to_delete = Measurement . objects . filter ( id__in = ids ) del ids if obj_to_delete . exists (): with transaction . atomic (): n_del , detail_del = obj_to_delete . delete () logger . info ( ( 'Deleting all previous forced measurement and association' ' objects for this run. Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del )","title":"remove_forced_meas()"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.write_group_to_parquet","text":"Write a dataframe correpondent to a single group/image to a parquet file. Parameters: Name Type Description Default df DataFrame Dataframe containing all the extracted measurements. required fname str The file name of the output parquet. required add_mode bool True when the pipeline is running in add image mode. required Returns: Type Description None None Source code in vast_pipeline/pipeline/forced_extraction.py def write_group_to_parquet ( df : pd . DataFrame , fname : str , add_mode : bool ) -> None : ''' Write a dataframe correpondent to a single group/image to a parquet file. Args: df: Dataframe containing all the extracted measurements. fname: The file name of the output parquet. add_mode: True when the pipeline is running in add image mode. Returns: None ''' out_df = df . drop ([ 'd2d' , 'dr' , 'source' , 'image' ], axis = 1 ) if os . path . isfile ( fname ) and add_mode : exist_df = pd . read_parquet ( fname ) out_df = exist_df . append ( out_df ) out_df . to_parquet ( fname , index = False ) pass","title":"write_group_to_parquet()"},{"location":"reference/pipeline/loading/","text":"bulk_upload_model ( djmodel , generator , batch_size = 10000 , return_ids = False ) \u00b6 Bulk upload a list of generator objects of django models to db. Parameters: Name Type Description Default djmodel Model The Django pipeline model to be uploaded. required generator Iterable[Generator[django.db.models.base.Model, NoneType, NoneType]] The generator objects of the model to upload. required batch_size int How many records to upload at once. 10000 return_ids bool When set to True, the database IDs of the uploaded objects are returned. False Returns: Type Description List[int] None or a list of the database IDs of the uploaded objects. Source code in vast_pipeline/pipeline/loading.py @transaction . atomic def bulk_upload_model ( djmodel : models . Model , generator : Iterable [ Generator [ models . Model , None , None ]], batch_size : int = 10_000 , return_ids : bool = False ) -> List [ int ]: ''' Bulk upload a list of generator objects of django models to db. Args: djmodel: The Django pipeline model to be uploaded. generator: The generator objects of the model to upload. batch_size: How many records to upload at once. return_ids: When set to True, the database IDs of the uploaded objects are returned. Returns: None or a list of the database IDs of the uploaded objects. ''' bulk_ids = [] while True : items = list ( islice ( generator , batch_size )) if not items : break out_bulk = djmodel . objects . bulk_create ( items ) logger . info ( 'Bulk created # %i %s ' , len ( out_bulk ), djmodel . __name__ ) # save the DB ids to return if return_ids : bulk_ids . extend ( list ( map ( lambda i : i . id , out_bulk ))) if return_ids : return bulk_ids make_upload_associations ( associations_df ) \u00b6 Uploads the associations from the supplied associations DataFrame. Parameters: Name Type Description Default associations_df DataFrame DataFrame containing the associations information from the pipeline. required Returns: Type Description None None. Source code in vast_pipeline/pipeline/loading.py def make_upload_associations ( associations_df : pd . DataFrame ) -> None : \"\"\" Uploads the associations from the supplied associations DataFrame. Args: associations_df: DataFrame containing the associations information from the pipeline. Returns: None. \"\"\" logger . info ( 'Upload associations...' ) bulk_upload_model ( Association , association_models_generator ( associations_df ) ) make_upload_images ( paths , config , pipeline_run ) \u00b6 Carry the first part of the pipeline, by uploading all the images to the image table and populated band and skyregion objects. Parameters: Name Type Description Default paths Dict[str, Dict[str, str]] Dictionary containing the image, noise and background paths of all the images in the pipeline run. The primary keys are selavy , 'noise' and 'background' with the secondary key being the image name. required config config The config object of the pipeline run. required pipeline_run Run The pipeline run object. required Returns: Type Description Tuple[List[vast_pipeline.models.Image], pandas.core.frame.DataFrame] A list of image objects that have been uploaded along with a DataFrame containing the information of the sky regions associated with the run. Source code in vast_pipeline/pipeline/loading.py def make_upload_images ( paths : Dict [ str , Dict [ str , str ]], config , pipeline_run : Run ) -> Tuple [ List [ Image ], pd . DataFrame ]: ''' Carry the first part of the pipeline, by uploading all the images to the image table and populated band and skyregion objects. Args: paths: Dictionary containing the image, noise and background paths of all the images in the pipeline run. The primary keys are `selavy`, 'noise' and 'background' with the secondary key being the image name. config (config): The config object of the pipeline run. pipeline_run: The pipeline run object. Returns: A list of image objects that have been uploaded along with a DataFrame containing the information of the sky regions associated with the run. ''' timer = StopWatch () images = [] skyregions = [] bands = [] for path in paths [ 'selavy' ]: # STEP #1: Load image and measurements image = SelavyImage ( path , paths , config = config ) logger . info ( 'Reading image %s ...' , image . name ) # 1.1 get/create the frequency band with transaction . atomic (): band = get_create_img_band ( image ) if band not in bands : bands . append ( band ) # 1.2 create image and skyregion entry in DB with transaction . atomic (): img , skyreg , exists_f = get_create_img ( pipeline_run , band . id , image ) # add image and skyregion to respective lists images . append ( img ) if skyreg not in skyregions : skyregions . append ( skyreg ) if exists_f : logger . info ( 'Image %s already processed, grab measurements' , img . name ) # grab the measurements and skip to process next image measurements = ( pd . Series ( Measurement . objects . filter ( forced = False , image__id = img . id ), name = 'meas_dj' ) . to_frame () ) measurements [ 'id' ] = measurements [ 'meas_dj' ] . apply ( lambda x : x . id ) continue # 1.3 get the image measurements and save them in DB measurements = image . read_selavy ( img ) logger . info ( 'Processed measurements dataframe of shape: ( %i , %i )' , measurements . shape [ 0 ], measurements . shape [ 1 ] ) # upload measurements, a column with the db is added to the df measurements = make_upload_measurements ( measurements ) # save measurements to parquet file in pipeline run folder base_folder = os . path . dirname ( img . measurements_path ) if not os . path . exists ( base_folder ): os . makedirs ( base_folder ) measurements . to_parquet ( img . measurements_path , index = False ) del measurements , image , band , img # write images parquet file under pipeline run folder images_df = pd . DataFrame ( map ( lambda x : x . __dict__ , images )) images_df = images_df . drop ( '_state' , axis = 1 ) images_df . to_parquet ( os . path . join ( config . PIPE_RUN_PATH , 'images.parquet' ), index = False ) # write skyregions parquet file under pipeline run folder skyregs_df = pd . DataFrame ( map ( lambda x : x . __dict__ , skyregions )) skyregs_df = skyregs_df . drop ( '_state' , axis = 1 ) skyregs_df . to_parquet ( os . path . join ( config . PIPE_RUN_PATH , 'skyregions.parquet' ), index = False ) # write skyregions parquet file under pipeline run folder bands_df = pd . DataFrame ( map ( lambda x : x . __dict__ , bands )) bands_df = bands_df . drop ( '_state' , axis = 1 ) bands_df . to_parquet ( os . path . join ( config . PIPE_RUN_PATH , 'bands.parquet' ), index = False ) logger . info ( 'Total images upload/loading time: %.2f seconds' , timer . reset_init () ) return images , skyregs_df make_upload_measurement_pairs ( measurement_pairs_df ) \u00b6 Uploads the measurement pairs from the supplied measurement pairs DataFrame. Parameters: Name Type Description Default measurement_pairs_df DataFrame DataFrame containing the measurement pairs information from the pipeline. required Returns: Type Description DataFrame Original DataFrame with the database ID attached to each row. Source code in vast_pipeline/pipeline/loading.py def make_upload_measurement_pairs ( measurement_pairs_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Uploads the measurement pairs from the supplied measurement pairs DataFrame. Args: measurement_pairs_df: DataFrame containing the measurement pairs information from the pipeline. Returns: Original DataFrame with the database ID attached to each row. \"\"\" meas_pair_dj_ids = bulk_upload_model ( MeasurementPair , measurement_pair_models_generator ( measurement_pairs_df ), return_ids = True ) measurement_pairs_df [ \"id\" ] = meas_pair_dj_ids return measurement_pairs_df make_upload_measurements ( measurements_df ) \u00b6 Uploads the measurements from the supplied measurements DataFrame. Parameters: Name Type Description Default measurements_df DataFrame DataFrame containing the measurements information from the pipeline. required Returns: Type Description DataFrame Original DataFrame with the database ID attached to each row. Source code in vast_pipeline/pipeline/loading.py def make_upload_measurements ( measurements_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Uploads the measurements from the supplied measurements DataFrame. Args: measurements_df: DataFrame containing the measurements information from the pipeline. Returns: Original DataFrame with the database ID attached to each row. \"\"\" meas_dj_ids = bulk_upload_model ( Measurement , measurement_models_generator ( measurements_df ), return_ids = True ) measurements_df [ 'id' ] = meas_dj_ids return measurements_df make_upload_related_sources ( related_df ) \u00b6 Uploads the related sources from the supplied related sources DataFrame. Parameters: Name Type Description Default related_df DataFrame DataFrame containing the related sources information from the pipeline. required Returns: Type Description None None. Source code in vast_pipeline/pipeline/loading.py def make_upload_related_sources ( related_df : pd . DataFrame ) -> None : \"\"\" Uploads the related sources from the supplied related sources DataFrame. Args: related_df: DataFrame containing the related sources information from the pipeline. Returns: None. \"\"\" logger . info ( 'Populate \"related\" field of sources...' ) bulk_upload_model ( RelatedSource , related_models_generator ( related_df )) make_upload_sources ( sources_df , pipeline_run , add_mode = False ) \u00b6 Delete previous sources for given pipeline run and bulk upload new found sources as well as related sources. Parameters: Name Type Description Default sources_df DataFrame Holds the measurements associated into sources. The output of of thE association step. required pipeline_run Run The pipeline Run object. required add_mode bool Whether the pipeline is running in add image mode. False Returns: Type Description DataFrame The input dataframe with the 'id' column added. Source code in vast_pipeline/pipeline/loading.py def make_upload_sources ( sources_df : pd . DataFrame , pipeline_run : Run , add_mode : bool = False ) -> pd . DataFrame : ''' Delete previous sources for given pipeline run and bulk upload new found sources as well as related sources. Args: sources_df: Holds the measurements associated into sources. The output of of thE association step. pipeline_run: The pipeline Run object. add_mode: Whether the pipeline is running in add image mode. Returns: The input dataframe with the 'id' column added. ''' # create sources in DB with transaction . atomic (): if ( add_mode is False and Source . objects . filter ( run = pipeline_run ) . exists ()): logger . info ( 'Removing objects from previous pipeline run' ) n_del , detail_del = ( Source . objects . filter ( run = pipeline_run ) . delete () ) logger . info ( ( 'Deleting all sources and related objects for this run. ' 'Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) src_dj_ids = bulk_upload_model ( Source , source_models_generator ( sources_df , pipeline_run = pipeline_run ), return_ids = True ) sources_df [ 'id' ] = src_dj_ids return sources_df SQL_update ( df , model , index = None , columns = None ) \u00b6 Generate the SQL code required to update the database. Parameters: Name Type Description Default df DataFrame DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. required model Model The model that is being updated. required index Optional[str] Header of the column to join on, determines which rows in the different tables match. If None, then use the primary key column. None columns Optional[List[str]] The column headers of the columns to be updated. If None, updates all columns except the index column. None Returns: Type Description str The SQL command to update the database. Source code in vast_pipeline/pipeline/loading.py def SQL_update ( df : pd . DataFrame , model : models . Model , index : Optional [ str ] = None , columns : Optional [ List [ str ]] = None ) -> str : ''' Generate the SQL code required to update the database. Args: df: DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. model: The model that is being updated. index: Header of the column to join on, determines which rows in the different tables match. If None, then use the primary key column. columns: The column headers of the columns to be updated. If None, updates all columns except the index column. Returns: The SQL command to update the database. ''' # set index and columns if None if index is None : index = model . _meta . pk . name if columns is None : columns = df . columns . tolist () columns . remove ( index ) # get names table = model . _meta . db_table new_columns = ', ' . join ( 'new_' + c for c in columns ) set_columns = ', ' . join ( c + '=new_' + c for c in columns ) # get index values and new values column_headers = [ index ] column_headers . extend ( columns ) data_arr = df [ column_headers ] . to_numpy () values = [] for row in data_arr : val_row = '(' + ', ' . join ( f ' { val } ' for val in row ) + ')' values . append ( val_row ) values = ', ' . join ( values ) # update database SQL_comm = f \"\"\" UPDATE { table } SET { set_columns } FROM (VALUES { values } ) AS new_values (index_col, { new_columns } ) WHERE { index } =index_col; \"\"\" return SQL_comm update_sources ( sources_df , batch_size = 10000 ) \u00b6 Update database using SQL code. This function opens one connection to the database, and closes it after the update is done. Parameters: Name Type Description Default sources_df DataFrame DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. required batch_size int The df rows are broken into chunks, each chunk is executed in a separate SQL command, batch_size determines the maximum size of the chunk. 10000 Returns: Type Description DataFrame DataFrame containing the new data to be uploaded to the database. Source code in vast_pipeline/pipeline/loading.py def update_sources ( sources_df : pd . DataFrame , batch_size : int = 10_000 ) -> pd . DataFrame : ''' Update database using SQL code. This function opens one connection to the database, and closes it after the update is done. Args: sources_df: DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. batch_size: The df rows are broken into chunks, each chunk is executed in a separate SQL command, batch_size determines the maximum size of the chunk. Returns: DataFrame containing the new data to be uploaded to the database. ''' # Get all possible columns from the model all_source_table_cols = [ fld . attname for fld in Source . _meta . get_fields () if getattr ( fld , 'attname' , None ) is not None ] # Filter to those present in sources_df columns = [ col for col in all_source_table_cols if col in sources_df . columns ] sources_df [ 'id' ] = sources_df . index . values batches = np . ceil ( len ( sources_df ) / batch_size ) dfs = np . array_split ( sources_df , batches ) with connection . cursor () as cursor : for df_batch in dfs : SQL_comm = SQL_update ( df_batch , Source , index = 'id' , columns = columns ) cursor . execute ( SQL_comm ) return sources_df","title":"loading.py"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.bulk_upload_model","text":"Bulk upload a list of generator objects of django models to db. Parameters: Name Type Description Default djmodel Model The Django pipeline model to be uploaded. required generator Iterable[Generator[django.db.models.base.Model, NoneType, NoneType]] The generator objects of the model to upload. required batch_size int How many records to upload at once. 10000 return_ids bool When set to True, the database IDs of the uploaded objects are returned. False Returns: Type Description List[int] None or a list of the database IDs of the uploaded objects. Source code in vast_pipeline/pipeline/loading.py @transaction . atomic def bulk_upload_model ( djmodel : models . Model , generator : Iterable [ Generator [ models . Model , None , None ]], batch_size : int = 10_000 , return_ids : bool = False ) -> List [ int ]: ''' Bulk upload a list of generator objects of django models to db. Args: djmodel: The Django pipeline model to be uploaded. generator: The generator objects of the model to upload. batch_size: How many records to upload at once. return_ids: When set to True, the database IDs of the uploaded objects are returned. Returns: None or a list of the database IDs of the uploaded objects. ''' bulk_ids = [] while True : items = list ( islice ( generator , batch_size )) if not items : break out_bulk = djmodel . objects . bulk_create ( items ) logger . info ( 'Bulk created # %i %s ' , len ( out_bulk ), djmodel . __name__ ) # save the DB ids to return if return_ids : bulk_ids . extend ( list ( map ( lambda i : i . id , out_bulk ))) if return_ids : return bulk_ids","title":"bulk_upload_model()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_associations","text":"Uploads the associations from the supplied associations DataFrame. Parameters: Name Type Description Default associations_df DataFrame DataFrame containing the associations information from the pipeline. required Returns: Type Description None None. Source code in vast_pipeline/pipeline/loading.py def make_upload_associations ( associations_df : pd . DataFrame ) -> None : \"\"\" Uploads the associations from the supplied associations DataFrame. Args: associations_df: DataFrame containing the associations information from the pipeline. Returns: None. \"\"\" logger . info ( 'Upload associations...' ) bulk_upload_model ( Association , association_models_generator ( associations_df ) )","title":"make_upload_associations()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_images","text":"Carry the first part of the pipeline, by uploading all the images to the image table and populated band and skyregion objects. Parameters: Name Type Description Default paths Dict[str, Dict[str, str]] Dictionary containing the image, noise and background paths of all the images in the pipeline run. The primary keys are selavy , 'noise' and 'background' with the secondary key being the image name. required config config The config object of the pipeline run. required pipeline_run Run The pipeline run object. required Returns: Type Description Tuple[List[vast_pipeline.models.Image], pandas.core.frame.DataFrame] A list of image objects that have been uploaded along with a DataFrame containing the information of the sky regions associated with the run. Source code in vast_pipeline/pipeline/loading.py def make_upload_images ( paths : Dict [ str , Dict [ str , str ]], config , pipeline_run : Run ) -> Tuple [ List [ Image ], pd . DataFrame ]: ''' Carry the first part of the pipeline, by uploading all the images to the image table and populated band and skyregion objects. Args: paths: Dictionary containing the image, noise and background paths of all the images in the pipeline run. The primary keys are `selavy`, 'noise' and 'background' with the secondary key being the image name. config (config): The config object of the pipeline run. pipeline_run: The pipeline run object. Returns: A list of image objects that have been uploaded along with a DataFrame containing the information of the sky regions associated with the run. ''' timer = StopWatch () images = [] skyregions = [] bands = [] for path in paths [ 'selavy' ]: # STEP #1: Load image and measurements image = SelavyImage ( path , paths , config = config ) logger . info ( 'Reading image %s ...' , image . name ) # 1.1 get/create the frequency band with transaction . atomic (): band = get_create_img_band ( image ) if band not in bands : bands . append ( band ) # 1.2 create image and skyregion entry in DB with transaction . atomic (): img , skyreg , exists_f = get_create_img ( pipeline_run , band . id , image ) # add image and skyregion to respective lists images . append ( img ) if skyreg not in skyregions : skyregions . append ( skyreg ) if exists_f : logger . info ( 'Image %s already processed, grab measurements' , img . name ) # grab the measurements and skip to process next image measurements = ( pd . Series ( Measurement . objects . filter ( forced = False , image__id = img . id ), name = 'meas_dj' ) . to_frame () ) measurements [ 'id' ] = measurements [ 'meas_dj' ] . apply ( lambda x : x . id ) continue # 1.3 get the image measurements and save them in DB measurements = image . read_selavy ( img ) logger . info ( 'Processed measurements dataframe of shape: ( %i , %i )' , measurements . shape [ 0 ], measurements . shape [ 1 ] ) # upload measurements, a column with the db is added to the df measurements = make_upload_measurements ( measurements ) # save measurements to parquet file in pipeline run folder base_folder = os . path . dirname ( img . measurements_path ) if not os . path . exists ( base_folder ): os . makedirs ( base_folder ) measurements . to_parquet ( img . measurements_path , index = False ) del measurements , image , band , img # write images parquet file under pipeline run folder images_df = pd . DataFrame ( map ( lambda x : x . __dict__ , images )) images_df = images_df . drop ( '_state' , axis = 1 ) images_df . to_parquet ( os . path . join ( config . PIPE_RUN_PATH , 'images.parquet' ), index = False ) # write skyregions parquet file under pipeline run folder skyregs_df = pd . DataFrame ( map ( lambda x : x . __dict__ , skyregions )) skyregs_df = skyregs_df . drop ( '_state' , axis = 1 ) skyregs_df . to_parquet ( os . path . join ( config . PIPE_RUN_PATH , 'skyregions.parquet' ), index = False ) # write skyregions parquet file under pipeline run folder bands_df = pd . DataFrame ( map ( lambda x : x . __dict__ , bands )) bands_df = bands_df . drop ( '_state' , axis = 1 ) bands_df . to_parquet ( os . path . join ( config . PIPE_RUN_PATH , 'bands.parquet' ), index = False ) logger . info ( 'Total images upload/loading time: %.2f seconds' , timer . reset_init () ) return images , skyregs_df","title":"make_upload_images()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_measurement_pairs","text":"Uploads the measurement pairs from the supplied measurement pairs DataFrame. Parameters: Name Type Description Default measurement_pairs_df DataFrame DataFrame containing the measurement pairs information from the pipeline. required Returns: Type Description DataFrame Original DataFrame with the database ID attached to each row. Source code in vast_pipeline/pipeline/loading.py def make_upload_measurement_pairs ( measurement_pairs_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Uploads the measurement pairs from the supplied measurement pairs DataFrame. Args: measurement_pairs_df: DataFrame containing the measurement pairs information from the pipeline. Returns: Original DataFrame with the database ID attached to each row. \"\"\" meas_pair_dj_ids = bulk_upload_model ( MeasurementPair , measurement_pair_models_generator ( measurement_pairs_df ), return_ids = True ) measurement_pairs_df [ \"id\" ] = meas_pair_dj_ids return measurement_pairs_df","title":"make_upload_measurement_pairs()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_measurements","text":"Uploads the measurements from the supplied measurements DataFrame. Parameters: Name Type Description Default measurements_df DataFrame DataFrame containing the measurements information from the pipeline. required Returns: Type Description DataFrame Original DataFrame with the database ID attached to each row. Source code in vast_pipeline/pipeline/loading.py def make_upload_measurements ( measurements_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Uploads the measurements from the supplied measurements DataFrame. Args: measurements_df: DataFrame containing the measurements information from the pipeline. Returns: Original DataFrame with the database ID attached to each row. \"\"\" meas_dj_ids = bulk_upload_model ( Measurement , measurement_models_generator ( measurements_df ), return_ids = True ) measurements_df [ 'id' ] = meas_dj_ids return measurements_df","title":"make_upload_measurements()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_related_sources","text":"Uploads the related sources from the supplied related sources DataFrame. Parameters: Name Type Description Default related_df DataFrame DataFrame containing the related sources information from the pipeline. required Returns: Type Description None None. Source code in vast_pipeline/pipeline/loading.py def make_upload_related_sources ( related_df : pd . DataFrame ) -> None : \"\"\" Uploads the related sources from the supplied related sources DataFrame. Args: related_df: DataFrame containing the related sources information from the pipeline. Returns: None. \"\"\" logger . info ( 'Populate \"related\" field of sources...' ) bulk_upload_model ( RelatedSource , related_models_generator ( related_df ))","title":"make_upload_related_sources()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_sources","text":"Delete previous sources for given pipeline run and bulk upload new found sources as well as related sources. Parameters: Name Type Description Default sources_df DataFrame Holds the measurements associated into sources. The output of of thE association step. required pipeline_run Run The pipeline Run object. required add_mode bool Whether the pipeline is running in add image mode. False Returns: Type Description DataFrame The input dataframe with the 'id' column added. Source code in vast_pipeline/pipeline/loading.py def make_upload_sources ( sources_df : pd . DataFrame , pipeline_run : Run , add_mode : bool = False ) -> pd . DataFrame : ''' Delete previous sources for given pipeline run and bulk upload new found sources as well as related sources. Args: sources_df: Holds the measurements associated into sources. The output of of thE association step. pipeline_run: The pipeline Run object. add_mode: Whether the pipeline is running in add image mode. Returns: The input dataframe with the 'id' column added. ''' # create sources in DB with transaction . atomic (): if ( add_mode is False and Source . objects . filter ( run = pipeline_run ) . exists ()): logger . info ( 'Removing objects from previous pipeline run' ) n_del , detail_del = ( Source . objects . filter ( run = pipeline_run ) . delete () ) logger . info ( ( 'Deleting all sources and related objects for this run. ' 'Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) src_dj_ids = bulk_upload_model ( Source , source_models_generator ( sources_df , pipeline_run = pipeline_run ), return_ids = True ) sources_df [ 'id' ] = src_dj_ids return sources_df","title":"make_upload_sources()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.SQL_update","text":"Generate the SQL code required to update the database. Parameters: Name Type Description Default df DataFrame DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. required model Model The model that is being updated. required index Optional[str] Header of the column to join on, determines which rows in the different tables match. If None, then use the primary key column. None columns Optional[List[str]] The column headers of the columns to be updated. If None, updates all columns except the index column. None Returns: Type Description str The SQL command to update the database. Source code in vast_pipeline/pipeline/loading.py def SQL_update ( df : pd . DataFrame , model : models . Model , index : Optional [ str ] = None , columns : Optional [ List [ str ]] = None ) -> str : ''' Generate the SQL code required to update the database. Args: df: DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. model: The model that is being updated. index: Header of the column to join on, determines which rows in the different tables match. If None, then use the primary key column. columns: The column headers of the columns to be updated. If None, updates all columns except the index column. Returns: The SQL command to update the database. ''' # set index and columns if None if index is None : index = model . _meta . pk . name if columns is None : columns = df . columns . tolist () columns . remove ( index ) # get names table = model . _meta . db_table new_columns = ', ' . join ( 'new_' + c for c in columns ) set_columns = ', ' . join ( c + '=new_' + c for c in columns ) # get index values and new values column_headers = [ index ] column_headers . extend ( columns ) data_arr = df [ column_headers ] . to_numpy () values = [] for row in data_arr : val_row = '(' + ', ' . join ( f ' { val } ' for val in row ) + ')' values . append ( val_row ) values = ', ' . join ( values ) # update database SQL_comm = f \"\"\" UPDATE { table } SET { set_columns } FROM (VALUES { values } ) AS new_values (index_col, { new_columns } ) WHERE { index } =index_col; \"\"\" return SQL_comm","title":"SQL_update()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.update_sources","text":"Update database using SQL code. This function opens one connection to the database, and closes it after the update is done. Parameters: Name Type Description Default sources_df DataFrame DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. required batch_size int The df rows are broken into chunks, each chunk is executed in a separate SQL command, batch_size determines the maximum size of the chunk. 10000 Returns: Type Description DataFrame DataFrame containing the new data to be uploaded to the database. Source code in vast_pipeline/pipeline/loading.py def update_sources ( sources_df : pd . DataFrame , batch_size : int = 10_000 ) -> pd . DataFrame : ''' Update database using SQL code. This function opens one connection to the database, and closes it after the update is done. Args: sources_df: DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. batch_size: The df rows are broken into chunks, each chunk is executed in a separate SQL command, batch_size determines the maximum size of the chunk. Returns: DataFrame containing the new data to be uploaded to the database. ''' # Get all possible columns from the model all_source_table_cols = [ fld . attname for fld in Source . _meta . get_fields () if getattr ( fld , 'attname' , None ) is not None ] # Filter to those present in sources_df columns = [ col for col in all_source_table_cols if col in sources_df . columns ] sources_df [ 'id' ] = sources_df . index . values batches = np . ceil ( len ( sources_df ) / batch_size ) dfs = np . array_split ( sources_df , batches ) with connection . cursor () as cursor : for df_batch in dfs : SQL_comm = SQL_update ( df_batch , Source , index = 'id' , columns = columns ) cursor . execute ( SQL_comm ) return sources_df","title":"update_sources()"},{"location":"reference/pipeline/main/","text":"This module contains the main pipeline class used for processing a pipeline run. Pipeline \u00b6 Instance of a pipeline. All the methods run stages of the pipeline processing. Attributes: Name Type Description name str The name of the pipeline run. config config The configuration options of the pipeline run. epoch_based bool Whether the pipeline run is in epoch_based mode or not. __init__ ( self , name , config_path ) special \u00b6 Initialise the pipeline with attributed such as configuration file path, name, and list of images and related files (e.g. selavy) Parameters: Name Type Description Default name str The pipeline name. required config_path str The system path to the configuration file. required Returns: Type Description None None. Source code in vast_pipeline/pipeline/main.py def __init__ ( self , name : str , config_path : str ) -> None : ''' Initialise the pipeline with attributed such as configuration file path, name, and list of images and related files (e.g. selavy) Args: name: The pipeline name. config_path: The system path to the configuration file. Returns: None. ''' self . name = name self . config = self . load_cfg ( config_path ) # The epoch_based parameter below is for # if the user has entered just lists we don't have # access to the dates until the Image instances are # created. So we flag this as true so that we can # reorder the epochs once the date information is available. # It is also recorded in the database such that there is a record # of the fact that the run was processed in an epoch based mode. self . epoch_based = False # Check if provided files are lists and convert to # dictionaries if so self . config , self . epoch_based = self . check_for_epoch_based ( self . config ) check_current_runs () staticmethod \u00b6 Checks the number of pipeline runs currently being processed. Returns: Type Description None None Exceptions: Type Description MaxPipelineRunsError Raised if the number of pipeline runs currently being processed is larger than the allowed maximum. Source code in vast_pipeline/pipeline/main.py @staticmethod def check_current_runs () -> None : \"\"\" Checks the number of pipeline runs currently being processed. Returns: None Raises: MaxPipelineRunsError: Raised if the number of pipeline runs currently being processed is larger than the allowed maximum. \"\"\" if Run . objects . check_max_runs ( settings . MAX_PIPELINE_RUNS ): raise MaxPipelineRunsError check_for_epoch_based ( cfg ) staticmethod \u00b6 Checks whether the images have been provided in a Dictionary format which means that epoch_based has been requested. If they have been provided with just lists then the inputs are converted to dictionaries with an epoch defined for each individual image. Parameters: Name Type Description Default cfg module The config object. required Returns: Type Description Tuple[module, bool] The config (with converted List -> Dict inputs if required) and epoch_based boolean flag. Exceptions: Type Description PipelineConfigError Raised if the images are entered into the configuration file in a format that cannot be read. Source code in vast_pipeline/pipeline/main.py @staticmethod def check_for_epoch_based ( cfg : ModuleType ) -> Tuple [ ModuleType , bool ]: # Config typing above is unknown what to put. \"\"\" Checks whether the images have been provided in a Dictionary format which means that epoch_based has been requested. If they have been provided with just lists then the inputs are converted to dictionaries with an epoch defined for each individual image. Args: cfg: The config object. Returns: The config (with converted List -> Dict inputs if required) and epoch_based boolean flag. Raises: PipelineConfigError: Raised if the images are entered into the configuration file in a format that cannot be read. \"\"\" epoch_based = False for cfg_key in [ 'IMAGE_FILES' , 'SELAVY_FILES' , 'BACKGROUND_FILES' , 'NOISE_FILES' ]: if isinstance ( getattr ( cfg , cfg_key ), list ): setattr ( cfg , cfg_key , convert_list_to_dict ( getattr ( cfg , cfg_key )) ) elif isinstance ( getattr ( cfg , cfg_key ), dict ): # Set to True if dictionaries are passed. epoch_based = True else : raise PipelineConfigError (( 'Unknown images entry format!' f ' Must be a list or dictionary.' )) return cfg , epoch_based check_prev_config_diff ( self , p_run_path ) \u00b6 Checks if the previous config file differs from the current config file. Used in add mode. Only returns true if the images are different and the other general settings are the same (the requirement for add mode). Otherwise False is returned. Parameters: Name Type Description Default p_run_path str The path of the pipeline run where the parquets are stored. required Returns: Type Description bool True if images are different but general settings are the same. Otherwise False is returned. Source code in vast_pipeline/pipeline/main.py def check_prev_config_diff ( self , p_run_path : str ) -> bool : \"\"\" Checks if the previous config file differs from the current config file. Used in add mode. Only returns true if the images are different and the other general settings are the same (the requirement for add mode). Otherwise False is returned. Args: p_run_path: The path of the pipeline run where the parquets are stored. Returns: True if images are different but general settings are the same. Otherwise False is returned. \"\"\" valid_keys = self . _get_valid_keys ( upper = True ) prev_config , _ = self . check_for_epoch_based ( self . load_cfg ( os . path . join ( p_run_path , 'config_prev.py' ))) prev_config_dict = { k : getattr ( prev_config , k ) for k in valid_keys } current_config_dict = { k : getattr ( self . config , k ) for k in valid_keys } if prev_config_dict == current_config_dict : return True image_check = ( prev_config_dict [ 'IMAGE_FILES' ] == current_config_dict [ 'IMAGE_FILES' ] ) for i in [ 'IMAGE_FILES' , 'SELAVY_FILES' , 'NOISE_FILES' , 'BACKGROUND_FILES' ]: prev_config_dict . pop ( i ) current_config_dict . pop ( i ) settings_check = prev_config_dict == current_config_dict if not image_check and settings_check : return False return True load_cfg ( cfg ) staticmethod \u00b6 Check the given Config path. Throw exception if any problems return the config object as module/class. Parameters: Name Type Description Default cfg str The system path to the pipeline run configuration file. required Returns: Type Description module The configuration options read from the input file. Exceptions: Type Description PipelineConfigError Raised if the configuration file cannot be found. Source code in vast_pipeline/pipeline/main.py @staticmethod def load_cfg ( cfg : str ) -> ModuleType : \"\"\" Check the given Config path. Throw exception if any problems return the config object as module/class. Args: cfg: The system path to the pipeline run configuration file. Returns: The configuration options read from the input file. Raises: PipelineConfigError: Raised if the configuration file cannot be found. \"\"\" if not os . path . exists ( cfg ): raise PipelineConfigError ( 'pipeline run config file not existent' ) # load the run config as a Python module spec = spec_from_file_location ( 'run_config' , cfg ) mod = module_from_spec ( spec ) spec . loader . exec_module ( mod ) return mod match_images_to_data ( self ) \u00b6 Loops through images and matches the selavy, noise and bkg images. Assumes that user has enteted images and other data in the same order. Returns: Type Description None None Source code in vast_pipeline/pipeline/main.py def match_images_to_data ( self ) -> None : \"\"\" Loops through images and matches the selavy, noise and bkg images. Assumes that user has enteted images and other data in the same order. Returns: None \"\"\" self . img_paths = { 'selavy' : {}, 'noise' : {}, 'background' : {} } self . img_epochs = {} for key in sorted ( self . config . IMAGE_FILES . keys ()): for x , y in zip ( self . config . IMAGE_FILES [ key ], self . config . SELAVY_FILES [ key ] ): self . img_paths [ 'selavy' ][ x ] = y for x , y in zip ( self . config . IMAGE_FILES [ key ], self . config . NOISE_FILES [ key ] ): self . img_paths [ 'noise' ][ x ] = y # check if backgound files have been given before # attempting to match if key in self . config . BACKGROUND_FILES : for x , y in zip ( self . config . IMAGE_FILES [ key ], self . config . BACKGROUND_FILES [ key ] ): self . img_paths [ 'background' ][ x ] = y for x in self . config . IMAGE_FILES [ key ]: self . img_epochs [ os . path . basename ( x )] = key process_pipeline ( self , p_run ) \u00b6 The function that performs the processing operations of the pipeline run. Parameters: Name Type Description Default p_run Run The pipeline run model object. required Returns: Type Description None None Source code in vast_pipeline/pipeline/main.py def process_pipeline ( self , p_run : Run ) -> None : \"\"\" The function that performs the processing operations of the pipeline run. Args: p_run: The pipeline run model object. Returns: None \"\"\" logger . info ( f 'Epoch based association: { self . epoch_based } ' ) if self . add_mode : logger . info ( 'Running in image add mode.' ) # Update epoch based flag to not cause user confusion when running # the pipeline (i.e. if it was only updated at the end). It is not # updated if the pipeline is being run in add mode. if self . epoch_based and not self . add_mode : with transaction . atomic (): p_run . epoch_based = self . epoch_based p_run . save () # Match the image files to the respective selavy, noise and bkg files. # Do this after validation is successful. self . match_images_to_data () # upload/retrieve image data images , skyregs_df = make_upload_images ( self . img_paths , self . config , p_run ) # STEP #2: measurements association # order images by time images . sort ( key = operator . attrgetter ( 'datetime' )) # If the user has given lists we need to reorder the # image epochs such that they are in date order. if self . epoch_based is False : self . img_epochs = {} for i , img in enumerate ( images ): self . img_epochs [ img . name ] = i + 1 image_epochs = [ self . img_epochs [ img . name ] for img in images ] limit = Angle ( self . config . ASSOCIATION_RADIUS * u . arcsec ) dr_limit = self . config . ASSOCIATION_DE_RUITER_RADIUS bw_limit = self . config . ASSOCIATION_BEAMWIDTH_LIMIT duplicate_limit = Angle ( self . config . ASSOCIATION_EPOCH_DUPLICATE_RADIUS * u . arcsec ) # 2.1 Check if sky regions to be associated can be # split into connected point groups skyregion_groups = group_skyregions ( skyregs_df [[ 'id' , 'centre_ra' , 'centre_dec' , 'xtr_radius' ]] ) n_skyregion_groups = skyregion_groups [ 'skyreg_group' ] . unique () . shape [ 0 ] # Get already done images if in add mode if self . add_mode : done_images_df = pd . read_parquet ( self . previous_parquets [ 'images' ], columns = [ 'id' , 'name' ] ) done_source_ids = pd . read_parquet ( self . previous_parquets [ 'sources' ], columns = [ 'wavg_ra' ] ) . index . tolist () else : done_images_df = None done_source_ids = None # 2.2 Associate with other measurements if self . config . ASSOCIATION_PARALLEL and n_skyregion_groups > 1 : images_df = get_parallel_assoc_image_df ( images , skyregion_groups ) images_df [ 'epoch' ] = image_epochs sources_df = parallel_association ( images_df , limit , dr_limit , bw_limit , duplicate_limit , self . config , n_skyregion_groups , self . add_mode , self . previous_parquets , done_images_df , done_source_ids ) else : images_df = pd . DataFrame . from_dict ( { 'image_dj' : images , 'epoch' : image_epochs } ) images_df [ 'skyreg_id' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . skyreg_id ) images_df [ 'image_name' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . name ) sources_df = association ( images_df , limit , dr_limit , bw_limit , duplicate_limit , self . config , self . add_mode , self . previous_parquets , done_images_df ) # 2.3 Associate Measurements with reference survey sources if SurveySource . objects . exists (): pass # Obtain the number of selavy measurements for the run # n_selavy_measurements = sources_df. nr_selavy_measurements = sources_df [ 'id' ] . unique () . shape [ 0 ] # STEP #3: Merge sky regions and sources ready for # steps 4 and 5 below. missing_source_cols = [ 'source' , 'datetime' , 'image' , 'epoch' , 'interim_ew' , 'weight_ew' , 'interim_ns' , 'weight_ns' ] # need to make sure no forced measurments are being passed which # could happen in add mode, otherwise the wrong detection image is # assigned. missing_sources_df = get_src_skyregion_merged_df ( sources_df . loc [ sources_df [ 'forced' ] == False , missing_source_cols ], images_df , skyregs_df , ) # STEP #4 New source analysis new_sources_df = new_sources ( sources_df , missing_sources_df , self . config . NEW_SOURCE_MIN_SIGMA , self . config . MONITOR_EDGE_BUFFER_SCALE , p_run ) # Drop column no longer required in missing_sources_df. missing_sources_df = ( missing_sources_df . drop ([ 'in_primary' ], axis = 1 ) ) # STEP #5: Run forced extraction/photometry if asked if self . config . MONITOR : ( sources_df , nr_forced_measurements ) = forced_extraction ( sources_df , self . config . ASTROMETRIC_UNCERTAINTY_RA / 3600. , self . config . ASTROMETRIC_UNCERTAINTY_DEC / 3600. , p_run , missing_sources_df , self . config . MONITOR_MIN_SIGMA , self . config . MONITOR_EDGE_BUFFER_SCALE , self . config . MONITOR_CLUSTER_THRESHOLD , self . config . MONITOR_ALLOW_NAN , self . add_mode , done_images_df , done_source_ids ) del missing_sources_df # STEP #6: finalise the df getting unique sources, calculating # metrics and upload data to database nr_sources = final_operations ( sources_df , p_run , new_sources_df , self . config . SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS , self . add_mode , done_source_ids , self . previous_parquets ) # calculate number processed images nr_img_processed = len ( images ) # update pipeline run with the nr images and sources with transaction . atomic (): p_run . n_images = nr_img_processed p_run . n_sources = nr_sources p_run . n_selavy_measurements = nr_selavy_measurements p_run . n_forced_measurements = ( nr_forced_measurements if self . config . MONITOR else 0 ) p_run . save () pass set_status ( pipe_run , status = None ) staticmethod \u00b6 Function to change the status of a pipeline run model object and save to the database. Parameters: Name Type Description Default pipe_run Run The pipeline run model object. required status str The status to set. None Returns: Type Description None None Source code in vast_pipeline/pipeline/main.py @staticmethod def set_status ( pipe_run : Run , status : str = None ) -> None : \"\"\" Function to change the status of a pipeline run model object and save to the database. Args: pipe_run: The pipeline run model object. status: The status to set. Returns: None \"\"\" #TODO: This function gives no feedback if the status is not accepted? choices = [ x [ 0 ] for x in Run . _meta . get_field ( 'status' ) . choices ] if status and status in choices and pipe_run . status != status : with transaction . atomic (): pipe_run . status = status pipe_run . save () validate_cfg ( self , user = None ) \u00b6 Validates a pipeline run configuration against default parameters and for different settings (e.g. force extraction). Parameters: Name Type Description Default user User The User of the request if made through the UI. Defaults to None. None Returns: Type Description None None Exceptions: Type Description PipelineConfigError Raised if any errors are encountered during the configuration validation. Source code in vast_pipeline/pipeline/main.py def validate_cfg ( self , user : User = None ) -> None : \"\"\" Validates a pipeline run configuration against default parameters and for different settings (e.g. force extraction). Args: user: The User of the request if made through the UI. Defaults to None. Returns: None Raises: PipelineConfigError: Raised if any errors are encountered during the configuration validation. \"\"\" # validate every config from the config template config_keys = [ k for k in dir ( self . config ) if k . isupper ()] valid_keys = self . _get_valid_keys () # Check that there are no 'unknown' options given by the user for key in config_keys : if key . lower () not in valid_keys : raise PipelineConfigError ( f 'Configuration not valid, unknown option: { key } !' ) # Check that all options are provided by the user for key in settings . PIPE_RUN_CONFIG_DEFAULTS . keys (): if key . upper () not in config_keys : raise PipelineConfigError ( f 'Configuration not valid, missing option: { key . upper () } !' ) # do sanity checks if ( getattr ( self . config , 'IMAGE_FILES' ) and getattr ( self . config , 'SELAVY_FILES' ) and getattr ( self . config , 'NOISE_FILES' ) ): img_f_list = getattr ( self . config , 'IMAGE_FILES' ) img_f_list = [ item for sublist in img_f_list . values () for item in sublist ] # creates a flat list of all the dictionary value lists len_img_f_list = len ( img_f_list ) # maximum number of images check. If the user is `None` then it # means the run was initiated through the command line hence no # check is performed. if ( user is not None and len_img_f_list > settings . MAX_PIPERUN_IMAGES ): if user . is_staff : logger . warning ( 'Maximum number of images' f ' ( { settings . MAX_PIPERUN_IMAGES } ) rule bypassed with' ' admin status.' ) else : raise PipelineConfigError ( f 'The number of images entered ( { len_img_f_list } )' ' exceeds the maximum number of images currently' f ' allowed ( { settings . MAX_PIPERUN_IMAGES } ). Please ask' ' an administrator for advice on processing your run' ) for lst in [ 'IMAGE_FILES' , 'SELAVY_FILES' , 'NOISE_FILES' ]: cfg_list = getattr ( self . config , lst ) cfg_list = [ item for sublist in cfg_list . values () for item in sublist ] # checks for duplicates in each list if len ( set ( cfg_list )) != len ( cfg_list ): raise PipelineConfigError ( f 'Duplicated files in: \\n { lst } ' ) # check if nr of files match nr of images if len ( cfg_list ) != len_img_f_list : raise PipelineConfigError ( f 'Number of { lst } files not matching number of images' ) for key in getattr ( self . config , lst ): for file in getattr ( self . config , lst )[ key ]: if not os . path . exists ( file ): raise PipelineConfigError ( f 'file: \\n { file } \\n does not exists!' ) else : raise PipelineConfigError ( 'No image and/or Selavy and/or noise file paths passed!' ) # need more than 1 image file to generate a lightcurve if len ( getattr ( self . config , 'IMAGE_FILES' )) < 2 : raise PipelineConfigError ( 'Number of image files needs to be larger than 1!' ) source_finder_names = settings . SOURCE_FINDERS if getattr ( self . config , 'SOURCE_FINDER' ) not in source_finder_names : raise PipelineConfigError (( f \"Invalid source finder { getattr ( self . config , 'SOURCE_FINDER' ) } .\" f ' Choices are { source_finder_names } ' )) association_methods = settings . DEFAULT_ASSOCIATION_METHODS if getattr ( self . config , 'ASSOCIATION_METHOD' ) not in association_methods : raise PipelineConfigError (( 'ASSOCIATION_METHOD is not valid!' f ' Must be a value contained in: { association_methods } .' )) # validate Forced extraction settings if getattr ( self . config , 'MONITOR' ): if not getattr ( self . config , 'BACKGROUND_FILES' ): raise PipelineConfigError ( 'Expecting list of background MAP files!' ) # if defined, check background files regardless of monitor if getattr ( self . config , 'BACKGROUND_FILES' ): # check for duplicated values backgrd_f_list = getattr ( self . config , 'BACKGROUND_FILES' ) backgrd_f_list = [ item for sublist in backgrd_f_list . values () for item in sublist ] if len ( set ( backgrd_f_list )) != len ( backgrd_f_list ): raise PipelineConfigError ( 'Duplicated files in: BACKGROUND_FILES list' ) # check if provided more background files than images if len ( backgrd_f_list ) != len_img_f_list : raise PipelineConfigError (( 'Number of BACKGROUND_FILES different from number of' ' IMAGE_FILES files' )) for key in getattr ( self . config , 'BACKGROUND_FILES' ): for file in getattr ( self . config , 'BACKGROUND_FILES' )[ key ]: if not os . path . exists ( file ): raise PipelineConfigError ( f 'file: \\n { file } \\n does not exists!' ) pass","title":"main.py"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline","text":"Instance of a pipeline. All the methods run stages of the pipeline processing. Attributes: Name Type Description name str The name of the pipeline run. config config The configuration options of the pipeline run. epoch_based bool Whether the pipeline run is in epoch_based mode or not.","title":"Pipeline"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.__init__","text":"Initialise the pipeline with attributed such as configuration file path, name, and list of images and related files (e.g. selavy) Parameters: Name Type Description Default name str The pipeline name. required config_path str The system path to the configuration file. required Returns: Type Description None None. Source code in vast_pipeline/pipeline/main.py def __init__ ( self , name : str , config_path : str ) -> None : ''' Initialise the pipeline with attributed such as configuration file path, name, and list of images and related files (e.g. selavy) Args: name: The pipeline name. config_path: The system path to the configuration file. Returns: None. ''' self . name = name self . config = self . load_cfg ( config_path ) # The epoch_based parameter below is for # if the user has entered just lists we don't have # access to the dates until the Image instances are # created. So we flag this as true so that we can # reorder the epochs once the date information is available. # It is also recorded in the database such that there is a record # of the fact that the run was processed in an epoch based mode. self . epoch_based = False # Check if provided files are lists and convert to # dictionaries if so self . config , self . epoch_based = self . check_for_epoch_based ( self . config )","title":"__init__()"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.check_current_runs","text":"Checks the number of pipeline runs currently being processed. Returns: Type Description None None Exceptions: Type Description MaxPipelineRunsError Raised if the number of pipeline runs currently being processed is larger than the allowed maximum. Source code in vast_pipeline/pipeline/main.py @staticmethod def check_current_runs () -> None : \"\"\" Checks the number of pipeline runs currently being processed. Returns: None Raises: MaxPipelineRunsError: Raised if the number of pipeline runs currently being processed is larger than the allowed maximum. \"\"\" if Run . objects . check_max_runs ( settings . MAX_PIPELINE_RUNS ): raise MaxPipelineRunsError","title":"check_current_runs()"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.check_for_epoch_based","text":"Checks whether the images have been provided in a Dictionary format which means that epoch_based has been requested. If they have been provided with just lists then the inputs are converted to dictionaries with an epoch defined for each individual image. Parameters: Name Type Description Default cfg module The config object. required Returns: Type Description Tuple[module, bool] The config (with converted List -> Dict inputs if required) and epoch_based boolean flag. Exceptions: Type Description PipelineConfigError Raised if the images are entered into the configuration file in a format that cannot be read. Source code in vast_pipeline/pipeline/main.py @staticmethod def check_for_epoch_based ( cfg : ModuleType ) -> Tuple [ ModuleType , bool ]: # Config typing above is unknown what to put. \"\"\" Checks whether the images have been provided in a Dictionary format which means that epoch_based has been requested. If they have been provided with just lists then the inputs are converted to dictionaries with an epoch defined for each individual image. Args: cfg: The config object. Returns: The config (with converted List -> Dict inputs if required) and epoch_based boolean flag. Raises: PipelineConfigError: Raised if the images are entered into the configuration file in a format that cannot be read. \"\"\" epoch_based = False for cfg_key in [ 'IMAGE_FILES' , 'SELAVY_FILES' , 'BACKGROUND_FILES' , 'NOISE_FILES' ]: if isinstance ( getattr ( cfg , cfg_key ), list ): setattr ( cfg , cfg_key , convert_list_to_dict ( getattr ( cfg , cfg_key )) ) elif isinstance ( getattr ( cfg , cfg_key ), dict ): # Set to True if dictionaries are passed. epoch_based = True else : raise PipelineConfigError (( 'Unknown images entry format!' f ' Must be a list or dictionary.' )) return cfg , epoch_based","title":"check_for_epoch_based()"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.check_prev_config_diff","text":"Checks if the previous config file differs from the current config file. Used in add mode. Only returns true if the images are different and the other general settings are the same (the requirement for add mode). Otherwise False is returned. Parameters: Name Type Description Default p_run_path str The path of the pipeline run where the parquets are stored. required Returns: Type Description bool True if images are different but general settings are the same. Otherwise False is returned. Source code in vast_pipeline/pipeline/main.py def check_prev_config_diff ( self , p_run_path : str ) -> bool : \"\"\" Checks if the previous config file differs from the current config file. Used in add mode. Only returns true if the images are different and the other general settings are the same (the requirement for add mode). Otherwise False is returned. Args: p_run_path: The path of the pipeline run where the parquets are stored. Returns: True if images are different but general settings are the same. Otherwise False is returned. \"\"\" valid_keys = self . _get_valid_keys ( upper = True ) prev_config , _ = self . check_for_epoch_based ( self . load_cfg ( os . path . join ( p_run_path , 'config_prev.py' ))) prev_config_dict = { k : getattr ( prev_config , k ) for k in valid_keys } current_config_dict = { k : getattr ( self . config , k ) for k in valid_keys } if prev_config_dict == current_config_dict : return True image_check = ( prev_config_dict [ 'IMAGE_FILES' ] == current_config_dict [ 'IMAGE_FILES' ] ) for i in [ 'IMAGE_FILES' , 'SELAVY_FILES' , 'NOISE_FILES' , 'BACKGROUND_FILES' ]: prev_config_dict . pop ( i ) current_config_dict . pop ( i ) settings_check = prev_config_dict == current_config_dict if not image_check and settings_check : return False return True","title":"check_prev_config_diff()"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.load_cfg","text":"Check the given Config path. Throw exception if any problems return the config object as module/class. Parameters: Name Type Description Default cfg str The system path to the pipeline run configuration file. required Returns: Type Description module The configuration options read from the input file. Exceptions: Type Description PipelineConfigError Raised if the configuration file cannot be found. Source code in vast_pipeline/pipeline/main.py @staticmethod def load_cfg ( cfg : str ) -> ModuleType : \"\"\" Check the given Config path. Throw exception if any problems return the config object as module/class. Args: cfg: The system path to the pipeline run configuration file. Returns: The configuration options read from the input file. Raises: PipelineConfigError: Raised if the configuration file cannot be found. \"\"\" if not os . path . exists ( cfg ): raise PipelineConfigError ( 'pipeline run config file not existent' ) # load the run config as a Python module spec = spec_from_file_location ( 'run_config' , cfg ) mod = module_from_spec ( spec ) spec . loader . exec_module ( mod ) return mod","title":"load_cfg()"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.match_images_to_data","text":"Loops through images and matches the selavy, noise and bkg images. Assumes that user has enteted images and other data in the same order. Returns: Type Description None None Source code in vast_pipeline/pipeline/main.py def match_images_to_data ( self ) -> None : \"\"\" Loops through images and matches the selavy, noise and bkg images. Assumes that user has enteted images and other data in the same order. Returns: None \"\"\" self . img_paths = { 'selavy' : {}, 'noise' : {}, 'background' : {} } self . img_epochs = {} for key in sorted ( self . config . IMAGE_FILES . keys ()): for x , y in zip ( self . config . IMAGE_FILES [ key ], self . config . SELAVY_FILES [ key ] ): self . img_paths [ 'selavy' ][ x ] = y for x , y in zip ( self . config . IMAGE_FILES [ key ], self . config . NOISE_FILES [ key ] ): self . img_paths [ 'noise' ][ x ] = y # check if backgound files have been given before # attempting to match if key in self . config . BACKGROUND_FILES : for x , y in zip ( self . config . IMAGE_FILES [ key ], self . config . BACKGROUND_FILES [ key ] ): self . img_paths [ 'background' ][ x ] = y for x in self . config . IMAGE_FILES [ key ]: self . img_epochs [ os . path . basename ( x )] = key","title":"match_images_to_data()"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.process_pipeline","text":"The function that performs the processing operations of the pipeline run. Parameters: Name Type Description Default p_run Run The pipeline run model object. required Returns: Type Description None None Source code in vast_pipeline/pipeline/main.py def process_pipeline ( self , p_run : Run ) -> None : \"\"\" The function that performs the processing operations of the pipeline run. Args: p_run: The pipeline run model object. Returns: None \"\"\" logger . info ( f 'Epoch based association: { self . epoch_based } ' ) if self . add_mode : logger . info ( 'Running in image add mode.' ) # Update epoch based flag to not cause user confusion when running # the pipeline (i.e. if it was only updated at the end). It is not # updated if the pipeline is being run in add mode. if self . epoch_based and not self . add_mode : with transaction . atomic (): p_run . epoch_based = self . epoch_based p_run . save () # Match the image files to the respective selavy, noise and bkg files. # Do this after validation is successful. self . match_images_to_data () # upload/retrieve image data images , skyregs_df = make_upload_images ( self . img_paths , self . config , p_run ) # STEP #2: measurements association # order images by time images . sort ( key = operator . attrgetter ( 'datetime' )) # If the user has given lists we need to reorder the # image epochs such that they are in date order. if self . epoch_based is False : self . img_epochs = {} for i , img in enumerate ( images ): self . img_epochs [ img . name ] = i + 1 image_epochs = [ self . img_epochs [ img . name ] for img in images ] limit = Angle ( self . config . ASSOCIATION_RADIUS * u . arcsec ) dr_limit = self . config . ASSOCIATION_DE_RUITER_RADIUS bw_limit = self . config . ASSOCIATION_BEAMWIDTH_LIMIT duplicate_limit = Angle ( self . config . ASSOCIATION_EPOCH_DUPLICATE_RADIUS * u . arcsec ) # 2.1 Check if sky regions to be associated can be # split into connected point groups skyregion_groups = group_skyregions ( skyregs_df [[ 'id' , 'centre_ra' , 'centre_dec' , 'xtr_radius' ]] ) n_skyregion_groups = skyregion_groups [ 'skyreg_group' ] . unique () . shape [ 0 ] # Get already done images if in add mode if self . add_mode : done_images_df = pd . read_parquet ( self . previous_parquets [ 'images' ], columns = [ 'id' , 'name' ] ) done_source_ids = pd . read_parquet ( self . previous_parquets [ 'sources' ], columns = [ 'wavg_ra' ] ) . index . tolist () else : done_images_df = None done_source_ids = None # 2.2 Associate with other measurements if self . config . ASSOCIATION_PARALLEL and n_skyregion_groups > 1 : images_df = get_parallel_assoc_image_df ( images , skyregion_groups ) images_df [ 'epoch' ] = image_epochs sources_df = parallel_association ( images_df , limit , dr_limit , bw_limit , duplicate_limit , self . config , n_skyregion_groups , self . add_mode , self . previous_parquets , done_images_df , done_source_ids ) else : images_df = pd . DataFrame . from_dict ( { 'image_dj' : images , 'epoch' : image_epochs } ) images_df [ 'skyreg_id' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . skyreg_id ) images_df [ 'image_name' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . name ) sources_df = association ( images_df , limit , dr_limit , bw_limit , duplicate_limit , self . config , self . add_mode , self . previous_parquets , done_images_df ) # 2.3 Associate Measurements with reference survey sources if SurveySource . objects . exists (): pass # Obtain the number of selavy measurements for the run # n_selavy_measurements = sources_df. nr_selavy_measurements = sources_df [ 'id' ] . unique () . shape [ 0 ] # STEP #3: Merge sky regions and sources ready for # steps 4 and 5 below. missing_source_cols = [ 'source' , 'datetime' , 'image' , 'epoch' , 'interim_ew' , 'weight_ew' , 'interim_ns' , 'weight_ns' ] # need to make sure no forced measurments are being passed which # could happen in add mode, otherwise the wrong detection image is # assigned. missing_sources_df = get_src_skyregion_merged_df ( sources_df . loc [ sources_df [ 'forced' ] == False , missing_source_cols ], images_df , skyregs_df , ) # STEP #4 New source analysis new_sources_df = new_sources ( sources_df , missing_sources_df , self . config . NEW_SOURCE_MIN_SIGMA , self . config . MONITOR_EDGE_BUFFER_SCALE , p_run ) # Drop column no longer required in missing_sources_df. missing_sources_df = ( missing_sources_df . drop ([ 'in_primary' ], axis = 1 ) ) # STEP #5: Run forced extraction/photometry if asked if self . config . MONITOR : ( sources_df , nr_forced_measurements ) = forced_extraction ( sources_df , self . config . ASTROMETRIC_UNCERTAINTY_RA / 3600. , self . config . ASTROMETRIC_UNCERTAINTY_DEC / 3600. , p_run , missing_sources_df , self . config . MONITOR_MIN_SIGMA , self . config . MONITOR_EDGE_BUFFER_SCALE , self . config . MONITOR_CLUSTER_THRESHOLD , self . config . MONITOR_ALLOW_NAN , self . add_mode , done_images_df , done_source_ids ) del missing_sources_df # STEP #6: finalise the df getting unique sources, calculating # metrics and upload data to database nr_sources = final_operations ( sources_df , p_run , new_sources_df , self . config . SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS , self . add_mode , done_source_ids , self . previous_parquets ) # calculate number processed images nr_img_processed = len ( images ) # update pipeline run with the nr images and sources with transaction . atomic (): p_run . n_images = nr_img_processed p_run . n_sources = nr_sources p_run . n_selavy_measurements = nr_selavy_measurements p_run . n_forced_measurements = ( nr_forced_measurements if self . config . MONITOR else 0 ) p_run . save () pass","title":"process_pipeline()"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.set_status","text":"Function to change the status of a pipeline run model object and save to the database. Parameters: Name Type Description Default pipe_run Run The pipeline run model object. required status str The status to set. None Returns: Type Description None None Source code in vast_pipeline/pipeline/main.py @staticmethod def set_status ( pipe_run : Run , status : str = None ) -> None : \"\"\" Function to change the status of a pipeline run model object and save to the database. Args: pipe_run: The pipeline run model object. status: The status to set. Returns: None \"\"\" #TODO: This function gives no feedback if the status is not accepted? choices = [ x [ 0 ] for x in Run . _meta . get_field ( 'status' ) . choices ] if status and status in choices and pipe_run . status != status : with transaction . atomic (): pipe_run . status = status pipe_run . save ()","title":"set_status()"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.validate_cfg","text":"Validates a pipeline run configuration against default parameters and for different settings (e.g. force extraction). Parameters: Name Type Description Default user User The User of the request if made through the UI. Defaults to None. None Returns: Type Description None None Exceptions: Type Description PipelineConfigError Raised if any errors are encountered during the configuration validation. Source code in vast_pipeline/pipeline/main.py def validate_cfg ( self , user : User = None ) -> None : \"\"\" Validates a pipeline run configuration against default parameters and for different settings (e.g. force extraction). Args: user: The User of the request if made through the UI. Defaults to None. Returns: None Raises: PipelineConfigError: Raised if any errors are encountered during the configuration validation. \"\"\" # validate every config from the config template config_keys = [ k for k in dir ( self . config ) if k . isupper ()] valid_keys = self . _get_valid_keys () # Check that there are no 'unknown' options given by the user for key in config_keys : if key . lower () not in valid_keys : raise PipelineConfigError ( f 'Configuration not valid, unknown option: { key } !' ) # Check that all options are provided by the user for key in settings . PIPE_RUN_CONFIG_DEFAULTS . keys (): if key . upper () not in config_keys : raise PipelineConfigError ( f 'Configuration not valid, missing option: { key . upper () } !' ) # do sanity checks if ( getattr ( self . config , 'IMAGE_FILES' ) and getattr ( self . config , 'SELAVY_FILES' ) and getattr ( self . config , 'NOISE_FILES' ) ): img_f_list = getattr ( self . config , 'IMAGE_FILES' ) img_f_list = [ item for sublist in img_f_list . values () for item in sublist ] # creates a flat list of all the dictionary value lists len_img_f_list = len ( img_f_list ) # maximum number of images check. If the user is `None` then it # means the run was initiated through the command line hence no # check is performed. if ( user is not None and len_img_f_list > settings . MAX_PIPERUN_IMAGES ): if user . is_staff : logger . warning ( 'Maximum number of images' f ' ( { settings . MAX_PIPERUN_IMAGES } ) rule bypassed with' ' admin status.' ) else : raise PipelineConfigError ( f 'The number of images entered ( { len_img_f_list } )' ' exceeds the maximum number of images currently' f ' allowed ( { settings . MAX_PIPERUN_IMAGES } ). Please ask' ' an administrator for advice on processing your run' ) for lst in [ 'IMAGE_FILES' , 'SELAVY_FILES' , 'NOISE_FILES' ]: cfg_list = getattr ( self . config , lst ) cfg_list = [ item for sublist in cfg_list . values () for item in sublist ] # checks for duplicates in each list if len ( set ( cfg_list )) != len ( cfg_list ): raise PipelineConfigError ( f 'Duplicated files in: \\n { lst } ' ) # check if nr of files match nr of images if len ( cfg_list ) != len_img_f_list : raise PipelineConfigError ( f 'Number of { lst } files not matching number of images' ) for key in getattr ( self . config , lst ): for file in getattr ( self . config , lst )[ key ]: if not os . path . exists ( file ): raise PipelineConfigError ( f 'file: \\n { file } \\n does not exists!' ) else : raise PipelineConfigError ( 'No image and/or Selavy and/or noise file paths passed!' ) # need more than 1 image file to generate a lightcurve if len ( getattr ( self . config , 'IMAGE_FILES' )) < 2 : raise PipelineConfigError ( 'Number of image files needs to be larger than 1!' ) source_finder_names = settings . SOURCE_FINDERS if getattr ( self . config , 'SOURCE_FINDER' ) not in source_finder_names : raise PipelineConfigError (( f \"Invalid source finder { getattr ( self . config , 'SOURCE_FINDER' ) } .\" f ' Choices are { source_finder_names } ' )) association_methods = settings . DEFAULT_ASSOCIATION_METHODS if getattr ( self . config , 'ASSOCIATION_METHOD' ) not in association_methods : raise PipelineConfigError (( 'ASSOCIATION_METHOD is not valid!' f ' Must be a value contained in: { association_methods } .' )) # validate Forced extraction settings if getattr ( self . config , 'MONITOR' ): if not getattr ( self . config , 'BACKGROUND_FILES' ): raise PipelineConfigError ( 'Expecting list of background MAP files!' ) # if defined, check background files regardless of monitor if getattr ( self . config , 'BACKGROUND_FILES' ): # check for duplicated values backgrd_f_list = getattr ( self . config , 'BACKGROUND_FILES' ) backgrd_f_list = [ item for sublist in backgrd_f_list . values () for item in sublist ] if len ( set ( backgrd_f_list )) != len ( backgrd_f_list ): raise PipelineConfigError ( 'Duplicated files in: BACKGROUND_FILES list' ) # check if provided more background files than images if len ( backgrd_f_list ) != len_img_f_list : raise PipelineConfigError (( 'Number of BACKGROUND_FILES different from number of' ' IMAGE_FILES files' )) for key in getattr ( self . config , 'BACKGROUND_FILES' ): for file in getattr ( self . config , 'BACKGROUND_FILES' )[ key ]: if not os . path . exists ( file ): raise PipelineConfigError ( f 'file: \\n { file } \\n does not exists!' ) pass","title":"validate_cfg()"},{"location":"reference/pipeline/model_generator/","text":"association_models_generator ( assoc_df ) \u00b6 Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Parameters: Name Type Description Default assoc_df DataFrame The dataframe from the pipeline containing the associations between measurements and sources. required Returns: Type Description Iterable[Generator[vast_pipeline.models.Association, NoneType, NoneType]] An iterable generator object containing the yielded Association objects. Source code in vast_pipeline/pipeline/model_generator.py def association_models_generator ( assoc_df : pd . DataFrame ) -> Iterable [ Generator [ Association , None , None ]]: \"\"\" Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Args: assoc_df: The dataframe from the pipeline containing the associations between measurements and sources. Returns: An iterable generator object containing the yielded Association objects. \"\"\" for i , row in assoc_df . iterrows (): yield Association ( meas_id = row [ 'id' ], source_id = row [ 'source_id' ], d2d = row [ 'd2d' ], dr = row [ 'dr' ], ) measurement_models_generator ( meas_df ) \u00b6 Creates a generator object containing yielded Measurement objects from an input pipeline measurement dataframe. Parameters: Name Type Description Default meas_df DataFrame The dataframe from the pipeline containing the measurements of an image. required Returns: Type Description Iterable[Generator[vast_pipeline.models.Measurement, NoneType, NoneType]] An iterable generator object containing the yielded Measurement objects. Source code in vast_pipeline/pipeline/model_generator.py def measurement_models_generator ( meas_df : pd . DataFrame ) -> Iterable [ Generator [ Measurement , None , None ]]: \"\"\" Creates a generator object containing yielded Measurement objects from an input pipeline measurement dataframe. Args: meas_df: The dataframe from the pipeline containing the measurements of an image. Returns: An iterable generator object containing the yielded Measurement objects. \"\"\" for i , row in meas_df . iterrows (): one_m = Measurement () for fld in one_m . _meta . get_fields (): if getattr ( fld , 'attname' , None ) and fld . attname in row . index : setattr ( one_m , fld . attname , row [ fld . attname ]) yield one_m measurement_pair_models_generator ( measurement_pairs_df ) \u00b6 Creates a generator of MeasurementPair objects from an input pipeline measurement pair dataframe. Parameters: Name Type Description Default measurement_pairs_df DataFrame The DataFrame of measurement pairs. required Returns: Type Description Iterable[Generator[vast_pipeline.models.MeasurementPair, NoneType, NoneType]] An iterable of MeasurementPair objects, one for each row of measurement_pairs_df . Source code in vast_pipeline/pipeline/model_generator.py def measurement_pair_models_generator ( measurement_pairs_df : pd . DataFrame , ) -> Iterable [ Generator [ MeasurementPair , None , None ]]: \"\"\" Creates a generator of MeasurementPair objects from an input pipeline measurement pair dataframe. Args: measurement_pairs_df: The DataFrame of measurement pairs. Returns: An iterable of MeasurementPair objects, one for each row of `measurement_pairs_df`. \"\"\" for i , row in measurement_pairs_df . iterrows (): yield MeasurementPair ( source_id = row [ \"source_id\" ], measurement_a_id = row [ \"id_a\" ], measurement_b_id = row [ \"id_b\" ], vs_peak = row [ \"vs_peak\" ], vs_int = row [ \"vs_int\" ], m_peak = row [ \"m_peak\" ], m_int = row [ \"m_int\" ], ) related_models_generator ( related_df ) \u00b6 Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Parameters: Name Type Description Default related_df DataFrame The dataframe from the pipeline containing the relations between sources. required Returns: Type Description Iterable[Generator[vast_pipeline.models.RelatedSource, NoneType, NoneType]] An iterable generator object containing the yielded Association objects. Source code in vast_pipeline/pipeline/model_generator.py def related_models_generator ( related_df : pd . DataFrame ) -> Iterable [ Generator [ RelatedSource , None , None ]]: \"\"\" Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Args: related_df: The dataframe from the pipeline containing the relations between sources. Returns: An iterable generator object containing the yielded Association objects. \"\"\" for i , row in related_df . iterrows (): yield RelatedSource ( ** row . to_dict ()) source_models_generator ( src_df , pipeline_run ) \u00b6 Creates a generator object containing yielded Source objects from an input pipeline sources dataframe. Parameters: Name Type Description Default src_df DataFrame The dataframe from the pipeline containing the measurements of an image. required pipeline_run Run The pipeline Run object of which the sources are associated with. required Returns: Type Description Iterable[Generator[vast_pipeline.models.Source, NoneType, NoneType]] An iterable generator object containing the yielded Source objects. Source code in vast_pipeline/pipeline/model_generator.py def source_models_generator ( src_df : pd . DataFrame , pipeline_run : Run ) -> Iterable [ Generator [ Source , None , None ]]: \"\"\" Creates a generator object containing yielded Source objects from an input pipeline sources dataframe. Args: src_df: The dataframe from the pipeline containing the measurements of an image. pipeline_run: The pipeline Run object of which the sources are associated with. Returns: An iterable generator object containing the yielded Source objects. \"\"\" for i , row in src_df . iterrows (): name = ( f \"ASKAP_ { deg2hms ( row [ 'wavg_ra' ]) } \" f \" { deg2dms ( row [ 'wavg_dec' ]) } \" . replace ( \":\" , \"\" ) ) src = Source () src . run_id = pipeline_run . id src . name = name for fld in src . _meta . get_fields (): if getattr ( fld , 'attname' , None ) and fld . attname in row . index : setattr ( src , fld . attname , row [ fld . attname ]) yield src","title":"model_generator.py"},{"location":"reference/pipeline/model_generator/#vast_pipeline.pipeline.model_generator.association_models_generator","text":"Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Parameters: Name Type Description Default assoc_df DataFrame The dataframe from the pipeline containing the associations between measurements and sources. required Returns: Type Description Iterable[Generator[vast_pipeline.models.Association, NoneType, NoneType]] An iterable generator object containing the yielded Association objects. Source code in vast_pipeline/pipeline/model_generator.py def association_models_generator ( assoc_df : pd . DataFrame ) -> Iterable [ Generator [ Association , None , None ]]: \"\"\" Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Args: assoc_df: The dataframe from the pipeline containing the associations between measurements and sources. Returns: An iterable generator object containing the yielded Association objects. \"\"\" for i , row in assoc_df . iterrows (): yield Association ( meas_id = row [ 'id' ], source_id = row [ 'source_id' ], d2d = row [ 'd2d' ], dr = row [ 'dr' ], )","title":"association_models_generator()"},{"location":"reference/pipeline/model_generator/#vast_pipeline.pipeline.model_generator.measurement_models_generator","text":"Creates a generator object containing yielded Measurement objects from an input pipeline measurement dataframe. Parameters: Name Type Description Default meas_df DataFrame The dataframe from the pipeline containing the measurements of an image. required Returns: Type Description Iterable[Generator[vast_pipeline.models.Measurement, NoneType, NoneType]] An iterable generator object containing the yielded Measurement objects. Source code in vast_pipeline/pipeline/model_generator.py def measurement_models_generator ( meas_df : pd . DataFrame ) -> Iterable [ Generator [ Measurement , None , None ]]: \"\"\" Creates a generator object containing yielded Measurement objects from an input pipeline measurement dataframe. Args: meas_df: The dataframe from the pipeline containing the measurements of an image. Returns: An iterable generator object containing the yielded Measurement objects. \"\"\" for i , row in meas_df . iterrows (): one_m = Measurement () for fld in one_m . _meta . get_fields (): if getattr ( fld , 'attname' , None ) and fld . attname in row . index : setattr ( one_m , fld . attname , row [ fld . attname ]) yield one_m","title":"measurement_models_generator()"},{"location":"reference/pipeline/model_generator/#vast_pipeline.pipeline.model_generator.measurement_pair_models_generator","text":"Creates a generator of MeasurementPair objects from an input pipeline measurement pair dataframe. Parameters: Name Type Description Default measurement_pairs_df DataFrame The DataFrame of measurement pairs. required Returns: Type Description Iterable[Generator[vast_pipeline.models.MeasurementPair, NoneType, NoneType]] An iterable of MeasurementPair objects, one for each row of measurement_pairs_df . Source code in vast_pipeline/pipeline/model_generator.py def measurement_pair_models_generator ( measurement_pairs_df : pd . DataFrame , ) -> Iterable [ Generator [ MeasurementPair , None , None ]]: \"\"\" Creates a generator of MeasurementPair objects from an input pipeline measurement pair dataframe. Args: measurement_pairs_df: The DataFrame of measurement pairs. Returns: An iterable of MeasurementPair objects, one for each row of `measurement_pairs_df`. \"\"\" for i , row in measurement_pairs_df . iterrows (): yield MeasurementPair ( source_id = row [ \"source_id\" ], measurement_a_id = row [ \"id_a\" ], measurement_b_id = row [ \"id_b\" ], vs_peak = row [ \"vs_peak\" ], vs_int = row [ \"vs_int\" ], m_peak = row [ \"m_peak\" ], m_int = row [ \"m_int\" ], )","title":"measurement_pair_models_generator()"},{"location":"reference/pipeline/model_generator/#vast_pipeline.pipeline.model_generator.related_models_generator","text":"Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Parameters: Name Type Description Default related_df DataFrame The dataframe from the pipeline containing the relations between sources. required Returns: Type Description Iterable[Generator[vast_pipeline.models.RelatedSource, NoneType, NoneType]] An iterable generator object containing the yielded Association objects. Source code in vast_pipeline/pipeline/model_generator.py def related_models_generator ( related_df : pd . DataFrame ) -> Iterable [ Generator [ RelatedSource , None , None ]]: \"\"\" Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Args: related_df: The dataframe from the pipeline containing the relations between sources. Returns: An iterable generator object containing the yielded Association objects. \"\"\" for i , row in related_df . iterrows (): yield RelatedSource ( ** row . to_dict ())","title":"related_models_generator()"},{"location":"reference/pipeline/model_generator/#vast_pipeline.pipeline.model_generator.source_models_generator","text":"Creates a generator object containing yielded Source objects from an input pipeline sources dataframe. Parameters: Name Type Description Default src_df DataFrame The dataframe from the pipeline containing the measurements of an image. required pipeline_run Run The pipeline Run object of which the sources are associated with. required Returns: Type Description Iterable[Generator[vast_pipeline.models.Source, NoneType, NoneType]] An iterable generator object containing the yielded Source objects. Source code in vast_pipeline/pipeline/model_generator.py def source_models_generator ( src_df : pd . DataFrame , pipeline_run : Run ) -> Iterable [ Generator [ Source , None , None ]]: \"\"\" Creates a generator object containing yielded Source objects from an input pipeline sources dataframe. Args: src_df: The dataframe from the pipeline containing the measurements of an image. pipeline_run: The pipeline Run object of which the sources are associated with. Returns: An iterable generator object containing the yielded Source objects. \"\"\" for i , row in src_df . iterrows (): name = ( f \"ASKAP_ { deg2hms ( row [ 'wavg_ra' ]) } \" f \" { deg2dms ( row [ 'wavg_dec' ]) } \" . replace ( \":\" , \"\" ) ) src = Source () src . run_id = pipeline_run . id src . name = name for fld in src . _meta . get_fields (): if getattr ( fld , 'attname' , None ) and fld . attname in row . index : setattr ( src , fld . attname , row [ fld . attname ]) yield src","title":"source_models_generator()"},{"location":"reference/pipeline/new_sources/","text":"check_primary_image ( row ) \u00b6 Checks if the primary image is in the image list. Parameters: Name Type Description Default row Series Row of the missing_sources_df, need the keys 'primary' and 'img_list'. required Returns: Type Description bool True if the primary image is in the image list. Source code in vast_pipeline/pipeline/new_sources.py def check_primary_image ( row : pd . Series ) -> bool : \"\"\" Checks if the primary image is in the image list. Args: row: Row of the missing_sources_df, need the keys 'primary' and 'img_list'. Returns: True if the primary image is in the image list. \"\"\" return row [ 'primary' ] in row [ 'img_list' ] gen_array_coords_from_wcs ( coords , wcs ) \u00b6 Converts SkyCoord coordinates to array coordinates given a wcs. Parameters: Name Type Description Default coords SkyCoord The coordinates to convert. required wcs WCS The WCS to use for the conversion. required Returns: Type Description ndarray Array containing the x and y array coordinates of the input sky coordinates, e.g.: np.array([[x1, x2, x3], [y1, y2, y3]]) Source code in vast_pipeline/pipeline/new_sources.py def gen_array_coords_from_wcs ( coords : SkyCoord , wcs : WCS ) -> np . ndarray : \"\"\" Converts SkyCoord coordinates to array coordinates given a wcs. Args: coords: The coordinates to convert. wcs: The WCS to use for the conversion. Returns: Array containing the x and y array coordinates of the input sky coordinates, e.g.: np.array([[x1, x2, x3], [y1, y2, y3]]) \"\"\" array_coords = wcs . world_to_array_index ( coords ) array_coords = np . array ([ np . array ( array_coords [ 0 ]), np . array ( array_coords [ 1 ]), ]) return array_coords get_image_rms_measurements ( group , nbeam = 3 , edge_buffer = 1.0 ) \u00b6 Take the coordinates provided from the group and measure the array cell value in the provided image. Parameters: Name Type Description Default group DataFrame The group of sources to measure in the image, requiring the columns: 'source', 'wavg_ra', 'wavg_dec' and 'img_diff_rms_path'. required nbeam int The number of half beamwidths (BMAJ) away from the edge of the image or a NaN value that is acceptable. 3 edge_buffer float Multiplicative factor applied to nbeam to act as a buffer. 1.0 Returns: Type Description DataFrame The group dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Source code in vast_pipeline/pipeline/new_sources.py def get_image_rms_measurements ( group : pd . DataFrame , nbeam : int = 3 , edge_buffer : float = 1.0 ) -> pd . DataFrame : \"\"\" Take the coordinates provided from the group and measure the array cell value in the provided image. Args: group: The group of sources to measure in the image, requiring the columns: 'source', 'wavg_ra', 'wavg_dec' and 'img_diff_rms_path'. nbeam: The number of half beamwidths (BMAJ) away from the edge of the image or a NaN value that is acceptable. edge_buffer: Multiplicative factor applied to nbeam to act as a buffer. Returns: The group dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. \"\"\" image = group . iloc [ 0 ][ 'img_diff_rms_path' ] with fits . open ( image ) as hdul : header = hdul [ 0 ] . header wcs = WCS ( header , naxis = 2 ) data = hdul [ 0 ] . data . squeeze () # Here we mimic the forced fits behaviour, # sources within 3 half BMAJ widths of the image # edges are ignored. The user buffer is also # applied for consistency. pixelscale = ( proj_plane_pixel_scales ( wcs )[ 1 ] * u . deg ) . to ( u . arcsec ) bmaj = header [ \"BMAJ\" ] * u . deg npix = round ( ( nbeam / 2. * bmaj . to ( 'arcsec' ) / pixelscale ) . value ) npix = int ( round ( npix * edge_buffer )) coords = SkyCoord ( group . wavg_ra , group . wavg_dec , unit = ( u . deg , u . deg ) ) array_coords = gen_array_coords_from_wcs ( coords , wcs ) # check for pixel wrapping x_valid = np . logical_or ( array_coords [ 0 ] >= ( data . shape [ 0 ] - npix ), array_coords [ 0 ] < npix ) y_valid = np . logical_or ( array_coords [ 1 ] >= ( data . shape [ 1 ] - npix ), array_coords [ 1 ] < npix ) valid = ~ np . logical_or ( x_valid , y_valid ) valid_indexes = group [ valid ] . index . values group = group . loc [ valid_indexes ] if group . empty : # early return if all sources failed range check logger . debug ( 'All sources out of range in new source rms measurement' f ' for image { image } .' ) group [ 'img_diff_true_rms' ] = np . nan return group # Now we also need to check proximity to NaN values # as forced fits may also drop these values coords = SkyCoord ( group . wavg_ra , group . wavg_dec , unit = ( u . deg , u . deg ) ) array_coords = gen_array_coords_from_wcs ( coords , wcs ) acceptable_no_nan_dist = int ( round ( bmaj . to ( 'arcsec' ) . value / 2. / pixelscale . value ) ) nan_valid = [] # Get slices of each source and check NaN is not included. for i , j in zip ( array_coords [ 0 ], array_coords [ 1 ]): sl = tuple (( slice ( i - acceptable_no_nan_dist , i + acceptable_no_nan_dist ), slice ( j - acceptable_no_nan_dist , j + acceptable_no_nan_dist ) )) if np . any ( np . isnan ( data [ sl ])): nan_valid . append ( False ) else : nan_valid . append ( True ) valid_indexes = group [ nan_valid ] . index . values if np . any ( nan_valid ): # only run if there are actual values to measure rms_values = data [ array_coords [ 0 ][ nan_valid ], array_coords [ 1 ][ nan_valid ] ] # not matched ones will be NaN. group . loc [ valid_indexes , 'img_diff_true_rms' ] = rms_values . astype ( np . float64 ) * 1.e3 else : group [ 'img_diff_true_rms' ] = np . nan return group new_sources ( sources_df , missing_sources_df , min_sigma , edge_buffer , p_run ) \u00b6 Processes the new sources detected to check that they are valid new sources. This involves checking to see that the source should be seen at all in the images where it is not detected. For valid new sources the snr value the source would have in non-detected images is also calculated. Parameters: Name Type Description Default sources_df DataFrame The sources found from the association step. required missing_sources_df DataFrame The dataframe containing the 'missing detections' for each source. +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 776 | ['VAST_0127-73A.EPOCH01.I.fits'] | 31.8223 | -70.4674 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+ img_diff | ----------------------------------| ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ----------------------------------+ required min_sigma float The minimum sigma value acceptable when compared to the minimum rms of the respective image. required edge_buffer float Multiplicative factor to be passed to the 'get_image_rms_measurements' function. required p_run Run The pipeline run. required Returns: Type Description DataFrame The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Columns: source - source id, int. img_list - list of images, List. wavg_ra - weighted average RA, float. wavg_dec - weighted average Dec, float. skyreg_img_list - list of sky regions of images in img_list, List. img_diff - The images missing from coverage, List. primary - What should be the first image, str. detection - The first detection image, str. detection_time - Datetime of detection, datetime.datetime. img_diff_time - Datetime of img_diff list, datetime.datetime. img_diff_rms_min - Minimum rms of diff images, float. img_diff_rms_median - Median rms of diff images, float. img_diff_rms_path - rms path of diff images, str. flux_peak - Flux peak of source (detection), float. diff_sigma - SNR in differnce images (compared to minimum), float. img_diff_true_rms - The true rms value from the diff images, float. new_high_sigma - peak flux / true rms value, float. Source code in vast_pipeline/pipeline/new_sources.py def new_sources ( sources_df : pd . DataFrame , missing_sources_df : pd . DataFrame , min_sigma : float , edge_buffer : float , p_run : Run ) -> pd . DataFrame : \"\"\" Processes the new sources detected to check that they are valid new sources. This involves checking to see that the source *should* be seen at all in the images where it is not detected. For valid new sources the snr value the source would have in non-detected images is also calculated. Args: sources_df: The sources found from the association step. missing_sources_df: The dataframe containing the 'missing detections' for each source. +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 776 | ['VAST_0127-73A.EPOCH01.I.fits'] | 31.8223 | -70.4674 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+ img_diff | ----------------------------------| ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ----------------------------------+ min_sigma: The minimum sigma value acceptable when compared to the minimum rms of the respective image. edge_buffer: Multiplicative factor to be passed to the 'get_image_rms_measurements' function. p_run: The pipeline run. Returns: The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Columns: source - source id, int. img_list - list of images, List. wavg_ra - weighted average RA, float. wavg_dec - weighted average Dec, float. skyreg_img_list - list of sky regions of images in img_list, List. img_diff - The images missing from coverage, List. primary - What should be the first image, str. detection - The first detection image, str. detection_time - Datetime of detection, datetime.datetime. img_diff_time - Datetime of img_diff list, datetime.datetime. img_diff_rms_min - Minimum rms of diff images, float. img_diff_rms_median - Median rms of diff images, float. img_diff_rms_path - rms path of diff images, str. flux_peak - Flux peak of source (detection), float. diff_sigma - SNR in differnce images (compared to minimum), float. img_diff_true_rms - The true rms value from the diff images, float. new_high_sigma - peak flux / true rms value, float. \"\"\" timer = StopWatch () logger . info ( \"Starting new source analysis.\" ) cols = [ 'id' , 'name' , 'noise_path' , 'datetime' , 'rms_median' , 'rms_min' , 'rms_max' , ] images_df = pd . DataFrame ( list ( Image . objects . filter ( run = p_run ) . values ( * tuple ( cols )) )) . set_index ( 'name' ) # Get rid of sources that are not 'new', i.e. sources which the # first sky region image is not in the image list new_sources_df = missing_sources_df [ missing_sources_df [ 'in_primary' ] == False ] . drop ( columns = [ 'in_primary' ] ) # Check if the previous sources would have actually been seen # i.e. are the previous images sensitive enough # save the index before exploding new_sources_df = new_sources_df . reset_index () # Explode now to avoid two loops below new_sources_df = new_sources_df . explode ( 'img_diff' ) # Merge the respective image information to the df new_sources_df = new_sources_df . merge ( images_df [[ 'datetime' ]], left_on = 'detection' , right_on = 'name' , how = 'left' ) . rename ( columns = { 'datetime' : 'detection_time' }) new_sources_df = new_sources_df . merge ( images_df [[ 'datetime' , 'rms_min' , 'rms_median' , 'noise_path' ]], left_on = 'img_diff' , right_on = 'name' , how = 'left' ) . rename ( columns = { 'datetime' : 'img_diff_time' , 'rms_min' : 'img_diff_rms_min' , 'rms_median' : 'img_diff_rms_median' , 'noise_path' : 'img_diff_rms_path' }) # Select only those images that come before the detection image # in time. new_sources_df = new_sources_df [ new_sources_df . img_diff_time < new_sources_df . detection_time ] # merge the detection fluxes in new_sources_df = pd . merge ( new_sources_df , sources_df [[ 'source' , 'image' , 'flux_peak' ]], left_on = [ 'source' , 'detection' ], right_on = [ 'source' , 'image' ], how = 'left' ) . drop ( columns = [ 'image' ]) # calculate the sigma of the source if it was placed in the # minimum rms region of the previous images new_sources_df [ 'diff_sigma' ] = ( new_sources_df [ 'flux_peak' ] . values / new_sources_df [ 'img_diff_rms_min' ] . values ) # keep those that are above the user specified threshold new_sources_df = new_sources_df . loc [ new_sources_df [ 'diff_sigma' ] >= min_sigma ] # Now have list of sources that should have been seen before given # previous images minimum rms values. # Current inaccurate sky regions may mean that the source # was in a previous 'NaN' area of the image. This needs to be # checked. Currently the check is done by filtering out of range # pixels once the values have been obtained (below). # This could be done using MOCpy however this is reasonably # fast and the io of a MOC fits may take more time. # So these sources will be flagged as new sources, but we can also # make a guess of how signficant they are. For this the next step is # to measure the true rms at the source location. # measure the actual rms in the previous images at # the source location. new_sources_df = parallel_get_rms_measurements ( new_sources_df , edge_buffer = edge_buffer ) # this removes those that are out of range new_sources_df [ 'img_diff_true_rms' ] = ( new_sources_df [ 'img_diff_true_rms' ] . fillna ( 0. ) ) new_sources_df = new_sources_df [ new_sources_df [ 'img_diff_true_rms' ] != 0 ] # calculate the true sigma new_sources_df [ 'true_sigma' ] = ( new_sources_df [ 'flux_peak' ] . values / new_sources_df [ 'img_diff_true_rms' ] . values ) # We only care about the highest true sigma new_sources_df = new_sources_df . sort_values ( by = [ 'source' , 'true_sigma' ] ) # keep only the highest for each source, rename for the daatabase new_sources_df = ( new_sources_df . drop_duplicates ( 'source' ) . set_index ( 'source' ) . rename ( columns = { 'true_sigma' : 'new_high_sigma' }) ) # moving forward only the new_high_sigma columns is needed, drop all others. new_sources_df = new_sources_df [[ 'new_high_sigma' ]] logger . info ( 'Total new source analysis time: %.2f seconds' , timer . reset_init () ) return new_sources_df parallel_get_rms_measurements ( df , edge_buffer = 1.0 ) \u00b6 Wrapper function to use 'get_image_rms_measurements' in parallel with Dask. nbeam is not an option here as that parameter is fixed in forced extraction and so is made sure to be fixed here to. This may change in the future. Parameters: Name Type Description Default df DataFrame The group of sources to measure in the images. required edge_buffer float Multiplicative factor to be passed to the 'get_image_rms_measurements' function. 1.0 Returns: Type Description DataFrame The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Source code in vast_pipeline/pipeline/new_sources.py def parallel_get_rms_measurements ( df : pd . DataFrame , edge_buffer : float = 1.0 ) -> pd . DataFrame : \"\"\" Wrapper function to use 'get_image_rms_measurements' in parallel with Dask. nbeam is not an option here as that parameter is fixed in forced extraction and so is made sure to be fixed here to. This may change in the future. Args: df: The group of sources to measure in the images. edge_buffer: Multiplicative factor to be passed to the 'get_image_rms_measurements' function. Returns: The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. \"\"\" out = df [[ 'source' , 'wavg_ra' , 'wavg_dec' , 'img_diff_rms_path' ]] col_dtype = { 'source' : 'i' , 'wavg_ra' : 'f' , 'wavg_dec' : 'f' , 'img_diff_rms_path' : 'U' , 'img_diff_true_rms' : 'f' , } n_cpu = cpu_count () - 1 out = ( dd . from_pandas ( out , n_cpu ) . groupby ( 'img_diff_rms_path' ) . apply ( get_image_rms_measurements , edge_buffer = edge_buffer , meta = col_dtype ) . compute ( num_workers = n_cpu , scheduler = 'processes' ) ) df = df . merge ( out [[ 'source' , 'img_diff_true_rms' ]], left_on = 'source' , right_on = 'source' , how = 'left' ) return df","title":"new_sources.py"},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.check_primary_image","text":"Checks if the primary image is in the image list. Parameters: Name Type Description Default row Series Row of the missing_sources_df, need the keys 'primary' and 'img_list'. required Returns: Type Description bool True if the primary image is in the image list. Source code in vast_pipeline/pipeline/new_sources.py def check_primary_image ( row : pd . Series ) -> bool : \"\"\" Checks if the primary image is in the image list. Args: row: Row of the missing_sources_df, need the keys 'primary' and 'img_list'. Returns: True if the primary image is in the image list. \"\"\" return row [ 'primary' ] in row [ 'img_list' ]","title":"check_primary_image()"},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.gen_array_coords_from_wcs","text":"Converts SkyCoord coordinates to array coordinates given a wcs. Parameters: Name Type Description Default coords SkyCoord The coordinates to convert. required wcs WCS The WCS to use for the conversion. required Returns: Type Description ndarray Array containing the x and y array coordinates of the input sky coordinates, e.g.: np.array([[x1, x2, x3], [y1, y2, y3]]) Source code in vast_pipeline/pipeline/new_sources.py def gen_array_coords_from_wcs ( coords : SkyCoord , wcs : WCS ) -> np . ndarray : \"\"\" Converts SkyCoord coordinates to array coordinates given a wcs. Args: coords: The coordinates to convert. wcs: The WCS to use for the conversion. Returns: Array containing the x and y array coordinates of the input sky coordinates, e.g.: np.array([[x1, x2, x3], [y1, y2, y3]]) \"\"\" array_coords = wcs . world_to_array_index ( coords ) array_coords = np . array ([ np . array ( array_coords [ 0 ]), np . array ( array_coords [ 1 ]), ]) return array_coords","title":"gen_array_coords_from_wcs()"},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.get_image_rms_measurements","text":"Take the coordinates provided from the group and measure the array cell value in the provided image. Parameters: Name Type Description Default group DataFrame The group of sources to measure in the image, requiring the columns: 'source', 'wavg_ra', 'wavg_dec' and 'img_diff_rms_path'. required nbeam int The number of half beamwidths (BMAJ) away from the edge of the image or a NaN value that is acceptable. 3 edge_buffer float Multiplicative factor applied to nbeam to act as a buffer. 1.0 Returns: Type Description DataFrame The group dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Source code in vast_pipeline/pipeline/new_sources.py def get_image_rms_measurements ( group : pd . DataFrame , nbeam : int = 3 , edge_buffer : float = 1.0 ) -> pd . DataFrame : \"\"\" Take the coordinates provided from the group and measure the array cell value in the provided image. Args: group: The group of sources to measure in the image, requiring the columns: 'source', 'wavg_ra', 'wavg_dec' and 'img_diff_rms_path'. nbeam: The number of half beamwidths (BMAJ) away from the edge of the image or a NaN value that is acceptable. edge_buffer: Multiplicative factor applied to nbeam to act as a buffer. Returns: The group dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. \"\"\" image = group . iloc [ 0 ][ 'img_diff_rms_path' ] with fits . open ( image ) as hdul : header = hdul [ 0 ] . header wcs = WCS ( header , naxis = 2 ) data = hdul [ 0 ] . data . squeeze () # Here we mimic the forced fits behaviour, # sources within 3 half BMAJ widths of the image # edges are ignored. The user buffer is also # applied for consistency. pixelscale = ( proj_plane_pixel_scales ( wcs )[ 1 ] * u . deg ) . to ( u . arcsec ) bmaj = header [ \"BMAJ\" ] * u . deg npix = round ( ( nbeam / 2. * bmaj . to ( 'arcsec' ) / pixelscale ) . value ) npix = int ( round ( npix * edge_buffer )) coords = SkyCoord ( group . wavg_ra , group . wavg_dec , unit = ( u . deg , u . deg ) ) array_coords = gen_array_coords_from_wcs ( coords , wcs ) # check for pixel wrapping x_valid = np . logical_or ( array_coords [ 0 ] >= ( data . shape [ 0 ] - npix ), array_coords [ 0 ] < npix ) y_valid = np . logical_or ( array_coords [ 1 ] >= ( data . shape [ 1 ] - npix ), array_coords [ 1 ] < npix ) valid = ~ np . logical_or ( x_valid , y_valid ) valid_indexes = group [ valid ] . index . values group = group . loc [ valid_indexes ] if group . empty : # early return if all sources failed range check logger . debug ( 'All sources out of range in new source rms measurement' f ' for image { image } .' ) group [ 'img_diff_true_rms' ] = np . nan return group # Now we also need to check proximity to NaN values # as forced fits may also drop these values coords = SkyCoord ( group . wavg_ra , group . wavg_dec , unit = ( u . deg , u . deg ) ) array_coords = gen_array_coords_from_wcs ( coords , wcs ) acceptable_no_nan_dist = int ( round ( bmaj . to ( 'arcsec' ) . value / 2. / pixelscale . value ) ) nan_valid = [] # Get slices of each source and check NaN is not included. for i , j in zip ( array_coords [ 0 ], array_coords [ 1 ]): sl = tuple (( slice ( i - acceptable_no_nan_dist , i + acceptable_no_nan_dist ), slice ( j - acceptable_no_nan_dist , j + acceptable_no_nan_dist ) )) if np . any ( np . isnan ( data [ sl ])): nan_valid . append ( False ) else : nan_valid . append ( True ) valid_indexes = group [ nan_valid ] . index . values if np . any ( nan_valid ): # only run if there are actual values to measure rms_values = data [ array_coords [ 0 ][ nan_valid ], array_coords [ 1 ][ nan_valid ] ] # not matched ones will be NaN. group . loc [ valid_indexes , 'img_diff_true_rms' ] = rms_values . astype ( np . float64 ) * 1.e3 else : group [ 'img_diff_true_rms' ] = np . nan return group","title":"get_image_rms_measurements()"},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.new_sources","text":"Processes the new sources detected to check that they are valid new sources. This involves checking to see that the source should be seen at all in the images where it is not detected. For valid new sources the snr value the source would have in non-detected images is also calculated. Parameters: Name Type Description Default sources_df DataFrame The sources found from the association step. required missing_sources_df DataFrame The dataframe containing the 'missing detections' for each source. +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 776 | ['VAST_0127-73A.EPOCH01.I.fits'] | 31.8223 | -70.4674 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+ img_diff | ----------------------------------| ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ----------------------------------+ required min_sigma float The minimum sigma value acceptable when compared to the minimum rms of the respective image. required edge_buffer float Multiplicative factor to be passed to the 'get_image_rms_measurements' function. required p_run Run The pipeline run. required Returns: Type Description DataFrame The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Columns: source - source id, int. img_list - list of images, List. wavg_ra - weighted average RA, float. wavg_dec - weighted average Dec, float. skyreg_img_list - list of sky regions of images in img_list, List. img_diff - The images missing from coverage, List. primary - What should be the first image, str. detection - The first detection image, str. detection_time - Datetime of detection, datetime.datetime. img_diff_time - Datetime of img_diff list, datetime.datetime. img_diff_rms_min - Minimum rms of diff images, float. img_diff_rms_median - Median rms of diff images, float. img_diff_rms_path - rms path of diff images, str. flux_peak - Flux peak of source (detection), float. diff_sigma - SNR in differnce images (compared to minimum), float. img_diff_true_rms - The true rms value from the diff images, float. new_high_sigma - peak flux / true rms value, float. Source code in vast_pipeline/pipeline/new_sources.py def new_sources ( sources_df : pd . DataFrame , missing_sources_df : pd . DataFrame , min_sigma : float , edge_buffer : float , p_run : Run ) -> pd . DataFrame : \"\"\" Processes the new sources detected to check that they are valid new sources. This involves checking to see that the source *should* be seen at all in the images where it is not detected. For valid new sources the snr value the source would have in non-detected images is also calculated. Args: sources_df: The sources found from the association step. missing_sources_df: The dataframe containing the 'missing detections' for each source. +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 776 | ['VAST_0127-73A.EPOCH01.I.fits'] | 31.8223 | -70.4674 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+ img_diff | ----------------------------------| ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ----------------------------------+ min_sigma: The minimum sigma value acceptable when compared to the minimum rms of the respective image. edge_buffer: Multiplicative factor to be passed to the 'get_image_rms_measurements' function. p_run: The pipeline run. Returns: The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Columns: source - source id, int. img_list - list of images, List. wavg_ra - weighted average RA, float. wavg_dec - weighted average Dec, float. skyreg_img_list - list of sky regions of images in img_list, List. img_diff - The images missing from coverage, List. primary - What should be the first image, str. detection - The first detection image, str. detection_time - Datetime of detection, datetime.datetime. img_diff_time - Datetime of img_diff list, datetime.datetime. img_diff_rms_min - Minimum rms of diff images, float. img_diff_rms_median - Median rms of diff images, float. img_diff_rms_path - rms path of diff images, str. flux_peak - Flux peak of source (detection), float. diff_sigma - SNR in differnce images (compared to minimum), float. img_diff_true_rms - The true rms value from the diff images, float. new_high_sigma - peak flux / true rms value, float. \"\"\" timer = StopWatch () logger . info ( \"Starting new source analysis.\" ) cols = [ 'id' , 'name' , 'noise_path' , 'datetime' , 'rms_median' , 'rms_min' , 'rms_max' , ] images_df = pd . DataFrame ( list ( Image . objects . filter ( run = p_run ) . values ( * tuple ( cols )) )) . set_index ( 'name' ) # Get rid of sources that are not 'new', i.e. sources which the # first sky region image is not in the image list new_sources_df = missing_sources_df [ missing_sources_df [ 'in_primary' ] == False ] . drop ( columns = [ 'in_primary' ] ) # Check if the previous sources would have actually been seen # i.e. are the previous images sensitive enough # save the index before exploding new_sources_df = new_sources_df . reset_index () # Explode now to avoid two loops below new_sources_df = new_sources_df . explode ( 'img_diff' ) # Merge the respective image information to the df new_sources_df = new_sources_df . merge ( images_df [[ 'datetime' ]], left_on = 'detection' , right_on = 'name' , how = 'left' ) . rename ( columns = { 'datetime' : 'detection_time' }) new_sources_df = new_sources_df . merge ( images_df [[ 'datetime' , 'rms_min' , 'rms_median' , 'noise_path' ]], left_on = 'img_diff' , right_on = 'name' , how = 'left' ) . rename ( columns = { 'datetime' : 'img_diff_time' , 'rms_min' : 'img_diff_rms_min' , 'rms_median' : 'img_diff_rms_median' , 'noise_path' : 'img_diff_rms_path' }) # Select only those images that come before the detection image # in time. new_sources_df = new_sources_df [ new_sources_df . img_diff_time < new_sources_df . detection_time ] # merge the detection fluxes in new_sources_df = pd . merge ( new_sources_df , sources_df [[ 'source' , 'image' , 'flux_peak' ]], left_on = [ 'source' , 'detection' ], right_on = [ 'source' , 'image' ], how = 'left' ) . drop ( columns = [ 'image' ]) # calculate the sigma of the source if it was placed in the # minimum rms region of the previous images new_sources_df [ 'diff_sigma' ] = ( new_sources_df [ 'flux_peak' ] . values / new_sources_df [ 'img_diff_rms_min' ] . values ) # keep those that are above the user specified threshold new_sources_df = new_sources_df . loc [ new_sources_df [ 'diff_sigma' ] >= min_sigma ] # Now have list of sources that should have been seen before given # previous images minimum rms values. # Current inaccurate sky regions may mean that the source # was in a previous 'NaN' area of the image. This needs to be # checked. Currently the check is done by filtering out of range # pixels once the values have been obtained (below). # This could be done using MOCpy however this is reasonably # fast and the io of a MOC fits may take more time. # So these sources will be flagged as new sources, but we can also # make a guess of how signficant they are. For this the next step is # to measure the true rms at the source location. # measure the actual rms in the previous images at # the source location. new_sources_df = parallel_get_rms_measurements ( new_sources_df , edge_buffer = edge_buffer ) # this removes those that are out of range new_sources_df [ 'img_diff_true_rms' ] = ( new_sources_df [ 'img_diff_true_rms' ] . fillna ( 0. ) ) new_sources_df = new_sources_df [ new_sources_df [ 'img_diff_true_rms' ] != 0 ] # calculate the true sigma new_sources_df [ 'true_sigma' ] = ( new_sources_df [ 'flux_peak' ] . values / new_sources_df [ 'img_diff_true_rms' ] . values ) # We only care about the highest true sigma new_sources_df = new_sources_df . sort_values ( by = [ 'source' , 'true_sigma' ] ) # keep only the highest for each source, rename for the daatabase new_sources_df = ( new_sources_df . drop_duplicates ( 'source' ) . set_index ( 'source' ) . rename ( columns = { 'true_sigma' : 'new_high_sigma' }) ) # moving forward only the new_high_sigma columns is needed, drop all others. new_sources_df = new_sources_df [[ 'new_high_sigma' ]] logger . info ( 'Total new source analysis time: %.2f seconds' , timer . reset_init () ) return new_sources_df","title":"new_sources()"},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.parallel_get_rms_measurements","text":"Wrapper function to use 'get_image_rms_measurements' in parallel with Dask. nbeam is not an option here as that parameter is fixed in forced extraction and so is made sure to be fixed here to. This may change in the future. Parameters: Name Type Description Default df DataFrame The group of sources to measure in the images. required edge_buffer float Multiplicative factor to be passed to the 'get_image_rms_measurements' function. 1.0 Returns: Type Description DataFrame The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Source code in vast_pipeline/pipeline/new_sources.py def parallel_get_rms_measurements ( df : pd . DataFrame , edge_buffer : float = 1.0 ) -> pd . DataFrame : \"\"\" Wrapper function to use 'get_image_rms_measurements' in parallel with Dask. nbeam is not an option here as that parameter is fixed in forced extraction and so is made sure to be fixed here to. This may change in the future. Args: df: The group of sources to measure in the images. edge_buffer: Multiplicative factor to be passed to the 'get_image_rms_measurements' function. Returns: The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. \"\"\" out = df [[ 'source' , 'wavg_ra' , 'wavg_dec' , 'img_diff_rms_path' ]] col_dtype = { 'source' : 'i' , 'wavg_ra' : 'f' , 'wavg_dec' : 'f' , 'img_diff_rms_path' : 'U' , 'img_diff_true_rms' : 'f' , } n_cpu = cpu_count () - 1 out = ( dd . from_pandas ( out , n_cpu ) . groupby ( 'img_diff_rms_path' ) . apply ( get_image_rms_measurements , edge_buffer = edge_buffer , meta = col_dtype ) . compute ( num_workers = n_cpu , scheduler = 'processes' ) ) df = df . merge ( out [[ 'source' , 'img_diff_true_rms' ]], left_on = 'source' , right_on = 'source' , how = 'left' ) return df","title":"parallel_get_rms_measurements()"},{"location":"reference/pipeline/utils/","text":"This module contains utility functions that are used by the pipeline during the processing of a run. add_new_many_to_one_relations ( row ) \u00b6 This handles the relation information being created from the many_to_one function in advanced association. It is a lot simpler than the one_to_many case as it purely just adds the new relations to the relation column, taking into account if it is already a list of relations or not (i.e. no previous relations). Parameters: Name Type Description Default row Series The relation information Series from the assoication dataframe. Only the columns ['related_skyc1', 'new_relations'] are required. required Returns: Type Description List[int] The new related field for the source in question, containing the appended ids. Source code in vast_pipeline/pipeline/utils.py def add_new_many_to_one_relations ( row : pd . Series ) -> List [ int ]: \"\"\" This handles the relation information being created from the many_to_one function in advanced association. It is a lot simpler than the one_to_many case as it purely just adds the new relations to the relation column, taking into account if it is already a list of relations or not (i.e. no previous relations). Args: row: The relation information Series from the assoication dataframe. Only the columns ['related_skyc1', 'new_relations'] are required. Returns: The new related field for the source in question, containing the appended ids. \"\"\" out = row [ 'new_relations' ] . copy () if isinstance ( row [ 'related_skyc1' ], list ): out += row [ 'related_skyc1' ] . copy () return out add_new_one_to_many_relations ( row , advanced = False , source_ids = None ) \u00b6 This handles the relation information being created from the one_to_many functions in association. Parameters: Name Type Description Default row Series The relation information Series from the assoication dataframe. Only the columns ['related_skyc1', 'source_skyc1'] are required for advanced, these are instead called ['related', 'source'] for basic. required advanced bool Whether advanced association is being used which changes the names of the columns involved. False source_ids Optional[pandas.core.frame.DataFrame] A dataframe that contains the other ids to append to related for each original source. +----------------+--------+ | source_skyc1 | 0 | |----------------+--------| | 122 | [5542] | | 254 | [5543] | | 262 | [5544] | | 405 | [5545] | | 656 | [5546] | +----------------+--------+ None Returns: Type Description List[int] The new related field for the source in question, containing the appended ids. Source code in vast_pipeline/pipeline/utils.py def add_new_one_to_many_relations ( row : pd . Series , advanced : bool = False , source_ids : Optional [ pd . DataFrame ] = None ) -> List [ int ]: \"\"\" This handles the relation information being created from the one_to_many functions in association. Args: row: The relation information Series from the assoication dataframe. Only the columns ['related_skyc1', 'source_skyc1'] are required for advanced, these are instead called ['related', 'source'] for basic. advanced: Whether advanced association is being used which changes the names of the columns involved. source_ids: A dataframe that contains the other ids to append to related for each original source. +----------------+--------+ | source_skyc1 | 0 | |----------------+--------| | 122 | [5542] | | 254 | [5543] | | 262 | [5544] | | 405 | [5545] | | 656 | [5546] | +----------------+--------+ Returns: The new related field for the source in question, containing the appended ids. \"\"\" if source_ids is None : source_ids = pd . DataFrame () related_col = 'related_skyc1' if advanced else 'related' source_col = 'source_skyc1' if advanced else 'source' # this is the not_original case where the original source id is appended. if source_ids . empty : if isinstance ( row [ related_col ], list ): out = row [ related_col ] out . append ( row [ source_col ]) else : out = [ row [ source_col ],] else : # the original case to append all the new ids. source_ids = source_ids . loc [ row [ source_col ]] . iloc [ 0 ] if isinstance ( row [ related_col ], list ): out = row [ related_col ] + source_ids else : out = source_ids return out backup_parquets ( p_run_path ) \u00b6 Backups up all the existing parquet files in a pipeline run directory. Backups are named with a '.bak' suffix in the pipeline run directory. Parameters: Name Type Description Default p_run_path str The path of the pipeline run where the parquets are stored. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def backup_parquets ( p_run_path : str ) -> None : \"\"\" Backups up all the existing parquet files in a pipeline run directory. Backups are named with a '.bak' suffix in the pipeline run directory. Args: p_run_path: The path of the pipeline run where the parquets are stored. Returns: None \"\"\" parquets = ( glob . glob ( os . path . join ( p_run_path , \"*.parquet\" )) # TODO Remove arrow when vaex support is dropped. + glob . glob ( os . path . join ( p_run_path , \"*.arrow\" ))) for i in parquets : backup_name = i + '.bak' if os . path . isfile ( backup_name ): logger . debug ( f 'Removing old backup file: { backup_name } .' ) os . remove ( backup_name ) shutil . copyfile ( i , backup_name ) calc_ave_coord ( grp ) \u00b6 Calculates the average coordinate of the grouped by sources dataframe for each unique group, along with defining the image and epoch list for each unique source (group). Parameters: Name Type Description Default grp DataFrame The current group dataframe (unique source) of the grouped by dataframe being acted upon. required Returns: Type Description Series A pandas series containing the average coordinate along with the image and epoch lists. Source code in vast_pipeline/pipeline/utils.py def calc_ave_coord ( grp : pd . DataFrame ) -> pd . Series : \"\"\" Calculates the average coordinate of the grouped by sources dataframe for each unique group, along with defining the image and epoch list for each unique source (group). Args: grp: The current group dataframe (unique source) of the grouped by dataframe being acted upon. Returns: A pandas series containing the average coordinate along with the image and epoch lists. \"\"\" d = {} grp = grp . sort_values ( by = 'datetime' ) d [ 'img_list' ] = grp [ 'image' ] . values . tolist () d [ 'epoch_list' ] = grp [ 'epoch' ] . values . tolist () d [ 'wavg_ra' ] = grp [ 'interim_ew' ] . sum () / grp [ 'weight_ew' ] . sum () d [ 'wavg_dec' ] = grp [ 'interim_ns' ] . sum () / grp [ 'weight_ns' ] . sum () return pd . Series ( d ) calculate_m_metric ( flux_a , flux_b ) \u00b6 Calculate the m variability metric which is the modulation index between two fluxes. This is proportional to the fractional variability. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Parameters: Name Type Description Default flux_a float flux value \"A\". required flux_b float flux value \"B\". required Returns: Type Description float float: the m metric for flux values \"A\" and \"B\". Source code in vast_pipeline/pipeline/utils.py def calculate_m_metric ( flux_a : float , flux_b : float ) -> float : \"\"\"Calculate the m variability metric which is the modulation index between two fluxes. This is proportional to the fractional variability. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Args: flux_a (float): flux value \"A\". flux_b (float): flux value \"B\". Returns: float: the m metric for flux values \"A\" and \"B\". \"\"\" return 2 * (( flux_a - flux_b ) / ( flux_a + flux_b )) calculate_measurement_pair_metrics ( df ) \u00b6 Generate a DataFrame of measurement pairs and their 2-epoch variability metrics from a DataFrame of measurements. For more information on the variability metrics, see Section 5 of Mooley et al. (2016), DOI: 10.3847/0004-637X/818/2/105. Parameters: Name Type Description Default df DataFrame Input measurements. Must contain columns: id, source, flux_int, flux_int_err, flux_peak, flux_peak_err, has_siblings. required Returns: Type Description DataFrame pd.DataFrame: Measurement pairs and 2-epoch metrics. Will contain columns: source - the source ID id_a, id_b - the measurement IDs flux_int_a, flux_int_b - measurement integrated fluxes in mJy flux_int_err_a, flux_int_err_b - measurement integrated flux errors in mJy flux_peak_a, flux_peak_b - measurement peak fluxes in mJy/beam flux_peak_err_a, flux_peak_err_b - measurement peak flux errors in mJy/beam vs_peak, vs_int - variability t-statistic m_peak, m_int - variability modulation index Source code in vast_pipeline/pipeline/utils.py def calculate_measurement_pair_metrics ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Generate a DataFrame of measurement pairs and their 2-epoch variability metrics from a DataFrame of measurements. For more information on the variability metrics, see Section 5 of Mooley et al. (2016), DOI: 10.3847/0004-637X/818/2/105. Args: df (pd.DataFrame): Input measurements. Must contain columns: id, source, flux_int, flux_int_err, flux_peak, flux_peak_err, has_siblings. Returns: pd.DataFrame: Measurement pairs and 2-epoch metrics. Will contain columns: source - the source ID id_a, id_b - the measurement IDs flux_int_a, flux_int_b - measurement integrated fluxes in mJy flux_int_err_a, flux_int_err_b - measurement integrated flux errors in mJy flux_peak_a, flux_peak_b - measurement peak fluxes in mJy/beam flux_peak_err_a, flux_peak_err_b - measurement peak flux errors in mJy/beam vs_peak, vs_int - variability t-statistic m_peak, m_int - variability modulation index \"\"\" n_cpu = cpu_count () - 1 \"\"\"Create a DataFrame containing all measurement ID combinations per source. Resultant DataFrame will have a MultiIndex([\"source\", RangeIndex]) where \"source\" is the source ID and RangeIndex is an unnamed temporary ID for each measurement pair, unique only together with source. DataFrame will have columns [0, 1], each containing a measurement ID. e.g. 0 1 source 1 0 1 9284 1 1 17597 2 1 26984 3 9284 17597 4 9284 26984 ... ... ... 11105 2 11845 19961 11124 0 3573 12929 1 3573 21994 2 12929 21994 11128 0 6216 23534 \"\"\" measurement_combinations = ( dd . from_pandas ( df , n_cpu ) . groupby ( \"source\" )[ \"id\" ] . apply ( lambda x : pd . DataFrame ( list ( combinations ( x , 2 )) ), meta = { 0 : \"i\" , 1 : \"i\" },) . compute ( num_workers = n_cpu , scheduler = \"processes\" ) ) \"\"\"Drop the RangeIndex from the MultiIndex as it isn't required and rename the columns. Example resultant DataFrame: source id_a id_b 0 1 1 9284 1 1 1 17597 2 1 1 26984 3 1 9284 17597 4 1 9284 26984 ... ... ... ... 33640 11105 11845 19961 33641 11124 3573 12929 33642 11124 3573 21994 33643 11124 12929 21994 33644 11128 6216 23534 Where source is the source ID, id_a and id_b are measurement IDs. \"\"\" measurement_combinations = measurement_combinations . reset_index ( level = 1 , drop = True ) . rename ( columns = { 0 : \"id_a\" , 1 : \"id_b\" }) . astype ( int ) . reset_index () # Dask has a tendency to swap which order the measurement pairs are # defined in, even if the dataframe is pre-sorted. We want the pairs to be # in date order (a < b) so the code below corrects any that are not. measurement_combinations = measurement_combinations . join ( df [[ 'source' , 'id' , 'datetime' ]] . set_index ([ 'source' , 'id' ]), on = [ 'source' , 'id_a' ], ) measurement_combinations = measurement_combinations . join ( df [[ 'source' , 'id' , 'datetime' ]] . set_index ([ 'source' , 'id' ]), on = [ 'source' , 'id_b' ], lsuffix = '_a' , rsuffix = '_b' ) to_correct_mask = ( measurement_combinations [ 'datetime_a' ] > measurement_combinations [ 'datetime_b' ] ) if np . any ( to_correct_mask ): logger . debug ( 'Correcting measurement pairs order' ) ( measurement_combinations . loc [ to_correct_mask , 'id_a' ], measurement_combinations . loc [ to_correct_mask , 'id_b' ] ) = np . array ([ measurement_combinations . loc [ to_correct_mask , 'id_b' ] . values , measurement_combinations . loc [ to_correct_mask , 'id_a' ] . values ]) measurement_combinations = measurement_combinations . drop ( [ 'datetime_a' , 'datetime_b' ], axis = 1 ) # add the measurement fluxes and errors association_fluxes = df . set_index ([ \"source\" , \"id\" ])[ [ \"flux_int\" , \"flux_int_err\" , \"flux_peak\" , \"flux_peak_err\" , \"image\" ] ] . rename ( columns = { \"image\" : \"image_name\" }) measurement_combinations = measurement_combinations . join ( association_fluxes , on = [ \"source\" , \"id_a\" ], ) . join ( association_fluxes , on = [ \"source\" , \"id_b\" ], lsuffix = \"_a\" , rsuffix = \"_b\" , ) # calculate 2-epoch metrics measurement_combinations [ \"vs_peak\" ] = calculate_vs_metric ( measurement_combinations . flux_peak_a , measurement_combinations . flux_peak_b , measurement_combinations . flux_peak_err_a , measurement_combinations . flux_peak_err_b , ) measurement_combinations [ \"vs_int\" ] = calculate_vs_metric ( measurement_combinations . flux_int_a , measurement_combinations . flux_int_b , measurement_combinations . flux_int_err_a , measurement_combinations . flux_int_err_b , ) measurement_combinations [ \"m_peak\" ] = calculate_m_metric ( measurement_combinations . flux_peak_a , measurement_combinations . flux_peak_b , ) measurement_combinations [ \"m_int\" ] = calculate_m_metric ( measurement_combinations . flux_int_a , measurement_combinations . flux_int_b , ) return measurement_combinations calculate_vs_metric ( flux_a , flux_b , flux_err_a , flux_err_b ) \u00b6 Calculate the Vs variability metric which is the t-statistic that the provided fluxes are variable. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Parameters: Name Type Description Default flux_a float flux value \"A\". required flux_b float flux value \"B\". required flux_err_a float error of flux_a . required flux_err_b float error of flux_b . required Returns: Type Description float float: the Vs metric for flux values \"A\" and \"B\". Source code in vast_pipeline/pipeline/utils.py def calculate_vs_metric ( flux_a : float , flux_b : float , flux_err_a : float , flux_err_b : float ) -> float : \"\"\"Calculate the Vs variability metric which is the t-statistic that the provided fluxes are variable. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Args: flux_a (float): flux value \"A\". flux_b (float): flux value \"B\". flux_err_a (float): error of `flux_a`. flux_err_b (float): error of `flux_b`. Returns: float: the Vs metric for flux values \"A\" and \"B\". \"\"\" return ( flux_a - flux_b ) / np . hypot ( flux_err_a , flux_err_b ) check_primary_image ( row ) \u00b6 Checks whether the primary image of the ideal source dataframe is in the image list for the source. Parameters: Name Type Description Default row Series Input dataframe row, with columns ['primary'] and ['img_list']. required Returns: Type Description bool True if primary in image list else False. Source code in vast_pipeline/pipeline/utils.py def check_primary_image ( row : pd . Series ) -> bool : \"\"\" Checks whether the primary image of the ideal source dataframe is in the image list for the source. Args: row: Input dataframe row, with columns ['primary'] and ['img_list']. Returns: True if primary in image list else False. \"\"\" return row [ 'primary' ] in row [ 'img_list' ] create_measurement_pairs_arrow_file ( p_run ) \u00b6 Creates a measurement_pairs.arrow file using the parquet outputs of a pipeline run. Vaex is used to do the exporting to arrow to ensure compatibility with Vaex. Parameters: Name Type Description Default p_run Run Pipeline model instance. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def create_measurement_pairs_arrow_file ( p_run : Run ) -> None : \"\"\" Creates a measurement_pairs.arrow file using the parquet outputs of a pipeline run. Vaex is used to do the exporting to arrow to ensure compatibility with Vaex. Args: p_run: Pipeline model instance. Returns: None \"\"\" logger . info ( 'Creating measurement_pairs.arrow for run %s .' , p_run . name ) measurement_pairs_df = pd . read_parquet ( os . path . join ( p_run . path , 'measurement_pairs.parquet' ) ) logger . debug ( 'Optimising dataframe.' ) measurement_pairs_df = optimize_ints ( optimize_floats ( measurement_pairs_df )) # use vaex to export to arrow logger . debug ( \"Loading to vaex.\" ) measurement_pairs_df = vaex . from_pandas ( measurement_pairs_df ) logger . debug ( \"Exporting to arrow.\" ) outname = os . path . join ( p_run . path , 'measurement_pairs.arrow' ) measurement_pairs_df . export_arrow ( outname ) create_measurements_arrow_file ( p_run ) \u00b6 Creates a measurements.arrow file using the parquet outputs of a pipeline run. Vaex is used to do the exporting to arrow to ensure compatibility with Vaex. Parameters: Name Type Description Default p_run Run Pipeline model instance. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def create_measurements_arrow_file ( p_run : Run ) -> None : \"\"\" Creates a measurements.arrow file using the parquet outputs of a pipeline run. Vaex is used to do the exporting to arrow to ensure compatibility with Vaex. Args: p_run: Pipeline model instance. Returns: None \"\"\" logger . info ( 'Creating measurements.arrow for run %s .' , p_run . name ) associations = pd . read_parquet ( os . path . join ( p_run . path , 'associations.parquet' ) ) images = pd . read_parquet ( os . path . join ( p_run . path , 'images.parquet' ) ) m_files = images [ 'measurements_path' ] . tolist () m_files += glob . glob ( os . path . join ( p_run . path , 'forced*.parquet' )) logger . debug ( 'Loading %i files...' , len ( m_files )) measurements = dd . read_parquet ( m_files , engine = 'pyarrow' ) . compute () measurements = measurements . loc [ measurements [ 'id' ] . isin ( associations [ 'meas_id' ] . values ) ] measurements = ( associations . loc [:, [ 'meas_id' , 'source_id' ]] . set_index ( 'meas_id' ) . merge ( measurements , left_index = True , right_on = 'id' ) . rename ( columns = { 'source_id' : 'source' }) ) logger . debug ( 'Optimising dataframes.' ) measurements = optimize_ints ( optimize_floats ( measurements )) # use vaex to export to arrow logger . debug ( \"Loading to vaex.\" ) measurements = vaex . from_pandas ( measurements ) logger . debug ( \"Exporting to arrow.\" ) outname = os . path . join ( p_run . path , 'measurements.arrow' ) measurements . export_arrow ( outname ) cross_join ( left , right ) \u00b6 A convenience function to merge two dataframes. Parameters: Name Type Description Default left DataFrame The base pandas DataFrame to merge. required right DataFrame The pandas DataFrame to merge to the left. required Returns: Type Description DataFrame The resultant merged DataFrame. Source code in vast_pipeline/pipeline/utils.py def cross_join ( left : pd . DataFrame , right : pd . DataFrame ) -> pd . DataFrame : \"\"\" A convenience function to merge two dataframes. Args: left: The base pandas DataFrame to merge. right: The pandas DataFrame to merge to the left. Returns: The resultant merged DataFrame. \"\"\" return ( left . assign ( key = 1 ) . merge ( right . assign ( key = 1 ), on = 'key' ) . drop ( 'key' , axis = 1 ) ) get_create_img ( p_run , band_id , image ) \u00b6 Function to fetch or create the Image and Sky Region objects for the images in the pipeline run. Parameters: Name Type Description Default p_run Run The pipeline run Django ORM object. required band_id int The integer database id value of the frequency band of the image. required image Image The image Django ORM object. required Returns: Type Description Tuple[vast_pipeline.models.Image, vast_pipeline.models.SkyRegion, bool] The resulting image django ORM object, the sky region Django ORM object and a bool value denoting if the image already existed in the database. Source code in vast_pipeline/pipeline/utils.py def get_create_img ( p_run : Run , band_id : int , image : Image ) -> Tuple [ Image , SkyRegion , bool ]: \"\"\" Function to fetch or create the Image and Sky Region objects for the images in the pipeline run. Args: p_run: The pipeline run Django ORM object. band_id: The integer database id value of the frequency band of the image. image: The image Django ORM object. Returns: The resulting image django ORM object, the sky region Django ORM object and a bool value denoting if the image already existed in the database. \"\"\" img = Image . objects . filter ( name__exact = image . name ) if img . exists (): img = img . get () # Add background path if not originally provided if image . background_path and not img . background_path : img . background_path = image . background_path img . save () skyreg = get_create_skyreg ( p_run , img ) # check and add the many to many if not existent if not Image . objects . filter ( id = img . id , run__id = p_run . id ) . exists (): img . run . add ( p_run ) return ( img , skyreg , True ) # at this stage, measurement parquet file is not created but # assume location img_folder_name = image . name . replace ( '.' , '_' ) measurements_path = os . path . join ( settings . PIPELINE_WORKING_DIR , 'images' , img_folder_name , 'measurements.parquet' ) img = Image ( band_id = band_id , measurements_path = measurements_path ) # set the attributes and save the image, # by selecting only valid (not hidden) attributes # FYI attributs and/or method starting with _ are hidden # and with __ can't be modified/called for fld in img . _meta . get_fields (): if getattr ( fld , 'attname' , None ) and ( getattr ( image , fld . attname , None ) is not None ): setattr ( img , fld . attname , getattr ( image , fld . attname )) # get create the sky region and associate with image skyreg = get_create_skyreg ( p_run , img ) img . skyreg = skyreg img . rms_median , img . rms_min , img . rms_max = get_rms_noise_image_values ( img . noise_path ) img . save () img . run . add ( p_run ) return ( img , skyreg , False ) get_create_img_band ( image ) \u00b6 Return the existing Band row for the given FitsImage. An image is considered to belong to a band if its frequency is within some tolerance of the band's frequency. Returns a Band row or None if no matching band. Parameters: Name Type Description Default image Image The image Django ORM object. required Returns: Type Description Band The band Django ORM object. Source code in vast_pipeline/pipeline/utils.py def get_create_img_band ( image : Image ) -> Band : ''' Return the existing Band row for the given FitsImage. An image is considered to belong to a band if its frequency is within some tolerance of the band's frequency. Returns a Band row or None if no matching band. Args: image: The image Django ORM object. Returns: The band Django ORM object. ''' # For now we match bands using the central frequency. # This assumes that every band has a unique frequency, # which is true for the data we've used so far. freq = int ( image . freq_eff * 1.e-6 ) freq_band = int ( image . freq_bw * 1.e-6 ) # TODO: refine the band query for band in Band . objects . all (): diff = abs ( freq - band . frequency ) / float ( band . frequency ) if diff < 0.02 : return band # no band has been found so create it band = Band ( name = str ( freq ), frequency = freq , bandwidth = freq_band ) logger . info ( 'Adding new frequency band: %s ' , band ) band . save () return band get_create_p_run ( name , path , description = None , user = None ) \u00b6 Get or create a pipeline run in db, return the run django object and a flag True/False if has been created or already exists. Parameters: Name Type Description Default name str The name of the pipeline run. required path str The system path to the pipeline run folder which contains the configuration file and where outputs will be saved. required description str An optional description of the pipeline run. None user User The Django user that launched the pipeline run. None Returns: Type Description Tuple[vast_pipeline.models.Run, bool] The pipeline run object and a boolean object representing whether the pipeline run already existed ('True') or not ('False'). Source code in vast_pipeline/pipeline/utils.py def get_create_p_run ( name : str , path : str , description : str = None , user : User = None ) -> Tuple [ Run , bool ]: ''' Get or create a pipeline run in db, return the run django object and a flag True/False if has been created or already exists. Args: name: The name of the pipeline run. path: The system path to the pipeline run folder which contains the configuration file and where outputs will be saved. description: An optional description of the pipeline run. user: The Django user that launched the pipeline run. Returns: The pipeline run object and a boolean object representing whether the pipeline run already existed ('True') or not ('False'). ''' p_run = Run . objects . filter ( name__exact = name ) if p_run : return p_run . get (), True description = \"\" if description is None else description p_run = Run ( name = name , description = description , path = path ) if user : p_run . user = user p_run . save () return p_run , False get_create_skyreg ( p_run , image ) \u00b6 This creates a Sky Region object in Django ORM given the related image object. Parameters: Name Type Description Default p_run Run The pipeline run Django ORM object. required image Image The image Django ORM object. required Returns: Type Description SkyRegion The sky region Django ORM object. Source code in vast_pipeline/pipeline/utils.py def get_create_skyreg ( p_run : Run , image : Image ) -> SkyRegion : ''' This creates a Sky Region object in Django ORM given the related image object. Args: p_run: The pipeline run Django ORM object. image: The image Django ORM object. Returns: The sky region Django ORM object. ''' # In the calculations below, it is assumed the image has square # pixels (this pipeline has been designed for ASKAP images, so it # should always be square). It will likely give wrong results if not skyr = SkyRegion . objects . filter ( centre_ra = image . ra , centre_dec = image . dec , xtr_radius = image . fov_bmin ) if skyr : skyr = skyr . get () logger . info ( 'Found sky region %s ' , skyr ) if p_run not in skyr . run . all (): logger . info ( 'Adding %s to sky region %s ' , p_run , skyr ) skyr . run . add ( p_run ) return skyr x , y , z = eq_to_cart ( image . ra , image . dec ) skyr = SkyRegion ( centre_ra = image . ra , centre_dec = image . dec , width_ra = image . physical_bmin , width_dec = image . physical_bmaj , xtr_radius = image . fov_bmin , x = x , y = y , z = z , ) skyr . save () logger . info ( 'Created sky region %s ' , skyr ) skyr . run . add ( p_run ) logger . info ( 'Adding %s to sky region %s ' , p_run , skyr ) return skyr get_eta_metric ( row , df , peak = False ) \u00b6 Calculates the eta variability metric of a source. Works on the grouped by dataframe using the fluxes of the assoicated measurements. Parameters: Name Type Description Default row Dict[str, float] Dictionary containg statistics for the current source. required df DataFrame The grouped by sources dataframe of the measurements containing all the flux and flux error information, required peak bool Whether to use peak_flux for the calculation. If False then the integrated flux is used. False Returns: Type Description float The calculated eta value. Source code in vast_pipeline/pipeline/utils.py def get_eta_metric ( row : Dict [ str , float ], df : pd . DataFrame , peak : bool = False ) -> float : ''' Calculates the eta variability metric of a source. Works on the grouped by dataframe using the fluxes of the assoicated measurements. Args: row: Dictionary containg statistics for the current source. df: The grouped by sources dataframe of the measurements containing all the flux and flux error information, peak: Whether to use peak_flux for the calculation. If False then the integrated flux is used. Returns: The calculated eta value. ''' if row [ 'n_meas' ] == 1 : return 0. suffix = 'peak' if peak else 'int' weights = 1. / df [ f 'flux_ { suffix } _err' ] . values ** 2 fluxes = df [ f 'flux_ { suffix } ' ] . values eta = ( row [ 'n_meas' ] / ( row [ 'n_meas' ] - 1 )) * ( ( weights * fluxes ** 2 ) . mean () - ( ( weights * fluxes ) . mean () ** 2 / weights . mean () ) ) return eta get_image_list_diff ( row ) \u00b6 Calculate the difference between the ideal coverage image list of a source and the actual observed image list. Also checks whether an epoch does in fact contain a detection but is not in the expected 'ideal' image for that epoch. Parameters: Name Type Description Default row Series The row from the sources dataframe that is being iterated over. required Returns: Type Description List[str] A list of the images missing from the observed image list. Will be returned as '-1' integer value if there are no missing images. Source code in vast_pipeline/pipeline/utils.py def get_image_list_diff ( row : pd . Series ) -> List [ str ]: \"\"\" Calculate the difference between the ideal coverage image list of a source and the actual observed image list. Also checks whether an epoch does in fact contain a detection but is not in the expected 'ideal' image for that epoch. Args: row: The row from the sources dataframe that is being iterated over. Returns: A list of the images missing from the observed image list. Will be returned as '-1' integer value if there are no missing images. \"\"\" out = list ( filter ( lambda arg : arg not in row [ 'img_list' ], row [ 'skyreg_img_list' ]) ) # set empty list to -1 if not out : return - 1 # Check that an epoch has not already been seen (just not in the 'ideal' # image) out_epochs = [ row [ 'skyreg_epoch' ][ pair [ 0 ]] for pair in enumerate ( row [ 'skyreg_img_list' ] ) if pair [ 1 ] in out ] out = [ out [ pair [ 0 ]] for pair in enumerate ( out_epochs ) if pair [ 1 ] not in row [ 'epoch_list' ] ] if not out : out = - 1 return out get_names_and_epochs ( grp ) \u00b6 Convenience function to group together the image names, epochs and datetimes into one list object which is then returned as a pandas series. This is necessary for easier processing in the ideal coverage analysis. Parameters: Name Type Description Default grp DataFrame A group from the grouped by sources DataFrame. required Returns: Type Description Series Pandas series containing the list object that contains the lists of the image names, epochs and datetimes. Source code in vast_pipeline/pipeline/utils.py def get_names_and_epochs ( grp : pd . DataFrame ) -> pd . Series : \"\"\" Convenience function to group together the image names, epochs and datetimes into one list object which is then returned as a pandas series. This is necessary for easier processing in the ideal coverage analysis. Args: grp: A group from the grouped by sources DataFrame. Returns: Pandas series containing the list object that contains the lists of the image names, epochs and datetimes. \"\"\" d = {} d [ 'skyreg_img_epoch_list' ] = [[[ x ,], y , z ] for x , y , z in zip ( grp [ 'name' ] . values . tolist (), grp [ 'epoch' ] . values . tolist (), grp [ 'datetime' ] . values . tolist () )] return pd . Series ( d ) get_parallel_assoc_image_df ( images , skyregion_groups ) \u00b6 Merge the sky region groups with the images and skyreg_ids. Parameters: Name Type Description Default images List[vast_pipeline.models.Image] A list of the Image objects. required skyregion_groups DataFrame The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ required Returns: Type Description DataFrame Dataframe containing the merged images and skyreg_id and skyreg_group. +----+-------------------------------+-------------+----------------+ | | image | skyreg_id | skyreg_group | |----+-------------------------------+-------------+----------------| | 0 | VAST_2118+00A.EPOCH01.I.fits | 2 | 1 | | 1 | VAST_2118-06A.EPOCH01.I.fits | 3 | 1 | | 2 | VAST_0127-73A.EPOCH01.I.fits | 1 | 2 | | 3 | VAST_2118-06A.EPOCH03x.I.fits | 3 | 1 | | 4 | VAST_2118-06A.EPOCH02.I.fits | 3 | 1 | | 5 | VAST_2118-06A.EPOCH05x.I.fits | 3 | 1 | | 6 | VAST_2118-06A.EPOCH06x.I.fits | 3 | 1 | | 7 | VAST_0127-73A.EPOCH08.I.fits | 1 | 2 | +----+-------------------------------+-------------+----------------+ Source code in vast_pipeline/pipeline/utils.py def get_parallel_assoc_image_df ( images : List [ Image ], skyregion_groups : pd . DataFrame ) -> pd . DataFrame : \"\"\" Merge the sky region groups with the images and skyreg_ids. Args: images: A list of the Image objects. skyregion_groups: The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ Returns: Dataframe containing the merged images and skyreg_id and skyreg_group. +----+-------------------------------+-------------+----------------+ | | image | skyreg_id | skyreg_group | |----+-------------------------------+-------------+----------------| | 0 | VAST_2118+00A.EPOCH01.I.fits | 2 | 1 | | 1 | VAST_2118-06A.EPOCH01.I.fits | 3 | 1 | | 2 | VAST_0127-73A.EPOCH01.I.fits | 1 | 2 | | 3 | VAST_2118-06A.EPOCH03x.I.fits | 3 | 1 | | 4 | VAST_2118-06A.EPOCH02.I.fits | 3 | 1 | | 5 | VAST_2118-06A.EPOCH05x.I.fits | 3 | 1 | | 6 | VAST_2118-06A.EPOCH06x.I.fits | 3 | 1 | | 7 | VAST_0127-73A.EPOCH08.I.fits | 1 | 2 | +----+-------------------------------+-------------+----------------+ \"\"\" skyreg_ids = [ i . skyreg_id for i in images ] images_df = pd . DataFrame ({ 'image_dj' : images , 'skyreg_id' : skyreg_ids , }) images_df = images_df . merge ( skyregion_groups , how = 'left' , left_on = 'skyreg_id' , right_index = True ) images_df [ 'image_name' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . name ) images_df [ 'image_datetime' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . datetime ) return images_df get_rms_noise_image_values ( rms_path ) \u00b6 Open the RMS noise FITS file and compute the median, max and min rms values to be added to the image model and then used in the calculations. Parameters: Name Type Description Default rms_path str The system path to the RMS FITS image. required Returns: Type Description Tuple[float, float, float] The median, minimum and maximum values of the RMS image. Exceptions: Type Description IOError Raised when the RMS FITS file cannot be found. Source code in vast_pipeline/pipeline/utils.py def get_rms_noise_image_values ( rms_path : str ) -> Tuple [ float , float , float ]: ''' Open the RMS noise FITS file and compute the median, max and min rms values to be added to the image model and then used in the calculations. Args: rms_path: The system path to the RMS FITS image. Returns: The median, minimum and maximum values of the RMS image. Raises: IOError: Raised when the RMS FITS file cannot be found. ''' logger . debug ( 'Extracting Image RMS values from Noise file...' ) med_val = min_val = max_val = 0. try : with fits . open ( rms_path ) as f : data = f [ 0 ] . data data = data [ np . logical_not ( np . isnan ( data ))] data = data [ data != 0 ] med_val = np . median ( data ) * 1e+3 min_val = np . min ( data ) * 1e+3 max_val = np . max ( data ) * 1e+3 del data except Exception : raise IOError ( f 'Could not read this RMS FITS file: { rms_path } ' ) return med_val , min_val , max_val get_src_skyregion_merged_df ( sources_df , images_df , skyreg_df ) \u00b6 Analyses the current sources_df to determine what the 'ideal coverage' for each source should be. In other words, what images is the source missing in when it should have been seen. Parameters: Name Type Description Default sources_df DataFrame The output of the assoication step containing the measurements assoicated into sources. required images_df DataFrame Contains the images of the pipeline run. I.e. all image objects for the run loaded into a dataframe. required skyreg_df DataFrame Contains the sky regions of the pipeline run. I.e. all sky region objects for the run loaded into a dataframe. required Returns: Type Description DataFrame DataFrame containing missing image information. Output format: +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | | 1290 | ['VAST_0127-73A.EPOCH01.I.fits'] | 20.8455 | -76.8269 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+------------------------------+ img_diff | primary | ----------------------------------+------------------------------+ ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ----------------------------------+------------------------------+ ------------------------------+--------------+ detection | in_primary | ------------------------------+--------------| VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | ------------------------------+--------------+ Source code in vast_pipeline/pipeline/utils.py def get_src_skyregion_merged_df ( sources_df : pd . DataFrame , images_df : pd . DataFrame , skyreg_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Analyses the current sources_df to determine what the 'ideal coverage' for each source should be. In other words, what images is the source missing in when it should have been seen. Args: sources_df: The output of the assoication step containing the measurements assoicated into sources. images_df: Contains the images of the pipeline run. I.e. all image objects for the run loaded into a dataframe. skyreg_df: Contains the sky regions of the pipeline run. I.e. all sky region objects for the run loaded into a dataframe. Returns: DataFrame containing missing image information. Output format: +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | | 1290 | ['VAST_0127-73A.EPOCH01.I.fits'] | 20.8455 | -76.8269 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+------------------------------+ img_diff | primary | ----------------------------------+------------------------------+ ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ----------------------------------+------------------------------+ ------------------------------+--------------+ detection | in_primary | ------------------------------+--------------| VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | ------------------------------+--------------+ \"\"\" logger . info ( \"Creating ideal source coverage df...\" ) merged_timer = StopWatch () skyreg_df = skyreg_df . drop ( [ 'x' , 'y' , 'z' , 'width_ra' , 'width_dec' ], axis = 1 ) images_df [ 'name' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . name ) images_df [ 'datetime' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . datetime ) skyreg_df = skyreg_df . join ( pd . DataFrame ( images_df . groupby ( 'skyreg_id' ) . apply ( get_names_and_epochs ) ), on = 'id' ) sources_df = sources_df . sort_values ( by = 'datetime' ) # calculate some metrics on sources # compute only some necessary metrics in the groupby timer = StopWatch () srcs_df = parallel_groupby_coord ( sources_df ) logger . debug ( 'Groupby-apply time: %.2f seconds' , timer . reset ()) del sources_df # create dataframe with all skyregions and sources combinations src_skyrg_df = cross_join ( srcs_df . drop ([ 'epoch_list' , 'img_list' ], axis = 1 ) . reset_index (), skyreg_df . drop ( 'skyreg_img_epoch_list' , axis = 1 ) ) skyreg_df = skyreg_df . drop ( [ 'centre_ra' , 'centre_dec' , 'xtr_radius' ], axis = 1 ) . set_index ( 'id' ) src_skyrg_df [ 'sep' ] = np . rad2deg ( on_sky_sep ( np . deg2rad ( src_skyrg_df [ 'wavg_ra' ] . values ), np . deg2rad ( src_skyrg_df [ 'centre_ra' ] . values ), np . deg2rad ( src_skyrg_df [ 'wavg_dec' ] . values ), np . deg2rad ( src_skyrg_df [ 'centre_dec' ] . values ), ) ) # select rows where separation is less than sky region radius # drop not more useful columns and groupby source id # compute list of images src_skyrg_df = ( src_skyrg_df . loc [ src_skyrg_df . sep < src_skyrg_df . xtr_radius , [ 'source' , 'id' , 'sep' ] ] . merge ( skyreg_df , left_on = 'id' , right_index = True ) . drop ( 'id' , axis = 1 ) . explode ( 'skyreg_img_epoch_list' ) ) del skyreg_df src_skyrg_df [ [ 'skyreg_img_list' , 'skyreg_epoch' , 'skyreg_datetime' ] ] = pd . DataFrame ( src_skyrg_df [ 'skyreg_img_epoch_list' ] . tolist (), index = src_skyrg_df . index ) src_skyrg_df = src_skyrg_df . drop ( 'skyreg_img_epoch_list' , axis = 1 ) src_skyrg_df = ( src_skyrg_df . sort_values ( [ 'source' , 'sep' ] ) . drop_duplicates ([ 'source' , 'skyreg_epoch' ]) . sort_values ( by = 'skyreg_datetime' ) . drop ( [ 'sep' , 'skyreg_datetime' ], axis = 1 ) ) # annoyingly epoch needs to be not a list to drop duplicates # but then we need to sum the epochs into a list src_skyrg_df [ 'skyreg_epoch' ] = src_skyrg_df [ 'skyreg_epoch' ] . apply ( lambda x : [ x ,] ) src_skyrg_df = ( src_skyrg_df . groupby ( 'source' ) . agg ( 'sum' ) # sum because we need to preserve order ) # merge into main df and compare the images srcs_df = srcs_df . merge ( src_skyrg_df , left_index = True , right_index = True ) del src_skyrg_df srcs_df [ 'img_diff' ] = srcs_df [ [ 'img_list' , 'skyreg_img_list' , 'epoch_list' , 'skyreg_epoch' ] ] . apply ( get_image_list_diff , axis = 1 ) srcs_df = srcs_df . loc [ srcs_df [ 'img_diff' ] != - 1 ] srcs_df = srcs_df . drop ( [ 'epoch_list' , 'skyreg_epoch' ], axis = 1 ) srcs_df [ 'primary' ] = srcs_df [ 'skyreg_img_list' ] . apply ( lambda x : x [ 0 ]) srcs_df [ 'detection' ] = srcs_df [ 'img_list' ] . apply ( lambda x : x [ 0 ]) srcs_df [ 'in_primary' ] = srcs_df [ [ 'primary' , 'img_list' ] ] . apply ( check_primary_image , axis = 1 ) srcs_df = srcs_df . drop ([ 'img_list' , 'skyreg_img_list' , 'primary' ], axis = 1 ) logger . info ( 'Ideal source coverage time: %.2f seconds' , merged_timer . reset () ) return srcs_df group_skyregions ( df ) \u00b6 Logic to group sky regions into overlapping groups. Returns a dataframe containing the sky region id as the index and a column containing a list of the sky region group number it belongs to. Parameters: Name Type Description Default df DataFrame A dataframe containing all the sky regions of the run. Only the 'id', 'centre_ra', 'centre_dec' and 'xtr_radius' columns are required. +------+-------------+--------------+--------------+ | id | centre_ra | centre_dec | xtr_radius | |------+-------------+--------------+--------------| | 2 | 319.652 | 0.0030765 | 6.72488 | | 3 | 319.652 | -6.2989 | 6.7401 | | 1 | 21.8361 | -73.121 | 7.24662 | +------+-------------+--------------+--------------+ required Returns: Type Description DataFrame The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ Source code in vast_pipeline/pipeline/utils.py def group_skyregions ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Logic to group sky regions into overlapping groups. Returns a dataframe containing the sky region id as the index and a column containing a list of the sky region group number it belongs to. Args: df: A dataframe containing all the sky regions of the run. Only the 'id', 'centre_ra', 'centre_dec' and 'xtr_radius' columns are required. +------+-------------+--------------+--------------+ | id | centre_ra | centre_dec | xtr_radius | |------+-------------+--------------+--------------| | 2 | 319.652 | 0.0030765 | 6.72488 | | 3 | 319.652 | -6.2989 | 6.7401 | | 1 | 21.8361 | -73.121 | 7.24662 | +------+-------------+--------------+--------------+ Returns: The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ \"\"\" sr_coords = SkyCoord ( df [ 'centre_ra' ], df [ 'centre_dec' ], unit = ( u . deg , u . deg ) ) df = df . set_index ( 'id' ) results = df . apply ( _get_skyregion_relations , args = ( sr_coords , df . index ), axis = 1 ) skyreg_groups = {} master_done = [] # keep track of all checked ids in master done for skyreg_id , neighbours in results . iteritems (): if skyreg_id not in master_done : local_done = [] # a local done list for the sky region group. # add the current skyreg_id to both master and local done. master_done . append ( skyreg_id ) local_done . append ( skyreg_id ) # Define the new group number based on the existing ones. skyreg_group = len ( skyreg_groups ) + 1 # Add all the ones that we know are neighbours that were obtained # from _get_skyregion_relations. skyreg_groups [ skyreg_group ] = list ( neighbours ) # Now the sky region group is extended out to include all those sky # regions that overlap with the neighbours. # Each neighbour is checked and added to the local done list. # Checked means that for each neighbour, it's own neighbours are # added to the current group if not in already. # When the local done is equal to the skyreg group we know that # we have exhausted all possible neighbours and that results in a # sky region group. while sorted ( local_done ) != sorted ( skyreg_groups [ skyreg_group ]): # Loop over each neighbour for other_skyreg_id in skyreg_groups [ skyreg_group ]: # If we haven't checked this neighbour locally proceed. if other_skyreg_id not in local_done : # Add it to the local checked. local_done . append ( other_skyreg_id ) # Get the neighbours neighbour and add these. new_vals = results . loc [ other_skyreg_id ] for k in new_vals : if k not in skyreg_groups [ skyreg_group ]: skyreg_groups [ skyreg_group ] . append ( k ) # Reached the end of the group so append all to the master # done list for j in skyreg_groups [ skyreg_group ]: master_done . append ( j ) else : # continue if already placed in group continue # flip the dictionary around skyreg_group_ids = {} for i in skyreg_groups : for j in skyreg_groups [ i ]: skyreg_group_ids [ j ] = i skyreg_group_ids = pd . DataFrame . from_dict ( skyreg_group_ids , orient = 'index' ) . rename ( columns = { 0 : 'skyreg_group' }) return skyreg_group_ids groupby_funcs ( df ) \u00b6 Performs calculations on the unique sources to get the lightcurve properties. Works on the grouped by source dataframe. Parameters: Name Type Description Default df DataFrame The current iteration dataframe of the grouped by sources dataframe. required Returns: Type Description Series Pandas series containing the calculated metrics of the source. Source code in vast_pipeline/pipeline/utils.py def groupby_funcs ( df : pd . DataFrame ) -> pd . Series : ''' Performs calculations on the unique sources to get the lightcurve properties. Works on the grouped by source dataframe. Args: df: The current iteration dataframe of the grouped by sources dataframe. Returns: Pandas series containing the calculated metrics of the source. ''' # calculated average ra, dec, fluxes and metrics d = {} d [ 'img_list' ] = df [ 'image' ] . values . tolist () d [ 'n_meas_forced' ] = df [ 'forced' ] . sum () d [ 'n_meas' ] = df [ 'id' ] . count () d [ 'n_meas_sel' ] = d [ 'n_meas' ] - d [ 'n_meas_forced' ] d [ 'n_sibl' ] = df [ 'has_siblings' ] . sum () if d [ 'n_meas_forced' ] > 0 : non_forced_sel = df [ 'forced' ] != True d [ 'wavg_ra' ] = ( df . loc [ non_forced_sel , 'interim_ew' ] . sum () / df . loc [ non_forced_sel , 'weight_ew' ] . sum () ) d [ 'wavg_dec' ] = ( df . loc [ non_forced_sel , 'interim_ns' ] . sum () / df . loc [ non_forced_sel , 'weight_ns' ] . sum () ) d [ 'avg_compactness' ] = df . loc [ non_forced_sel , 'compactness' ] . mean () d [ 'min_snr' ] = df . loc [ non_forced_sel , 'snr' ] . min () d [ 'max_snr' ] = df . loc [ non_forced_sel , 'snr' ] . max () else : d [ 'wavg_ra' ] = df [ 'interim_ew' ] . sum () / df [ 'weight_ew' ] . sum () d [ 'wavg_dec' ] = df [ 'interim_ns' ] . sum () / df [ 'weight_ns' ] . sum () d [ 'avg_compactness' ] = df [ 'compactness' ] . mean () d [ 'min_snr' ] = df [ 'snr' ] . min () d [ 'max_snr' ] = df [ 'snr' ] . max () d [ 'wavg_uncertainty_ew' ] = 1. / np . sqrt ( df [ 'weight_ew' ] . sum ()) d [ 'wavg_uncertainty_ns' ] = 1. / np . sqrt ( df [ 'weight_ns' ] . sum ()) for col in [ 'avg_flux_int' , 'avg_flux_peak' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . mean () for col in [ 'max_flux_peak' , 'max_flux_int' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . max () for col in [ 'min_flux_peak' , 'min_flux_int' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . min () for col in [ 'min_flux_peak_isl_ratio' , 'min_flux_int_isl_ratio' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . min () for col in [ 'flux_int' , 'flux_peak' ]: d [ f ' { col } _sq' ] = ( df [ col ] ** 2 ) . mean () d [ 'v_int' ] = df [ 'flux_int' ] . std () / df [ 'flux_int' ] . mean () d [ 'v_peak' ] = df [ 'flux_peak' ] . std () / df [ 'flux_peak' ] . mean () d [ 'eta_int' ] = get_eta_metric ( d , df ) d [ 'eta_peak' ] = get_eta_metric ( d , df , peak = True ) # remove not used cols for col in [ 'flux_int_sq' , 'flux_peak_sq' ]: d . pop ( col ) # get unique related sources list_uniq_related = list ( set ( chain . from_iterable ( lst for lst in df [ 'related' ] if isinstance ( lst , list ) ) )) d [ 'related_list' ] = list_uniq_related if list_uniq_related else - 1 return pd . Series ( d ) . fillna ( value = { \"v_int\" : 0.0 , \"v_peak\" : 0.0 }) parallel_groupby ( df ) \u00b6 Performs the parallel source dataframe operations to calculate the source metrics using Dask and returns the resulting dataframe. Parameters: Name Type Description Default df DataFrame The sources dataframe produced by the previous pipeline stages. required Returns: Type Description DataFrame The source dataframe with the calculated metric columns. Source code in vast_pipeline/pipeline/utils.py def parallel_groupby ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Performs the parallel source dataframe operations to calculate the source metrics using Dask and returns the resulting dataframe. Args: df: The sources dataframe produced by the previous pipeline stages. Returns: The source dataframe with the calculated metric columns. \"\"\" col_dtype = { 'img_list' : 'O' , 'n_meas_forced' : 'i' , 'n_meas' : 'i' , 'n_meas_sel' : 'i' , 'n_sibl' : 'i' , 'wavg_ra' : 'f' , 'wavg_dec' : 'f' , 'avg_compactness' : 'f' , 'min_snr' : 'f' , 'max_snr' : 'f' , 'wavg_uncertainty_ew' : 'f' , 'wavg_uncertainty_ns' : 'f' , 'avg_flux_int' : 'f' , 'avg_flux_peak' : 'f' , 'max_flux_peak' : 'f' , 'max_flux_int' : 'f' , 'min_flux_peak' : 'f' , 'min_flux_int' : 'f' , 'min_flux_peak_isl_ratio' : 'f' , 'min_flux_int_isl_ratio' : 'f' , 'v_int' : 'f' , 'v_peak' : 'f' , 'eta_int' : 'f' , 'eta_peak' : 'f' , 'related_list' : 'O' } n_cpu = cpu_count () - 1 out = dd . from_pandas ( df , n_cpu ) out = ( out . groupby ( 'source' ) . apply ( groupby_funcs , meta = col_dtype ) . compute ( num_workers = n_cpu , scheduler = 'processes' ) ) out [ 'n_rel' ] = out [ 'related_list' ] . apply ( lambda x : 0 if x == - 1 else len ( x )) return out parallel_groupby_coord ( df ) \u00b6 This function uses Dask to perform the average coordinate and unique image and epoch lists calculation. The result from the Dask compute is returned which is a dataframe containing the results for each source. Parameters: Name Type Description Default df DataFrame The sources dataframe produced by the pipeline. required Returns: Type Description DataFrame The resulting average coordinate values and unique image and epoch lists for each unique source (group). Source code in vast_pipeline/pipeline/utils.py def parallel_groupby_coord ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" This function uses Dask to perform the average coordinate and unique image and epoch lists calculation. The result from the Dask compute is returned which is a dataframe containing the results for each source. Args: df: The sources dataframe produced by the pipeline. Returns: The resulting average coordinate values and unique image and epoch lists for each unique source (group). \"\"\" col_dtype = { 'img_list' : 'O' , 'epoch_list' : 'O' , 'wavg_ra' : 'f' , 'wavg_dec' : 'f' , } n_cpu = cpu_count () - 1 out = dd . from_pandas ( df , n_cpu ) out = ( out . groupby ( 'source' ) . apply ( calc_ave_coord , meta = col_dtype ) . compute ( num_workers = n_cpu , scheduler = 'processes' ) ) return out prep_skysrc_df ( images , perc_error = 0.0 , duplicate_limit = None , ini_df = False ) \u00b6 Initiliase the source dataframe to use in association logic by reading the measurement parquet file and creating columns. When epoch based assoication is used it will also remove duplicate measurements from the list of sources. Parameters: Name Type Description Default images List[vast_pipeline.models.Image] A list holding the Image objects of the images to load measurements for. required perc_error float A percentage flux error to apply to the flux errors of the measurements. Defaults to 0. 0.0 duplicate_limit Optional[astropy.coordinates.angles.Angle] The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used in the 'remove_duplicate_measurements' function (usual ASKAP pixel size). None ini_df bool Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. False Returns: Type Description DataFrame The measurements of the image(s) with some extra values set ready for assoication and duplicates removed if necessary. Source code in vast_pipeline/pipeline/utils.py def prep_skysrc_df ( images : List [ Image ], perc_error : float = 0. , duplicate_limit : Optional [ Angle ] = None , ini_df : bool = False ) -> pd . DataFrame : ''' Initiliase the source dataframe to use in association logic by reading the measurement parquet file and creating columns. When epoch based assoication is used it will also remove duplicate measurements from the list of sources. Args: images: A list holding the Image objects of the images to load measurements for. perc_error: A percentage flux error to apply to the flux errors of the measurements. Defaults to 0. duplicate_limit: The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used in the 'remove_duplicate_measurements' function (usual ASKAP pixel size). ini_df: Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. Returns: The measurements of the image(s) with some extra values set ready for assoication and duplicates removed if necessary. ''' cols = [ 'id' , 'ra' , 'uncertainty_ew' , 'weight_ew' , 'dec' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' ] df = _load_measurements ( images [ 0 ], cols , ini_df = ini_df ) if len ( images ) > 1 : for img in images [ 1 :]: df = df . append ( _load_measurements ( img , cols , df . source . max (), ini_df = ini_df ), ignore_index = True ) df = remove_duplicate_measurements ( df , dup_lim = duplicate_limit , ini_df = ini_df ) df = df . drop ( 'dist_from_centre' , axis = 1 ) if perc_error != 0.0 : logger . info ( 'Correcting flux errors with config error setting...' ) for col in [ 'flux_int' , 'flux_peak' ]: df [ f ' { col } _err' ] = np . hypot ( df [ f ' { col } _err' ] . values , perc_error * df [ col ] . values ) return df reconstruct_associtaion_dfs ( images_df_done , previous_parquet_paths ) \u00b6 This function is used with add image mode and performs the necessary manipulations to reconstruct the sources_df and skyc1_srcs required by association. Parameters: Name Type Description Default images_df_done DataFrame The images_df output from the existing run (from the parquet). required previous_parquet_paths Dict[str, str] Dictionary that contains the paths for the previous run parquet files. Keys are 'images', 'associations', 'sources', 'relations' and 'measurement_pairs'. required Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] The reconstructed sources_df and skyc1_srs dataframes. Source code in vast_pipeline/pipeline/utils.py def reconstruct_associtaion_dfs ( images_df_done : pd . DataFrame , previous_parquet_paths : Dict [ str , str ] ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" This function is used with add image mode and performs the necessary manipulations to reconstruct the sources_df and skyc1_srcs required by association. Args: images_df_done: The images_df output from the existing run (from the parquet). previous_parquet_paths: Dictionary that contains the paths for the previous run parquet files. Keys are 'images', 'associations', 'sources', 'relations' and 'measurement_pairs'. Returns: The reconstructed sources_df and skyc1_srs dataframes. \"\"\" prev_associations = pd . read_parquet ( previous_parquet_paths [ 'associations' ]) # Get the parquet paths from the image objects img_meas_paths = ( images_df_done [ 'image_dj' ] . apply ( lambda x : x . measurements_path ) . to_list () ) # Obtain the pipeline run path in order to fetch forced measurements. run_path = previous_parquet_paths [ 'sources' ] . replace ( 'sources.parquet.bak' , '' ) # Get the forced measurement paths. img_fmeas_paths = [] for i in images_df_done . image_name . values : forced_parquet = os . path . join ( run_path , \"forced_measurements_ {} .parquet\" . format ( i . replace ( \".\" , \"_\" ) ) ) if os . path . isfile ( forced_parquet ): img_fmeas_paths . append ( forced_parquet ) # Create union of paths. img_meas_paths += img_fmeas_paths # Define the columns that are required cols = [ 'id' , 'ra' , 'uncertainty_ew' , 'weight_ew' , 'dec' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image_id' , 'time' , ] # Open all the parquets logger . debug ( \"Opening all measurement parquet files to use in reconstruction...\" ) measurements = pd . concat ( [ pd . read_parquet ( f , columns = cols ) for f in img_meas_paths ] ) # Create mask to drop measurements for epoch mode (epoch based mode). measurements_mask = measurements [ 'id' ] . isin ( prev_associations [ 'meas_id' ]) measurements = measurements . loc [ measurements_mask ] . set_index ( 'id' ) # Set the index on images_df for faster merging. images_df_done [ 'image_id' ] = images_df_done [ 'image_dj' ] . apply ( lambda x : x . id ) . values images_df_done = images_df_done . set_index ( 'image_id' ) # Merge image information to measurements measurements = ( measurements . merge ( images_df_done [[ 'image_name' , 'epoch' ]], left_on = 'image_id' , right_index = True ) . rename ( columns = { 'image_name' : 'image' }) ) # Drop any associations that are not used in this sky region group. associations_mask = prev_associations [ 'meas_id' ] . isin ( measurements . index . values ) prev_associations = prev_associations . loc [ associations_mask ] # Merge measurements into the associations to form the sources_df. sources_df = ( prev_associations . merge ( measurements , left_on = 'meas_id' , right_index = True ) . rename ( columns = { 'source_id' : 'source' , 'time' : 'datetime' , 'meas_id' : 'id' , 'ra' : 'ra_source' , 'dec' : 'dec_source' , 'uncertainty_ew' : 'uncertainty_ew_source' , 'uncertainty_ns' : 'uncertainty_ns_source' , }) ) # Load up the previous unique sources. prev_sources = pd . read_parquet ( previous_parquet_paths [ 'sources' ], columns = [ 'wavg_ra' , 'wavg_dec' , 'wavg_uncertainty_ew' , 'wavg_uncertainty_ns' , ] ) # Merge the wavg ra and dec to the sources_df - this is required to # create the skyc1_srcs below (but MUST be converted back to the source # ra and dec) sources_df = ( sources_df . merge ( prev_sources , left_on = 'source' , right_index = True ) . rename ( columns = { 'wavg_ra' : 'ra' , 'wavg_dec' : 'dec' , 'wavg_uncertainty_ew' : 'uncertainty_ew' , 'wavg_uncertainty_ns' : 'uncertainty_ns' , }) ) # Load the previous relations prev_relations = pd . read_parquet ( previous_parquet_paths [ 'relations' ]) # Form relation lists to merge in. prev_relations = pd . DataFrame ( prev_relations . groupby ( 'from_source_id' )[ 'to_source_id' ] . apply ( lambda x : x . values . tolist ()) ) . rename ( columns = { 'to_source_id' : 'related' }) # Append the relations to only the last instance of each source # First get the ids of the sources relation_ids = sources_df [ sources_df . source . isin ( prev_relations . index . values )] . drop_duplicates ( 'source' , keep = 'last' ) . index . values # Make sure we attach the correct source id source_ids = sources_df . loc [ relation_ids ] . source . values sources_df [ 'related' ] = np . nan relations_to_update = prev_relations . loc [ source_ids ] . to_numpy () . copy () relations_to_update = np . reshape ( relations_to_update , relations_to_update . shape [ 0 ]) sources_df . loc [ relation_ids , 'related' ] = relations_to_update # Reorder so we don't mess up the dask metas. sources_df = sources_df [[ 'id' , 'uncertainty_ew' , 'weight_ew' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image' , 'datetime' , 'source' , 'ra' , 'dec' , 'ra_source' , 'dec_source' , 'd2d' , 'dr' , 'related' , 'epoch' , 'uncertainty_ew_source' , 'uncertainty_ns_source' ]] # Create the unique skyc1_srcs dataframe. skyc1_srcs = ( sources_df [ sources_df [ 'forced' ] == False ] . sort_values ( by = 'id' ) . drop ( 'related' , axis = 1 ) . drop_duplicates ( 'source' ) ) . copy ( deep = True ) # Get relations into the skyc1_srcs (as we only keep the first instance # which does not have the relation information) skyc1_srcs = skyc1_srcs . merge ( prev_relations , how = 'left' , left_on = 'source' , right_index = True ) # Need to break the pointer relationship between the related sources ( # deep=True copy does not truly copy mutable type objects) relation_mask = skyc1_srcs . related . notna () relation_vals = skyc1_srcs . loc [ relation_mask , 'related' ] . to_list () new_relation_vals = [ x . copy () for x in relation_vals ] skyc1_srcs . loc [ relation_mask , 'related' ] = new_relation_vals # Reorder so we don't mess up the dask metas. skyc1_srcs = skyc1_srcs [[ 'id' , 'ra' , 'uncertainty_ew' , 'weight_ew' , 'dec' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image' , 'datetime' , 'source' , 'ra_source' , 'dec_source' , 'd2d' , 'dr' , 'related' , 'epoch' ]] . reset_index ( drop = True ) # Finally move the source ra and dec back to the sources_df ra and dec # columns sources_df [ 'ra' ] = sources_df [ 'ra_source' ] sources_df [ 'dec' ] = sources_df [ 'dec_source' ] sources_df [ 'uncertainty_ew' ] = sources_df [ 'uncertainty_ew_source' ] sources_df [ 'uncertainty_ns' ] = sources_df [ 'uncertainty_ns_source' ] # Drop not needed columns for the sources_df. sources_df = sources_df . drop ([ 'uncertainty_ew_source' , 'uncertainty_ns_source' ], axis = 1 ) . reset_index ( drop = True ) return sources_df , skyc1_srcs remove_duplicate_measurements ( sources_df , dup_lim = None , ini_df = False ) \u00b6 Remove perceived duplicate sources from a dataframe of loaded measurements. Duplicates are determined by their separation and whether this distances is within the 'dup_lim'. Parameters: Name Type Description Default sources_df DataFrame The loaded measurements from two or more images. required dup_lim Optional[astropy.coordinates.angles.Angle] The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used (usual ASKAP pixel size). None ini_df bool Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. False Returns: Type Description DataFrame The input sources_df with duplicate sources removed. Source code in vast_pipeline/pipeline/utils.py def remove_duplicate_measurements ( sources_df : pd . DataFrame , dup_lim : Optional [ Angle ] = None , ini_df : bool = False ) -> pd . DataFrame : \"\"\" Remove perceived duplicate sources from a dataframe of loaded measurements. Duplicates are determined by their separation and whether this distances is within the 'dup_lim'. Args: sources_df: The loaded measurements from two or more images. dup_lim: The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used (usual ASKAP pixel size). ini_df: Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. Returns: The input sources_df with duplicate sources removed. \"\"\" logger . debug ( 'Cleaning duplicate sources from epoch...' ) if dup_lim is None : dup_lim = Angle ( 2.5 * u . arcsec ) logger . debug ( 'Using duplicate crossmatch radius of %.2f arcsec.' , dup_lim . arcsec ) min_source = sources_df [ 'source' ] . min () # sort by the distance from the image centre so we know # that the first source is always the one to keep sources_df = sources_df . sort_values ( by = 'dist_from_centre' ) sources_sc = SkyCoord ( sources_df [ 'ra' ], sources_df [ 'dec' ], unit = ( u . deg , u . deg ) ) # perform search around sky to get all self matches idxc , idxcatalog , d2d_around , _ = sources_sc . search_around_sky ( sources_sc , dup_lim ) # create df from results results = pd . DataFrame ( data = { 'source_id' : idxc , 'match_id' : idxcatalog , 'source_image' : sources_df . iloc [ idxc ][ 'image' ] . tolist (), 'match_image' : sources_df . iloc [ idxcatalog ][ 'image' ] . tolist () } ) # Drop those that are matched from the same image matching_image_mask = ( results [ 'source_image' ] != results [ 'match_image' ] ) results = ( results . loc [ matching_image_mask ] . drop ([ 'source_image' , 'match_image' ], axis = 1 ) ) # create a pair column defining each pair ith index results [ 'pair' ] = results . apply ( tuple , 1 ) . apply ( sorted ) . apply ( tuple ) # Drop the duplicate pairs (pairs are sorted so this works) results = results . drop_duplicates ( 'pair' ) # No longer need pair results = results . drop ( 'pair' , axis = 1 ) # Drop all self matches and we are left with those to drop # in the match id column. to_drop = results . loc [ results [ 'source_id' ] != results [ 'match_id' ], 'match_id' ] # Get the index values from the ith values to_drop_indexes = sources_df . iloc [ to_drop ] . index . values logger . debug ( \"Dropping %i duplicate measurements.\" , to_drop_indexes . shape [ 0 ] ) # Drop them from sources sources_df = sources_df . drop ( to_drop_indexes ) . sort_values ( by = 'ra' ) # reset the source_df index sources_df = sources_df . reset_index ( drop = True ) # Reset the source number if ini_df : sources_df [ 'source' ] = sources_df . index + 1 del results return sources_df","title":"utils.py"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.add_new_many_to_one_relations","text":"This handles the relation information being created from the many_to_one function in advanced association. It is a lot simpler than the one_to_many case as it purely just adds the new relations to the relation column, taking into account if it is already a list of relations or not (i.e. no previous relations). Parameters: Name Type Description Default row Series The relation information Series from the assoication dataframe. Only the columns ['related_skyc1', 'new_relations'] are required. required Returns: Type Description List[int] The new related field for the source in question, containing the appended ids. Source code in vast_pipeline/pipeline/utils.py def add_new_many_to_one_relations ( row : pd . Series ) -> List [ int ]: \"\"\" This handles the relation information being created from the many_to_one function in advanced association. It is a lot simpler than the one_to_many case as it purely just adds the new relations to the relation column, taking into account if it is already a list of relations or not (i.e. no previous relations). Args: row: The relation information Series from the assoication dataframe. Only the columns ['related_skyc1', 'new_relations'] are required. Returns: The new related field for the source in question, containing the appended ids. \"\"\" out = row [ 'new_relations' ] . copy () if isinstance ( row [ 'related_skyc1' ], list ): out += row [ 'related_skyc1' ] . copy () return out","title":"add_new_many_to_one_relations()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.add_new_one_to_many_relations","text":"This handles the relation information being created from the one_to_many functions in association. Parameters: Name Type Description Default row Series The relation information Series from the assoication dataframe. Only the columns ['related_skyc1', 'source_skyc1'] are required for advanced, these are instead called ['related', 'source'] for basic. required advanced bool Whether advanced association is being used which changes the names of the columns involved. False source_ids Optional[pandas.core.frame.DataFrame] A dataframe that contains the other ids to append to related for each original source. +----------------+--------+ | source_skyc1 | 0 | |----------------+--------| | 122 | [5542] | | 254 | [5543] | | 262 | [5544] | | 405 | [5545] | | 656 | [5546] | +----------------+--------+ None Returns: Type Description List[int] The new related field for the source in question, containing the appended ids. Source code in vast_pipeline/pipeline/utils.py def add_new_one_to_many_relations ( row : pd . Series , advanced : bool = False , source_ids : Optional [ pd . DataFrame ] = None ) -> List [ int ]: \"\"\" This handles the relation information being created from the one_to_many functions in association. Args: row: The relation information Series from the assoication dataframe. Only the columns ['related_skyc1', 'source_skyc1'] are required for advanced, these are instead called ['related', 'source'] for basic. advanced: Whether advanced association is being used which changes the names of the columns involved. source_ids: A dataframe that contains the other ids to append to related for each original source. +----------------+--------+ | source_skyc1 | 0 | |----------------+--------| | 122 | [5542] | | 254 | [5543] | | 262 | [5544] | | 405 | [5545] | | 656 | [5546] | +----------------+--------+ Returns: The new related field for the source in question, containing the appended ids. \"\"\" if source_ids is None : source_ids = pd . DataFrame () related_col = 'related_skyc1' if advanced else 'related' source_col = 'source_skyc1' if advanced else 'source' # this is the not_original case where the original source id is appended. if source_ids . empty : if isinstance ( row [ related_col ], list ): out = row [ related_col ] out . append ( row [ source_col ]) else : out = [ row [ source_col ],] else : # the original case to append all the new ids. source_ids = source_ids . loc [ row [ source_col ]] . iloc [ 0 ] if isinstance ( row [ related_col ], list ): out = row [ related_col ] + source_ids else : out = source_ids return out","title":"add_new_one_to_many_relations()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.backup_parquets","text":"Backups up all the existing parquet files in a pipeline run directory. Backups are named with a '.bak' suffix in the pipeline run directory. Parameters: Name Type Description Default p_run_path str The path of the pipeline run where the parquets are stored. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def backup_parquets ( p_run_path : str ) -> None : \"\"\" Backups up all the existing parquet files in a pipeline run directory. Backups are named with a '.bak' suffix in the pipeline run directory. Args: p_run_path: The path of the pipeline run where the parquets are stored. Returns: None \"\"\" parquets = ( glob . glob ( os . path . join ( p_run_path , \"*.parquet\" )) # TODO Remove arrow when vaex support is dropped. + glob . glob ( os . path . join ( p_run_path , \"*.arrow\" ))) for i in parquets : backup_name = i + '.bak' if os . path . isfile ( backup_name ): logger . debug ( f 'Removing old backup file: { backup_name } .' ) os . remove ( backup_name ) shutil . copyfile ( i , backup_name )","title":"backup_parquets()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.calc_ave_coord","text":"Calculates the average coordinate of the grouped by sources dataframe for each unique group, along with defining the image and epoch list for each unique source (group). Parameters: Name Type Description Default grp DataFrame The current group dataframe (unique source) of the grouped by dataframe being acted upon. required Returns: Type Description Series A pandas series containing the average coordinate along with the image and epoch lists. Source code in vast_pipeline/pipeline/utils.py def calc_ave_coord ( grp : pd . DataFrame ) -> pd . Series : \"\"\" Calculates the average coordinate of the grouped by sources dataframe for each unique group, along with defining the image and epoch list for each unique source (group). Args: grp: The current group dataframe (unique source) of the grouped by dataframe being acted upon. Returns: A pandas series containing the average coordinate along with the image and epoch lists. \"\"\" d = {} grp = grp . sort_values ( by = 'datetime' ) d [ 'img_list' ] = grp [ 'image' ] . values . tolist () d [ 'epoch_list' ] = grp [ 'epoch' ] . values . tolist () d [ 'wavg_ra' ] = grp [ 'interim_ew' ] . sum () / grp [ 'weight_ew' ] . sum () d [ 'wavg_dec' ] = grp [ 'interim_ns' ] . sum () / grp [ 'weight_ns' ] . sum () return pd . Series ( d )","title":"calc_ave_coord()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.calculate_m_metric","text":"Calculate the m variability metric which is the modulation index between two fluxes. This is proportional to the fractional variability. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Parameters: Name Type Description Default flux_a float flux value \"A\". required flux_b float flux value \"B\". required Returns: Type Description float float: the m metric for flux values \"A\" and \"B\". Source code in vast_pipeline/pipeline/utils.py def calculate_m_metric ( flux_a : float , flux_b : float ) -> float : \"\"\"Calculate the m variability metric which is the modulation index between two fluxes. This is proportional to the fractional variability. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Args: flux_a (float): flux value \"A\". flux_b (float): flux value \"B\". Returns: float: the m metric for flux values \"A\" and \"B\". \"\"\" return 2 * (( flux_a - flux_b ) / ( flux_a + flux_b ))","title":"calculate_m_metric()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.calculate_measurement_pair_metrics","text":"Generate a DataFrame of measurement pairs and their 2-epoch variability metrics from a DataFrame of measurements. For more information on the variability metrics, see Section 5 of Mooley et al. (2016), DOI: 10.3847/0004-637X/818/2/105. Parameters: Name Type Description Default df DataFrame Input measurements. Must contain columns: id, source, flux_int, flux_int_err, flux_peak, flux_peak_err, has_siblings. required Returns: Type Description DataFrame pd.DataFrame: Measurement pairs and 2-epoch metrics. Will contain columns: source - the source ID id_a, id_b - the measurement IDs flux_int_a, flux_int_b - measurement integrated fluxes in mJy flux_int_err_a, flux_int_err_b - measurement integrated flux errors in mJy flux_peak_a, flux_peak_b - measurement peak fluxes in mJy/beam flux_peak_err_a, flux_peak_err_b - measurement peak flux errors in mJy/beam vs_peak, vs_int - variability t-statistic m_peak, m_int - variability modulation index Source code in vast_pipeline/pipeline/utils.py def calculate_measurement_pair_metrics ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Generate a DataFrame of measurement pairs and their 2-epoch variability metrics from a DataFrame of measurements. For more information on the variability metrics, see Section 5 of Mooley et al. (2016), DOI: 10.3847/0004-637X/818/2/105. Args: df (pd.DataFrame): Input measurements. Must contain columns: id, source, flux_int, flux_int_err, flux_peak, flux_peak_err, has_siblings. Returns: pd.DataFrame: Measurement pairs and 2-epoch metrics. Will contain columns: source - the source ID id_a, id_b - the measurement IDs flux_int_a, flux_int_b - measurement integrated fluxes in mJy flux_int_err_a, flux_int_err_b - measurement integrated flux errors in mJy flux_peak_a, flux_peak_b - measurement peak fluxes in mJy/beam flux_peak_err_a, flux_peak_err_b - measurement peak flux errors in mJy/beam vs_peak, vs_int - variability t-statistic m_peak, m_int - variability modulation index \"\"\" n_cpu = cpu_count () - 1 \"\"\"Create a DataFrame containing all measurement ID combinations per source. Resultant DataFrame will have a MultiIndex([\"source\", RangeIndex]) where \"source\" is the source ID and RangeIndex is an unnamed temporary ID for each measurement pair, unique only together with source. DataFrame will have columns [0, 1], each containing a measurement ID. e.g. 0 1 source 1 0 1 9284 1 1 17597 2 1 26984 3 9284 17597 4 9284 26984 ... ... ... 11105 2 11845 19961 11124 0 3573 12929 1 3573 21994 2 12929 21994 11128 0 6216 23534 \"\"\" measurement_combinations = ( dd . from_pandas ( df , n_cpu ) . groupby ( \"source\" )[ \"id\" ] . apply ( lambda x : pd . DataFrame ( list ( combinations ( x , 2 )) ), meta = { 0 : \"i\" , 1 : \"i\" },) . compute ( num_workers = n_cpu , scheduler = \"processes\" ) ) \"\"\"Drop the RangeIndex from the MultiIndex as it isn't required and rename the columns. Example resultant DataFrame: source id_a id_b 0 1 1 9284 1 1 1 17597 2 1 1 26984 3 1 9284 17597 4 1 9284 26984 ... ... ... ... 33640 11105 11845 19961 33641 11124 3573 12929 33642 11124 3573 21994 33643 11124 12929 21994 33644 11128 6216 23534 Where source is the source ID, id_a and id_b are measurement IDs. \"\"\" measurement_combinations = measurement_combinations . reset_index ( level = 1 , drop = True ) . rename ( columns = { 0 : \"id_a\" , 1 : \"id_b\" }) . astype ( int ) . reset_index () # Dask has a tendency to swap which order the measurement pairs are # defined in, even if the dataframe is pre-sorted. We want the pairs to be # in date order (a < b) so the code below corrects any that are not. measurement_combinations = measurement_combinations . join ( df [[ 'source' , 'id' , 'datetime' ]] . set_index ([ 'source' , 'id' ]), on = [ 'source' , 'id_a' ], ) measurement_combinations = measurement_combinations . join ( df [[ 'source' , 'id' , 'datetime' ]] . set_index ([ 'source' , 'id' ]), on = [ 'source' , 'id_b' ], lsuffix = '_a' , rsuffix = '_b' ) to_correct_mask = ( measurement_combinations [ 'datetime_a' ] > measurement_combinations [ 'datetime_b' ] ) if np . any ( to_correct_mask ): logger . debug ( 'Correcting measurement pairs order' ) ( measurement_combinations . loc [ to_correct_mask , 'id_a' ], measurement_combinations . loc [ to_correct_mask , 'id_b' ] ) = np . array ([ measurement_combinations . loc [ to_correct_mask , 'id_b' ] . values , measurement_combinations . loc [ to_correct_mask , 'id_a' ] . values ]) measurement_combinations = measurement_combinations . drop ( [ 'datetime_a' , 'datetime_b' ], axis = 1 ) # add the measurement fluxes and errors association_fluxes = df . set_index ([ \"source\" , \"id\" ])[ [ \"flux_int\" , \"flux_int_err\" , \"flux_peak\" , \"flux_peak_err\" , \"image\" ] ] . rename ( columns = { \"image\" : \"image_name\" }) measurement_combinations = measurement_combinations . join ( association_fluxes , on = [ \"source\" , \"id_a\" ], ) . join ( association_fluxes , on = [ \"source\" , \"id_b\" ], lsuffix = \"_a\" , rsuffix = \"_b\" , ) # calculate 2-epoch metrics measurement_combinations [ \"vs_peak\" ] = calculate_vs_metric ( measurement_combinations . flux_peak_a , measurement_combinations . flux_peak_b , measurement_combinations . flux_peak_err_a , measurement_combinations . flux_peak_err_b , ) measurement_combinations [ \"vs_int\" ] = calculate_vs_metric ( measurement_combinations . flux_int_a , measurement_combinations . flux_int_b , measurement_combinations . flux_int_err_a , measurement_combinations . flux_int_err_b , ) measurement_combinations [ \"m_peak\" ] = calculate_m_metric ( measurement_combinations . flux_peak_a , measurement_combinations . flux_peak_b , ) measurement_combinations [ \"m_int\" ] = calculate_m_metric ( measurement_combinations . flux_int_a , measurement_combinations . flux_int_b , ) return measurement_combinations","title":"calculate_measurement_pair_metrics()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.calculate_vs_metric","text":"Calculate the Vs variability metric which is the t-statistic that the provided fluxes are variable. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Parameters: Name Type Description Default flux_a float flux value \"A\". required flux_b float flux value \"B\". required flux_err_a float error of flux_a . required flux_err_b float error of flux_b . required Returns: Type Description float float: the Vs metric for flux values \"A\" and \"B\". Source code in vast_pipeline/pipeline/utils.py def calculate_vs_metric ( flux_a : float , flux_b : float , flux_err_a : float , flux_err_b : float ) -> float : \"\"\"Calculate the Vs variability metric which is the t-statistic that the provided fluxes are variable. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Args: flux_a (float): flux value \"A\". flux_b (float): flux value \"B\". flux_err_a (float): error of `flux_a`. flux_err_b (float): error of `flux_b`. Returns: float: the Vs metric for flux values \"A\" and \"B\". \"\"\" return ( flux_a - flux_b ) / np . hypot ( flux_err_a , flux_err_b )","title":"calculate_vs_metric()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.check_primary_image","text":"Checks whether the primary image of the ideal source dataframe is in the image list for the source. Parameters: Name Type Description Default row Series Input dataframe row, with columns ['primary'] and ['img_list']. required Returns: Type Description bool True if primary in image list else False. Source code in vast_pipeline/pipeline/utils.py def check_primary_image ( row : pd . Series ) -> bool : \"\"\" Checks whether the primary image of the ideal source dataframe is in the image list for the source. Args: row: Input dataframe row, with columns ['primary'] and ['img_list']. Returns: True if primary in image list else False. \"\"\" return row [ 'primary' ] in row [ 'img_list' ]","title":"check_primary_image()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.create_measurement_pairs_arrow_file","text":"Creates a measurement_pairs.arrow file using the parquet outputs of a pipeline run. Vaex is used to do the exporting to arrow to ensure compatibility with Vaex. Parameters: Name Type Description Default p_run Run Pipeline model instance. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def create_measurement_pairs_arrow_file ( p_run : Run ) -> None : \"\"\" Creates a measurement_pairs.arrow file using the parquet outputs of a pipeline run. Vaex is used to do the exporting to arrow to ensure compatibility with Vaex. Args: p_run: Pipeline model instance. Returns: None \"\"\" logger . info ( 'Creating measurement_pairs.arrow for run %s .' , p_run . name ) measurement_pairs_df = pd . read_parquet ( os . path . join ( p_run . path , 'measurement_pairs.parquet' ) ) logger . debug ( 'Optimising dataframe.' ) measurement_pairs_df = optimize_ints ( optimize_floats ( measurement_pairs_df )) # use vaex to export to arrow logger . debug ( \"Loading to vaex.\" ) measurement_pairs_df = vaex . from_pandas ( measurement_pairs_df ) logger . debug ( \"Exporting to arrow.\" ) outname = os . path . join ( p_run . path , 'measurement_pairs.arrow' ) measurement_pairs_df . export_arrow ( outname )","title":"create_measurement_pairs_arrow_file()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.create_measurements_arrow_file","text":"Creates a measurements.arrow file using the parquet outputs of a pipeline run. Vaex is used to do the exporting to arrow to ensure compatibility with Vaex. Parameters: Name Type Description Default p_run Run Pipeline model instance. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def create_measurements_arrow_file ( p_run : Run ) -> None : \"\"\" Creates a measurements.arrow file using the parquet outputs of a pipeline run. Vaex is used to do the exporting to arrow to ensure compatibility with Vaex. Args: p_run: Pipeline model instance. Returns: None \"\"\" logger . info ( 'Creating measurements.arrow for run %s .' , p_run . name ) associations = pd . read_parquet ( os . path . join ( p_run . path , 'associations.parquet' ) ) images = pd . read_parquet ( os . path . join ( p_run . path , 'images.parquet' ) ) m_files = images [ 'measurements_path' ] . tolist () m_files += glob . glob ( os . path . join ( p_run . path , 'forced*.parquet' )) logger . debug ( 'Loading %i files...' , len ( m_files )) measurements = dd . read_parquet ( m_files , engine = 'pyarrow' ) . compute () measurements = measurements . loc [ measurements [ 'id' ] . isin ( associations [ 'meas_id' ] . values ) ] measurements = ( associations . loc [:, [ 'meas_id' , 'source_id' ]] . set_index ( 'meas_id' ) . merge ( measurements , left_index = True , right_on = 'id' ) . rename ( columns = { 'source_id' : 'source' }) ) logger . debug ( 'Optimising dataframes.' ) measurements = optimize_ints ( optimize_floats ( measurements )) # use vaex to export to arrow logger . debug ( \"Loading to vaex.\" ) measurements = vaex . from_pandas ( measurements ) logger . debug ( \"Exporting to arrow.\" ) outname = os . path . join ( p_run . path , 'measurements.arrow' ) measurements . export_arrow ( outname )","title":"create_measurements_arrow_file()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.cross_join","text":"A convenience function to merge two dataframes. Parameters: Name Type Description Default left DataFrame The base pandas DataFrame to merge. required right DataFrame The pandas DataFrame to merge to the left. required Returns: Type Description DataFrame The resultant merged DataFrame. Source code in vast_pipeline/pipeline/utils.py def cross_join ( left : pd . DataFrame , right : pd . DataFrame ) -> pd . DataFrame : \"\"\" A convenience function to merge two dataframes. Args: left: The base pandas DataFrame to merge. right: The pandas DataFrame to merge to the left. Returns: The resultant merged DataFrame. \"\"\" return ( left . assign ( key = 1 ) . merge ( right . assign ( key = 1 ), on = 'key' ) . drop ( 'key' , axis = 1 ) )","title":"cross_join()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_create_img","text":"Function to fetch or create the Image and Sky Region objects for the images in the pipeline run. Parameters: Name Type Description Default p_run Run The pipeline run Django ORM object. required band_id int The integer database id value of the frequency band of the image. required image Image The image Django ORM object. required Returns: Type Description Tuple[vast_pipeline.models.Image, vast_pipeline.models.SkyRegion, bool] The resulting image django ORM object, the sky region Django ORM object and a bool value denoting if the image already existed in the database. Source code in vast_pipeline/pipeline/utils.py def get_create_img ( p_run : Run , band_id : int , image : Image ) -> Tuple [ Image , SkyRegion , bool ]: \"\"\" Function to fetch or create the Image and Sky Region objects for the images in the pipeline run. Args: p_run: The pipeline run Django ORM object. band_id: The integer database id value of the frequency band of the image. image: The image Django ORM object. Returns: The resulting image django ORM object, the sky region Django ORM object and a bool value denoting if the image already existed in the database. \"\"\" img = Image . objects . filter ( name__exact = image . name ) if img . exists (): img = img . get () # Add background path if not originally provided if image . background_path and not img . background_path : img . background_path = image . background_path img . save () skyreg = get_create_skyreg ( p_run , img ) # check and add the many to many if not existent if not Image . objects . filter ( id = img . id , run__id = p_run . id ) . exists (): img . run . add ( p_run ) return ( img , skyreg , True ) # at this stage, measurement parquet file is not created but # assume location img_folder_name = image . name . replace ( '.' , '_' ) measurements_path = os . path . join ( settings . PIPELINE_WORKING_DIR , 'images' , img_folder_name , 'measurements.parquet' ) img = Image ( band_id = band_id , measurements_path = measurements_path ) # set the attributes and save the image, # by selecting only valid (not hidden) attributes # FYI attributs and/or method starting with _ are hidden # and with __ can't be modified/called for fld in img . _meta . get_fields (): if getattr ( fld , 'attname' , None ) and ( getattr ( image , fld . attname , None ) is not None ): setattr ( img , fld . attname , getattr ( image , fld . attname )) # get create the sky region and associate with image skyreg = get_create_skyreg ( p_run , img ) img . skyreg = skyreg img . rms_median , img . rms_min , img . rms_max = get_rms_noise_image_values ( img . noise_path ) img . save () img . run . add ( p_run ) return ( img , skyreg , False )","title":"get_create_img()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_create_img_band","text":"Return the existing Band row for the given FitsImage. An image is considered to belong to a band if its frequency is within some tolerance of the band's frequency. Returns a Band row or None if no matching band. Parameters: Name Type Description Default image Image The image Django ORM object. required Returns: Type Description Band The band Django ORM object. Source code in vast_pipeline/pipeline/utils.py def get_create_img_band ( image : Image ) -> Band : ''' Return the existing Band row for the given FitsImage. An image is considered to belong to a band if its frequency is within some tolerance of the band's frequency. Returns a Band row or None if no matching band. Args: image: The image Django ORM object. Returns: The band Django ORM object. ''' # For now we match bands using the central frequency. # This assumes that every band has a unique frequency, # which is true for the data we've used so far. freq = int ( image . freq_eff * 1.e-6 ) freq_band = int ( image . freq_bw * 1.e-6 ) # TODO: refine the band query for band in Band . objects . all (): diff = abs ( freq - band . frequency ) / float ( band . frequency ) if diff < 0.02 : return band # no band has been found so create it band = Band ( name = str ( freq ), frequency = freq , bandwidth = freq_band ) logger . info ( 'Adding new frequency band: %s ' , band ) band . save () return band","title":"get_create_img_band()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_create_p_run","text":"Get or create a pipeline run in db, return the run django object and a flag True/False if has been created or already exists. Parameters: Name Type Description Default name str The name of the pipeline run. required path str The system path to the pipeline run folder which contains the configuration file and where outputs will be saved. required description str An optional description of the pipeline run. None user User The Django user that launched the pipeline run. None Returns: Type Description Tuple[vast_pipeline.models.Run, bool] The pipeline run object and a boolean object representing whether the pipeline run already existed ('True') or not ('False'). Source code in vast_pipeline/pipeline/utils.py def get_create_p_run ( name : str , path : str , description : str = None , user : User = None ) -> Tuple [ Run , bool ]: ''' Get or create a pipeline run in db, return the run django object and a flag True/False if has been created or already exists. Args: name: The name of the pipeline run. path: The system path to the pipeline run folder which contains the configuration file and where outputs will be saved. description: An optional description of the pipeline run. user: The Django user that launched the pipeline run. Returns: The pipeline run object and a boolean object representing whether the pipeline run already existed ('True') or not ('False'). ''' p_run = Run . objects . filter ( name__exact = name ) if p_run : return p_run . get (), True description = \"\" if description is None else description p_run = Run ( name = name , description = description , path = path ) if user : p_run . user = user p_run . save () return p_run , False","title":"get_create_p_run()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_create_skyreg","text":"This creates a Sky Region object in Django ORM given the related image object. Parameters: Name Type Description Default p_run Run The pipeline run Django ORM object. required image Image The image Django ORM object. required Returns: Type Description SkyRegion The sky region Django ORM object. Source code in vast_pipeline/pipeline/utils.py def get_create_skyreg ( p_run : Run , image : Image ) -> SkyRegion : ''' This creates a Sky Region object in Django ORM given the related image object. Args: p_run: The pipeline run Django ORM object. image: The image Django ORM object. Returns: The sky region Django ORM object. ''' # In the calculations below, it is assumed the image has square # pixels (this pipeline has been designed for ASKAP images, so it # should always be square). It will likely give wrong results if not skyr = SkyRegion . objects . filter ( centre_ra = image . ra , centre_dec = image . dec , xtr_radius = image . fov_bmin ) if skyr : skyr = skyr . get () logger . info ( 'Found sky region %s ' , skyr ) if p_run not in skyr . run . all (): logger . info ( 'Adding %s to sky region %s ' , p_run , skyr ) skyr . run . add ( p_run ) return skyr x , y , z = eq_to_cart ( image . ra , image . dec ) skyr = SkyRegion ( centre_ra = image . ra , centre_dec = image . dec , width_ra = image . physical_bmin , width_dec = image . physical_bmaj , xtr_radius = image . fov_bmin , x = x , y = y , z = z , ) skyr . save () logger . info ( 'Created sky region %s ' , skyr ) skyr . run . add ( p_run ) logger . info ( 'Adding %s to sky region %s ' , p_run , skyr ) return skyr","title":"get_create_skyreg()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_eta_metric","text":"Calculates the eta variability metric of a source. Works on the grouped by dataframe using the fluxes of the assoicated measurements. Parameters: Name Type Description Default row Dict[str, float] Dictionary containg statistics for the current source. required df DataFrame The grouped by sources dataframe of the measurements containing all the flux and flux error information, required peak bool Whether to use peak_flux for the calculation. If False then the integrated flux is used. False Returns: Type Description float The calculated eta value. Source code in vast_pipeline/pipeline/utils.py def get_eta_metric ( row : Dict [ str , float ], df : pd . DataFrame , peak : bool = False ) -> float : ''' Calculates the eta variability metric of a source. Works on the grouped by dataframe using the fluxes of the assoicated measurements. Args: row: Dictionary containg statistics for the current source. df: The grouped by sources dataframe of the measurements containing all the flux and flux error information, peak: Whether to use peak_flux for the calculation. If False then the integrated flux is used. Returns: The calculated eta value. ''' if row [ 'n_meas' ] == 1 : return 0. suffix = 'peak' if peak else 'int' weights = 1. / df [ f 'flux_ { suffix } _err' ] . values ** 2 fluxes = df [ f 'flux_ { suffix } ' ] . values eta = ( row [ 'n_meas' ] / ( row [ 'n_meas' ] - 1 )) * ( ( weights * fluxes ** 2 ) . mean () - ( ( weights * fluxes ) . mean () ** 2 / weights . mean () ) ) return eta","title":"get_eta_metric()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_image_list_diff","text":"Calculate the difference between the ideal coverage image list of a source and the actual observed image list. Also checks whether an epoch does in fact contain a detection but is not in the expected 'ideal' image for that epoch. Parameters: Name Type Description Default row Series The row from the sources dataframe that is being iterated over. required Returns: Type Description List[str] A list of the images missing from the observed image list. Will be returned as '-1' integer value if there are no missing images. Source code in vast_pipeline/pipeline/utils.py def get_image_list_diff ( row : pd . Series ) -> List [ str ]: \"\"\" Calculate the difference between the ideal coverage image list of a source and the actual observed image list. Also checks whether an epoch does in fact contain a detection but is not in the expected 'ideal' image for that epoch. Args: row: The row from the sources dataframe that is being iterated over. Returns: A list of the images missing from the observed image list. Will be returned as '-1' integer value if there are no missing images. \"\"\" out = list ( filter ( lambda arg : arg not in row [ 'img_list' ], row [ 'skyreg_img_list' ]) ) # set empty list to -1 if not out : return - 1 # Check that an epoch has not already been seen (just not in the 'ideal' # image) out_epochs = [ row [ 'skyreg_epoch' ][ pair [ 0 ]] for pair in enumerate ( row [ 'skyreg_img_list' ] ) if pair [ 1 ] in out ] out = [ out [ pair [ 0 ]] for pair in enumerate ( out_epochs ) if pair [ 1 ] not in row [ 'epoch_list' ] ] if not out : out = - 1 return out","title":"get_image_list_diff()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_names_and_epochs","text":"Convenience function to group together the image names, epochs and datetimes into one list object which is then returned as a pandas series. This is necessary for easier processing in the ideal coverage analysis. Parameters: Name Type Description Default grp DataFrame A group from the grouped by sources DataFrame. required Returns: Type Description Series Pandas series containing the list object that contains the lists of the image names, epochs and datetimes. Source code in vast_pipeline/pipeline/utils.py def get_names_and_epochs ( grp : pd . DataFrame ) -> pd . Series : \"\"\" Convenience function to group together the image names, epochs and datetimes into one list object which is then returned as a pandas series. This is necessary for easier processing in the ideal coverage analysis. Args: grp: A group from the grouped by sources DataFrame. Returns: Pandas series containing the list object that contains the lists of the image names, epochs and datetimes. \"\"\" d = {} d [ 'skyreg_img_epoch_list' ] = [[[ x ,], y , z ] for x , y , z in zip ( grp [ 'name' ] . values . tolist (), grp [ 'epoch' ] . values . tolist (), grp [ 'datetime' ] . values . tolist () )] return pd . Series ( d )","title":"get_names_and_epochs()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_parallel_assoc_image_df","text":"Merge the sky region groups with the images and skyreg_ids. Parameters: Name Type Description Default images List[vast_pipeline.models.Image] A list of the Image objects. required skyregion_groups DataFrame The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ required Returns: Type Description DataFrame Dataframe containing the merged images and skyreg_id and skyreg_group. +----+-------------------------------+-------------+----------------+ | | image | skyreg_id | skyreg_group | |----+-------------------------------+-------------+----------------| | 0 | VAST_2118+00A.EPOCH01.I.fits | 2 | 1 | | 1 | VAST_2118-06A.EPOCH01.I.fits | 3 | 1 | | 2 | VAST_0127-73A.EPOCH01.I.fits | 1 | 2 | | 3 | VAST_2118-06A.EPOCH03x.I.fits | 3 | 1 | | 4 | VAST_2118-06A.EPOCH02.I.fits | 3 | 1 | | 5 | VAST_2118-06A.EPOCH05x.I.fits | 3 | 1 | | 6 | VAST_2118-06A.EPOCH06x.I.fits | 3 | 1 | | 7 | VAST_0127-73A.EPOCH08.I.fits | 1 | 2 | +----+-------------------------------+-------------+----------------+ Source code in vast_pipeline/pipeline/utils.py def get_parallel_assoc_image_df ( images : List [ Image ], skyregion_groups : pd . DataFrame ) -> pd . DataFrame : \"\"\" Merge the sky region groups with the images and skyreg_ids. Args: images: A list of the Image objects. skyregion_groups: The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ Returns: Dataframe containing the merged images and skyreg_id and skyreg_group. +----+-------------------------------+-------------+----------------+ | | image | skyreg_id | skyreg_group | |----+-------------------------------+-------------+----------------| | 0 | VAST_2118+00A.EPOCH01.I.fits | 2 | 1 | | 1 | VAST_2118-06A.EPOCH01.I.fits | 3 | 1 | | 2 | VAST_0127-73A.EPOCH01.I.fits | 1 | 2 | | 3 | VAST_2118-06A.EPOCH03x.I.fits | 3 | 1 | | 4 | VAST_2118-06A.EPOCH02.I.fits | 3 | 1 | | 5 | VAST_2118-06A.EPOCH05x.I.fits | 3 | 1 | | 6 | VAST_2118-06A.EPOCH06x.I.fits | 3 | 1 | | 7 | VAST_0127-73A.EPOCH08.I.fits | 1 | 2 | +----+-------------------------------+-------------+----------------+ \"\"\" skyreg_ids = [ i . skyreg_id for i in images ] images_df = pd . DataFrame ({ 'image_dj' : images , 'skyreg_id' : skyreg_ids , }) images_df = images_df . merge ( skyregion_groups , how = 'left' , left_on = 'skyreg_id' , right_index = True ) images_df [ 'image_name' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . name ) images_df [ 'image_datetime' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . datetime ) return images_df","title":"get_parallel_assoc_image_df()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_rms_noise_image_values","text":"Open the RMS noise FITS file and compute the median, max and min rms values to be added to the image model and then used in the calculations. Parameters: Name Type Description Default rms_path str The system path to the RMS FITS image. required Returns: Type Description Tuple[float, float, float] The median, minimum and maximum values of the RMS image. Exceptions: Type Description IOError Raised when the RMS FITS file cannot be found. Source code in vast_pipeline/pipeline/utils.py def get_rms_noise_image_values ( rms_path : str ) -> Tuple [ float , float , float ]: ''' Open the RMS noise FITS file and compute the median, max and min rms values to be added to the image model and then used in the calculations. Args: rms_path: The system path to the RMS FITS image. Returns: The median, minimum and maximum values of the RMS image. Raises: IOError: Raised when the RMS FITS file cannot be found. ''' logger . debug ( 'Extracting Image RMS values from Noise file...' ) med_val = min_val = max_val = 0. try : with fits . open ( rms_path ) as f : data = f [ 0 ] . data data = data [ np . logical_not ( np . isnan ( data ))] data = data [ data != 0 ] med_val = np . median ( data ) * 1e+3 min_val = np . min ( data ) * 1e+3 max_val = np . max ( data ) * 1e+3 del data except Exception : raise IOError ( f 'Could not read this RMS FITS file: { rms_path } ' ) return med_val , min_val , max_val","title":"get_rms_noise_image_values()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_src_skyregion_merged_df","text":"Analyses the current sources_df to determine what the 'ideal coverage' for each source should be. In other words, what images is the source missing in when it should have been seen. Parameters: Name Type Description Default sources_df DataFrame The output of the assoication step containing the measurements assoicated into sources. required images_df DataFrame Contains the images of the pipeline run. I.e. all image objects for the run loaded into a dataframe. required skyreg_df DataFrame Contains the sky regions of the pipeline run. I.e. all sky region objects for the run loaded into a dataframe. required Returns: Type Description DataFrame DataFrame containing missing image information. Output format: +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | | 1290 | ['VAST_0127-73A.EPOCH01.I.fits'] | 20.8455 | -76.8269 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+------------------------------+ img_diff | primary | ----------------------------------+------------------------------+ ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ----------------------------------+------------------------------+ ------------------------------+--------------+ detection | in_primary | ------------------------------+--------------| VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | ------------------------------+--------------+ Source code in vast_pipeline/pipeline/utils.py def get_src_skyregion_merged_df ( sources_df : pd . DataFrame , images_df : pd . DataFrame , skyreg_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Analyses the current sources_df to determine what the 'ideal coverage' for each source should be. In other words, what images is the source missing in when it should have been seen. Args: sources_df: The output of the assoication step containing the measurements assoicated into sources. images_df: Contains the images of the pipeline run. I.e. all image objects for the run loaded into a dataframe. skyreg_df: Contains the sky regions of the pipeline run. I.e. all sky region objects for the run loaded into a dataframe. Returns: DataFrame containing missing image information. Output format: +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | | 1290 | ['VAST_0127-73A.EPOCH01.I.fits'] | 20.8455 | -76.8269 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+------------------------------+ img_diff | primary | ----------------------------------+------------------------------+ ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ----------------------------------+------------------------------+ ------------------------------+--------------+ detection | in_primary | ------------------------------+--------------| VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | ------------------------------+--------------+ \"\"\" logger . info ( \"Creating ideal source coverage df...\" ) merged_timer = StopWatch () skyreg_df = skyreg_df . drop ( [ 'x' , 'y' , 'z' , 'width_ra' , 'width_dec' ], axis = 1 ) images_df [ 'name' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . name ) images_df [ 'datetime' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . datetime ) skyreg_df = skyreg_df . join ( pd . DataFrame ( images_df . groupby ( 'skyreg_id' ) . apply ( get_names_and_epochs ) ), on = 'id' ) sources_df = sources_df . sort_values ( by = 'datetime' ) # calculate some metrics on sources # compute only some necessary metrics in the groupby timer = StopWatch () srcs_df = parallel_groupby_coord ( sources_df ) logger . debug ( 'Groupby-apply time: %.2f seconds' , timer . reset ()) del sources_df # create dataframe with all skyregions and sources combinations src_skyrg_df = cross_join ( srcs_df . drop ([ 'epoch_list' , 'img_list' ], axis = 1 ) . reset_index (), skyreg_df . drop ( 'skyreg_img_epoch_list' , axis = 1 ) ) skyreg_df = skyreg_df . drop ( [ 'centre_ra' , 'centre_dec' , 'xtr_radius' ], axis = 1 ) . set_index ( 'id' ) src_skyrg_df [ 'sep' ] = np . rad2deg ( on_sky_sep ( np . deg2rad ( src_skyrg_df [ 'wavg_ra' ] . values ), np . deg2rad ( src_skyrg_df [ 'centre_ra' ] . values ), np . deg2rad ( src_skyrg_df [ 'wavg_dec' ] . values ), np . deg2rad ( src_skyrg_df [ 'centre_dec' ] . values ), ) ) # select rows where separation is less than sky region radius # drop not more useful columns and groupby source id # compute list of images src_skyrg_df = ( src_skyrg_df . loc [ src_skyrg_df . sep < src_skyrg_df . xtr_radius , [ 'source' , 'id' , 'sep' ] ] . merge ( skyreg_df , left_on = 'id' , right_index = True ) . drop ( 'id' , axis = 1 ) . explode ( 'skyreg_img_epoch_list' ) ) del skyreg_df src_skyrg_df [ [ 'skyreg_img_list' , 'skyreg_epoch' , 'skyreg_datetime' ] ] = pd . DataFrame ( src_skyrg_df [ 'skyreg_img_epoch_list' ] . tolist (), index = src_skyrg_df . index ) src_skyrg_df = src_skyrg_df . drop ( 'skyreg_img_epoch_list' , axis = 1 ) src_skyrg_df = ( src_skyrg_df . sort_values ( [ 'source' , 'sep' ] ) . drop_duplicates ([ 'source' , 'skyreg_epoch' ]) . sort_values ( by = 'skyreg_datetime' ) . drop ( [ 'sep' , 'skyreg_datetime' ], axis = 1 ) ) # annoyingly epoch needs to be not a list to drop duplicates # but then we need to sum the epochs into a list src_skyrg_df [ 'skyreg_epoch' ] = src_skyrg_df [ 'skyreg_epoch' ] . apply ( lambda x : [ x ,] ) src_skyrg_df = ( src_skyrg_df . groupby ( 'source' ) . agg ( 'sum' ) # sum because we need to preserve order ) # merge into main df and compare the images srcs_df = srcs_df . merge ( src_skyrg_df , left_index = True , right_index = True ) del src_skyrg_df srcs_df [ 'img_diff' ] = srcs_df [ [ 'img_list' , 'skyreg_img_list' , 'epoch_list' , 'skyreg_epoch' ] ] . apply ( get_image_list_diff , axis = 1 ) srcs_df = srcs_df . loc [ srcs_df [ 'img_diff' ] != - 1 ] srcs_df = srcs_df . drop ( [ 'epoch_list' , 'skyreg_epoch' ], axis = 1 ) srcs_df [ 'primary' ] = srcs_df [ 'skyreg_img_list' ] . apply ( lambda x : x [ 0 ]) srcs_df [ 'detection' ] = srcs_df [ 'img_list' ] . apply ( lambda x : x [ 0 ]) srcs_df [ 'in_primary' ] = srcs_df [ [ 'primary' , 'img_list' ] ] . apply ( check_primary_image , axis = 1 ) srcs_df = srcs_df . drop ([ 'img_list' , 'skyreg_img_list' , 'primary' ], axis = 1 ) logger . info ( 'Ideal source coverage time: %.2f seconds' , merged_timer . reset () ) return srcs_df","title":"get_src_skyregion_merged_df()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.group_skyregions","text":"Logic to group sky regions into overlapping groups. Returns a dataframe containing the sky region id as the index and a column containing a list of the sky region group number it belongs to. Parameters: Name Type Description Default df DataFrame A dataframe containing all the sky regions of the run. Only the 'id', 'centre_ra', 'centre_dec' and 'xtr_radius' columns are required. +------+-------------+--------------+--------------+ | id | centre_ra | centre_dec | xtr_radius | |------+-------------+--------------+--------------| | 2 | 319.652 | 0.0030765 | 6.72488 | | 3 | 319.652 | -6.2989 | 6.7401 | | 1 | 21.8361 | -73.121 | 7.24662 | +------+-------------+--------------+--------------+ required Returns: Type Description DataFrame The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ Source code in vast_pipeline/pipeline/utils.py def group_skyregions ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Logic to group sky regions into overlapping groups. Returns a dataframe containing the sky region id as the index and a column containing a list of the sky region group number it belongs to. Args: df: A dataframe containing all the sky regions of the run. Only the 'id', 'centre_ra', 'centre_dec' and 'xtr_radius' columns are required. +------+-------------+--------------+--------------+ | id | centre_ra | centre_dec | xtr_radius | |------+-------------+--------------+--------------| | 2 | 319.652 | 0.0030765 | 6.72488 | | 3 | 319.652 | -6.2989 | 6.7401 | | 1 | 21.8361 | -73.121 | 7.24662 | +------+-------------+--------------+--------------+ Returns: The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ \"\"\" sr_coords = SkyCoord ( df [ 'centre_ra' ], df [ 'centre_dec' ], unit = ( u . deg , u . deg ) ) df = df . set_index ( 'id' ) results = df . apply ( _get_skyregion_relations , args = ( sr_coords , df . index ), axis = 1 ) skyreg_groups = {} master_done = [] # keep track of all checked ids in master done for skyreg_id , neighbours in results . iteritems (): if skyreg_id not in master_done : local_done = [] # a local done list for the sky region group. # add the current skyreg_id to both master and local done. master_done . append ( skyreg_id ) local_done . append ( skyreg_id ) # Define the new group number based on the existing ones. skyreg_group = len ( skyreg_groups ) + 1 # Add all the ones that we know are neighbours that were obtained # from _get_skyregion_relations. skyreg_groups [ skyreg_group ] = list ( neighbours ) # Now the sky region group is extended out to include all those sky # regions that overlap with the neighbours. # Each neighbour is checked and added to the local done list. # Checked means that for each neighbour, it's own neighbours are # added to the current group if not in already. # When the local done is equal to the skyreg group we know that # we have exhausted all possible neighbours and that results in a # sky region group. while sorted ( local_done ) != sorted ( skyreg_groups [ skyreg_group ]): # Loop over each neighbour for other_skyreg_id in skyreg_groups [ skyreg_group ]: # If we haven't checked this neighbour locally proceed. if other_skyreg_id not in local_done : # Add it to the local checked. local_done . append ( other_skyreg_id ) # Get the neighbours neighbour and add these. new_vals = results . loc [ other_skyreg_id ] for k in new_vals : if k not in skyreg_groups [ skyreg_group ]: skyreg_groups [ skyreg_group ] . append ( k ) # Reached the end of the group so append all to the master # done list for j in skyreg_groups [ skyreg_group ]: master_done . append ( j ) else : # continue if already placed in group continue # flip the dictionary around skyreg_group_ids = {} for i in skyreg_groups : for j in skyreg_groups [ i ]: skyreg_group_ids [ j ] = i skyreg_group_ids = pd . DataFrame . from_dict ( skyreg_group_ids , orient = 'index' ) . rename ( columns = { 0 : 'skyreg_group' }) return skyreg_group_ids","title":"group_skyregions()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.groupby_funcs","text":"Performs calculations on the unique sources to get the lightcurve properties. Works on the grouped by source dataframe. Parameters: Name Type Description Default df DataFrame The current iteration dataframe of the grouped by sources dataframe. required Returns: Type Description Series Pandas series containing the calculated metrics of the source. Source code in vast_pipeline/pipeline/utils.py def groupby_funcs ( df : pd . DataFrame ) -> pd . Series : ''' Performs calculations on the unique sources to get the lightcurve properties. Works on the grouped by source dataframe. Args: df: The current iteration dataframe of the grouped by sources dataframe. Returns: Pandas series containing the calculated metrics of the source. ''' # calculated average ra, dec, fluxes and metrics d = {} d [ 'img_list' ] = df [ 'image' ] . values . tolist () d [ 'n_meas_forced' ] = df [ 'forced' ] . sum () d [ 'n_meas' ] = df [ 'id' ] . count () d [ 'n_meas_sel' ] = d [ 'n_meas' ] - d [ 'n_meas_forced' ] d [ 'n_sibl' ] = df [ 'has_siblings' ] . sum () if d [ 'n_meas_forced' ] > 0 : non_forced_sel = df [ 'forced' ] != True d [ 'wavg_ra' ] = ( df . loc [ non_forced_sel , 'interim_ew' ] . sum () / df . loc [ non_forced_sel , 'weight_ew' ] . sum () ) d [ 'wavg_dec' ] = ( df . loc [ non_forced_sel , 'interim_ns' ] . sum () / df . loc [ non_forced_sel , 'weight_ns' ] . sum () ) d [ 'avg_compactness' ] = df . loc [ non_forced_sel , 'compactness' ] . mean () d [ 'min_snr' ] = df . loc [ non_forced_sel , 'snr' ] . min () d [ 'max_snr' ] = df . loc [ non_forced_sel , 'snr' ] . max () else : d [ 'wavg_ra' ] = df [ 'interim_ew' ] . sum () / df [ 'weight_ew' ] . sum () d [ 'wavg_dec' ] = df [ 'interim_ns' ] . sum () / df [ 'weight_ns' ] . sum () d [ 'avg_compactness' ] = df [ 'compactness' ] . mean () d [ 'min_snr' ] = df [ 'snr' ] . min () d [ 'max_snr' ] = df [ 'snr' ] . max () d [ 'wavg_uncertainty_ew' ] = 1. / np . sqrt ( df [ 'weight_ew' ] . sum ()) d [ 'wavg_uncertainty_ns' ] = 1. / np . sqrt ( df [ 'weight_ns' ] . sum ()) for col in [ 'avg_flux_int' , 'avg_flux_peak' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . mean () for col in [ 'max_flux_peak' , 'max_flux_int' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . max () for col in [ 'min_flux_peak' , 'min_flux_int' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . min () for col in [ 'min_flux_peak_isl_ratio' , 'min_flux_int_isl_ratio' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . min () for col in [ 'flux_int' , 'flux_peak' ]: d [ f ' { col } _sq' ] = ( df [ col ] ** 2 ) . mean () d [ 'v_int' ] = df [ 'flux_int' ] . std () / df [ 'flux_int' ] . mean () d [ 'v_peak' ] = df [ 'flux_peak' ] . std () / df [ 'flux_peak' ] . mean () d [ 'eta_int' ] = get_eta_metric ( d , df ) d [ 'eta_peak' ] = get_eta_metric ( d , df , peak = True ) # remove not used cols for col in [ 'flux_int_sq' , 'flux_peak_sq' ]: d . pop ( col ) # get unique related sources list_uniq_related = list ( set ( chain . from_iterable ( lst for lst in df [ 'related' ] if isinstance ( lst , list ) ) )) d [ 'related_list' ] = list_uniq_related if list_uniq_related else - 1 return pd . Series ( d ) . fillna ( value = { \"v_int\" : 0.0 , \"v_peak\" : 0.0 })","title":"groupby_funcs()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.parallel_groupby","text":"Performs the parallel source dataframe operations to calculate the source metrics using Dask and returns the resulting dataframe. Parameters: Name Type Description Default df DataFrame The sources dataframe produced by the previous pipeline stages. required Returns: Type Description DataFrame The source dataframe with the calculated metric columns. Source code in vast_pipeline/pipeline/utils.py def parallel_groupby ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Performs the parallel source dataframe operations to calculate the source metrics using Dask and returns the resulting dataframe. Args: df: The sources dataframe produced by the previous pipeline stages. Returns: The source dataframe with the calculated metric columns. \"\"\" col_dtype = { 'img_list' : 'O' , 'n_meas_forced' : 'i' , 'n_meas' : 'i' , 'n_meas_sel' : 'i' , 'n_sibl' : 'i' , 'wavg_ra' : 'f' , 'wavg_dec' : 'f' , 'avg_compactness' : 'f' , 'min_snr' : 'f' , 'max_snr' : 'f' , 'wavg_uncertainty_ew' : 'f' , 'wavg_uncertainty_ns' : 'f' , 'avg_flux_int' : 'f' , 'avg_flux_peak' : 'f' , 'max_flux_peak' : 'f' , 'max_flux_int' : 'f' , 'min_flux_peak' : 'f' , 'min_flux_int' : 'f' , 'min_flux_peak_isl_ratio' : 'f' , 'min_flux_int_isl_ratio' : 'f' , 'v_int' : 'f' , 'v_peak' : 'f' , 'eta_int' : 'f' , 'eta_peak' : 'f' , 'related_list' : 'O' } n_cpu = cpu_count () - 1 out = dd . from_pandas ( df , n_cpu ) out = ( out . groupby ( 'source' ) . apply ( groupby_funcs , meta = col_dtype ) . compute ( num_workers = n_cpu , scheduler = 'processes' ) ) out [ 'n_rel' ] = out [ 'related_list' ] . apply ( lambda x : 0 if x == - 1 else len ( x )) return out","title":"parallel_groupby()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.parallel_groupby_coord","text":"This function uses Dask to perform the average coordinate and unique image and epoch lists calculation. The result from the Dask compute is returned which is a dataframe containing the results for each source. Parameters: Name Type Description Default df DataFrame The sources dataframe produced by the pipeline. required Returns: Type Description DataFrame The resulting average coordinate values and unique image and epoch lists for each unique source (group). Source code in vast_pipeline/pipeline/utils.py def parallel_groupby_coord ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" This function uses Dask to perform the average coordinate and unique image and epoch lists calculation. The result from the Dask compute is returned which is a dataframe containing the results for each source. Args: df: The sources dataframe produced by the pipeline. Returns: The resulting average coordinate values and unique image and epoch lists for each unique source (group). \"\"\" col_dtype = { 'img_list' : 'O' , 'epoch_list' : 'O' , 'wavg_ra' : 'f' , 'wavg_dec' : 'f' , } n_cpu = cpu_count () - 1 out = dd . from_pandas ( df , n_cpu ) out = ( out . groupby ( 'source' ) . apply ( calc_ave_coord , meta = col_dtype ) . compute ( num_workers = n_cpu , scheduler = 'processes' ) ) return out","title":"parallel_groupby_coord()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.prep_skysrc_df","text":"Initiliase the source dataframe to use in association logic by reading the measurement parquet file and creating columns. When epoch based assoication is used it will also remove duplicate measurements from the list of sources. Parameters: Name Type Description Default images List[vast_pipeline.models.Image] A list holding the Image objects of the images to load measurements for. required perc_error float A percentage flux error to apply to the flux errors of the measurements. Defaults to 0. 0.0 duplicate_limit Optional[astropy.coordinates.angles.Angle] The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used in the 'remove_duplicate_measurements' function (usual ASKAP pixel size). None ini_df bool Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. False Returns: Type Description DataFrame The measurements of the image(s) with some extra values set ready for assoication and duplicates removed if necessary. Source code in vast_pipeline/pipeline/utils.py def prep_skysrc_df ( images : List [ Image ], perc_error : float = 0. , duplicate_limit : Optional [ Angle ] = None , ini_df : bool = False ) -> pd . DataFrame : ''' Initiliase the source dataframe to use in association logic by reading the measurement parquet file and creating columns. When epoch based assoication is used it will also remove duplicate measurements from the list of sources. Args: images: A list holding the Image objects of the images to load measurements for. perc_error: A percentage flux error to apply to the flux errors of the measurements. Defaults to 0. duplicate_limit: The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used in the 'remove_duplicate_measurements' function (usual ASKAP pixel size). ini_df: Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. Returns: The measurements of the image(s) with some extra values set ready for assoication and duplicates removed if necessary. ''' cols = [ 'id' , 'ra' , 'uncertainty_ew' , 'weight_ew' , 'dec' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' ] df = _load_measurements ( images [ 0 ], cols , ini_df = ini_df ) if len ( images ) > 1 : for img in images [ 1 :]: df = df . append ( _load_measurements ( img , cols , df . source . max (), ini_df = ini_df ), ignore_index = True ) df = remove_duplicate_measurements ( df , dup_lim = duplicate_limit , ini_df = ini_df ) df = df . drop ( 'dist_from_centre' , axis = 1 ) if perc_error != 0.0 : logger . info ( 'Correcting flux errors with config error setting...' ) for col in [ 'flux_int' , 'flux_peak' ]: df [ f ' { col } _err' ] = np . hypot ( df [ f ' { col } _err' ] . values , perc_error * df [ col ] . values ) return df","title":"prep_skysrc_df()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.reconstruct_associtaion_dfs","text":"This function is used with add image mode and performs the necessary manipulations to reconstruct the sources_df and skyc1_srcs required by association. Parameters: Name Type Description Default images_df_done DataFrame The images_df output from the existing run (from the parquet). required previous_parquet_paths Dict[str, str] Dictionary that contains the paths for the previous run parquet files. Keys are 'images', 'associations', 'sources', 'relations' and 'measurement_pairs'. required Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] The reconstructed sources_df and skyc1_srs dataframes. Source code in vast_pipeline/pipeline/utils.py def reconstruct_associtaion_dfs ( images_df_done : pd . DataFrame , previous_parquet_paths : Dict [ str , str ] ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" This function is used with add image mode and performs the necessary manipulations to reconstruct the sources_df and skyc1_srcs required by association. Args: images_df_done: The images_df output from the existing run (from the parquet). previous_parquet_paths: Dictionary that contains the paths for the previous run parquet files. Keys are 'images', 'associations', 'sources', 'relations' and 'measurement_pairs'. Returns: The reconstructed sources_df and skyc1_srs dataframes. \"\"\" prev_associations = pd . read_parquet ( previous_parquet_paths [ 'associations' ]) # Get the parquet paths from the image objects img_meas_paths = ( images_df_done [ 'image_dj' ] . apply ( lambda x : x . measurements_path ) . to_list () ) # Obtain the pipeline run path in order to fetch forced measurements. run_path = previous_parquet_paths [ 'sources' ] . replace ( 'sources.parquet.bak' , '' ) # Get the forced measurement paths. img_fmeas_paths = [] for i in images_df_done . image_name . values : forced_parquet = os . path . join ( run_path , \"forced_measurements_ {} .parquet\" . format ( i . replace ( \".\" , \"_\" ) ) ) if os . path . isfile ( forced_parquet ): img_fmeas_paths . append ( forced_parquet ) # Create union of paths. img_meas_paths += img_fmeas_paths # Define the columns that are required cols = [ 'id' , 'ra' , 'uncertainty_ew' , 'weight_ew' , 'dec' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image_id' , 'time' , ] # Open all the parquets logger . debug ( \"Opening all measurement parquet files to use in reconstruction...\" ) measurements = pd . concat ( [ pd . read_parquet ( f , columns = cols ) for f in img_meas_paths ] ) # Create mask to drop measurements for epoch mode (epoch based mode). measurements_mask = measurements [ 'id' ] . isin ( prev_associations [ 'meas_id' ]) measurements = measurements . loc [ measurements_mask ] . set_index ( 'id' ) # Set the index on images_df for faster merging. images_df_done [ 'image_id' ] = images_df_done [ 'image_dj' ] . apply ( lambda x : x . id ) . values images_df_done = images_df_done . set_index ( 'image_id' ) # Merge image information to measurements measurements = ( measurements . merge ( images_df_done [[ 'image_name' , 'epoch' ]], left_on = 'image_id' , right_index = True ) . rename ( columns = { 'image_name' : 'image' }) ) # Drop any associations that are not used in this sky region group. associations_mask = prev_associations [ 'meas_id' ] . isin ( measurements . index . values ) prev_associations = prev_associations . loc [ associations_mask ] # Merge measurements into the associations to form the sources_df. sources_df = ( prev_associations . merge ( measurements , left_on = 'meas_id' , right_index = True ) . rename ( columns = { 'source_id' : 'source' , 'time' : 'datetime' , 'meas_id' : 'id' , 'ra' : 'ra_source' , 'dec' : 'dec_source' , 'uncertainty_ew' : 'uncertainty_ew_source' , 'uncertainty_ns' : 'uncertainty_ns_source' , }) ) # Load up the previous unique sources. prev_sources = pd . read_parquet ( previous_parquet_paths [ 'sources' ], columns = [ 'wavg_ra' , 'wavg_dec' , 'wavg_uncertainty_ew' , 'wavg_uncertainty_ns' , ] ) # Merge the wavg ra and dec to the sources_df - this is required to # create the skyc1_srcs below (but MUST be converted back to the source # ra and dec) sources_df = ( sources_df . merge ( prev_sources , left_on = 'source' , right_index = True ) . rename ( columns = { 'wavg_ra' : 'ra' , 'wavg_dec' : 'dec' , 'wavg_uncertainty_ew' : 'uncertainty_ew' , 'wavg_uncertainty_ns' : 'uncertainty_ns' , }) ) # Load the previous relations prev_relations = pd . read_parquet ( previous_parquet_paths [ 'relations' ]) # Form relation lists to merge in. prev_relations = pd . DataFrame ( prev_relations . groupby ( 'from_source_id' )[ 'to_source_id' ] . apply ( lambda x : x . values . tolist ()) ) . rename ( columns = { 'to_source_id' : 'related' }) # Append the relations to only the last instance of each source # First get the ids of the sources relation_ids = sources_df [ sources_df . source . isin ( prev_relations . index . values )] . drop_duplicates ( 'source' , keep = 'last' ) . index . values # Make sure we attach the correct source id source_ids = sources_df . loc [ relation_ids ] . source . values sources_df [ 'related' ] = np . nan relations_to_update = prev_relations . loc [ source_ids ] . to_numpy () . copy () relations_to_update = np . reshape ( relations_to_update , relations_to_update . shape [ 0 ]) sources_df . loc [ relation_ids , 'related' ] = relations_to_update # Reorder so we don't mess up the dask metas. sources_df = sources_df [[ 'id' , 'uncertainty_ew' , 'weight_ew' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image' , 'datetime' , 'source' , 'ra' , 'dec' , 'ra_source' , 'dec_source' , 'd2d' , 'dr' , 'related' , 'epoch' , 'uncertainty_ew_source' , 'uncertainty_ns_source' ]] # Create the unique skyc1_srcs dataframe. skyc1_srcs = ( sources_df [ sources_df [ 'forced' ] == False ] . sort_values ( by = 'id' ) . drop ( 'related' , axis = 1 ) . drop_duplicates ( 'source' ) ) . copy ( deep = True ) # Get relations into the skyc1_srcs (as we only keep the first instance # which does not have the relation information) skyc1_srcs = skyc1_srcs . merge ( prev_relations , how = 'left' , left_on = 'source' , right_index = True ) # Need to break the pointer relationship between the related sources ( # deep=True copy does not truly copy mutable type objects) relation_mask = skyc1_srcs . related . notna () relation_vals = skyc1_srcs . loc [ relation_mask , 'related' ] . to_list () new_relation_vals = [ x . copy () for x in relation_vals ] skyc1_srcs . loc [ relation_mask , 'related' ] = new_relation_vals # Reorder so we don't mess up the dask metas. skyc1_srcs = skyc1_srcs [[ 'id' , 'ra' , 'uncertainty_ew' , 'weight_ew' , 'dec' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image' , 'datetime' , 'source' , 'ra_source' , 'dec_source' , 'd2d' , 'dr' , 'related' , 'epoch' ]] . reset_index ( drop = True ) # Finally move the source ra and dec back to the sources_df ra and dec # columns sources_df [ 'ra' ] = sources_df [ 'ra_source' ] sources_df [ 'dec' ] = sources_df [ 'dec_source' ] sources_df [ 'uncertainty_ew' ] = sources_df [ 'uncertainty_ew_source' ] sources_df [ 'uncertainty_ns' ] = sources_df [ 'uncertainty_ns_source' ] # Drop not needed columns for the sources_df. sources_df = sources_df . drop ([ 'uncertainty_ew_source' , 'uncertainty_ns_source' ], axis = 1 ) . reset_index ( drop = True ) return sources_df , skyc1_srcs","title":"reconstruct_associtaion_dfs()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.remove_duplicate_measurements","text":"Remove perceived duplicate sources from a dataframe of loaded measurements. Duplicates are determined by their separation and whether this distances is within the 'dup_lim'. Parameters: Name Type Description Default sources_df DataFrame The loaded measurements from two or more images. required dup_lim Optional[astropy.coordinates.angles.Angle] The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used (usual ASKAP pixel size). None ini_df bool Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. False Returns: Type Description DataFrame The input sources_df with duplicate sources removed. Source code in vast_pipeline/pipeline/utils.py def remove_duplicate_measurements ( sources_df : pd . DataFrame , dup_lim : Optional [ Angle ] = None , ini_df : bool = False ) -> pd . DataFrame : \"\"\" Remove perceived duplicate sources from a dataframe of loaded measurements. Duplicates are determined by their separation and whether this distances is within the 'dup_lim'. Args: sources_df: The loaded measurements from two or more images. dup_lim: The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used (usual ASKAP pixel size). ini_df: Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. Returns: The input sources_df with duplicate sources removed. \"\"\" logger . debug ( 'Cleaning duplicate sources from epoch...' ) if dup_lim is None : dup_lim = Angle ( 2.5 * u . arcsec ) logger . debug ( 'Using duplicate crossmatch radius of %.2f arcsec.' , dup_lim . arcsec ) min_source = sources_df [ 'source' ] . min () # sort by the distance from the image centre so we know # that the first source is always the one to keep sources_df = sources_df . sort_values ( by = 'dist_from_centre' ) sources_sc = SkyCoord ( sources_df [ 'ra' ], sources_df [ 'dec' ], unit = ( u . deg , u . deg ) ) # perform search around sky to get all self matches idxc , idxcatalog , d2d_around , _ = sources_sc . search_around_sky ( sources_sc , dup_lim ) # create df from results results = pd . DataFrame ( data = { 'source_id' : idxc , 'match_id' : idxcatalog , 'source_image' : sources_df . iloc [ idxc ][ 'image' ] . tolist (), 'match_image' : sources_df . iloc [ idxcatalog ][ 'image' ] . tolist () } ) # Drop those that are matched from the same image matching_image_mask = ( results [ 'source_image' ] != results [ 'match_image' ] ) results = ( results . loc [ matching_image_mask ] . drop ([ 'source_image' , 'match_image' ], axis = 1 ) ) # create a pair column defining each pair ith index results [ 'pair' ] = results . apply ( tuple , 1 ) . apply ( sorted ) . apply ( tuple ) # Drop the duplicate pairs (pairs are sorted so this works) results = results . drop_duplicates ( 'pair' ) # No longer need pair results = results . drop ( 'pair' , axis = 1 ) # Drop all self matches and we are left with those to drop # in the match id column. to_drop = results . loc [ results [ 'source_id' ] != results [ 'match_id' ], 'match_id' ] # Get the index values from the ith values to_drop_indexes = sources_df . iloc [ to_drop ] . index . values logger . debug ( \"Dropping %i duplicate measurements.\" , to_drop_indexes . shape [ 0 ] ) # Drop them from sources sources_df = sources_df . drop ( to_drop_indexes ) . sort_values ( by = 'ra' ) # reset the source_df index sources_df = sources_df . reset_index ( drop = True ) # Reset the source number if ini_df : sources_df [ 'source' ] = sources_df . index + 1 del results return sources_df","title":"remove_duplicate_measurements()"},{"location":"reference/survey/catalogue/","text":"Original VAST file: PATH/vast-pipeline/vast/import_survey_catalogue.py get_survey ( filename , survey_name , survey_id , tr = { 'prefix' : 'IM' , 'ra' : 'ra' , 'err_ra' : 'er_ra' , 'dec' : 'dec' , 'err_dec' : 'err_dec' , 'pos_err_ang_scale' : 1 , 'peak_flux' : 'peak_flux' , 'err_peak_flux' : 'err_peak_flux' , 'total_flux' : 'int_flux' , 'err_total_flux' : 'err_int_flux' , 'flux_scale' : 1000.0 , 'bmaj' : 'a' , 'bmin' : 'b' , 'ang_scale' : 3600 , 'pa' : 'pa' , 'freq' : 999 }) \u00b6 Read a table and extract the columns of interest. Parameters: Name Type Description Default filename str The catalogue file name. required survey_name str The name of the survey. required survey_id The ID of the survey. required tr Dict[str, str] The dictionary containing the translators of the source finders. {'prefix': 'IM', 'ra': 'ra', 'err_ra': 'er_ra', 'dec': 'dec', 'err_dec': 'err_dec', 'pos_err_ang_scale': 1, 'peak_flux': 'peak_flux', 'err_peak_flux': 'err_peak_flux', 'total_flux': 'int_flux', 'err_total_flux': 'err_int_flux', 'flux_scale': 1000.0, 'bmaj': 'a', 'bmin': 'b', 'ang_scale': 3600, 'pa': 'pa', 'freq': 999} Returns: Type Description DataFrame A dataframe of the translated source catalogue. Source code in vast_pipeline/survey/catalogue.py def get_survey ( filename : str , survey_name : str , survey_id , tr : Dict [ str , str ] = translators [ 'DEFAULT' ] ) -> pd . DataFrame : \"\"\" Read a table and extract the columns of interest. Args: filename: The catalogue file name. survey_name: The name of the survey. survey_id: The ID of the survey. tr: The dictionary containing the translators of the source finders. Returns: A dataframe of the translated source catalogue. \"\"\" #TODO: Unsure of what the survey ID is. # moving to Pandas (in future to Dask) watch = StopWatch () ext = os . path . splitext ( filename )[ - 1 ] if ext . upper () in [ 'VOT' , 'VO' , 'XML' ]: tab = ( parse_single_table ( filename ) . to_table ( use_names_over_ids = True ) ) else : tab = Table . read ( filename ) logger . info ( 'total time to load catalogue: %f s' , watch . reset ()) # grab the columns names and dtypes col_dtype_map = { col : tab [ col ] . dtype . name for col in tab . colnames } tab = tab . to_pandas () # remove not used columns for col in list ( col_dtype_map . keys ()): if col not in tr . values (): col_dtype_map . pop ( col ) tab = tab . loc [:, col_dtype_map . keys ()] # fix data types for bytes and convert float32 to float64 for col in tab . columns : if 'bytes' in col_dtype_map [ col ]: tab [ col ] = tab . loc [:, col ] . astype ( str ) . str . strip ( \"'b\" ) if tab . loc [:, col ] . dtype == np . float32 : tab [ col ] = tab . loc [:, col ] . astype ( np . float64 ) # calculate extra columns tab [ 'survey_id' ] = survey_id tab [ 'name' ] = f \" { tr [ 'prefix' ] } _\" + tab . index . map ( lambda x : f ' { x : 06 } ' ) . values tab = tab . rename ( columns = { tr [ 'ra' ]: 'ra' , tr [ 'dec' ]: 'dec' , tr [ 'pa' ]: 'pa' }, copy = False ) tab [ 'err_ra' ] = tab . loc [:, tr [ 'err_ra' ]] * tr [ 'pos_err_ang_scale' ] tab [ 'err_dec' ] = tab . loc [:, tr [ 'err_dec' ]] * tr [ 'pos_err_ang_scale' ] tab [ 'peak_flux' ] = tab . loc [:, tr [ 'peak_flux' ]] * tr [ 'flux_scale' ] tab [ 'err_peak_flux' ] = tab . loc [:, tr [ 'err_peak_flux' ]] * tr [ 'flux_scale' ] tab [ 'total_flux' ] = tab . loc [:, tr [ 'total_flux' ]] * tr [ 'flux_scale' ] tab [ 'err_total_flux' ] = tab . loc [:, tr [ 'err_total_flux' ]] * tr [ 'flux_scale' ] tab [ 'bmaj' ] = tab . loc [:, tr [ 'bmaj' ]] * tr [ 'ang_scale' ] tab [ 'bmin' ] = tab . loc [:, tr [ 'bmin' ]] * tr [ 'ang_scale' ] tab [ 'image_name' ] = 'None' if 'alpha' in tr : tab = tab . rename ( columns = { tr [ 'alpha' ]: 'alpha' }, copy = False ) else : tab [ 'alpha' ] = 0 # drop not used columns cols = [ 'survey_id' , 'name' , 'ra' , 'err_ra' , 'dec' , 'err_dec' , 'peak_flux' , 'err_peak_flux' , 'total_flux' , 'err_total_flux' , 'bmaj' , 'bmin' , 'pa' , 'alpha' , 'image_name' ] tab = tab . loc [:, cols ] # fix NaNs in numeric types for col in cols : if tab [ col ] . isnull () . any (): if tab [ col ] . dtype == float : tab [ col ] = tab [ col ] . fillna ( 0. ) elif tab [ col ] . dtype == int : tab [ col ] = tab [ col ] . fillna ( 0 ) logger . info ( 'total time to process catalogue: %f s' , watch . reset ()) return tab","title":"catalogue.py"},{"location":"reference/survey/catalogue/#vast_pipeline.survey.catalogue.get_survey","text":"Read a table and extract the columns of interest. Parameters: Name Type Description Default filename str The catalogue file name. required survey_name str The name of the survey. required survey_id The ID of the survey. required tr Dict[str, str] The dictionary containing the translators of the source finders. {'prefix': 'IM', 'ra': 'ra', 'err_ra': 'er_ra', 'dec': 'dec', 'err_dec': 'err_dec', 'pos_err_ang_scale': 1, 'peak_flux': 'peak_flux', 'err_peak_flux': 'err_peak_flux', 'total_flux': 'int_flux', 'err_total_flux': 'err_int_flux', 'flux_scale': 1000.0, 'bmaj': 'a', 'bmin': 'b', 'ang_scale': 3600, 'pa': 'pa', 'freq': 999} Returns: Type Description DataFrame A dataframe of the translated source catalogue. Source code in vast_pipeline/survey/catalogue.py def get_survey ( filename : str , survey_name : str , survey_id , tr : Dict [ str , str ] = translators [ 'DEFAULT' ] ) -> pd . DataFrame : \"\"\" Read a table and extract the columns of interest. Args: filename: The catalogue file name. survey_name: The name of the survey. survey_id: The ID of the survey. tr: The dictionary containing the translators of the source finders. Returns: A dataframe of the translated source catalogue. \"\"\" #TODO: Unsure of what the survey ID is. # moving to Pandas (in future to Dask) watch = StopWatch () ext = os . path . splitext ( filename )[ - 1 ] if ext . upper () in [ 'VOT' , 'VO' , 'XML' ]: tab = ( parse_single_table ( filename ) . to_table ( use_names_over_ids = True ) ) else : tab = Table . read ( filename ) logger . info ( 'total time to load catalogue: %f s' , watch . reset ()) # grab the columns names and dtypes col_dtype_map = { col : tab [ col ] . dtype . name for col in tab . colnames } tab = tab . to_pandas () # remove not used columns for col in list ( col_dtype_map . keys ()): if col not in tr . values (): col_dtype_map . pop ( col ) tab = tab . loc [:, col_dtype_map . keys ()] # fix data types for bytes and convert float32 to float64 for col in tab . columns : if 'bytes' in col_dtype_map [ col ]: tab [ col ] = tab . loc [:, col ] . astype ( str ) . str . strip ( \"'b\" ) if tab . loc [:, col ] . dtype == np . float32 : tab [ col ] = tab . loc [:, col ] . astype ( np . float64 ) # calculate extra columns tab [ 'survey_id' ] = survey_id tab [ 'name' ] = f \" { tr [ 'prefix' ] } _\" + tab . index . map ( lambda x : f ' { x : 06 } ' ) . values tab = tab . rename ( columns = { tr [ 'ra' ]: 'ra' , tr [ 'dec' ]: 'dec' , tr [ 'pa' ]: 'pa' }, copy = False ) tab [ 'err_ra' ] = tab . loc [:, tr [ 'err_ra' ]] * tr [ 'pos_err_ang_scale' ] tab [ 'err_dec' ] = tab . loc [:, tr [ 'err_dec' ]] * tr [ 'pos_err_ang_scale' ] tab [ 'peak_flux' ] = tab . loc [:, tr [ 'peak_flux' ]] * tr [ 'flux_scale' ] tab [ 'err_peak_flux' ] = tab . loc [:, tr [ 'err_peak_flux' ]] * tr [ 'flux_scale' ] tab [ 'total_flux' ] = tab . loc [:, tr [ 'total_flux' ]] * tr [ 'flux_scale' ] tab [ 'err_total_flux' ] = tab . loc [:, tr [ 'err_total_flux' ]] * tr [ 'flux_scale' ] tab [ 'bmaj' ] = tab . loc [:, tr [ 'bmaj' ]] * tr [ 'ang_scale' ] tab [ 'bmin' ] = tab . loc [:, tr [ 'bmin' ]] * tr [ 'ang_scale' ] tab [ 'image_name' ] = 'None' if 'alpha' in tr : tab = tab . rename ( columns = { tr [ 'alpha' ]: 'alpha' }, copy = False ) else : tab [ 'alpha' ] = 0 # drop not used columns cols = [ 'survey_id' , 'name' , 'ra' , 'err_ra' , 'dec' , 'err_dec' , 'peak_flux' , 'err_peak_flux' , 'total_flux' , 'err_total_flux' , 'bmaj' , 'bmin' , 'pa' , 'alpha' , 'image_name' ] tab = tab . loc [:, cols ] # fix NaNs in numeric types for col in cols : if tab [ col ] . isnull () . any (): if tab [ col ] . dtype == float : tab [ col ] = tab [ col ] . fillna ( 0. ) elif tab [ col ] . dtype == int : tab [ col ] = tab [ col ] . fillna ( 0 ) logger . info ( 'total time to process catalogue: %f s' , watch . reset ()) return tab","title":"get_survey()"},{"location":"reference/survey/translators/","text":"Translators to map the table column names in the input files into the required column names in our db. The flux/ang scale needs to be the multiplicative factor that converts the input flux into mJy and a/b into arcsec. tr_aegean \u00b6 The translator dictionary for a Agean catalogue input. Not complete. tr_gleam \u00b6 The translator dictionary for a GLEAM catalogue input. Not complete. tr_mwacs \u00b6 The translator dictionary for a MWACS catalogue input. Not complete. tr_nvss \u00b6 The translator dictionary for a NVSS catalogue input. Not complete. tr_selavy \u00b6 The translator dictionary for a Selavy catalogue input. tr_sumss \u00b6 The translator dictionary for a SUMSS catalogue input. Not complete. translators \u00b6 Dictionary containing all the translators defined in this module.","title":"translators.py"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_aegean","text":"The translator dictionary for a Agean catalogue input. Not complete.","title":"tr_aegean"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_gleam","text":"The translator dictionary for a GLEAM catalogue input. Not complete.","title":"tr_gleam"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_mwacs","text":"The translator dictionary for a MWACS catalogue input. Not complete.","title":"tr_mwacs"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_nvss","text":"The translator dictionary for a NVSS catalogue input. Not complete.","title":"tr_nvss"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_selavy","text":"The translator dictionary for a Selavy catalogue input.","title":"tr_selavy"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_sumss","text":"The translator dictionary for a SUMSS catalogue input. Not complete.","title":"tr_sumss"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.translators","text":"Dictionary containing all the translators defined in this module.","title":"translators"},{"location":"reference/utils/auth/","text":"create_admin_user ( uid , response , details , user , social , * args , ** kwargs ) \u00b6 Give Django admin privileges to a user who login via GitHub and belong to a specific team. The parameters are as per python-social-auth docs https://python-social-auth.readthedocs.io/en/latest/pipeline.html#extending-the-pipeline Parameters: Name Type Description Default uid int user id required response Dict request dictionary required details Dict user details generated by the backend required user User Django user model object required social UserSocialAuth Social auth user model object required Returns: Type Description Dict return a dictionary with the Django User object in it or empty if no action are taken Source code in vast_pipeline/utils/auth.py def create_admin_user ( uid : int , response : Dict , details : Dict , user : User , social : UserSocialAuth , * args , ** kwargs ) -> Dict : \"\"\" Give Django admin privileges to a user who login via GitHub and belong to a specific team. The parameters are as per python-social-auth docs https://python-social-auth.readthedocs.io/en/latest/pipeline.html#extending-the-pipeline Args: uid: user id response: request dictionary details: user details generated by the backend user: Django user model object social: Social auth user model object Returns: return a dictionary with the Django User object in it or empty if no action are taken \"\"\" # assume github-org backend, add <if backend.name == 'github-org'> # if other backend are implemented admin_team = settings . SOCIAL_AUTH_GITHUB_ADMIN_TEAM usr = response . get ( 'login' , '' ) if ( usr != '' and admin_team != '' and user and not user . is_staff and not user . is_superuser ): logger . info ( 'Trying to add Django admin privileges to user' ) # check if github user belong to admin team org = settings . SOCIAL_AUTH_GITHUB_ORG_NAME header = { 'Authorization' : f \"token { response . get ( 'access_token' , '' ) } \" } url = ( f 'https://api.github.com/orgs/ { org } /teams/ { admin_team } ' f '/memberships/ { usr } ' ) resp = requests . get ( url , headers = header ) if resp . ok : # add user to admin user . is_superuser = True user . is_staff = True user . save () logger . info ( 'Django admin privileges successfully added to user' ) return { 'user' : user } logger . info ( f 'GitHub request failed, reason: { resp . reason } ' ) return {} return {} load_github_avatar ( response , social , * args , ** kwargs ) \u00b6 Add GitHub avatar url to the extra data stored by social_django app Parameters: Name Type Description Default response Dict request dictionary required social UserSocialAuth Social auth user model object required *args Variable length argument list. () **kwargs Arbitrary keyword arguments. {} Returns: Type Description Dict return a dictionary with the Social auth user object in it or empty if no action are taken Source code in vast_pipeline/utils/auth.py def load_github_avatar ( response : Dict , social : UserSocialAuth , * args , ** kwargs ) -> Dict : \"\"\" Add GitHub avatar url to the extra data stored by social_django app Args: response: request dictionary social: Social auth user model object *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. Returns: return a dictionary with the Social auth user object in it or empty if no action are taken \"\"\" # assume github-org backend, add <if backend.name == 'github-org'> # if other backend are implemented # if social and social.get('extra_data', None) # print(vars(social)) if 'avatar_url' not in social . extra_data : logger . info ( 'Adding GitHub avatar url to user extra data' ) social . extra_data [ 'avatar_url' ] = response [ 'avatar_url' ] social . save () return { 'social' : social } return {}","title":"auth.py"},{"location":"reference/utils/auth/#vast_pipeline.utils.auth.create_admin_user","text":"Give Django admin privileges to a user who login via GitHub and belong to a specific team. The parameters are as per python-social-auth docs https://python-social-auth.readthedocs.io/en/latest/pipeline.html#extending-the-pipeline Parameters: Name Type Description Default uid int user id required response Dict request dictionary required details Dict user details generated by the backend required user User Django user model object required social UserSocialAuth Social auth user model object required Returns: Type Description Dict return a dictionary with the Django User object in it or empty if no action are taken Source code in vast_pipeline/utils/auth.py def create_admin_user ( uid : int , response : Dict , details : Dict , user : User , social : UserSocialAuth , * args , ** kwargs ) -> Dict : \"\"\" Give Django admin privileges to a user who login via GitHub and belong to a specific team. The parameters are as per python-social-auth docs https://python-social-auth.readthedocs.io/en/latest/pipeline.html#extending-the-pipeline Args: uid: user id response: request dictionary details: user details generated by the backend user: Django user model object social: Social auth user model object Returns: return a dictionary with the Django User object in it or empty if no action are taken \"\"\" # assume github-org backend, add <if backend.name == 'github-org'> # if other backend are implemented admin_team = settings . SOCIAL_AUTH_GITHUB_ADMIN_TEAM usr = response . get ( 'login' , '' ) if ( usr != '' and admin_team != '' and user and not user . is_staff and not user . is_superuser ): logger . info ( 'Trying to add Django admin privileges to user' ) # check if github user belong to admin team org = settings . SOCIAL_AUTH_GITHUB_ORG_NAME header = { 'Authorization' : f \"token { response . get ( 'access_token' , '' ) } \" } url = ( f 'https://api.github.com/orgs/ { org } /teams/ { admin_team } ' f '/memberships/ { usr } ' ) resp = requests . get ( url , headers = header ) if resp . ok : # add user to admin user . is_superuser = True user . is_staff = True user . save () logger . info ( 'Django admin privileges successfully added to user' ) return { 'user' : user } logger . info ( f 'GitHub request failed, reason: { resp . reason } ' ) return {} return {}","title":"create_admin_user()"},{"location":"reference/utils/auth/#vast_pipeline.utils.auth.load_github_avatar","text":"Add GitHub avatar url to the extra data stored by social_django app Parameters: Name Type Description Default response Dict request dictionary required social UserSocialAuth Social auth user model object required *args Variable length argument list. () **kwargs Arbitrary keyword arguments. {} Returns: Type Description Dict return a dictionary with the Social auth user object in it or empty if no action are taken Source code in vast_pipeline/utils/auth.py def load_github_avatar ( response : Dict , social : UserSocialAuth , * args , ** kwargs ) -> Dict : \"\"\" Add GitHub avatar url to the extra data stored by social_django app Args: response: request dictionary social: Social auth user model object *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. Returns: return a dictionary with the Social auth user object in it or empty if no action are taken \"\"\" # assume github-org backend, add <if backend.name == 'github-org'> # if other backend are implemented # if social and social.get('extra_data', None) # print(vars(social)) if 'avatar_url' not in social . extra_data : logger . info ( 'Adding GitHub avatar url to user extra data' ) social . extra_data [ 'avatar_url' ] = response [ 'avatar_url' ] social . save () return { 'social' : social } return {}","title":"load_github_avatar()"},{"location":"reference/utils/unit_tags/","text":"deg_to_arcmin ( angle ) \u00b6 Convert degrees to arcminutes. Parameters: Name Type Description Default angle float Angle in units of degrees. required Returns: Type Description float Angle in units of arcminutes. Source code in vast_pipeline/utils/unit_tags.py @register . filter def deg_to_arcmin ( angle : float ) -> float : \"\"\" Convert degrees to arcminutes. Args: angle: Angle in units of degrees. Returns: Angle in units of arcminutes. \"\"\" return float ( angle ) * 60. deg_to_arcsec ( angle ) \u00b6 Convert degrees to arcseconds. Parameters: Name Type Description Default angle float Angle in units of degrees. required Returns: Type Description float angle: Angle in units of arcseconds. Source code in vast_pipeline/utils/unit_tags.py @register . filter def deg_to_arcsec ( angle : float ) -> float : \"\"\" Convert degrees to arcseconds. Args: angle: Angle in units of degrees. Returns: angle: Angle in units of arcseconds. \"\"\" return float ( angle ) * 3600.","title":"unit_tags.py"},{"location":"reference/utils/unit_tags/#vast_pipeline.utils.unit_tags.deg_to_arcmin","text":"Convert degrees to arcminutes. Parameters: Name Type Description Default angle float Angle in units of degrees. required Returns: Type Description float Angle in units of arcminutes. Source code in vast_pipeline/utils/unit_tags.py @register . filter def deg_to_arcmin ( angle : float ) -> float : \"\"\" Convert degrees to arcminutes. Args: angle: Angle in units of degrees. Returns: Angle in units of arcminutes. \"\"\" return float ( angle ) * 60.","title":"deg_to_arcmin()"},{"location":"reference/utils/unit_tags/#vast_pipeline.utils.unit_tags.deg_to_arcsec","text":"Convert degrees to arcseconds. Parameters: Name Type Description Default angle float Angle in units of degrees. required Returns: Type Description float angle: Angle in units of arcseconds. Source code in vast_pipeline/utils/unit_tags.py @register . filter def deg_to_arcsec ( angle : float ) -> float : \"\"\" Convert degrees to arcseconds. Args: angle: Angle in units of degrees. Returns: angle: Angle in units of arcseconds. \"\"\" return float ( angle ) * 3600.","title":"deg_to_arcsec()"},{"location":"reference/utils/utils/","text":"This module contains general pipeline utility functions. StopWatch \u00b6 A simple stopwatch to simplify timing code. __init__ ( self ) special \u00b6 Initialise the StopWatch Returns: Type Description None None. Source code in vast_pipeline/utils/utils.py def __init__ ( self ) -> None : \"\"\" Initialise the StopWatch Returns: None. \"\"\" self . _init = datetime . now () self . _last = self . _init reset ( self ) \u00b6 Reset the stopwatch and return the time since last reset (seconds). Returns: Type Description float The time in seconds since the last reset. Source code in vast_pipeline/utils/utils.py def reset ( self ) -> float : \"\"\" Reset the stopwatch and return the time since last reset (seconds). Returns: The time in seconds since the last reset. \"\"\" now = datetime . now () diff = ( now - self . _last ) . total_seconds () self . _last = now return diff reset_init ( self ) \u00b6 Reset the stopwatch and return the total time since initialisation. Returns: Type Description float The time in seconds since the initialisation. Source code in vast_pipeline/utils/utils.py def reset_init ( self ) -> float : \"\"\" Reset the stopwatch and return the total time since initialisation. Returns: The time in seconds since the initialisation. \"\"\" now = datetime . now () diff = ( now - self . _init ) . total_seconds () self . _last = self . _init = now return diff check_read_write_perm ( path , perm = 'W' ) \u00b6 Assess the file permission on a path. Parameters: Name Type Description Default path str The system path to assess. required perm str The permission to check for. 'W' Returns: Type Description None None Exceptions: Type Description IOError The permission is not valid on the checked directory. Source code in vast_pipeline/utils/utils.py def check_read_write_perm ( path : str , perm : str = 'W' ) -> None : \"\"\" Assess the file permission on a path. Args: path: The system path to assess. perm: The permission to check for. Returns: None Raises: IOError: The permission is not valid on the checked directory. \"\"\" assert perm in ( 'R' , 'W' , 'X' ), 'permission not supported' perm_map = { 'R' : os . R_OK , 'W' : os . W_OK , 'X' : os . X_OK } if not os . access ( path , perm_map [ perm ]): msg = f 'permission not valid on folder: { path } ' logger . error ( msg ) raise IOError ( msg ) pass convert_list_to_dict ( l ) \u00b6 Converts a list to a dictionary where the keys are the enumerate iterator + 1. Parameters: Name Type Description Default l List[Any] input dataframe, no specific columns. required Returns: Type Description Dict[int, Any] A dictionary containing the list values. Source code in vast_pipeline/utils/utils.py def convert_list_to_dict ( l : List [ Any ]) -> Dict [ int , Any ]: \"\"\" Converts a list to a dictionary where the keys are the enumerate iterator + 1. Args: l: input dataframe, no specific columns. Returns: A dictionary containing the list values. \"\"\" conversion = { i + 1 : [ val ] for i , val in enumerate ( l )} return conversion deg2dms ( deg , dms_format = False ) \u00b6 Convert angle in degrees into DMS using format. Default: '02d:02d:05.2f' deg2dms(12.582438888888889) '+12:34:56.78' deg2dms(2.582438888888889) '+02:34:56.78' deg2dms(-12.582438888888889) '-12:34:56.78' Parameters: Name Type Description Default deg float The angle in degrees to convert. required dms_format bool If True then the result is returned in dms format, e.g. '+12d34m56.78s'. False Returns: Type Description str The string result. Source code in vast_pipeline/utils/utils.py def deg2dms ( deg : float , dms_format : bool = False ) -> str : \"\"\"Convert angle in degrees into DMS using format. Default: '02d:02d:05.2f' >>> deg2dms(12.582438888888889) '+12:34:56.78' >>> deg2dms(2.582438888888889) '+02:34:56.78' >>> deg2dms(-12.582438888888889) '-12:34:56.78' Args: deg: The angle in degrees to convert. dms_format: If `True` then the result is returned in dms format, e.g. '+12d34m56.78s'. Returns: The string result. \"\"\" sign , sex = deg2sex ( deg ) signchar = \"+\" if sign == 1 else \"-\" if dms_format : return f ' { signchar }{ sex [ 0 ] : 02d } d { sex [ 1 ] : 02d } m { sex [ 2 ] : 05.2f } s' return f ' { signchar }{ sex [ 0 ] : 02d } : { sex [ 1 ] : 02d } : { sex [ 2 ] : 05.2f } ' deg2hms ( deg , hms_format = False ) \u00b6 Convert angle in degrees into HMS using format. Default: '%d:%d:%.2f' deg2hms(188.73658333333333) '12:34:56.78' deg2hms(-188.73658333333333) '12:34:56.78' Parameters: Name Type Description Default deg float The angle in degrees to convert. required hms_format bool If True then the result is returned in hms format, e.g. '12h34m56.78s'. False Returns: Type Description str The string result. Source code in vast_pipeline/utils/utils.py def deg2hms ( deg : float , hms_format : bool = False ) -> str : \"\"\"Convert angle in degrees into HMS using format. Default: '%d:%d:%.2f' >>> deg2hms(188.73658333333333) '12:34:56.78' >>> deg2hms(-188.73658333333333) '12:34:56.78' Args: deg: The angle in degrees to convert. hms_format: If `True` then the result is returned in hms format, e.g. '12h34m56.78s'. Returns: The string result. \"\"\" # TODO: why it this? # We only handle positive RA values # assert deg >= 0 sign , sex = deg2sex ( deg / 15. ) if hms_format : return f ' { sex [ 0 ] : 02d } h { sex [ 1 ] : 02d } m { sex [ 2 ] : 05.2f } s' return f ' { sex [ 0 ] : 02d } : { sex [ 1 ] : 02d } : { sex [ 2 ] : 05.2f } ' deg2sex ( deg ) \u00b6 Converts angle in degrees to sexagesimal notation Returns tuple containing (sign, (degrees, minutes & seconds)) sign is -1 or 1 deg2sex(12.582438888888889) (12, 34, 56.78000000000182) deg2sex(-12.582438888888889) (-12, 34, 56.78000000000182) Parameters: Name Type Description Default deg float The angle to convert in degrees. required Returns: Type Description Tuple[int, Tuple[float, float, float]] A nested tuple with the sign as the 0th value followed by the sexagesimal values tuple. Source code in vast_pipeline/utils/utils.py def deg2sex ( deg : float ) -> Tuple [ int , Tuple [ float , float , float ]]: \"\"\" Converts angle in degrees to sexagesimal notation Returns tuple containing (sign, (degrees, minutes & seconds)) sign is -1 or 1 >>> deg2sex(12.582438888888889) (12, 34, 56.78000000000182) >>> deg2sex(-12.582438888888889) (-12, 34, 56.78000000000182) Args: deg: The angle to convert in degrees. Returns: A nested tuple with the sign as the 0th value followed by the sexagesimal values tuple. \"\"\" sign = - 1 if deg < 0 else 1 adeg = abs ( deg ) degf = m . floor ( adeg ) mins = ( adeg - degf ) * 60. minsf = int ( m . floor ( mins )) secs = ( mins - minsf ) * 60. return ( sign , ( degf , minsf , secs )) eq_to_cart ( ra , dec ) \u00b6 Find the cartesian co-ordinates on the unit sphere given the eq. co-ords. ra, dec should be in degrees. Parameters: Name Type Description Default ra float The right ascension coordinate, in degrees, to convert. required dec float The declination coordinate, in degrees, to convert. required Returns: Type Description Tuple[float, float, float] The cartesian coordinates. Source code in vast_pipeline/utils/utils.py def eq_to_cart ( ra : float , dec : float ) -> Tuple [ float , float , float ]: \"\"\" Find the cartesian co-ordinates on the unit sphere given the eq. co-ords. ra, dec should be in degrees. Args: ra: The right ascension coordinate, in degrees, to convert. dec: The declination coordinate, in degrees, to convert. Returns: The cartesian coordinates. \"\"\" # TODO: This part of the code can probably be removed along with the # storage of these coodinates on the image. return ( m . cos ( m . radians ( dec )) * m . cos ( m . radians ( ra )), # Cartesian x m . cos ( m . radians ( dec )) * m . sin ( m . radians ( ra )), # Cartesian y m . sin ( m . radians ( dec )) # Cartesian z ) equ2gal ( ra , dec ) \u00b6 Convert equatorial coordinates to galactic Parameters: Name Type Description Default ra float Right ascension in units of degrees. required dec float Declination in units of degrees. required Returns: Type Description Tuple[float, float] Tuple (float, float): Galactic longitude and latitude in degrees. Source code in vast_pipeline/utils/utils.py def equ2gal ( ra : float , dec : float ) -> Tuple [ float , float ]: \"\"\" Convert equatorial coordinates to galactic Args: ra (float): Right ascension in units of degrees. dec (float): Declination in units of degrees. Returns: Tuple (float, float): Galactic longitude and latitude in degrees. \"\"\" c = SkyCoord ( np . float ( ra ), np . float ( dec ), unit = ( u . deg , u . deg ), frame = 'icrs' ) l = c . galactic . l . deg b = c . galactic . b . deg return l , b gal2equ ( l , b ) \u00b6 Convert galactic coordinates to equatorial. Parameters: Name Type Description Default l float Galactic longitude in degrees. required b float Galactic latitude in degrees. required Returns: Type Description Tuple[float, float] Tuple (float, float): Right ascension and declination in units of degrees. Source code in vast_pipeline/utils/utils.py def gal2equ ( l : float , b : float ) -> Tuple [ float , float ]: \"\"\" Convert galactic coordinates to equatorial. Args: l (float): Galactic longitude in degrees. b (float): Galactic latitude in degrees. Returns: Tuple (float, float): Right ascension and declination in units of degrees. \"\"\" c = SkyCoord ( l = np . float ( l ) * u . deg , b = np . float ( b ) * u . deg , frame = 'galactic' ) ra = c . icrs . ra . deg dec = c . icrs . dec . deg return ra , dec optimize_floats ( df ) \u00b6 Downcast float columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Parameters: Name Type Description Default df DataFrame input dataframe, no specific columns. required Returns: Type Description DataFrame The input dataframe with the float64 type columns downcasted. Source code in vast_pipeline/utils/utils.py def optimize_floats ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Downcast float columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Args: df: input dataframe, no specific columns. Returns: The input dataframe with the `float64` type columns downcasted. \"\"\" floats = df . select_dtypes ( include = [ 'float64' ]) . columns . tolist () df [ floats ] = df [ floats ] . apply ( pd . to_numeric , downcast = 'float' ) return df optimize_ints ( df ) \u00b6 Downcast integer columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Parameters: Name Type Description Default df DataFrame Input dataframe, no specific columns. required Returns: Type Description DataFrame The input dataframe with the int64 type columns downcasted. Source code in vast_pipeline/utils/utils.py def optimize_ints ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Downcast integer columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Args: df: Input dataframe, no specific columns. Returns: The input dataframe with the `int64` type columns downcasted. \"\"\" ints = df . select_dtypes ( include = [ 'int64' ]) . columns . tolist () df [ ints ] = df [ ints ] . apply ( pd . to_numeric , downcast = 'integer' ) return df parse_coord ( coord_string , coord_frame = 'icrs' ) \u00b6 Parse a coordinate string and return a SkyCoord. The units may be expressed within coord_string e.g. \"21h52m03.1s -62d08m19.7s\", \"18.4d +43.1d\". If no units are given, the following assumptions are made: - if both coordinate components are decimals, they are assumed to be in degrees. - if a sexagesimal coordinate is given and the frame is galactic, both components are assumed to be in degrees. For any other frame, the first component is assumed to be in hourangles and the second in degrees. Will raise a ValueError if SkyCoord is unable to parse coord_string . Parameters: Name Type Description Default coord_string str The coordinate string to parse. required coord_frame str The frame of coord_string . Defaults to \"icrs\". 'icrs' Returns: Type Description SkyCoord SkyCoord Source code in vast_pipeline/utils/utils.py def parse_coord ( coord_string : str , coord_frame : str = \"icrs\" ) -> SkyCoord : \"\"\"Parse a coordinate string and return a SkyCoord. The units may be expressed within `coord_string` e.g. \"21h52m03.1s -62d08m19.7s\", \"18.4d +43.1d\". If no units are given, the following assumptions are made: - if both coordinate components are decimals, they are assumed to be in degrees. - if a sexagesimal coordinate is given and the frame is galactic, both components are assumed to be in degrees. For any other frame, the first component is assumed to be in hourangles and the second in degrees. Will raise a ValueError if SkyCoord is unable to parse `coord_string`. Args: coord_string (str): The coordinate string to parse. coord_frame (str, optional): The frame of `coord_string`. Defaults to \"icrs\". Returns: SkyCoord \"\"\" # if both coord components are decimals, assume they're in degrees, otherwise assume # hourangles and degrees. Note that the unit parameter is ignored if the units are # not ambiguous i.e. if coord_string contains the units (e.g. 18.4d, 5h35m, etc) try : _ = [ float ( x ) for x in coord_string . split ()] unit = \"deg\" except ValueError : if coord_frame == \"galactic\" : unit = \"deg\" else : unit = \"hourangle,deg\" coord = SkyCoord ( coord_string , unit = unit , frame = coord_frame ) return coord","title":"utils.py"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.StopWatch","text":"A simple stopwatch to simplify timing code.","title":"StopWatch"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.StopWatch.__init__","text":"Initialise the StopWatch Returns: Type Description None None. Source code in vast_pipeline/utils/utils.py def __init__ ( self ) -> None : \"\"\" Initialise the StopWatch Returns: None. \"\"\" self . _init = datetime . now () self . _last = self . _init","title":"__init__()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.StopWatch.reset","text":"Reset the stopwatch and return the time since last reset (seconds). Returns: Type Description float The time in seconds since the last reset. Source code in vast_pipeline/utils/utils.py def reset ( self ) -> float : \"\"\" Reset the stopwatch and return the time since last reset (seconds). Returns: The time in seconds since the last reset. \"\"\" now = datetime . now () diff = ( now - self . _last ) . total_seconds () self . _last = now return diff","title":"reset()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.StopWatch.reset_init","text":"Reset the stopwatch and return the total time since initialisation. Returns: Type Description float The time in seconds since the initialisation. Source code in vast_pipeline/utils/utils.py def reset_init ( self ) -> float : \"\"\" Reset the stopwatch and return the total time since initialisation. Returns: The time in seconds since the initialisation. \"\"\" now = datetime . now () diff = ( now - self . _init ) . total_seconds () self . _last = self . _init = now return diff","title":"reset_init()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.check_read_write_perm","text":"Assess the file permission on a path. Parameters: Name Type Description Default path str The system path to assess. required perm str The permission to check for. 'W' Returns: Type Description None None Exceptions: Type Description IOError The permission is not valid on the checked directory. Source code in vast_pipeline/utils/utils.py def check_read_write_perm ( path : str , perm : str = 'W' ) -> None : \"\"\" Assess the file permission on a path. Args: path: The system path to assess. perm: The permission to check for. Returns: None Raises: IOError: The permission is not valid on the checked directory. \"\"\" assert perm in ( 'R' , 'W' , 'X' ), 'permission not supported' perm_map = { 'R' : os . R_OK , 'W' : os . W_OK , 'X' : os . X_OK } if not os . access ( path , perm_map [ perm ]): msg = f 'permission not valid on folder: { path } ' logger . error ( msg ) raise IOError ( msg ) pass","title":"check_read_write_perm()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.convert_list_to_dict","text":"Converts a list to a dictionary where the keys are the enumerate iterator + 1. Parameters: Name Type Description Default l List[Any] input dataframe, no specific columns. required Returns: Type Description Dict[int, Any] A dictionary containing the list values. Source code in vast_pipeline/utils/utils.py def convert_list_to_dict ( l : List [ Any ]) -> Dict [ int , Any ]: \"\"\" Converts a list to a dictionary where the keys are the enumerate iterator + 1. Args: l: input dataframe, no specific columns. Returns: A dictionary containing the list values. \"\"\" conversion = { i + 1 : [ val ] for i , val in enumerate ( l )} return conversion","title":"convert_list_to_dict()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.deg2dms","text":"Convert angle in degrees into DMS using format. Default: '02d:02d:05.2f' deg2dms(12.582438888888889) '+12:34:56.78' deg2dms(2.582438888888889) '+02:34:56.78' deg2dms(-12.582438888888889) '-12:34:56.78' Parameters: Name Type Description Default deg float The angle in degrees to convert. required dms_format bool If True then the result is returned in dms format, e.g. '+12d34m56.78s'. False Returns: Type Description str The string result. Source code in vast_pipeline/utils/utils.py def deg2dms ( deg : float , dms_format : bool = False ) -> str : \"\"\"Convert angle in degrees into DMS using format. Default: '02d:02d:05.2f' >>> deg2dms(12.582438888888889) '+12:34:56.78' >>> deg2dms(2.582438888888889) '+02:34:56.78' >>> deg2dms(-12.582438888888889) '-12:34:56.78' Args: deg: The angle in degrees to convert. dms_format: If `True` then the result is returned in dms format, e.g. '+12d34m56.78s'. Returns: The string result. \"\"\" sign , sex = deg2sex ( deg ) signchar = \"+\" if sign == 1 else \"-\" if dms_format : return f ' { signchar }{ sex [ 0 ] : 02d } d { sex [ 1 ] : 02d } m { sex [ 2 ] : 05.2f } s' return f ' { signchar }{ sex [ 0 ] : 02d } : { sex [ 1 ] : 02d } : { sex [ 2 ] : 05.2f } '","title":"deg2dms()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.deg2hms","text":"Convert angle in degrees into HMS using format. Default: '%d:%d:%.2f' deg2hms(188.73658333333333) '12:34:56.78' deg2hms(-188.73658333333333) '12:34:56.78' Parameters: Name Type Description Default deg float The angle in degrees to convert. required hms_format bool If True then the result is returned in hms format, e.g. '12h34m56.78s'. False Returns: Type Description str The string result. Source code in vast_pipeline/utils/utils.py def deg2hms ( deg : float , hms_format : bool = False ) -> str : \"\"\"Convert angle in degrees into HMS using format. Default: '%d:%d:%.2f' >>> deg2hms(188.73658333333333) '12:34:56.78' >>> deg2hms(-188.73658333333333) '12:34:56.78' Args: deg: The angle in degrees to convert. hms_format: If `True` then the result is returned in hms format, e.g. '12h34m56.78s'. Returns: The string result. \"\"\" # TODO: why it this? # We only handle positive RA values # assert deg >= 0 sign , sex = deg2sex ( deg / 15. ) if hms_format : return f ' { sex [ 0 ] : 02d } h { sex [ 1 ] : 02d } m { sex [ 2 ] : 05.2f } s' return f ' { sex [ 0 ] : 02d } : { sex [ 1 ] : 02d } : { sex [ 2 ] : 05.2f } '","title":"deg2hms()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.deg2sex","text":"Converts angle in degrees to sexagesimal notation Returns tuple containing (sign, (degrees, minutes & seconds)) sign is -1 or 1 deg2sex(12.582438888888889) (12, 34, 56.78000000000182) deg2sex(-12.582438888888889) (-12, 34, 56.78000000000182) Parameters: Name Type Description Default deg float The angle to convert in degrees. required Returns: Type Description Tuple[int, Tuple[float, float, float]] A nested tuple with the sign as the 0th value followed by the sexagesimal values tuple. Source code in vast_pipeline/utils/utils.py def deg2sex ( deg : float ) -> Tuple [ int , Tuple [ float , float , float ]]: \"\"\" Converts angle in degrees to sexagesimal notation Returns tuple containing (sign, (degrees, minutes & seconds)) sign is -1 or 1 >>> deg2sex(12.582438888888889) (12, 34, 56.78000000000182) >>> deg2sex(-12.582438888888889) (-12, 34, 56.78000000000182) Args: deg: The angle to convert in degrees. Returns: A nested tuple with the sign as the 0th value followed by the sexagesimal values tuple. \"\"\" sign = - 1 if deg < 0 else 1 adeg = abs ( deg ) degf = m . floor ( adeg ) mins = ( adeg - degf ) * 60. minsf = int ( m . floor ( mins )) secs = ( mins - minsf ) * 60. return ( sign , ( degf , minsf , secs ))","title":"deg2sex()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.eq_to_cart","text":"Find the cartesian co-ordinates on the unit sphere given the eq. co-ords. ra, dec should be in degrees. Parameters: Name Type Description Default ra float The right ascension coordinate, in degrees, to convert. required dec float The declination coordinate, in degrees, to convert. required Returns: Type Description Tuple[float, float, float] The cartesian coordinates. Source code in vast_pipeline/utils/utils.py def eq_to_cart ( ra : float , dec : float ) -> Tuple [ float , float , float ]: \"\"\" Find the cartesian co-ordinates on the unit sphere given the eq. co-ords. ra, dec should be in degrees. Args: ra: The right ascension coordinate, in degrees, to convert. dec: The declination coordinate, in degrees, to convert. Returns: The cartesian coordinates. \"\"\" # TODO: This part of the code can probably be removed along with the # storage of these coodinates on the image. return ( m . cos ( m . radians ( dec )) * m . cos ( m . radians ( ra )), # Cartesian x m . cos ( m . radians ( dec )) * m . sin ( m . radians ( ra )), # Cartesian y m . sin ( m . radians ( dec )) # Cartesian z )","title":"eq_to_cart()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.equ2gal","text":"Convert equatorial coordinates to galactic Parameters: Name Type Description Default ra float Right ascension in units of degrees. required dec float Declination in units of degrees. required Returns: Type Description Tuple[float, float] Tuple (float, float): Galactic longitude and latitude in degrees. Source code in vast_pipeline/utils/utils.py def equ2gal ( ra : float , dec : float ) -> Tuple [ float , float ]: \"\"\" Convert equatorial coordinates to galactic Args: ra (float): Right ascension in units of degrees. dec (float): Declination in units of degrees. Returns: Tuple (float, float): Galactic longitude and latitude in degrees. \"\"\" c = SkyCoord ( np . float ( ra ), np . float ( dec ), unit = ( u . deg , u . deg ), frame = 'icrs' ) l = c . galactic . l . deg b = c . galactic . b . deg return l , b","title":"equ2gal()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.gal2equ","text":"Convert galactic coordinates to equatorial. Parameters: Name Type Description Default l float Galactic longitude in degrees. required b float Galactic latitude in degrees. required Returns: Type Description Tuple[float, float] Tuple (float, float): Right ascension and declination in units of degrees. Source code in vast_pipeline/utils/utils.py def gal2equ ( l : float , b : float ) -> Tuple [ float , float ]: \"\"\" Convert galactic coordinates to equatorial. Args: l (float): Galactic longitude in degrees. b (float): Galactic latitude in degrees. Returns: Tuple (float, float): Right ascension and declination in units of degrees. \"\"\" c = SkyCoord ( l = np . float ( l ) * u . deg , b = np . float ( b ) * u . deg , frame = 'galactic' ) ra = c . icrs . ra . deg dec = c . icrs . dec . deg return ra , dec","title":"gal2equ()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.optimize_floats","text":"Downcast float columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Parameters: Name Type Description Default df DataFrame input dataframe, no specific columns. required Returns: Type Description DataFrame The input dataframe with the float64 type columns downcasted. Source code in vast_pipeline/utils/utils.py def optimize_floats ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Downcast float columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Args: df: input dataframe, no specific columns. Returns: The input dataframe with the `float64` type columns downcasted. \"\"\" floats = df . select_dtypes ( include = [ 'float64' ]) . columns . tolist () df [ floats ] = df [ floats ] . apply ( pd . to_numeric , downcast = 'float' ) return df","title":"optimize_floats()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.optimize_ints","text":"Downcast integer columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Parameters: Name Type Description Default df DataFrame Input dataframe, no specific columns. required Returns: Type Description DataFrame The input dataframe with the int64 type columns downcasted. Source code in vast_pipeline/utils/utils.py def optimize_ints ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Downcast integer columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Args: df: Input dataframe, no specific columns. Returns: The input dataframe with the `int64` type columns downcasted. \"\"\" ints = df . select_dtypes ( include = [ 'int64' ]) . columns . tolist () df [ ints ] = df [ ints ] . apply ( pd . to_numeric , downcast = 'integer' ) return df","title":"optimize_ints()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.parse_coord","text":"Parse a coordinate string and return a SkyCoord. The units may be expressed within coord_string e.g. \"21h52m03.1s -62d08m19.7s\", \"18.4d +43.1d\". If no units are given, the following assumptions are made: - if both coordinate components are decimals, they are assumed to be in degrees. - if a sexagesimal coordinate is given and the frame is galactic, both components are assumed to be in degrees. For any other frame, the first component is assumed to be in hourangles and the second in degrees. Will raise a ValueError if SkyCoord is unable to parse coord_string . Parameters: Name Type Description Default coord_string str The coordinate string to parse. required coord_frame str The frame of coord_string . Defaults to \"icrs\". 'icrs' Returns: Type Description SkyCoord SkyCoord Source code in vast_pipeline/utils/utils.py def parse_coord ( coord_string : str , coord_frame : str = \"icrs\" ) -> SkyCoord : \"\"\"Parse a coordinate string and return a SkyCoord. The units may be expressed within `coord_string` e.g. \"21h52m03.1s -62d08m19.7s\", \"18.4d +43.1d\". If no units are given, the following assumptions are made: - if both coordinate components are decimals, they are assumed to be in degrees. - if a sexagesimal coordinate is given and the frame is galactic, both components are assumed to be in degrees. For any other frame, the first component is assumed to be in hourangles and the second in degrees. Will raise a ValueError if SkyCoord is unable to parse `coord_string`. Args: coord_string (str): The coordinate string to parse. coord_frame (str, optional): The frame of `coord_string`. Defaults to \"icrs\". Returns: SkyCoord \"\"\" # if both coord components are decimals, assume they're in degrees, otherwise assume # hourangles and degrees. Note that the unit parameter is ignored if the units are # not ambiguous i.e. if coord_string contains the units (e.g. 18.4d, 5h35m, etc) try : _ = [ float ( x ) for x in coord_string . split ()] unit = \"deg\" except ValueError : if coord_frame == \"galactic\" : unit = \"deg\" else : unit = \"hourangle,deg\" coord = SkyCoord ( coord_string , unit = unit , frame = coord_frame ) return coord","title":"parse_coord()"},{"location":"reference/utils/view/","text":"Functions and variables used in pipeline/views.py. generate_colsfields ( fields , url_prefix_dict , not_orderable_col = None , not_searchable_col = None ) \u00b6 Generate data to be included in context for datatables. Example of url_prefix_dict format: api_col_dict = { 'source.name': reverse('vast_pipeline:source_detail', args=[1])[:-2], 'source.run.name': reverse('vast_pipeline:run_detail', args=[1])[:-2] } Parameters: Name Type Description Default fields List[str] List of fields to include as columns. required url_prefix_dict Dict[str, str] Dict containing the url prefix to form href links in the datatables. required not_orderable_col Optional[List[str]] List of columns that should be set to be not orderable in the final table. None not_searchable_col Optional[List[str]] List of columns that should be set to be not searchable in the final table. None Returns: Type Description List[Dict[str, Any]] colsfields (list): List containing JSON object. Source code in vast_pipeline/utils/view.py def generate_colsfields ( fields : List [ str ], url_prefix_dict : Dict [ str , str ], not_orderable_col : Optional [ List [ str ]] = None , not_searchable_col : Optional [ List [ str ]] = None , ) -> List [ Dict [ str , Any ]]: \"\"\" Generate data to be included in context for datatables. Example of url_prefix_dict format: api_col_dict = { 'source.name': reverse('vast_pipeline:source_detail', args=[1])[:-2], 'source.run.name': reverse('vast_pipeline:run_detail', args=[1])[:-2] } Args: fields (list): List of fields to include as columns. url_prefix_dict (dict): Dict containing the url prefix to form href links in the datatables. not_orderable_col (list): List of columns that should be set to be not orderable in the final table. not_searchable_col (list): List of columns that should be set to be not searchable in the final table. Returns: colsfields (list): List containing JSON object. \"\"\" colsfields = [] if not_orderable_col is None : not_orderable_col = [] if not_searchable_col is None : not_searchable_col = [] for col in fields : field2append = {} if col == 'name' : field2append = { 'data' : col , 'render' : { 'url' : { 'prefix' : url_prefix_dict [ col ], 'col' : 'name' } } } elif '.name' in col : # this is for nested fields to build a render with column name # and id in url. The API results should look like: # {... , main_col : {'name': value, 'id': value, ... }} main_col = col . rsplit ( '.' , 1 )[ 0 ] field2append = { 'data' : col , 'render' : { 'url' : { 'prefix' : url_prefix_dict [ col ], 'col' : main_col , 'nested' : True , } } } elif col == 'n_sibl' : field2append = { 'data' : col , 'render' : { 'contains_sibl' : { 'col' : col } } } elif col in FLOAT_FIELDS : field2append = { 'data' : col , 'render' : { 'float' : { 'col' : col , 'precision' : FLOAT_FIELDS [ col ][ 'precision' ], 'scale' : FLOAT_FIELDS [ col ][ 'scale' ], } } } else : field2append = { 'data' : col } if col in not_orderable_col : field2append [ 'orderable' ] = False if col in not_searchable_col : field2append [ 'searchable' ] = False colsfields . append ( field2append ) return colsfields get_skyregions_collection ( run_id = None ) \u00b6 Produce Sky region geometry shapes JSON object for d3-celestial. Parameters: Name Type Description Default run_id Optional[int] Run ID to filter on if not None. None Returns: Type Description Dict[str, Any] skyregions_collection (dict): Dictionary representing a JSON obejct containing the sky regions. Source code in vast_pipeline/utils/view.py def get_skyregions_collection ( run_id : Optional [ int ] = None ) -> Dict [ str , Any ]: \"\"\" Produce Sky region geometry shapes JSON object for d3-celestial. Args: run_id (int, optional): Run ID to filter on if not None. Returns: skyregions_collection (dict): Dictionary representing a JSON obejct containing the sky regions. \"\"\" skyregions = SkyRegion . objects . all () if run_id is not None : skyregions = skyregions . filter ( run = run_id ) features = [] for skr in skyregions : ra_fix = 360. if skr . centre_ra > 180. else 0. ra = skr . centre_ra - ra_fix dec = skr . centre_dec width_ra = skr . width_ra / 2. width_dec = skr . width_dec / 2. id = skr . id features . append ( { \"type\" : \"Feature\" , \"id\" : f \"SkyRegion { id } \" , \"properties\" : { \"n\" : f \" { id : 02d } \" , \"loc\" : [ ra , dec ] }, \"geometry\" : { \"type\" : \"MultiLineString\" , \"coordinates\" : [[ [ ra + width_ra , dec + width_dec ], [ ra + width_ra , dec - width_dec ], [ ra - width_ra , dec - width_dec ], [ ra - width_ra , dec + width_dec ], [ ra + width_ra , dec + width_dec ] ]] } } ) skyregions_collection = { \"type\" : \"FeatureCollection\" , \"features\" : features } return skyregions_collection","title":"view.py"},{"location":"reference/utils/view/#vast_pipeline.utils.view.generate_colsfields","text":"Generate data to be included in context for datatables. Example of url_prefix_dict format: api_col_dict = { 'source.name': reverse('vast_pipeline:source_detail', args=[1])[:-2], 'source.run.name': reverse('vast_pipeline:run_detail', args=[1])[:-2] } Parameters: Name Type Description Default fields List[str] List of fields to include as columns. required url_prefix_dict Dict[str, str] Dict containing the url prefix to form href links in the datatables. required not_orderable_col Optional[List[str]] List of columns that should be set to be not orderable in the final table. None not_searchable_col Optional[List[str]] List of columns that should be set to be not searchable in the final table. None Returns: Type Description List[Dict[str, Any]] colsfields (list): List containing JSON object. Source code in vast_pipeline/utils/view.py def generate_colsfields ( fields : List [ str ], url_prefix_dict : Dict [ str , str ], not_orderable_col : Optional [ List [ str ]] = None , not_searchable_col : Optional [ List [ str ]] = None , ) -> List [ Dict [ str , Any ]]: \"\"\" Generate data to be included in context for datatables. Example of url_prefix_dict format: api_col_dict = { 'source.name': reverse('vast_pipeline:source_detail', args=[1])[:-2], 'source.run.name': reverse('vast_pipeline:run_detail', args=[1])[:-2] } Args: fields (list): List of fields to include as columns. url_prefix_dict (dict): Dict containing the url prefix to form href links in the datatables. not_orderable_col (list): List of columns that should be set to be not orderable in the final table. not_searchable_col (list): List of columns that should be set to be not searchable in the final table. Returns: colsfields (list): List containing JSON object. \"\"\" colsfields = [] if not_orderable_col is None : not_orderable_col = [] if not_searchable_col is None : not_searchable_col = [] for col in fields : field2append = {} if col == 'name' : field2append = { 'data' : col , 'render' : { 'url' : { 'prefix' : url_prefix_dict [ col ], 'col' : 'name' } } } elif '.name' in col : # this is for nested fields to build a render with column name # and id in url. The API results should look like: # {... , main_col : {'name': value, 'id': value, ... }} main_col = col . rsplit ( '.' , 1 )[ 0 ] field2append = { 'data' : col , 'render' : { 'url' : { 'prefix' : url_prefix_dict [ col ], 'col' : main_col , 'nested' : True , } } } elif col == 'n_sibl' : field2append = { 'data' : col , 'render' : { 'contains_sibl' : { 'col' : col } } } elif col in FLOAT_FIELDS : field2append = { 'data' : col , 'render' : { 'float' : { 'col' : col , 'precision' : FLOAT_FIELDS [ col ][ 'precision' ], 'scale' : FLOAT_FIELDS [ col ][ 'scale' ], } } } else : field2append = { 'data' : col } if col in not_orderable_col : field2append [ 'orderable' ] = False if col in not_searchable_col : field2append [ 'searchable' ] = False colsfields . append ( field2append ) return colsfields","title":"generate_colsfields()"},{"location":"reference/utils/view/#vast_pipeline.utils.view.get_skyregions_collection","text":"Produce Sky region geometry shapes JSON object for d3-celestial. Parameters: Name Type Description Default run_id Optional[int] Run ID to filter on if not None. None Returns: Type Description Dict[str, Any] skyregions_collection (dict): Dictionary representing a JSON obejct containing the sky regions. Source code in vast_pipeline/utils/view.py def get_skyregions_collection ( run_id : Optional [ int ] = None ) -> Dict [ str , Any ]: \"\"\" Produce Sky region geometry shapes JSON object for d3-celestial. Args: run_id (int, optional): Run ID to filter on if not None. Returns: skyregions_collection (dict): Dictionary representing a JSON obejct containing the sky regions. \"\"\" skyregions = SkyRegion . objects . all () if run_id is not None : skyregions = skyregions . filter ( run = run_id ) features = [] for skr in skyregions : ra_fix = 360. if skr . centre_ra > 180. else 0. ra = skr . centre_ra - ra_fix dec = skr . centre_dec width_ra = skr . width_ra / 2. width_dec = skr . width_dec / 2. id = skr . id features . append ( { \"type\" : \"Feature\" , \"id\" : f \"SkyRegion { id } \" , \"properties\" : { \"n\" : f \" { id : 02d } \" , \"loc\" : [ ra , dec ] }, \"geometry\" : { \"type\" : \"MultiLineString\" , \"coordinates\" : [[ [ ra + width_ra , dec + width_dec ], [ ra + width_ra , dec - width_dec ], [ ra - width_ra , dec - width_dec ], [ ra - width_ra , dec + width_dec ], [ ra + width_ra , dec + width_dec ] ]] } } ) skyregions_collection = { \"type\" : \"FeatureCollection\" , \"features\" : features } return skyregions_collection","title":"get_skyregions_collection()"},{"location":"using/access/","text":"Accessing the Pipeline \u00b6 Access to the pipeline website is done using GitHub as the authentification method. In particular it checks organisation membership to confirm that the user is allowed access. For example for those wanting to access the VAST instance hosted on Nimbus must make sure they are a member of the askap-vast GitHub organisation. Note If you are attempting to access an instance of the VAST pipeline not hosted by the VAST group, confirm with the administrator what GitHub organisation membership is required. Adminstrators can refer to Pipeline Login for information on how to configure the login system. Logging In \u00b6 Navigate to the VAST Pipeline (or other hosted instance) and the following page will appear. Click the Login with GitHub button and you will be presented with the following page to enter you GitHub details. Note that if you are already logged into GitHub in your browser then you will likely not see this page. After a successful login you will then be redirected to the Pipeline homepage. Troubleshooting \u00b6 Failures will commonly be caused by the user not being a member of the correct GitHub organisation or an error in the configuration of the login system by the administrator. Contact the administrator of the pipeline instance if you encounter problems logging in.","title":"Accessing the Pipeline"},{"location":"using/access/#accessing-the-pipeline","text":"Access to the pipeline website is done using GitHub as the authentification method. In particular it checks organisation membership to confirm that the user is allowed access. For example for those wanting to access the VAST instance hosted on Nimbus must make sure they are a member of the askap-vast GitHub organisation. Note If you are attempting to access an instance of the VAST pipeline not hosted by the VAST group, confirm with the administrator what GitHub organisation membership is required. Adminstrators can refer to Pipeline Login for information on how to configure the login system.","title":"Accessing the Pipeline"},{"location":"using/access/#logging-in","text":"Navigate to the VAST Pipeline (or other hosted instance) and the following page will appear. Click the Login with GitHub button and you will be presented with the following page to enter you GitHub details. Note that if you are already logged into GitHub in your browser then you will likely not see this page. After a successful login you will then be redirected to the Pipeline homepage.","title":"Logging In"},{"location":"using/access/#troubleshooting","text":"Failures will commonly be caused by the user not being a member of the correct GitHub organisation or an error in the configuration of the login system by the administrator. Contact the administrator of the pipeline instance if you encounter problems logging in.","title":"Troubleshooting"},{"location":"using/addtorun/","text":"Adding Images to a Run \u00b6 This page describes how to add images to a completed run, including how to restore the run to the previous state if an addition goes wrong. Note Adding images to an existing run will update the sources already present from the respective run, such that the existing source IDs, comments, and tags are retained. If a full re-run was used instead then new IDs would be created and the comments and tags lost. There is no limit on how many times images may be added to a run. Step-by-step Guide \u00b6 Warning A run must have a Completed status before images can be added to it. No other settings other than the input data can be changed in the config. 1. Navigate to the Run Detail Page \u00b6 Navigate to the detail page of the run you wish to process, and confirm that the job is marked as Completed . 2. Add the New Images to the Configuration \u00b6 Scroll to the configuration editor, enter edit mode and add the new images to the existing data inputs. If using epoch mode notation, add a new epoch(s) to the file. Once all the images, selavy files, rms images and background images have been added, select the Write Current Config option to save the file. Warning Do not remove the previous images from the configuration inputs or change any other options! Remember to make sure the order of the new input data is consistent between types! 3. Perform a Config Validation \u00b6 Check that the configuration file is still valid by selecting Validate Config from the same menu as shown in the previous step 2 screenshot. 4. Process the Run \u00b6 Select the Add Images or Re-Process Run button at the top right of the run detail page to open the processing modal window. Select whether to turn debugging log output On or Off and when ready select the Schedule Run . Warning Do not toggle Full Re-Run to On ! You can refresh the page to check the status of the run. You can confirm that the images have been added correctly by consulting the log output found below the configuration file. New images should have been ingested and output similar to the following should be present: 2021-01-31 12:20:37,885 association INFO Association mode selected: basic. 2021-01-31 12:20:37,975 association INFO Found 2 images to add to the run. 2021-01-31 12:20:37,979 association INFO Association iteration: #1 Once the processing has Completed the run detail page will now show the updated statistics and information of the run. Restore Run to Pre-Add Version \u00b6 When images are added to a run, a backup is made of the run before proceeding which can be used to restore the run to the pre-addition version. For example, perhaps the wrong images were added or an error occured mid-addition that could not be resolved. This command is currently unavailable to perform through the website interface but can be performed by an administrator of the pipeline via the command line interface. Please contact the adminstrator for the pipeline instance to request a run restoration. The command to perform this operation is restorepiperun and is described in the admin section here . Warning Do not add any further images if you wish to restore otherwise the backup version will be lost!","title":"Adding Images to a Run"},{"location":"using/addtorun/#adding-images-to-a-run","text":"This page describes how to add images to a completed run, including how to restore the run to the previous state if an addition goes wrong. Note Adding images to an existing run will update the sources already present from the respective run, such that the existing source IDs, comments, and tags are retained. If a full re-run was used instead then new IDs would be created and the comments and tags lost. There is no limit on how many times images may be added to a run.","title":"Adding Images to a Run"},{"location":"using/addtorun/#step-by-step-guide","text":"Warning A run must have a Completed status before images can be added to it. No other settings other than the input data can be changed in the config.","title":"Step-by-step Guide"},{"location":"using/addtorun/#1-navigate-to-the-run-detail-page","text":"Navigate to the detail page of the run you wish to process, and confirm that the job is marked as Completed .","title":"1. Navigate to the Run Detail Page"},{"location":"using/addtorun/#2-add-the-new-images-to-the-configuration","text":"Scroll to the configuration editor, enter edit mode and add the new images to the existing data inputs. If using epoch mode notation, add a new epoch(s) to the file. Once all the images, selavy files, rms images and background images have been added, select the Write Current Config option to save the file. Warning Do not remove the previous images from the configuration inputs or change any other options! Remember to make sure the order of the new input data is consistent between types!","title":"2. Add the New Images to the Configuration"},{"location":"using/addtorun/#3-perform-a-config-validation","text":"Check that the configuration file is still valid by selecting Validate Config from the same menu as shown in the previous step 2 screenshot.","title":"3. Perform a Config Validation"},{"location":"using/addtorun/#4-process-the-run","text":"Select the Add Images or Re-Process Run button at the top right of the run detail page to open the processing modal window. Select whether to turn debugging log output On or Off and when ready select the Schedule Run . Warning Do not toggle Full Re-Run to On ! You can refresh the page to check the status of the run. You can confirm that the images have been added correctly by consulting the log output found below the configuration file. New images should have been ingested and output similar to the following should be present: 2021-01-31 12:20:37,885 association INFO Association mode selected: basic. 2021-01-31 12:20:37,975 association INFO Found 2 images to add to the run. 2021-01-31 12:20:37,979 association INFO Association iteration: #1 Once the processing has Completed the run detail page will now show the updated statistics and information of the run.","title":"4. Process the Run"},{"location":"using/addtorun/#restore-run-to-pre-add-version","text":"When images are added to a run, a backup is made of the run before proceeding which can be used to restore the run to the pre-addition version. For example, perhaps the wrong images were added or an error occured mid-addition that could not be resolved. This command is currently unavailable to perform through the website interface but can be performed by an administrator of the pipeline via the command line interface. Please contact the adminstrator for the pipeline instance to request a run restoration. The command to perform this operation is restorepiperun and is described in the admin section here . Warning Do not add any further images if you wish to restore otherwise the backup version will be lost!","title":"Restore Run to Pre-Add Version"},{"location":"using/deleterun/","text":"Deleting a Run \u00b6 Currently a run can only be deleted by an administrator via the command line interface, and is not able to be performed using the website. Please contact the administrator if you wish to delete a run. Administrators can refer to the clearpiperun command for details on how to reset a pipeline run.","title":"Deleting a Run"},{"location":"using/deleterun/#deleting-a-run","text":"Currently a run can only be deleted by an administrator via the command line interface, and is not able to be performed using the website. Please contact the administrator if you wish to delete a run. Administrators can refer to the clearpiperun command for details on how to reset a pipeline run.","title":"Deleting a Run"},{"location":"using/initrun/","text":"Initialising a Pipeline Run \u00b6 This page outlines the steps required to create a pipeline run through the web interface. A description of the run configuration options can be found in the next section . Note Administrators please refer to this section in the admin documentation for instructions on how to initialise a pipeline run via the command line interface. Warning No data quality control is performed by the pipeline. Make sure your input data is clean and error free before processing using your preferred method. Step-by-step Guide \u00b6 1. Navigate to the Pipeline Runs Overview Page \u00b6 Navigate to the Pipeline Runs overview page by clicking on the Pipeline Runs option in the left hand side navigation bar, as highlighted below. 2. Select the New Pipeline Run Option \u00b6 From the Pipeline Runs overview page, select the New Pipeline Run button as highlighted in the screenshot below. This will open up a modal window to begin the run initialisation process. 3. Fill in the Run Details \u00b6 Fill in the name and description of the run and then press next to navigate to the next form to enter and select the configuration options. For full details on how to configure a pipeline run see the Run Configuration page, but a few notes here: Any settings entered here are not final, they can still be changed once the run is created. The order of the input files must match between the data types - i.e. the first selavy file, rms image and background image must all be the products of the first image, and so on. If you have a high number of images, selavy files, rms images and background images, it may be easier to leave these empty and instead use the text editor on the run detail page to directly enter the list to the configuration file. By default, non-admin users have a 200 image limit. Once you have finished filling in the configuration options, press the create button. 4. The Run Detail Page \u00b6 After pressing create, the run will be initialised in the pipeline, which means the required configuration files have been created and the run is ready to be processed. You will be navigated to the detail page of the created run (shown below). On this page you can: View details of the run. View and edit the configuration file. Leave a comment about the run. View tables of associated images and measurements (once processed). Submit the run to be processed. For full details on: how to use the text editor to edit the configuration file see Run Configuration , how to submit the job to be processed see Processing a Run , and how to add images to a run that has already been processed Adding Images to a Run .","title":"Initialising a Run"},{"location":"using/initrun/#initialising-a-pipeline-run","text":"This page outlines the steps required to create a pipeline run through the web interface. A description of the run configuration options can be found in the next section . Note Administrators please refer to this section in the admin documentation for instructions on how to initialise a pipeline run via the command line interface. Warning No data quality control is performed by the pipeline. Make sure your input data is clean and error free before processing using your preferred method.","title":"Initialising a Pipeline Run"},{"location":"using/initrun/#step-by-step-guide","text":"","title":"Step-by-step Guide"},{"location":"using/initrun/#1-navigate-to-the-pipeline-runs-overview-page","text":"Navigate to the Pipeline Runs overview page by clicking on the Pipeline Runs option in the left hand side navigation bar, as highlighted below.","title":"1. Navigate to the Pipeline Runs Overview Page"},{"location":"using/initrun/#2-select-the-new-pipeline-run-option","text":"From the Pipeline Runs overview page, select the New Pipeline Run button as highlighted in the screenshot below. This will open up a modal window to begin the run initialisation process.","title":"2. Select the New Pipeline Run Option"},{"location":"using/initrun/#3-fill-in-the-run-details","text":"Fill in the name and description of the run and then press next to navigate to the next form to enter and select the configuration options. For full details on how to configure a pipeline run see the Run Configuration page, but a few notes here: Any settings entered here are not final, they can still be changed once the run is created. The order of the input files must match between the data types - i.e. the first selavy file, rms image and background image must all be the products of the first image, and so on. If you have a high number of images, selavy files, rms images and background images, it may be easier to leave these empty and instead use the text editor on the run detail page to directly enter the list to the configuration file. By default, non-admin users have a 200 image limit. Once you have finished filling in the configuration options, press the create button.","title":"3. Fill in the Run Details"},{"location":"using/initrun/#4-the-run-detail-page","text":"After pressing create, the run will be initialised in the pipeline, which means the required configuration files have been created and the run is ready to be processed. You will be navigated to the detail page of the created run (shown below). On this page you can: View details of the run. View and edit the configuration file. Leave a comment about the run. View tables of associated images and measurements (once processed). Submit the run to be processed. For full details on: how to use the text editor to edit the configuration file see Run Configuration , how to submit the job to be processed see Processing a Run , and how to add images to a run that has already been processed Adding Images to a Run .","title":"4. The Run Detail Page"},{"location":"using/processrun/","text":"Processing a Run \u00b6 This page describes how to submit a pipeline run for processing. Note Administrators please refer to this section in the admin documentation for instructions on how to process a pipeline run via the command line interface. Tip Use the editor window on the run detail page to make adjustments to the run configuration file before processing. Step-by-step Guide \u00b6 1. Navigate to the Run Detail Page \u00b6 Navigate to the detail page of the run you wish to process. 2. Run a config validation \u00b6 Before processing it is recommended to check that the configuration file is valid. This is done by scrolling down to the config file card on the page and selecting the Validate Config option accessed by clicking the three dots menu button. Doing this will check if the configuration contains any errors prior to processing. Feedback will be provided on whether the configuration file is valid. In the event of an error, this can be corrected by using the edit option found in the same menu. 3. Confirm Processing \u00b6 With a successful configuration validation, scroll back up to the top of the page and click the Process Run button. This will open a modal window for you to confirm processing. For a newly initialised run, the only option that requires attention is whether to toggle the Debug Log Output on. This can be helpful if processing a new set of images which the pipeline hasn't seen before. If you are processing a run that has errored in the initial processing then the Full Re-Run option should be toggled to On . Once ready, press the Schedule Run button which will send the run to the queue for processing. Warning For non-admin users, by default there is a run image limit of 200. Monitoring the Run \u00b6 You can check the status of the run by refreshing the run detail page and seeing if the Run Status field has been updated. You can also check the log output by scrolling down to the log file card found below the configuration file. There is currently no automated notification on completion or errors. Full Re-Run \u00b6 A full re-run will be required if the run configuration needs to be changed, or in the event that an initial run has errored. Warning The Full Re-Run option will remove all associated existing data for that run. Note If images have been added to a run and the processing errors, there is a one time undo option that may avoid having to use the Full Re-Run command. Adding Images to a Run \u00b6 See the dedicated documentation page here .","title":"Processing a Run"},{"location":"using/processrun/#processing-a-run","text":"This page describes how to submit a pipeline run for processing. Note Administrators please refer to this section in the admin documentation for instructions on how to process a pipeline run via the command line interface. Tip Use the editor window on the run detail page to make adjustments to the run configuration file before processing.","title":"Processing a Run"},{"location":"using/processrun/#step-by-step-guide","text":"","title":"Step-by-step Guide"},{"location":"using/processrun/#1-navigate-to-the-run-detail-page","text":"Navigate to the detail page of the run you wish to process.","title":"1. Navigate to the Run Detail Page"},{"location":"using/processrun/#2-run-a-config-validation","text":"Before processing it is recommended to check that the configuration file is valid. This is done by scrolling down to the config file card on the page and selecting the Validate Config option accessed by clicking the three dots menu button. Doing this will check if the configuration contains any errors prior to processing. Feedback will be provided on whether the configuration file is valid. In the event of an error, this can be corrected by using the edit option found in the same menu.","title":"2. Run a config validation"},{"location":"using/processrun/#3-confirm-processing","text":"With a successful configuration validation, scroll back up to the top of the page and click the Process Run button. This will open a modal window for you to confirm processing. For a newly initialised run, the only option that requires attention is whether to toggle the Debug Log Output on. This can be helpful if processing a new set of images which the pipeline hasn't seen before. If you are processing a run that has errored in the initial processing then the Full Re-Run option should be toggled to On . Once ready, press the Schedule Run button which will send the run to the queue for processing. Warning For non-admin users, by default there is a run image limit of 200.","title":"3. Confirm Processing"},{"location":"using/processrun/#monitoring-the-run","text":"You can check the status of the run by refreshing the run detail page and seeing if the Run Status field has been updated. You can also check the log output by scrolling down to the log file card found below the configuration file. There is currently no automated notification on completion or errors.","title":"Monitoring the Run"},{"location":"using/processrun/#full-re-run","text":"A full re-run will be required if the run configuration needs to be changed, or in the event that an initial run has errored. Warning The Full Re-Run option will remove all associated existing data for that run. Note If images have been added to a run and the processing errors, there is a one time undo option that may avoid having to use the Full Re-Run command.","title":"Full Re-Run"},{"location":"using/processrun/#adding-images-to-a-run","text":"See the dedicated documentation page here .","title":"Adding Images to a Run"},{"location":"using/runconfig/","text":"Run Configuration \u00b6 This page gives an overview of the configuration options available for a pipeline run. Default Configuration File \u00b6 Below is an example of a default config.py file. Note that no images or other input files have been provided. The file can be either edited directly or through the editor available on the run detail page. # This file specifies the pipeline configuration for the current pipeline run. # You should review these settings before processing any images - some of the default # values will probably not be appropriate. import os # path of the pipeline run PIPE_RUN_PATH = os . path . dirname ( os . path . realpath ( __file__ )) # Images settings # NOTE: all the paths !!!MUST!!! match with each other, e.g. # IMAGE_FILES[0] image matches SELAVY_FILES[0] file IMAGE_FILES = [ # insert images file path(s) here ] # Selavy catalogue files SELAVY_FILES = [ # insert Selavy file path(s) here ] # Noise or RMS files NOISE_FILES = [ # insert RMS file path(s) here ] # background map files BACKGROUND_FILES = [ # insert background map file path(s) here ] ### # SOURCE FINDER OPTIONS ### # source finder used for this pipeline run SOURCE_FINDER = 'selavy' ### # SOURCE MONITORING OPTIONS ### # Source monitoring can be done both forward and backward in 'time'. # Monitoring backward means re-opening files that were previously processed and can be slow. MONITOR = False # MONITOR_MIN_SIGMA defines the minimum SNR ratio a source has to be if it was placed in the # area of minimum rms in the image from which it is to be extracted from. If lower than this # value it is skipped MONITOR_MIN_SIGMA = 3.0 # MONITOR_EDGE_BUFFER_SCALE is a multiplicative scaling factor to the buffer size of the # forced photometry from the image edge. MONITOR_EDGE_BUFFER_SCALE = 1.2 # MONITOR_CLUSTER_THRESHOLD represents the cluster_threshold parameter used in the forced # extraction. If unsure leave as default. MONITOR_CLUSTER_THRESHOLD = 3.0 # MONITOR_ALLOW_NAN governs whether a source is attempted to be fit even if there are NaN's # present in the rms or background maps. MONITOR_ALLOW_NAN = False # The position uncertainty is in reality a combination of the fitting errors and the # astrometric uncertainty of the image/survey/instrument. # These two uncertainties are combined in quadrature. # These two parameters are the astrometric uncertainty in ra/dec and they may be different ASTROMETRIC_UNCERTAINTY_RA = 1.0 # arcsec ASTROMETRIC_UNCERTAINTY_DEC = 1.0 # arcsec ### # OPTIONS THAT CONTROL THE SOURCE ASSOCIATION ### ASSOCIATION_METHOD = 'basic' # 'basic', 'advanced' or 'deruiter' # options that apply to basic and advanced association ASSOCIATION_RADIUS = 10.0 # arcsec, basic and advanced only # options that apply to deruiter association ASSOCIATION_DE_RUITER_RADIUS = 5.68 # unitless, deruiter only ASSOCIATION_BEAMWIDTH_LIMIT = 1.5 # multiplicative factor, deruiter only # If ASSOCIATION_PARALLEL is set to 'True' then the input images will be split into # 'sky region groups' and association run on these groups in parallel and combined at the end. # Setting to 'True' is best used when you have a large dataset with multiple patches of the sky, # for smaller searches of only 3 or below sky regions it is recommened to keep as 'False'. ASSOCIATION_PARALLEL = False # If images have been submitted in epoch dictionaries then an attempt will be made by the pipeline to # remove duplicate sources. To do this a crossmatch is made between catalgoues to match 'the same' # measurements from different catalogues. This parameter governs the distance for which a match is made. # Default is 2.5 arcsec (which is typically 1 pixel in ASKAP images). ASSOCIATION_EPOCH_DUPLICATE_RADIUS = 2.5 # arcsec ### # OPTIONS THAT CONTROL THE NEW SOURCE ANALYSIS ### # controls whether a source is labelled as a new source. The source in question # must meet the requirement of: # MIN_NEW_SOURCE_SIGMA > (source_peak_flux / lowest_previous_image_min_rms) NEW_SOURCE_MIN_SIGMA = 5.0 # Default survey. # Used by the website for analysis plots. DEFAULT_SURVEY = None # 'NVSS' # Minimum error to apply to all flux measurements. The actual value used will be the measured/ # reported value or this value, whichever is greater. # This is a fraction, 0 = No minimum error FLUX_PERC_ERROR = 0.0 #percent 0.05 is 5% # Replace the selavy errors with USE_CONDON_ERRORS = True # Sometimes the local rms for a source is reported as 0 by selavy. # Choose a value to use for the local rms in these cases SELAVY_LOCAL_RMS_ZERO_FILL_VALUE = 0.2 # mJy # Create 'measurements.arrow' and 'measurement_pairs.arrow' files at the end of # a successful run CREATE_MEASUREMENTS_ARROW_FILES = False # Hide astropy warnings SUPPRESS_ASTROPY_WARNINGS = True # Only measurement pairs where the Vs metric exceeds this value are selected for the # aggregate pair metrics that are stored in Source objects. SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS = 4.3 Configuration Options \u00b6 Images and Selavy Files \u00b6 IMAGE_FILES List or Dictionary. The full paths to the image FITS files to be processed. Also accepts a dictionary format that will activate epoch mode (see Epoch Based Association ) in which case all inputs must also be in dictionary format. The order of the entries must be consistent with the other input types. Normal mode IMAGE_FILES = [ \"/full/path/to/image1.fits\" , \"/full/path/to/image2.fits\" , \"/full/path/to/image3.fits\" ] Epoch mode IMAGE_FILES = { \"epoch01\" : [ \"/full/path/to/image1.fits\" , \"/full/path/to/image2.fits\" ], \"epoch02\" : [ \"/full/path/to/image3.fits\" ], } Tip Use glob to easily list a large amount of files. E.g. from glob import glob ... IMAGE_FILES = glob ( \"/full/path/to/image*.fits\" ) This can also be applied to any of the input options below. SELAVY_FILES List or Dictionary. The full paths to the selavy text files to be processed. Also accepts a dictionary format that will activate epoch mode (see Epoch Based Association ) in which case all inputs must also be in dictionary format. The order of the entries must be consistent with the other input types. Normal mode SELAVY_FILES = [ \"/full/path/to/image1_selavy.txt\" , \"/full/path/to/image2_selavy.txt\" , \"/full/path/to/image3_selavy.txt\" ] Epoch mode SELAVY_FILES = { \"epoch01\" : [ \"/full/path/to/image1_selavy.txt\" , \"/full/path/to/image2_selavy.txt\" ], \"epoch02\" : [ \"/full/path/to/image3_selavy.txt\" ], } NOISE_FILES List or Dictionary. The full paths to the image noise (RMS) FITS files to be processed. Also accepts a dictionary format that will activate epoch mode (see Epoch Based Association ) in which case all inputs must also be in dictionary format. The order of the entries must be consistent with the other input types. Normal mode NOISE_FILES = [ \"/full/path/to/image1_rms.fits\" , \"/full/path/to/image2_rms.fits\" , \"/full/path/to/image3_rms.fits\" ] Epoch mode NOISE_FILES = { \"epoch01\" : [ \"/full/path/to/image1_rms.fits\" , \"/full/path/to/image2_rms.fits\" ], \"epoch02\" : [ \"/full/path/to/image3_rms.fits\" ], } BACKGROUND_FILES List or Dictionary. The full paths to the image background (mean) FITS files to be processed. Also accepts a dictionary format that will activate epoch mode (see Epoch Based Association ) in which case all inputs must also be in dictionary format. The order of the entries must be consistent with the other input types. Only required to be defined if MONITOR is set to True . Normal mode BACKGROUND_FILES = [ \"/full/path/to/image1_bkg.fits\" , \"/full/path/to/image2_bkg.fits\" , \"/full/path/to/image3_bkg.fits\" ] Epoch mode BACKGROUND_FILES = { \"epoch01\" : [ \"/full/path/to/image1_bkg.fits\" , \"/full/path/to/image2_bkg.fits\" ], \"epoch02\" : [ \"/full/path/to/image3_bkg.fits\" ], } Source Finder Format \u00b6 SOURCE_FINDER String. Signifies the format of the source finder text file read by the pipeline. Currently only supports \"selavy\" . Warning Source finding is not performed by the pipeline and must be completed prior to processing. Source Monitoring \u00b6 MONITOR Boolean. Turns on or off forced extractions for non detections. If set to True then BACKGROUND_IMAGES must also be defined. Defaults to False . MONITOR_MIN_SIGMA Float. For forced extractions to be performed they must meet a minimum signal-to-noise threshold with respect to the minimum rms value of the respective image. If the proposed forced measurement does not meet the threshold then it is not performed. I.e. \\[ \\frac{f_{peak}}{\\text{rms}_{image, min}} \\geq \\small{\\text{MONITOR_MIN_SIGMA}}\\text{,} \\] where \\(f_{peak}\\) is the initial detection peak flux measurement of the source in question and \\(\\text{rms}_{image, min}\\) is the minimum RMS value of the image where the forced extraction is to take place. Defaults to 3.0 . MONITOR_EDGE_BUFFER_SCALE Float. Monitor forced extractions are not performed when the location is within 3 beamwidths of the image edge. This parameter scales this distance by the value set, which can help avoid errors when the 3 beamwidth limit is insufficient to avoid extraction failures. Defaults to 1.2. MONITOR_CLUSTER_THRESHOLD Float. A argument directly passed to the forced photometry package used by the pipeline. It defines the multiple of major_axes to use for identifying clusters. Defaults to 3.0. MONITOR_ALLOW_NAN Boolean. A argument directly passed to the forced photometry package used by the pipeline. It defines whether NaN values are allowed to be present in the extraction area in the rms or background maps. True would mean that NaN values are allowed. Defaults to False. Association \u00b6 Tip Refer to the association documentation for full details on the association methods. ASTROMETRIC_UNCERTAINTY_RA Float. Defines an uncertainty error to the RA that will be added in quadrature to the existing source extraction error. Used to represent a systematic positional error. Unit is arcseconds. Defaults to 1.0. ASTROMETRIC_UNCERTAINTY_DEC Float. Defines an uncertainty error to the Dec that will be added in quadrature to the existing source extraction error. Used to represent systematic positional error. Unit is arcseconds. Defaults to 1.0. ASSOCIATION_METHOD String. Select whether to use the basic , advanced or deruiter association method, entered as a string of the method name. Defaults to \"basic\" . ASSOCIATION_RADIUS Float. The distance limit to use during basic and advanced association. Unit is arcseconds. Defaults to 10.0 . ASSOCIATION_DE_RUITER_RADIUS Float. The de Ruiter radius limit to use during deruiter association only. The parameter is unitless. Defaults to 5.68 . ASSOCIATION_BEAMWIDTH_LIMIT Float. The beamwidth limit to use during deruiter association only. Multiplicative factor. Defaults to 1.5 . ASSOCIATION_PARALLEL Boolean. When True , association is performed in parallel on non-overlapping groups of sky regions. Defaults to False . ASSOCIATION_EPOCH_DUPLICATE_RADIUS Float. Applies to epoch based association only. Defines the limit at which a duplicate source is identified. Unit is arcseconds. Defaults to 2.5 (commonly one pixel for ASKAP images). New Sources \u00b6 NEW_SOURCE_MIN_SIGMA Float. Defines the limit at which a source is classed as a new source based upon the would-be significance of detections in previous images where no detection was made. I.e. \\[ \\frac{f_{peak}}{\\text{rms}_{image, min}} \\geq \\small{\\text{NEW_SOURCE_MIN_SIGMA}}\\text{,} \\] where \\(f_{peak}\\) is the initial detection peak flux measurement of the source in question and \\(\\text{rms}_{image, min}\\) is the minimum RMS value of the previous image(s) where no detection was made. If the requirement is met in any previous image then the source is flagged as new. Defaults to 5.0 . General \u00b6 DEFAULT_SURVEY Currently not used hence this option can be safely ignored. FLUX_PERC_ERROR Define a percentage flux error that will be added in quadrature to the extracted sources. Note that this will be reflected in the final source statistics and will not be applied directly to the measurements. Entered as a float between 0 - 1.0 which represents 0 - 100%. Defaults to 0.0 . USE_CONDON_ERRORS Boolean. Calculate the Condon errors of the extractions when read in from the source extraction file. If False then the errors directly from the source finder output are used. Recommended to set to True for selavy extractions. Defaults to True . SELAVY_LOCAL_RMS_ZERO_FILL_VALUE Float. Value to substitute for the local_rms parameter in selavy extractions if a 0.0 value is found. Unit is mJy. Defaults to 0.2 . CREATE_MEASUREMENTS_ARROW_FILES Boolean. When True then two arrow format files are produced: measurements.arrow - an arrow file containing all the measurements associated with the run. measurement_pairs.arrow - an arrow file containing the measurement pairs information pre-merged with extra information from the measurements. Producing these files for large runs (200+ images) is recommended for post-processing. Defaults to False . Note The arrow files can be produced after the run has completed by an administrator. SUPPRESS_ASTROPY_WARNINGS Boolean. Astropy warnings are suppressed in the logging output if set to True . Defaults to True . SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS Float. Defines the minimum Vs two-epoch metric value threshold used to attach the most significant pair value to the source. Defaults to 4.3 .","title":"Run Configuration"},{"location":"using/runconfig/#run-configuration","text":"This page gives an overview of the configuration options available for a pipeline run.","title":"Run Configuration"},{"location":"using/runconfig/#default-configuration-file","text":"Below is an example of a default config.py file. Note that no images or other input files have been provided. The file can be either edited directly or through the editor available on the run detail page. # This file specifies the pipeline configuration for the current pipeline run. # You should review these settings before processing any images - some of the default # values will probably not be appropriate. import os # path of the pipeline run PIPE_RUN_PATH = os . path . dirname ( os . path . realpath ( __file__ )) # Images settings # NOTE: all the paths !!!MUST!!! match with each other, e.g. # IMAGE_FILES[0] image matches SELAVY_FILES[0] file IMAGE_FILES = [ # insert images file path(s) here ] # Selavy catalogue files SELAVY_FILES = [ # insert Selavy file path(s) here ] # Noise or RMS files NOISE_FILES = [ # insert RMS file path(s) here ] # background map files BACKGROUND_FILES = [ # insert background map file path(s) here ] ### # SOURCE FINDER OPTIONS ### # source finder used for this pipeline run SOURCE_FINDER = 'selavy' ### # SOURCE MONITORING OPTIONS ### # Source monitoring can be done both forward and backward in 'time'. # Monitoring backward means re-opening files that were previously processed and can be slow. MONITOR = False # MONITOR_MIN_SIGMA defines the minimum SNR ratio a source has to be if it was placed in the # area of minimum rms in the image from which it is to be extracted from. If lower than this # value it is skipped MONITOR_MIN_SIGMA = 3.0 # MONITOR_EDGE_BUFFER_SCALE is a multiplicative scaling factor to the buffer size of the # forced photometry from the image edge. MONITOR_EDGE_BUFFER_SCALE = 1.2 # MONITOR_CLUSTER_THRESHOLD represents the cluster_threshold parameter used in the forced # extraction. If unsure leave as default. MONITOR_CLUSTER_THRESHOLD = 3.0 # MONITOR_ALLOW_NAN governs whether a source is attempted to be fit even if there are NaN's # present in the rms or background maps. MONITOR_ALLOW_NAN = False # The position uncertainty is in reality a combination of the fitting errors and the # astrometric uncertainty of the image/survey/instrument. # These two uncertainties are combined in quadrature. # These two parameters are the astrometric uncertainty in ra/dec and they may be different ASTROMETRIC_UNCERTAINTY_RA = 1.0 # arcsec ASTROMETRIC_UNCERTAINTY_DEC = 1.0 # arcsec ### # OPTIONS THAT CONTROL THE SOURCE ASSOCIATION ### ASSOCIATION_METHOD = 'basic' # 'basic', 'advanced' or 'deruiter' # options that apply to basic and advanced association ASSOCIATION_RADIUS = 10.0 # arcsec, basic and advanced only # options that apply to deruiter association ASSOCIATION_DE_RUITER_RADIUS = 5.68 # unitless, deruiter only ASSOCIATION_BEAMWIDTH_LIMIT = 1.5 # multiplicative factor, deruiter only # If ASSOCIATION_PARALLEL is set to 'True' then the input images will be split into # 'sky region groups' and association run on these groups in parallel and combined at the end. # Setting to 'True' is best used when you have a large dataset with multiple patches of the sky, # for smaller searches of only 3 or below sky regions it is recommened to keep as 'False'. ASSOCIATION_PARALLEL = False # If images have been submitted in epoch dictionaries then an attempt will be made by the pipeline to # remove duplicate sources. To do this a crossmatch is made between catalgoues to match 'the same' # measurements from different catalogues. This parameter governs the distance for which a match is made. # Default is 2.5 arcsec (which is typically 1 pixel in ASKAP images). ASSOCIATION_EPOCH_DUPLICATE_RADIUS = 2.5 # arcsec ### # OPTIONS THAT CONTROL THE NEW SOURCE ANALYSIS ### # controls whether a source is labelled as a new source. The source in question # must meet the requirement of: # MIN_NEW_SOURCE_SIGMA > (source_peak_flux / lowest_previous_image_min_rms) NEW_SOURCE_MIN_SIGMA = 5.0 # Default survey. # Used by the website for analysis plots. DEFAULT_SURVEY = None # 'NVSS' # Minimum error to apply to all flux measurements. The actual value used will be the measured/ # reported value or this value, whichever is greater. # This is a fraction, 0 = No minimum error FLUX_PERC_ERROR = 0.0 #percent 0.05 is 5% # Replace the selavy errors with USE_CONDON_ERRORS = True # Sometimes the local rms for a source is reported as 0 by selavy. # Choose a value to use for the local rms in these cases SELAVY_LOCAL_RMS_ZERO_FILL_VALUE = 0.2 # mJy # Create 'measurements.arrow' and 'measurement_pairs.arrow' files at the end of # a successful run CREATE_MEASUREMENTS_ARROW_FILES = False # Hide astropy warnings SUPPRESS_ASTROPY_WARNINGS = True # Only measurement pairs where the Vs metric exceeds this value are selected for the # aggregate pair metrics that are stored in Source objects. SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS = 4.3","title":"Default Configuration File"},{"location":"using/runconfig/#configuration-options","text":"","title":"Configuration Options"},{"location":"using/runconfig/#images-and-selavy-files","text":"IMAGE_FILES List or Dictionary. The full paths to the image FITS files to be processed. Also accepts a dictionary format that will activate epoch mode (see Epoch Based Association ) in which case all inputs must also be in dictionary format. The order of the entries must be consistent with the other input types. Normal mode IMAGE_FILES = [ \"/full/path/to/image1.fits\" , \"/full/path/to/image2.fits\" , \"/full/path/to/image3.fits\" ] Epoch mode IMAGE_FILES = { \"epoch01\" : [ \"/full/path/to/image1.fits\" , \"/full/path/to/image2.fits\" ], \"epoch02\" : [ \"/full/path/to/image3.fits\" ], } Tip Use glob to easily list a large amount of files. E.g. from glob import glob ... IMAGE_FILES = glob ( \"/full/path/to/image*.fits\" ) This can also be applied to any of the input options below. SELAVY_FILES List or Dictionary. The full paths to the selavy text files to be processed. Also accepts a dictionary format that will activate epoch mode (see Epoch Based Association ) in which case all inputs must also be in dictionary format. The order of the entries must be consistent with the other input types. Normal mode SELAVY_FILES = [ \"/full/path/to/image1_selavy.txt\" , \"/full/path/to/image2_selavy.txt\" , \"/full/path/to/image3_selavy.txt\" ] Epoch mode SELAVY_FILES = { \"epoch01\" : [ \"/full/path/to/image1_selavy.txt\" , \"/full/path/to/image2_selavy.txt\" ], \"epoch02\" : [ \"/full/path/to/image3_selavy.txt\" ], } NOISE_FILES List or Dictionary. The full paths to the image noise (RMS) FITS files to be processed. Also accepts a dictionary format that will activate epoch mode (see Epoch Based Association ) in which case all inputs must also be in dictionary format. The order of the entries must be consistent with the other input types. Normal mode NOISE_FILES = [ \"/full/path/to/image1_rms.fits\" , \"/full/path/to/image2_rms.fits\" , \"/full/path/to/image3_rms.fits\" ] Epoch mode NOISE_FILES = { \"epoch01\" : [ \"/full/path/to/image1_rms.fits\" , \"/full/path/to/image2_rms.fits\" ], \"epoch02\" : [ \"/full/path/to/image3_rms.fits\" ], } BACKGROUND_FILES List or Dictionary. The full paths to the image background (mean) FITS files to be processed. Also accepts a dictionary format that will activate epoch mode (see Epoch Based Association ) in which case all inputs must also be in dictionary format. The order of the entries must be consistent with the other input types. Only required to be defined if MONITOR is set to True . Normal mode BACKGROUND_FILES = [ \"/full/path/to/image1_bkg.fits\" , \"/full/path/to/image2_bkg.fits\" , \"/full/path/to/image3_bkg.fits\" ] Epoch mode BACKGROUND_FILES = { \"epoch01\" : [ \"/full/path/to/image1_bkg.fits\" , \"/full/path/to/image2_bkg.fits\" ], \"epoch02\" : [ \"/full/path/to/image3_bkg.fits\" ], }","title":"Images and Selavy Files"},{"location":"using/runconfig/#source-finder-format","text":"SOURCE_FINDER String. Signifies the format of the source finder text file read by the pipeline. Currently only supports \"selavy\" . Warning Source finding is not performed by the pipeline and must be completed prior to processing.","title":"Source Finder Format"},{"location":"using/runconfig/#source-monitoring","text":"MONITOR Boolean. Turns on or off forced extractions for non detections. If set to True then BACKGROUND_IMAGES must also be defined. Defaults to False . MONITOR_MIN_SIGMA Float. For forced extractions to be performed they must meet a minimum signal-to-noise threshold with respect to the minimum rms value of the respective image. If the proposed forced measurement does not meet the threshold then it is not performed. I.e. \\[ \\frac{f_{peak}}{\\text{rms}_{image, min}} \\geq \\small{\\text{MONITOR_MIN_SIGMA}}\\text{,} \\] where \\(f_{peak}\\) is the initial detection peak flux measurement of the source in question and \\(\\text{rms}_{image, min}\\) is the minimum RMS value of the image where the forced extraction is to take place. Defaults to 3.0 . MONITOR_EDGE_BUFFER_SCALE Float. Monitor forced extractions are not performed when the location is within 3 beamwidths of the image edge. This parameter scales this distance by the value set, which can help avoid errors when the 3 beamwidth limit is insufficient to avoid extraction failures. Defaults to 1.2. MONITOR_CLUSTER_THRESHOLD Float. A argument directly passed to the forced photometry package used by the pipeline. It defines the multiple of major_axes to use for identifying clusters. Defaults to 3.0. MONITOR_ALLOW_NAN Boolean. A argument directly passed to the forced photometry package used by the pipeline. It defines whether NaN values are allowed to be present in the extraction area in the rms or background maps. True would mean that NaN values are allowed. Defaults to False.","title":"Source Monitoring"},{"location":"using/runconfig/#association","text":"Tip Refer to the association documentation for full details on the association methods. ASTROMETRIC_UNCERTAINTY_RA Float. Defines an uncertainty error to the RA that will be added in quadrature to the existing source extraction error. Used to represent a systematic positional error. Unit is arcseconds. Defaults to 1.0. ASTROMETRIC_UNCERTAINTY_DEC Float. Defines an uncertainty error to the Dec that will be added in quadrature to the existing source extraction error. Used to represent systematic positional error. Unit is arcseconds. Defaults to 1.0. ASSOCIATION_METHOD String. Select whether to use the basic , advanced or deruiter association method, entered as a string of the method name. Defaults to \"basic\" . ASSOCIATION_RADIUS Float. The distance limit to use during basic and advanced association. Unit is arcseconds. Defaults to 10.0 . ASSOCIATION_DE_RUITER_RADIUS Float. The de Ruiter radius limit to use during deruiter association only. The parameter is unitless. Defaults to 5.68 . ASSOCIATION_BEAMWIDTH_LIMIT Float. The beamwidth limit to use during deruiter association only. Multiplicative factor. Defaults to 1.5 . ASSOCIATION_PARALLEL Boolean. When True , association is performed in parallel on non-overlapping groups of sky regions. Defaults to False . ASSOCIATION_EPOCH_DUPLICATE_RADIUS Float. Applies to epoch based association only. Defines the limit at which a duplicate source is identified. Unit is arcseconds. Defaults to 2.5 (commonly one pixel for ASKAP images).","title":"Association"},{"location":"using/runconfig/#new-sources","text":"NEW_SOURCE_MIN_SIGMA Float. Defines the limit at which a source is classed as a new source based upon the would-be significance of detections in previous images where no detection was made. I.e. \\[ \\frac{f_{peak}}{\\text{rms}_{image, min}} \\geq \\small{\\text{NEW_SOURCE_MIN_SIGMA}}\\text{,} \\] where \\(f_{peak}\\) is the initial detection peak flux measurement of the source in question and \\(\\text{rms}_{image, min}\\) is the minimum RMS value of the previous image(s) where no detection was made. If the requirement is met in any previous image then the source is flagged as new. Defaults to 5.0 .","title":"New Sources"},{"location":"using/runconfig/#general","text":"DEFAULT_SURVEY Currently not used hence this option can be safely ignored. FLUX_PERC_ERROR Define a percentage flux error that will be added in quadrature to the extracted sources. Note that this will be reflected in the final source statistics and will not be applied directly to the measurements. Entered as a float between 0 - 1.0 which represents 0 - 100%. Defaults to 0.0 . USE_CONDON_ERRORS Boolean. Calculate the Condon errors of the extractions when read in from the source extraction file. If False then the errors directly from the source finder output are used. Recommended to set to True for selavy extractions. Defaults to True . SELAVY_LOCAL_RMS_ZERO_FILL_VALUE Float. Value to substitute for the local_rms parameter in selavy extractions if a 0.0 value is found. Unit is mJy. Defaults to 0.2 . CREATE_MEASUREMENTS_ARROW_FILES Boolean. When True then two arrow format files are produced: measurements.arrow - an arrow file containing all the measurements associated with the run. measurement_pairs.arrow - an arrow file containing the measurement pairs information pre-merged with extra information from the measurements. Producing these files for large runs (200+ images) is recommended for post-processing. Defaults to False . Note The arrow files can be produced after the run has completed by an administrator. SUPPRESS_ASTROPY_WARNINGS Boolean. Astropy warnings are suppressed in the logging output if set to True . Defaults to True . SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS Float. Defines the minimum Vs two-epoch metric value threshold used to attach the most significant pair value to the source. Defaults to 4.3 .","title":"General"}]}