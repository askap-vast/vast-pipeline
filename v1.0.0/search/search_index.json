{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , with an added List of PRs section and links to the relevant PRs on the individual updates. This project adheres to Semantic Versioning . Unreleased \u00b6 Added \u00b6 Added the 0-based index of each measurement to the image cutout card headers #625 . Added Bokeh hover tooltip to measurement pair graph to display pair metrics #625 . Added new VAST surveys (13-21) to the Aladin Lite panel #622 . Added eta-V plot analysis page along with documentation #586 . Added thumbnails to light curve tooltips #586 . Added logfile dropdown selection to run detail page #595 . Added datetime stamps to all log files #595 . Added new log files for arrow file creation and restore run and added to run detail page #580 . Added restore run test #580 . Added new run status of DEL , Deleting #580 . Added documentation pages on new action buttons #580 . Added UI action buttons to run-detail page to allow arrow file generation, deletion and restoration #580 . Added try-except error capture on pre-run checks to correctly assign pipeline run as failed if an error occurs #576 . Added support for ingesting Selavy catalogues in VOTable (XML) and CSV format #565 Added new commands: initingest and ingestimages #544 Added documentation on the data required to run the pipeline #572 . Added support for ingesting Selavy catalogues in VOTable (XML) and CSV format #565 . Added new commands: initingest and ingestimages #544 . Added TNS cone search to the external search results on the source detail page #557 . Added HOME_DATA_ROOT to the pipeline settings to override the OS default home directory location #559 . Added processing spinner to source query table #551 . Added site_url to the mkdocs config so static asset URLs have the correct base URL #543 . Added basic linter to CI/CD #546 Changed \u00b6 Dependancies updated using npm audit fix (non-breaking) #620 . Refactored adding source to favourites button to use ajax to avoid page reload #614 . Bumped test python versions to 3.7.12, 3.8.12 and 3.9.10 #586 . Bumped various dependencies using a fresh poetry.lock file #586 . Bumped bokeh packages to 2.3.3 #586 . Django-Q config variable max_attempts is configurable in the .env file #595 . Replaced models.MeasurementPair model with a dataclass #590 . Django-Q config variables timeout and retry are configurable in the .env file #589 . Changed restore run command to only allow one run as input #580 . Changed existing documentation pages to reflect new buttons #580 . Moved creation of output backup files to occur before the config check #576 . Association test data updated with d2d fix #574 . Removed the timezone from the Timestamps being written to the the arrow file as this causes problems with vaex #571 . Reduced the memory footprint for computing the ideal source coverages by sky regions #555 . Gulp will only read webinterface/.env if the required vars are undefined in the current environment #548 . Fixed \u00b6 Fixed source names to be IAU compliant #618 . Fixed broken NED links for coordinates with many decimal places #623 . Added an error handler for the external source queries (e.g. SIMBAD) #616 . Stopped JS9 from changing the page titles #597 . Fixed regression issues with pandas 1.4 #586 . Fixed config being copied before run was confirmed to actually go ahead for existing runs #595 . Fixed forced measurements being removed from associations during the restore run process #600 . Fixed measurement FITS cutout bug #588 . Fixed removal of image and sky region objects when a run is deleted #585 . Fixed testing pandas equal deprecation warning #580 . Fixed restore run relations issue #580 . Fixed logic for full re-run requirement when UI run is being re-run from an error status #576 . Fixed d2d not being carried through the advanced association process #574 . Fixed old dictionary references in the documentation run config page #572 . Fixed a regression from pandas=1.3.0 that caused non-numeric columns to be dropped after a groupby sum operation #567 . Fixed permission error for regular users when trying to launch an initialised run #563 . Fixed outdated installation link in README #543 . Removed \u00b6 Removed the unique constraint on models.Measurement.name #583 . List of PRs \u00b6 #618 : fix: Produce IAU compliant source names. #625 : feat: Pair metrics hover tooltip. #620 : dep: Non-breaking npm audit fix update. #622 : feat: Updated Aladin VAST surveys. #623 : fix: fixed NED links. #616 : fix: added error handling to external queries. #614 : feat: Refactored add to favourites button to avoid refresh. #597 : fix: Update detail page titles. #586 : feat, dep, doc: Add an eta-v analysis page for the source query. #595 : fix: Add date and time stamp to log files. #600 : fix: Fixed restore run forced measurements associations. #590 : fix: Remove MeasurementPair model. #589 : fix: expose django-q timeout and retry to env vars. #588 : fix: change cutout endpoint to use measurement ID. #585 : fix: Clean up m2m related objects when deleting a run. #583 : fix: remove unique constraint from Measurement.name. #580 : feat, fix, doc: Added UI run action buttons. #576 : fix: Fixed UI re-run from errored status. #574 : fix: Fixed d2d assignment in advanced association. #572 : doc: Added required data page to documentation. #571 : fix: Removed timezone from measurements arrow file time column #565 : feat: added support for reading selavy VOTables and CSVs. #567 : fix: fixed pandas=1.3.0 groupby sum regression. #563 : fix: fixed launch run user permission bug. #544 : feat: new command to ingest images without running the full pipeline. #557 : feat: Add TNS external search for sources. #559 : feat: added HOME_DATA_ROOT setting. #555 : fix: compute ideal source coverage with astropy xmatch. #551 : feat: added processing spinner to source query table. #550 : fix: missing changelog entry #548 : fix: only read .env if required vars are undefined. #546 : feat, fix: remove unused imports, and added basic linter during CI/CD. #543 : fix, doc: Fix README link and documentation 404 assets. 1.0.0 (2021-05-21) \u00b6 Added \u00b6 When searching by source names, any \"VAST\" prefix on the name will be silently removed to make searching for published VAST sources easier #536 . Added acknowledgements and help section to docs #535 . Added vast_pipeline/_version.py to store the current software version and updated release documentation #532 . Added created and last updated dates to doc pages using mkdocs-git-revision-date-localized-plugin #514 . Added support for glob expressions when specifying input files in the run config file #504 Added DEFAULT_AUTO_FIELD to settings.py to silence Django 3.2 warnings #507 Added lightgallery support for all images in the documentation #494 . Added new entries in the documentation contributing section #494 . Added new entries in the documentation FAQ section #491 . Added new home page for documentation #491 . Added dark mode switch on documentation #487 . Added .env file information to documentation #487 . Added further epoch based association information to documentation page #487 . Added script to auto-generate code reference documentation pages #480 . Added code reference section to documentation #480 . Added new pages and sections to documentation #471 Added requirements/environment.yml so make it easier for Miniconda users to get the non-Python dependencies #472 . Added pyproject.toml and poetry.lock #472 . Added init-tools/init-db.py #472 . Added image add mode run restore command 'restorepiperun' #463 Added documentation folder and files for mkdocs and CI #433 Added add image to existing run feature #443 Added networkx to base reqiurements #460 . Added CI/CD workflow to run tests on pull requests #446 Added basic regression tests #425 Added image length validation for config #425 Changed \u00b6 Changed source naming convention to Jhhmmss.s(+/-)ddmmss to match VAST-P1 paper (Murphy, et al. 2021) convention #536 Updated npm packages to resolve dependabot security alert #533 . Updated homepage text to reflect new features and documentation #534 . Changed layout of source detail page #526 . Updated mkdocs-material to 7.1.4 for native creation date support #518 . Updated developing docs to specify the main development branch as dev instead of master #521 . Updated tests to account for relation fix #510 . All file examples in docs are now enclosed in an example admonition #494 . Further changes to layout of documentation #494 . Changed arrow file generation from vaex to pyarrow #503 . Changed layout of documentation to use tabs #491 . Dependabot: Bump y18n from 3.2.1 to 3.2.2 #482 . Replaced run config .py format with .yaml #483 . Changed docs VAST logo to icon format to avoid stretched appearence #487 . Bumped Browsersync from 2.26.13 to 2.26.14 #481 . Dependabot: Bump prismjs from 1.22.0 to 1.23.0 #469 . Changed non-google format docstrings to google format #480 . Changed some documentation layout and updated content #471 . Changed the vaex dependency to vaex-arrow #472 . Set CREATE_MEASUREMENTS_ARROW_FILES = True in the basic association test config #472 . Bumped minimum Python version to 3.7.1 #472 . Replaced npm package gulp-sass with @mr-hope/gulp-sass , a fork which drops the dependency on the deprecated node-sass which is difficult to install #472 . Changed the installation documentation to instruct users to use a PostgreSQL Docker image with Q3C already installed #472 . Changed 'cmd' flag in run pipeline to 'cli' #466 . Changed CONTRIBUTING.md and README.md #433 Changed forced extraction name suffix to run id rather than datetime #443 Changed tests to run on smaller cutouts #443 Changed particles style on login page #459 . Dependabot: Bump ini from 1.3.5 to 1.3.8 #436 Fixed \u00b6 Fixed the broken link to the image detail page on measurement detail pages #528 . Fixed simbad and ned external search results table nan values #523 . Fixed inaccurate total results reported by some paginators #517 . Removed excess whitespace from coordinates that get copied to the clipboard #515 Fixed rogue relations being created during one-to-many functions #510 . Fixed JS9 regions so that the selected source components are always on top #508 Fixed docstring in config.py #494 . Fixed arrow files being generated via the website #503 . Fixed a bug that returned all sources when performing a cone search where one of the coords = 0 #501 Fixed the missing hover tool for lightcurve plots of non-variable sources #493 Fixed the default Dask multiprocessing context to \"fork\" #472 . Fixed Selavy catalogue ingest to discard the unit row before reading the data #473 . Fixed initial job processing from the UI #466 . Fixed links in README.md #464 . Fixed basic association new sources created through relations #443 Fixed tests running pipeline multiple times #443 Fixed particles canvas sizing on login page #459 . Fixed breadcrumb new line on small resolutitons #459 . Fixed config files in tests #430 Fixed sources table on measurement detail page #429 . Fixed missing meta columns in parallel association #427 . Removed \u00b6 Removed SURVEYS_WORKING_DIR from settings and env file #538 . Removed default_survey from run configuration file #538 . Removed importsurvey command and catalogue.py #538 . Removed SurveySource, Survey and SurveySourceQuerySet models #538 . Removed email and Slack links from docs footer #535 . Removed bootstrap as the required version is bundled with startbootstrap-sb-admin-2 #533 . Removed docs/readme.md softlink as it is no longer used #494 . Removed vaex-arrow from the dependancies #503 . Removed requirements/*.txt files. Development dependency management moved to Poetry #472 . Removed init-tools/init-db.sh #472 . Removed INSTALL.md , PROFILE.md and static/README.md #433 Removed aplpy from base requirements #460 . List of PRs \u00b6 #538 feat: Removed survey source models, commands and references. #536 feat: changed source naming convention. #535 doc: added help and acknowledgement doc page. #534 feat: Update homepage text. #532 feat, doc: Versioning. #533 dep: updated npm deps; removed bootstrap. #528 fix: fixed broken image detail link. #526 feat: Updated source detail page layout. #518 dep: Updated mkdocs-material for native creation date support. #523 fix: Fixed external search results table nan values. #521 doc: update doc related to default dev branch. #517 fix: pin djangorestframework-datatables to 0.5.1. #515 fix: remove linebreaks from coordinates. #514 dep: Added created and updated dates to doc pages. #510 fix: Fix rogue relations. #508 fix: Draw selected source components on top in JS9. #504 feat: Add glob expression support to yaml run config. #507 fix: set default auto field model. #494 doc, dep: Docs: Added lightgallery support, layout update, minor fixes and additions. #503 fix, dep: Change arrow file generation from vaex to pyarrow. #501 fix: fix broken cone search when coord = 0 #491 doc: Updated the docs layout, home page and FAQs. #493 fix: Fix bokeh hover tool for lightcurve plots. #482 dep: Bump y18n from 3.2.1 to 3.2.2. #483 feat: replace run config .py files with .yaml. #487 doc: Minor documentation improvements. #481 dep: Bump Browsersync from 2.26.13 to 2.26.14. #469 dep: Bump prismjs from 1.22.0 to 1.23.0. #480 feat: Code reference documentation update. #471 feat: Documentation update. #472 feat: Simplify install. #473 fix: discard the selavy unit row before reading. #466 fix: Fixed initial job processing from the UI. #463 feat: Added image add mode run restore command. #433 doc: add documentation GitHub pages website with CI. #443 feat, fix: Adds the ability to add images to an existing run. #460 dep: Removed aplpy from base requirements. #446 feat: CI/CD workflow. #459 fix: Fix particles and breadcrumb issues on mobile. #436 dep: Bump ini from 1.3.5 to 1.3.8. #430 fix: Test config files. #425 feat: Basic regression tests. #429 fix: Fixed sources table on measurement detail page. #427 fix: Fixed missing meta columns in parallel association. 0.2.0 (2020-11-30) \u00b6 Added \u00b6 Added a check in the UI running that the job is not already running or queued #421 . Added the deletion of all parquet and arrow files upon a re-run #421 . Added source selection by name or ID on source query page #401 . Added test cases #412 Added askap-vast/forced_phot to pip requirements #408 . Added pipeline configuration parameter, SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS , to filter measurement pairs before calculating aggregate metrics #407 . Added custom 404.html and 500.html templates for error pages #415 Added ability to export measurement_pairs.parqyet as an arrow file #393 . Added new fields to detail pages and source and measurement tables #406 . Added new fields to source query page (island flux ratio, min and max fluxes) #406 . Added min, max flux values to sources and agg min island flux ratio field #406 . Added island flux ratio column to measurements, component flux divided by total island flux (peak and int) #406 . Added a maximum number of images for runs through the UI #404 . Added the ability to run a pipeline run through the UI #404 . Added Queued status to the list of pipeline run statuses #404 . Added the dependancy django-q that enables scheduled tasks to be processed #404 . Added source tagging #396 . Added link to measurement table from the lightcurve to source detail page #387 . Added 'epoch based' parameter to pipeline run detail page #387 . Adds basic commenting functionality for sources, measurements, images, and runs #368 . Custom CSS now processed with Sass: Bootstrap and sb-admin-2 theme are compiled into a single stylesheet #370 . Added vast_pipeline/pipeline/generators.py which contains generator functions #382 . Range and NaN check on new source analysis to match forced extraction #374 . Added the ability for the pipeline to read in groups of images which are defined as a single epoch #277 . Added the ability of the pipeline to remove duplicated measurements from an epoch #277 . Added option to control separation measurements which are defined as a duplicate #277 . Added the ability of the pipeline to separate images to associate into unique sky region groups #277 . Added option to perform assocication of separate sky region groups in parallel #277 . Added new options to webinterface pipeline run creation #277 . Added epoch_based column run model #277 . Added links to tables and postage stamps on source detail page #379 . Updates image background_path from current run when not originally provided #377 . Added csv export button to datatables on webinterface #363 . Added support for Excel export button to datatables on webinterface (waiting on datatables buttons fix) #363 . Added column visibility button to datatables on webinterface #363 . Added dependancy datatables-buttons 1.6.4 #363 . Added dependancy jszip (required for Excel export) #363 . Adds n_selavy_measurements and n_forced_measurements to run model #362 . Adds steps to populate new measurement count fields in pipeline run #362 . Source order from the query is preserved on source detail view #364 . Setting HOME_DATA_DIR to specify a directory relative to the user's home directory to scan for FITS and text files to use in a Run initialised with the UI #361 . Adds a node graph to accompany the lightcurve that shows which measurement pairs exceed the default variability metric thresholds ( Vs >= 4.3 , |m| >= 0.26 ) #305 . Adds the MeasurementPair model to store two variability metrics for each flux type: Vs, the t-statistic; and m, the modulation index. The maximum of these metrics are also added to the Source model for joinless queries. These metrics are calculated during the pipeline run #305 . Adds radio buttons to change the lightcurve data points between peak and integrated fluxes #305 . Fills out information on all webinterface detail pages #345 . Adds frequency information the measurements and images webinterface tables. #345 . Adds celestial plot and tables to webinterface pipeline detail page #345 . Adds useful links to webinterface navbar #345 . Adds tool tips to webinterface source query #345 . Adds hash reading to webinterface source query to allow filling from URL hash parameters #345 . Add links to number cards on webinterface #345 . Added home icon on hover on webinterface #345 . Added copy-to-clipboard functionality on coordinates on webinterface #345 . Changed \u00b6 Renamed 'alert-wrapper' container to 'toast-wrapper' #419 . Changed alerts to use the Bootstrap toasts system #419 . Bumped some npm package versions to address dependabot security alerts #411 . Images table on pipeline run detail page changed to order by datetime by default #417 . Changed config argument CREATE_MEASUREMENTS_ARROW_FILE -> CREATE_MEASUREMENTS_ARROW_FILES #393 . Naming of average flux query fields to account for other min max flux fields #406 . Expanded README.md to include DjangoQ and UI job scheduling information #404 . Shifted alerts location to the top right #404 . Log file card now expanded by default on pipeline run detail page #404 . Changed user comments on source detail pages to incorporate tagging feature #396 . Updated RACS HiPS URL in Aladin #399 . Changed home page changelog space to welcome/help messages #387 . The comment field in the Run model has been renamed to description . A comment many-to-many relationship was added to permit user comments on Run instances #368 . Moved sb-admin-2 Bootstrap theme static assets to NPM package dependency #370 . Refactored bulk uploading to use iterable generator objects #382 . Updated validation of config file to check that all options are present and valid #373 . Rewritten relation functions to improve speed #307 . Minor changes to association to increase speed #307 . Changes to decrease memory usage during the calculation of the ideal coverage dataframe #307 . Updated the get_src_skyregion_merged_df logic to account for epochs #277 . Updated the job creation modal layout #277 . Bumped datatables-buttons to 1.6.5 and enabled excel export buttton #380 . Bumped datatables to 1.10.22 #363 . Changed dom layout on datatables #363 . Changed external results table pagination buttons on source detail webinterface page pagination to include less numbers to avoid overlap #363 . Changes measurement counts view on website to use new model parameters #362 . Lightcurve plot now generated using Bokeh #305 . Multiple changes to webinterface page layouts #345 . Changes source names to the format ASKAP_hhmmss.ss(+/-)ddmmss.ss #345 . Simplified webinterface navbar #345 . Excludes sources and pipeline runs from being listed in the source query page that are not complete on the webinterface #345 . Clarifies number of measurements on webinterface detail pages #345 . Changed N.A. labels to N/A on the webinterface #345 . Fixed \u00b6 Fixed pipeline run DB loading in command line runpipeline command #401 . Fixed nodejs version #412 Fixed npm start failure #412 All queries using the 2-epoch metric Vs now operate on abs(Vs) . The original Vs stored in MeasurementPair objects is still signed #407 . Changed aggregate 2-epoch metric calculation for Source objects to ensure they come from the same pair #407 . Fixed new sources rms measurement returns when no measurements are valid #417 . Fixed measuring rms values from selavy created NAXIS=3 FITS images #417 . Fixed rms value calculation in non-cluster forced extractions #402 . Increase request limit for gunicorn #398 . Fixed max source Vs metric to being an absolute value #391 . Fixed misalignment of lightcurve card header text and the flux type radio buttons #386 . Fixes incorrently named GitHub social-auth settings variable that prevented users from logging in with GitHub #372 . Fixes webinterface navbar overspill at small sizes #345 . Fixes webinterface favourite source table #345 . Removed \u00b6 Removed/Disabled obsolete test cases #412 Removed vast_pipeline/pipeline/forced_phot.py #408 . Removed 'selavy' from homepage measurements count label #391 . Removed leftover pipeline/plots.py file #391 . Removed static/css/pipeline.css , this file is now produced by compiling the Sass ( scss/**/*.scss ) files with Gulp #370 . Removed any storage of meas_dj_obj or src_dj_obj in the pipeline #382 . Removed static/vendor/chart-js package #305 . Removed static/css/collapse-box.css , content moved to pipeline.css #345 . List of PRs \u00b6 #421 feat: Delete output files on re-run & UI run check. #401 feat: Added source selection by name or id to query page. #412 feat: added some unit tests. #419 feat: Update alerts to use toasts. #408 feat: use forced_phot dependency instead of copied code. #407 fix, model: modified 2-epoch metric calculation. #411 fix: updated npm deps to fix security vulnerabilities. #415 feat: Added custom 404 and 500 templates. #393 feat: Added measurement_pairs arrow export. #406 feat, model: Added island flux ratio columns. #402 fix: Fixed rms value calculation in non-cluster forced extractions. #404 feat, dep, model: Completed schedule pipe run. #396 feat: added source tagging. #398 fix: gunicorn request limit #399 fix: Updated RACS HiPS path. #391 fix: Vs metric fix and removed pipeline/plots.py. #387 feat: Minor website updates. #386 fix: fix lightcurve header floats. #368 feat: vast-candidates merger: Add user commenting #370 feat: moved sb-admin-2 assets to dependencies. #382 feat: Refactored bulk uploading of objects. #374 feat, fix: Bring new source checks inline with forced extraction. #373 fix: Check all options are valid and present in validate_cfg. #307 feat: Improve relation functions and general association speed ups. #277 feat,model: Parallel and epoch based association. #380 feat, dep: Enable Excel export button. #379 feat: Add links to source detail template. #377 fix: Update image bkg path when not originally provided. #363 feat, dep: Add export and column visibility buttons to tables. #362 feat, model: Added number of measurements to Run DB model. #364 feat: preserve source query order on detail view. #361 feat, fix: restrict home dir scan to specified directory. #372 fix: fix social auth scope setting name. #305 feat: 2 epoch metrics #345 feat, fix: Website improvements. 0.1.0 (2020-09-27) \u00b6 First release of the Vast Pipeline. This was able to process 707 images (EPOCH01 to EPOCH11x) on a machine with 64 GB of RAM. List of PRs \u00b6 #347 feat: Towards first release #354 fix, model: Updated Band model fields to floats #346 fix: fix JS9 overflow in measurement detail view #349 dep: Bump lodash from 4.17.15 to 4.17.20 #348 dep: Bump django from 3.0.5 to 3.0.7 in /requirements #344 fix: fixed aladin init for all pages #340 break: rename pipeline folder to vast_pipeline #342 fix: Hotfix - fixed parquet path on job detail view #336 feat: Simbad/NED async cone search #284 fix: Update Aladin surveys with RACS and VAST #333 feat: auth to GitHub org, add logging and docstring #325 fix, feat: fix forced extraction using Dask bags backend #334 doc: better migration management explanation #332 fix: added clean to build task, removed commented lines #322 fix, model: add unique to image name, remove timestamp from image folder #321 feat: added css and js sourcemaps #314 feat: query form redesign, sesame resolver, coord validator #318 feat: Suppress astropy warnings #317 fix: Forced photometry fixes for #298 and #312 #316 fix: fix migration file 0001_initial.py #310 fix: Fix run detail number of measurements display #309 fix: Added JS9 overlay filters and changed JS9 overlay behaviour on sources and measurements #303 fix: Fix write config feedback and validation #306 feat: Add config validation checks #302 fix: Fix RA correction for d3 celestial #300 fix: increase line limit for gunicorn server #299 fix: fix admin \"view site\" redirect #294 fix: Make lightcurves start at zero #268 feat: Production set up with static files and command #291 fix: Bug fix for forced_photom cluster allow_nan #289 fix: Fix broken UI run creation #287 fix: Fix forced measurement parquet files write #286 fix: compile JS9 without helper option #285 fix: Fix removing forced parquet and clear images from piperun","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , with an added List of PRs section and links to the relevant PRs on the individual updates. This project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"Unreleased"},{"location":"changelog/#added","text":"Added the 0-based index of each measurement to the image cutout card headers #625 . Added Bokeh hover tooltip to measurement pair graph to display pair metrics #625 . Added new VAST surveys (13-21) to the Aladin Lite panel #622 . Added eta-V plot analysis page along with documentation #586 . Added thumbnails to light curve tooltips #586 . Added logfile dropdown selection to run detail page #595 . Added datetime stamps to all log files #595 . Added new log files for arrow file creation and restore run and added to run detail page #580 . Added restore run test #580 . Added new run status of DEL , Deleting #580 . Added documentation pages on new action buttons #580 . Added UI action buttons to run-detail page to allow arrow file generation, deletion and restoration #580 . Added try-except error capture on pre-run checks to correctly assign pipeline run as failed if an error occurs #576 . Added support for ingesting Selavy catalogues in VOTable (XML) and CSV format #565 Added new commands: initingest and ingestimages #544 Added documentation on the data required to run the pipeline #572 . Added support for ingesting Selavy catalogues in VOTable (XML) and CSV format #565 . Added new commands: initingest and ingestimages #544 . Added TNS cone search to the external search results on the source detail page #557 . Added HOME_DATA_ROOT to the pipeline settings to override the OS default home directory location #559 . Added processing spinner to source query table #551 . Added site_url to the mkdocs config so static asset URLs have the correct base URL #543 . Added basic linter to CI/CD #546","title":"Added"},{"location":"changelog/#changed","text":"Dependancies updated using npm audit fix (non-breaking) #620 . Refactored adding source to favourites button to use ajax to avoid page reload #614 . Bumped test python versions to 3.7.12, 3.8.12 and 3.9.10 #586 . Bumped various dependencies using a fresh poetry.lock file #586 . Bumped bokeh packages to 2.3.3 #586 . Django-Q config variable max_attempts is configurable in the .env file #595 . Replaced models.MeasurementPair model with a dataclass #590 . Django-Q config variables timeout and retry are configurable in the .env file #589 . Changed restore run command to only allow one run as input #580 . Changed existing documentation pages to reflect new buttons #580 . Moved creation of output backup files to occur before the config check #576 . Association test data updated with d2d fix #574 . Removed the timezone from the Timestamps being written to the the arrow file as this causes problems with vaex #571 . Reduced the memory footprint for computing the ideal source coverages by sky regions #555 . Gulp will only read webinterface/.env if the required vars are undefined in the current environment #548 .","title":"Changed"},{"location":"changelog/#fixed","text":"Fixed source names to be IAU compliant #618 . Fixed broken NED links for coordinates with many decimal places #623 . Added an error handler for the external source queries (e.g. SIMBAD) #616 . Stopped JS9 from changing the page titles #597 . Fixed regression issues with pandas 1.4 #586 . Fixed config being copied before run was confirmed to actually go ahead for existing runs #595 . Fixed forced measurements being removed from associations during the restore run process #600 . Fixed measurement FITS cutout bug #588 . Fixed removal of image and sky region objects when a run is deleted #585 . Fixed testing pandas equal deprecation warning #580 . Fixed restore run relations issue #580 . Fixed logic for full re-run requirement when UI run is being re-run from an error status #576 . Fixed d2d not being carried through the advanced association process #574 . Fixed old dictionary references in the documentation run config page #572 . Fixed a regression from pandas=1.3.0 that caused non-numeric columns to be dropped after a groupby sum operation #567 . Fixed permission error for regular users when trying to launch an initialised run #563 . Fixed outdated installation link in README #543 .","title":"Fixed"},{"location":"changelog/#removed","text":"Removed the unique constraint on models.Measurement.name #583 .","title":"Removed"},{"location":"changelog/#list-of-prs","text":"#618 : fix: Produce IAU compliant source names. #625 : feat: Pair metrics hover tooltip. #620 : dep: Non-breaking npm audit fix update. #622 : feat: Updated Aladin VAST surveys. #623 : fix: fixed NED links. #616 : fix: added error handling to external queries. #614 : feat: Refactored add to favourites button to avoid refresh. #597 : fix: Update detail page titles. #586 : feat, dep, doc: Add an eta-v analysis page for the source query. #595 : fix: Add date and time stamp to log files. #600 : fix: Fixed restore run forced measurements associations. #590 : fix: Remove MeasurementPair model. #589 : fix: expose django-q timeout and retry to env vars. #588 : fix: change cutout endpoint to use measurement ID. #585 : fix: Clean up m2m related objects when deleting a run. #583 : fix: remove unique constraint from Measurement.name. #580 : feat, fix, doc: Added UI run action buttons. #576 : fix: Fixed UI re-run from errored status. #574 : fix: Fixed d2d assignment in advanced association. #572 : doc: Added required data page to documentation. #571 : fix: Removed timezone from measurements arrow file time column #565 : feat: added support for reading selavy VOTables and CSVs. #567 : fix: fixed pandas=1.3.0 groupby sum regression. #563 : fix: fixed launch run user permission bug. #544 : feat: new command to ingest images without running the full pipeline. #557 : feat: Add TNS external search for sources. #559 : feat: added HOME_DATA_ROOT setting. #555 : fix: compute ideal source coverage with astropy xmatch. #551 : feat: added processing spinner to source query table. #550 : fix: missing changelog entry #548 : fix: only read .env if required vars are undefined. #546 : feat, fix: remove unused imports, and added basic linter during CI/CD. #543 : fix, doc: Fix README link and documentation 404 assets.","title":"List of PRs"},{"location":"changelog/#100-2021-05-21","text":"","title":"1.0.0 (2021-05-21)"},{"location":"changelog/#added_1","text":"When searching by source names, any \"VAST\" prefix on the name will be silently removed to make searching for published VAST sources easier #536 . Added acknowledgements and help section to docs #535 . Added vast_pipeline/_version.py to store the current software version and updated release documentation #532 . Added created and last updated dates to doc pages using mkdocs-git-revision-date-localized-plugin #514 . Added support for glob expressions when specifying input files in the run config file #504 Added DEFAULT_AUTO_FIELD to settings.py to silence Django 3.2 warnings #507 Added lightgallery support for all images in the documentation #494 . Added new entries in the documentation contributing section #494 . Added new entries in the documentation FAQ section #491 . Added new home page for documentation #491 . Added dark mode switch on documentation #487 . Added .env file information to documentation #487 . Added further epoch based association information to documentation page #487 . Added script to auto-generate code reference documentation pages #480 . Added code reference section to documentation #480 . Added new pages and sections to documentation #471 Added requirements/environment.yml so make it easier for Miniconda users to get the non-Python dependencies #472 . Added pyproject.toml and poetry.lock #472 . Added init-tools/init-db.py #472 . Added image add mode run restore command 'restorepiperun' #463 Added documentation folder and files for mkdocs and CI #433 Added add image to existing run feature #443 Added networkx to base reqiurements #460 . Added CI/CD workflow to run tests on pull requests #446 Added basic regression tests #425 Added image length validation for config #425","title":"Added"},{"location":"changelog/#changed_1","text":"Changed source naming convention to Jhhmmss.s(+/-)ddmmss to match VAST-P1 paper (Murphy, et al. 2021) convention #536 Updated npm packages to resolve dependabot security alert #533 . Updated homepage text to reflect new features and documentation #534 . Changed layout of source detail page #526 . Updated mkdocs-material to 7.1.4 for native creation date support #518 . Updated developing docs to specify the main development branch as dev instead of master #521 . Updated tests to account for relation fix #510 . All file examples in docs are now enclosed in an example admonition #494 . Further changes to layout of documentation #494 . Changed arrow file generation from vaex to pyarrow #503 . Changed layout of documentation to use tabs #491 . Dependabot: Bump y18n from 3.2.1 to 3.2.2 #482 . Replaced run config .py format with .yaml #483 . Changed docs VAST logo to icon format to avoid stretched appearence #487 . Bumped Browsersync from 2.26.13 to 2.26.14 #481 . Dependabot: Bump prismjs from 1.22.0 to 1.23.0 #469 . Changed non-google format docstrings to google format #480 . Changed some documentation layout and updated content #471 . Changed the vaex dependency to vaex-arrow #472 . Set CREATE_MEASUREMENTS_ARROW_FILES = True in the basic association test config #472 . Bumped minimum Python version to 3.7.1 #472 . Replaced npm package gulp-sass with @mr-hope/gulp-sass , a fork which drops the dependency on the deprecated node-sass which is difficult to install #472 . Changed the installation documentation to instruct users to use a PostgreSQL Docker image with Q3C already installed #472 . Changed 'cmd' flag in run pipeline to 'cli' #466 . Changed CONTRIBUTING.md and README.md #433 Changed forced extraction name suffix to run id rather than datetime #443 Changed tests to run on smaller cutouts #443 Changed particles style on login page #459 . Dependabot: Bump ini from 1.3.5 to 1.3.8 #436","title":"Changed"},{"location":"changelog/#fixed_1","text":"Fixed the broken link to the image detail page on measurement detail pages #528 . Fixed simbad and ned external search results table nan values #523 . Fixed inaccurate total results reported by some paginators #517 . Removed excess whitespace from coordinates that get copied to the clipboard #515 Fixed rogue relations being created during one-to-many functions #510 . Fixed JS9 regions so that the selected source components are always on top #508 Fixed docstring in config.py #494 . Fixed arrow files being generated via the website #503 . Fixed a bug that returned all sources when performing a cone search where one of the coords = 0 #501 Fixed the missing hover tool for lightcurve plots of non-variable sources #493 Fixed the default Dask multiprocessing context to \"fork\" #472 . Fixed Selavy catalogue ingest to discard the unit row before reading the data #473 . Fixed initial job processing from the UI #466 . Fixed links in README.md #464 . Fixed basic association new sources created through relations #443 Fixed tests running pipeline multiple times #443 Fixed particles canvas sizing on login page #459 . Fixed breadcrumb new line on small resolutitons #459 . Fixed config files in tests #430 Fixed sources table on measurement detail page #429 . Fixed missing meta columns in parallel association #427 .","title":"Fixed"},{"location":"changelog/#removed_1","text":"Removed SURVEYS_WORKING_DIR from settings and env file #538 . Removed default_survey from run configuration file #538 . Removed importsurvey command and catalogue.py #538 . Removed SurveySource, Survey and SurveySourceQuerySet models #538 . Removed email and Slack links from docs footer #535 . Removed bootstrap as the required version is bundled with startbootstrap-sb-admin-2 #533 . Removed docs/readme.md softlink as it is no longer used #494 . Removed vaex-arrow from the dependancies #503 . Removed requirements/*.txt files. Development dependency management moved to Poetry #472 . Removed init-tools/init-db.sh #472 . Removed INSTALL.md , PROFILE.md and static/README.md #433 Removed aplpy from base requirements #460 .","title":"Removed"},{"location":"changelog/#list-of-prs_1","text":"#538 feat: Removed survey source models, commands and references. #536 feat: changed source naming convention. #535 doc: added help and acknowledgement doc page. #534 feat: Update homepage text. #532 feat, doc: Versioning. #533 dep: updated npm deps; removed bootstrap. #528 fix: fixed broken image detail link. #526 feat: Updated source detail page layout. #518 dep: Updated mkdocs-material for native creation date support. #523 fix: Fixed external search results table nan values. #521 doc: update doc related to default dev branch. #517 fix: pin djangorestframework-datatables to 0.5.1. #515 fix: remove linebreaks from coordinates. #514 dep: Added created and updated dates to doc pages. #510 fix: Fix rogue relations. #508 fix: Draw selected source components on top in JS9. #504 feat: Add glob expression support to yaml run config. #507 fix: set default auto field model. #494 doc, dep: Docs: Added lightgallery support, layout update, minor fixes and additions. #503 fix, dep: Change arrow file generation from vaex to pyarrow. #501 fix: fix broken cone search when coord = 0 #491 doc: Updated the docs layout, home page and FAQs. #493 fix: Fix bokeh hover tool for lightcurve plots. #482 dep: Bump y18n from 3.2.1 to 3.2.2. #483 feat: replace run config .py files with .yaml. #487 doc: Minor documentation improvements. #481 dep: Bump Browsersync from 2.26.13 to 2.26.14. #469 dep: Bump prismjs from 1.22.0 to 1.23.0. #480 feat: Code reference documentation update. #471 feat: Documentation update. #472 feat: Simplify install. #473 fix: discard the selavy unit row before reading. #466 fix: Fixed initial job processing from the UI. #463 feat: Added image add mode run restore command. #433 doc: add documentation GitHub pages website with CI. #443 feat, fix: Adds the ability to add images to an existing run. #460 dep: Removed aplpy from base requirements. #446 feat: CI/CD workflow. #459 fix: Fix particles and breadcrumb issues on mobile. #436 dep: Bump ini from 1.3.5 to 1.3.8. #430 fix: Test config files. #425 feat: Basic regression tests. #429 fix: Fixed sources table on measurement detail page. #427 fix: Fixed missing meta columns in parallel association.","title":"List of PRs"},{"location":"changelog/#020-2020-11-30","text":"","title":"0.2.0 (2020-11-30)"},{"location":"changelog/#added_2","text":"Added a check in the UI running that the job is not already running or queued #421 . Added the deletion of all parquet and arrow files upon a re-run #421 . Added source selection by name or ID on source query page #401 . Added test cases #412 Added askap-vast/forced_phot to pip requirements #408 . Added pipeline configuration parameter, SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS , to filter measurement pairs before calculating aggregate metrics #407 . Added custom 404.html and 500.html templates for error pages #415 Added ability to export measurement_pairs.parqyet as an arrow file #393 . Added new fields to detail pages and source and measurement tables #406 . Added new fields to source query page (island flux ratio, min and max fluxes) #406 . Added min, max flux values to sources and agg min island flux ratio field #406 . Added island flux ratio column to measurements, component flux divided by total island flux (peak and int) #406 . Added a maximum number of images for runs through the UI #404 . Added the ability to run a pipeline run through the UI #404 . Added Queued status to the list of pipeline run statuses #404 . Added the dependancy django-q that enables scheduled tasks to be processed #404 . Added source tagging #396 . Added link to measurement table from the lightcurve to source detail page #387 . Added 'epoch based' parameter to pipeline run detail page #387 . Adds basic commenting functionality for sources, measurements, images, and runs #368 . Custom CSS now processed with Sass: Bootstrap and sb-admin-2 theme are compiled into a single stylesheet #370 . Added vast_pipeline/pipeline/generators.py which contains generator functions #382 . Range and NaN check on new source analysis to match forced extraction #374 . Added the ability for the pipeline to read in groups of images which are defined as a single epoch #277 . Added the ability of the pipeline to remove duplicated measurements from an epoch #277 . Added option to control separation measurements which are defined as a duplicate #277 . Added the ability of the pipeline to separate images to associate into unique sky region groups #277 . Added option to perform assocication of separate sky region groups in parallel #277 . Added new options to webinterface pipeline run creation #277 . Added epoch_based column run model #277 . Added links to tables and postage stamps on source detail page #379 . Updates image background_path from current run when not originally provided #377 . Added csv export button to datatables on webinterface #363 . Added support for Excel export button to datatables on webinterface (waiting on datatables buttons fix) #363 . Added column visibility button to datatables on webinterface #363 . Added dependancy datatables-buttons 1.6.4 #363 . Added dependancy jszip (required for Excel export) #363 . Adds n_selavy_measurements and n_forced_measurements to run model #362 . Adds steps to populate new measurement count fields in pipeline run #362 . Source order from the query is preserved on source detail view #364 . Setting HOME_DATA_DIR to specify a directory relative to the user's home directory to scan for FITS and text files to use in a Run initialised with the UI #361 . Adds a node graph to accompany the lightcurve that shows which measurement pairs exceed the default variability metric thresholds ( Vs >= 4.3 , |m| >= 0.26 ) #305 . Adds the MeasurementPair model to store two variability metrics for each flux type: Vs, the t-statistic; and m, the modulation index. The maximum of these metrics are also added to the Source model for joinless queries. These metrics are calculated during the pipeline run #305 . Adds radio buttons to change the lightcurve data points between peak and integrated fluxes #305 . Fills out information on all webinterface detail pages #345 . Adds frequency information the measurements and images webinterface tables. #345 . Adds celestial plot and tables to webinterface pipeline detail page #345 . Adds useful links to webinterface navbar #345 . Adds tool tips to webinterface source query #345 . Adds hash reading to webinterface source query to allow filling from URL hash parameters #345 . Add links to number cards on webinterface #345 . Added home icon on hover on webinterface #345 . Added copy-to-clipboard functionality on coordinates on webinterface #345 .","title":"Added"},{"location":"changelog/#changed_2","text":"Renamed 'alert-wrapper' container to 'toast-wrapper' #419 . Changed alerts to use the Bootstrap toasts system #419 . Bumped some npm package versions to address dependabot security alerts #411 . Images table on pipeline run detail page changed to order by datetime by default #417 . Changed config argument CREATE_MEASUREMENTS_ARROW_FILE -> CREATE_MEASUREMENTS_ARROW_FILES #393 . Naming of average flux query fields to account for other min max flux fields #406 . Expanded README.md to include DjangoQ and UI job scheduling information #404 . Shifted alerts location to the top right #404 . Log file card now expanded by default on pipeline run detail page #404 . Changed user comments on source detail pages to incorporate tagging feature #396 . Updated RACS HiPS URL in Aladin #399 . Changed home page changelog space to welcome/help messages #387 . The comment field in the Run model has been renamed to description . A comment many-to-many relationship was added to permit user comments on Run instances #368 . Moved sb-admin-2 Bootstrap theme static assets to NPM package dependency #370 . Refactored bulk uploading to use iterable generator objects #382 . Updated validation of config file to check that all options are present and valid #373 . Rewritten relation functions to improve speed #307 . Minor changes to association to increase speed #307 . Changes to decrease memory usage during the calculation of the ideal coverage dataframe #307 . Updated the get_src_skyregion_merged_df logic to account for epochs #277 . Updated the job creation modal layout #277 . Bumped datatables-buttons to 1.6.5 and enabled excel export buttton #380 . Bumped datatables to 1.10.22 #363 . Changed dom layout on datatables #363 . Changed external results table pagination buttons on source detail webinterface page pagination to include less numbers to avoid overlap #363 . Changes measurement counts view on website to use new model parameters #362 . Lightcurve plot now generated using Bokeh #305 . Multiple changes to webinterface page layouts #345 . Changes source names to the format ASKAP_hhmmss.ss(+/-)ddmmss.ss #345 . Simplified webinterface navbar #345 . Excludes sources and pipeline runs from being listed in the source query page that are not complete on the webinterface #345 . Clarifies number of measurements on webinterface detail pages #345 . Changed N.A. labels to N/A on the webinterface #345 .","title":"Changed"},{"location":"changelog/#fixed_2","text":"Fixed pipeline run DB loading in command line runpipeline command #401 . Fixed nodejs version #412 Fixed npm start failure #412 All queries using the 2-epoch metric Vs now operate on abs(Vs) . The original Vs stored in MeasurementPair objects is still signed #407 . Changed aggregate 2-epoch metric calculation for Source objects to ensure they come from the same pair #407 . Fixed new sources rms measurement returns when no measurements are valid #417 . Fixed measuring rms values from selavy created NAXIS=3 FITS images #417 . Fixed rms value calculation in non-cluster forced extractions #402 . Increase request limit for gunicorn #398 . Fixed max source Vs metric to being an absolute value #391 . Fixed misalignment of lightcurve card header text and the flux type radio buttons #386 . Fixes incorrently named GitHub social-auth settings variable that prevented users from logging in with GitHub #372 . Fixes webinterface navbar overspill at small sizes #345 . Fixes webinterface favourite source table #345 .","title":"Fixed"},{"location":"changelog/#removed_2","text":"Removed/Disabled obsolete test cases #412 Removed vast_pipeline/pipeline/forced_phot.py #408 . Removed 'selavy' from homepage measurements count label #391 . Removed leftover pipeline/plots.py file #391 . Removed static/css/pipeline.css , this file is now produced by compiling the Sass ( scss/**/*.scss ) files with Gulp #370 . Removed any storage of meas_dj_obj or src_dj_obj in the pipeline #382 . Removed static/vendor/chart-js package #305 . Removed static/css/collapse-box.css , content moved to pipeline.css #345 .","title":"Removed"},{"location":"changelog/#list-of-prs_2","text":"#421 feat: Delete output files on re-run & UI run check. #401 feat: Added source selection by name or id to query page. #412 feat: added some unit tests. #419 feat: Update alerts to use toasts. #408 feat: use forced_phot dependency instead of copied code. #407 fix, model: modified 2-epoch metric calculation. #411 fix: updated npm deps to fix security vulnerabilities. #415 feat: Added custom 404 and 500 templates. #393 feat: Added measurement_pairs arrow export. #406 feat, model: Added island flux ratio columns. #402 fix: Fixed rms value calculation in non-cluster forced extractions. #404 feat, dep, model: Completed schedule pipe run. #396 feat: added source tagging. #398 fix: gunicorn request limit #399 fix: Updated RACS HiPS path. #391 fix: Vs metric fix and removed pipeline/plots.py. #387 feat: Minor website updates. #386 fix: fix lightcurve header floats. #368 feat: vast-candidates merger: Add user commenting #370 feat: moved sb-admin-2 assets to dependencies. #382 feat: Refactored bulk uploading of objects. #374 feat, fix: Bring new source checks inline with forced extraction. #373 fix: Check all options are valid and present in validate_cfg. #307 feat: Improve relation functions and general association speed ups. #277 feat,model: Parallel and epoch based association. #380 feat, dep: Enable Excel export button. #379 feat: Add links to source detail template. #377 fix: Update image bkg path when not originally provided. #363 feat, dep: Add export and column visibility buttons to tables. #362 feat, model: Added number of measurements to Run DB model. #364 feat: preserve source query order on detail view. #361 feat, fix: restrict home dir scan to specified directory. #372 fix: fix social auth scope setting name. #305 feat: 2 epoch metrics #345 feat, fix: Website improvements.","title":"List of PRs"},{"location":"changelog/#010-2020-09-27","text":"First release of the Vast Pipeline. This was able to process 707 images (EPOCH01 to EPOCH11x) on a machine with 64 GB of RAM.","title":"0.1.0 (2020-09-27)"},{"location":"changelog/#list-of-prs_3","text":"#347 feat: Towards first release #354 fix, model: Updated Band model fields to floats #346 fix: fix JS9 overflow in measurement detail view #349 dep: Bump lodash from 4.17.15 to 4.17.20 #348 dep: Bump django from 3.0.5 to 3.0.7 in /requirements #344 fix: fixed aladin init for all pages #340 break: rename pipeline folder to vast_pipeline #342 fix: Hotfix - fixed parquet path on job detail view #336 feat: Simbad/NED async cone search #284 fix: Update Aladin surveys with RACS and VAST #333 feat: auth to GitHub org, add logging and docstring #325 fix, feat: fix forced extraction using Dask bags backend #334 doc: better migration management explanation #332 fix: added clean to build task, removed commented lines #322 fix, model: add unique to image name, remove timestamp from image folder #321 feat: added css and js sourcemaps #314 feat: query form redesign, sesame resolver, coord validator #318 feat: Suppress astropy warnings #317 fix: Forced photometry fixes for #298 and #312 #316 fix: fix migration file 0001_initial.py #310 fix: Fix run detail number of measurements display #309 fix: Added JS9 overlay filters and changed JS9 overlay behaviour on sources and measurements #303 fix: Fix write config feedback and validation #306 feat: Add config validation checks #302 fix: Fix RA correction for d3 celestial #300 fix: increase line limit for gunicorn server #299 fix: fix admin \"view site\" redirect #294 fix: Make lightcurves start at zero #268 feat: Production set up with static files and command #291 fix: Bug fix for forced_photom cluster allow_nan #289 fix: Fix broken UI run creation #287 fix: Fix forced measurement parquet files write #286 fix: compile JS9 without helper option #285 fix: Fix removing forced parquet and clear images from piperun","title":"List of PRs"},{"location":"code_of_conduct/","text":"Code Of Conduct \u00b6 By joining the VAST collaboration you agree to adhere to the Code of Conduct below. We are committed to making this collaboration productive and enjoyable for everyone, regardless of gender, sexual orientation, disability, physical appearance, body size, race, nationality or religion. We will not tolerate harassment of colleagues and students in any form. To achieve this, VAST members must endeavour to work together in a cooperative way on scientific projects that fall within the scope of VAST. In particular all members must: Exercise their best professional and ethical judgement and carry out their duties and functions with integrity and objectivity; Act fairly and reasonably, and treat colleagues and students with respect, impartiality, courtesy and sensitivity; Avoid conflicts of interest; Adhere to the VAST membership and publication policies. Please follow these guidelines in all your interactions (including online) within VAST: Behave professionally . Harassment and sexist, racist, or exclusionary comments or jokes are not appropriate. Harassment includes sustained disruption of talks or other events, inappropriate physical contact, sexual attention or innuendo, deliberate intimidation, stalking, and photography or recording of an individual without consent. It also includes offensive comments related to gender, sexual orientation, disability, physical appearance, body size, race or religion. All communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery is not appropriate. Be kind to others . Do not insult or put down other collaboration members. Participants asked to stop any inappropriate behaviour are expected to comply immediately. Members violating these rules may be asked to leave the collaboration at the discretion of the PIs. Any participant who wishes to report a violation of this policy is asked to speak, in confidence, to Tara Murphy (tara.murphy@sydney.edu.au) or David Kaplan (kaplan@uwm.edu). Extra Guidelines \u00b6 We are a community based on openness, as well as friendly and didactic discussions. We aspire to treat everybody equally, and value their contributions. Decisions are made based on technical merit and consensus. Code is not the only way to help the project. Reviewing pull requests, answering questions to help others on mailing lists or issues, organizing and teaching tutorials, working on the website, improving the documentation, are all priceless contributions. We abide by the principles of openness, respect, and consideration of others of the Python Software Foundation: https://www.python.org/psf/codeofconduct/ Acknowledgements \u00b6 We ask that all VAST publications (papers, ATELs, etc) include the line: This work was done as part of the ASKAP Variables and Slow Transients (VAST) collaboration (Murphy et al. 2013, PASA, 30, 6). Separately, all refereed publications should carry the standard CSIRO acknowledgement : The Australian SKA Pathfinder is part of the Australia Telescope National Facility which is managed by CSIRO. Operation of ASKAP is funded by the Australian Government with support from the National Collaborative Research Infrastructure Strategy. ASKAP uses the resources of the Pawsey Supercomputing Centre. Establishment of ASKAP, the Murchison Radio-astronomy Observatory and the Pawsey Supercomputing Centre are initiatives of the Australian Government, with support from the Government of Western Australia and the Science and Industry Endowment Fund. We acknowledge the Wajarri Yamatji people as the traditional owners of the Observatory site. This project is supported by the University of Sydney, the Australian Research Council, and CSIRO.","title":"Code of Conduct"},{"location":"code_of_conduct/#code-of-conduct","text":"By joining the VAST collaboration you agree to adhere to the Code of Conduct below. We are committed to making this collaboration productive and enjoyable for everyone, regardless of gender, sexual orientation, disability, physical appearance, body size, race, nationality or religion. We will not tolerate harassment of colleagues and students in any form. To achieve this, VAST members must endeavour to work together in a cooperative way on scientific projects that fall within the scope of VAST. In particular all members must: Exercise their best professional and ethical judgement and carry out their duties and functions with integrity and objectivity; Act fairly and reasonably, and treat colleagues and students with respect, impartiality, courtesy and sensitivity; Avoid conflicts of interest; Adhere to the VAST membership and publication policies. Please follow these guidelines in all your interactions (including online) within VAST: Behave professionally . Harassment and sexist, racist, or exclusionary comments or jokes are not appropriate. Harassment includes sustained disruption of talks or other events, inappropriate physical contact, sexual attention or innuendo, deliberate intimidation, stalking, and photography or recording of an individual without consent. It also includes offensive comments related to gender, sexual orientation, disability, physical appearance, body size, race or religion. All communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery is not appropriate. Be kind to others . Do not insult or put down other collaboration members. Participants asked to stop any inappropriate behaviour are expected to comply immediately. Members violating these rules may be asked to leave the collaboration at the discretion of the PIs. Any participant who wishes to report a violation of this policy is asked to speak, in confidence, to Tara Murphy (tara.murphy@sydney.edu.au) or David Kaplan (kaplan@uwm.edu).","title":"Code Of Conduct"},{"location":"code_of_conduct/#extra-guidelines","text":"We are a community based on openness, as well as friendly and didactic discussions. We aspire to treat everybody equally, and value their contributions. Decisions are made based on technical merit and consensus. Code is not the only way to help the project. Reviewing pull requests, answering questions to help others on mailing lists or issues, organizing and teaching tutorials, working on the website, improving the documentation, are all priceless contributions. We abide by the principles of openness, respect, and consideration of others of the Python Software Foundation: https://www.python.org/psf/codeofconduct/","title":"Extra Guidelines"},{"location":"code_of_conduct/#acknowledgements","text":"We ask that all VAST publications (papers, ATELs, etc) include the line: This work was done as part of the ASKAP Variables and Slow Transients (VAST) collaboration (Murphy et al. 2013, PASA, 30, 6). Separately, all refereed publications should carry the standard CSIRO acknowledgement : The Australian SKA Pathfinder is part of the Australia Telescope National Facility which is managed by CSIRO. Operation of ASKAP is funded by the Australian Government with support from the National Collaborative Research Infrastructure Strategy. ASKAP uses the resources of the Pawsey Supercomputing Centre. Establishment of ASKAP, the Murchison Radio-astronomy Observatory and the Pawsey Supercomputing Centre are initiatives of the Australian Government, with support from the Government of Western Australia and the Science and Industry Endowment Fund. We acknowledge the Wajarri Yamatji people as the traditional owners of the Observatory site. This project is supported by the University of Sydney, the Australian Research Council, and CSIRO.","title":"Acknowledgements"},{"location":"faq/","text":"Frequently Asked Questions \u00b6 Can the VAST Pipeline be used with images from other telescopes? \u00b6 The base answer to this question is that the pipeline has been designed specifically for ASKAPsoft and ASKAPpipeline products, so compatibility with data from other telescopes is not supported. However, it's important to remember that the pipeline performs no source extraction itself, instead it reads in source catalogues that is expected to be in the format of the output of the Selavy source extractor. As seen from the Image Ingest page , the pipeline does not use any special or out of the ordinary FITS headers when reading the images, and the only inputs required are the images, catalogues, noise images and background images - which are standard products. Hence, the real answer to this question is yes, if one of the following is performed: Run the Selavy source extractor on the images to process. Convert the component output from a different source extractor to match that of the Selavy component file . The pipeline was also designed in a way such that other source extractor 'translators' could be plugged into the pipeline. So a further option is to develop new translators such that the pipeline can read in output from other source extractors. The translators can be found in vast_pipeline/surveys/translators.py . Please open a discussion or issue on GitHub if you intend to give this a go! Bug In reading the code recently I have a suspicion the FITS reading code is reliant on the TELESCOP FITS header being equal to ASKAP . This is unintentional as there is nothing special about the FITS headers being read. Worth to check if anyone goes down this path. - Adam, March 2021. Does the pipeline support any other Stokes products such as Stokes V? \u00b6 Currently the pipeline only supports Stokes I data. Users can view Stokes V HIPS maps of the RACS and VAST surveys in the Aladin Lite tool on the source detail page . The support of Stokes V is planned in a future update . Can the pipeline handle multi-frequency datasets? \u00b6 Currently the pipeline does not support multi-frequency datasets. Any images that are put through in a run are assumed by the pipeline to be directly comparable to one another. For example, all variability metrics are calculated directly on the fluxes provided from the source catalogues. Multi-frequency support is planned in a future update .","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#can-the-vast-pipeline-be-used-with-images-from-other-telescopes","text":"The base answer to this question is that the pipeline has been designed specifically for ASKAPsoft and ASKAPpipeline products, so compatibility with data from other telescopes is not supported. However, it's important to remember that the pipeline performs no source extraction itself, instead it reads in source catalogues that is expected to be in the format of the output of the Selavy source extractor. As seen from the Image Ingest page , the pipeline does not use any special or out of the ordinary FITS headers when reading the images, and the only inputs required are the images, catalogues, noise images and background images - which are standard products. Hence, the real answer to this question is yes, if one of the following is performed: Run the Selavy source extractor on the images to process. Convert the component output from a different source extractor to match that of the Selavy component file . The pipeline was also designed in a way such that other source extractor 'translators' could be plugged into the pipeline. So a further option is to develop new translators such that the pipeline can read in output from other source extractors. The translators can be found in vast_pipeline/surveys/translators.py . Please open a discussion or issue on GitHub if you intend to give this a go! Bug In reading the code recently I have a suspicion the FITS reading code is reliant on the TELESCOP FITS header being equal to ASKAP . This is unintentional as there is nothing special about the FITS headers being read. Worth to check if anyone goes down this path. - Adam, March 2021.","title":"Can the VAST Pipeline be used with images from other telescopes?"},{"location":"faq/#does-the-pipeline-support-any-other-stokes-products-such-as-stokes-v","text":"Currently the pipeline only supports Stokes I data. Users can view Stokes V HIPS maps of the RACS and VAST surveys in the Aladin Lite tool on the source detail page . The support of Stokes V is planned in a future update .","title":"Does the pipeline support any other Stokes products such as Stokes V?"},{"location":"faq/#can-the-pipeline-handle-multi-frequency-datasets","text":"Currently the pipeline does not support multi-frequency datasets. Any images that are put through in a run are assumed by the pipeline to be directly comparable to one another. For example, all variability metrics are calculated directly on the fluxes provided from the source catalogues. Multi-frequency support is planned in a future update .","title":"Can the pipeline handle multi-frequency datasets?"},{"location":"help_and_acknowledgements/","text":"Help and Acknowledgements \u00b6 Getting Help \u00b6 The best way to get help with the VAST Pipeline software is to contact the development team on GitHub. If you have encountered a specific problem while using the pipeline or attempting to install any of the components, please open a new issue . If you have a more general question or an idea for a new feature, please create a new discussion thread . General enquiries may also be sent via email to the VAST project Principal Investigators: Tara Murphy David Kaplan Contributors \u00b6 Sergio Pintaldi \u2013 Sydney Informatics Hub Adam Stewart \u2013 Sydney Institute for Astronomy Andrew O'Brien \u2013 Department of Physics, University of Wisconsin-Milwaukee Tara Murphy \u2013 Sydney Institute for Astronomy David Kaplan \u2013 Department of Physics, University of Wisconsin-Milwaukee Shibli Saleheen \u2013 ADACS David Liptai \u2013 ADACS Ella Xi Wang \u2013 ADACS Acknowledgements \u00b6 The VAST Pipeline development was supported by: The Australian Research Council through grants FT150100099 and DP190100561. The Sydney Informatics Hub (SIH), a core research facility at the University of Sydney. Software support resources awarded under the Astronomy Data and Computing Services (ADACS) Merit Allocation Program. ADACS is funded from the Astronomy National Collaborative Research Infrastructure Strategy (NCRIS) allocation provided by the Australian Government and managed by Astronomy Australia Limited (AAL). NSF grant AST-1816492. We also acknowledge the LOFAR Transients Pipeline (TraP) ( Swinbank, et al. 2015 ) from which various concepts and design choices have been implemented in the VAST Pipeline. The developers thank the creators of SB Admin 2 to make the dashboard template freely available.","title":"Help and Acknowledgements"},{"location":"help_and_acknowledgements/#help-and-acknowledgements","text":"","title":"Help and Acknowledgements"},{"location":"help_and_acknowledgements/#getting-help","text":"The best way to get help with the VAST Pipeline software is to contact the development team on GitHub. If you have encountered a specific problem while using the pipeline or attempting to install any of the components, please open a new issue . If you have a more general question or an idea for a new feature, please create a new discussion thread . General enquiries may also be sent via email to the VAST project Principal Investigators: Tara Murphy David Kaplan","title":"Getting Help"},{"location":"help_and_acknowledgements/#contributors","text":"Sergio Pintaldi \u2013 Sydney Informatics Hub Adam Stewart \u2013 Sydney Institute for Astronomy Andrew O'Brien \u2013 Department of Physics, University of Wisconsin-Milwaukee Tara Murphy \u2013 Sydney Institute for Astronomy David Kaplan \u2013 Department of Physics, University of Wisconsin-Milwaukee Shibli Saleheen \u2013 ADACS David Liptai \u2013 ADACS Ella Xi Wang \u2013 ADACS","title":"Contributors"},{"location":"help_and_acknowledgements/#acknowledgements","text":"The VAST Pipeline development was supported by: The Australian Research Council through grants FT150100099 and DP190100561. The Sydney Informatics Hub (SIH), a core research facility at the University of Sydney. Software support resources awarded under the Astronomy Data and Computing Services (ADACS) Merit Allocation Program. ADACS is funded from the Astronomy National Collaborative Research Infrastructure Strategy (NCRIS) allocation provided by the Australian Government and managed by Astronomy Australia Limited (AAL). NSF grant AST-1816492. We also acknowledge the LOFAR Transients Pipeline (TraP) ( Swinbank, et al. 2015 ) from which various concepts and design choices have been implemented in the VAST Pipeline. The developers thank the creators of SB Admin 2 to make the dashboard template freely available.","title":"Acknowledgements"},{"location":"license/","text":"MIT License Copyright (c) 2020-2025 ASKAP VAST Organisation, The University of Sydney (Sydney Informatics Hub), Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"adminusage/app/","text":"Web App \u00b6 This section describes how to run the pipeline Django web app/server. Starting the Pipeline Web App \u00b6 Make sure you installed and compiled correctly the frontend assets see guide Start the Django development web server: ( pipeline_env ) $ ./manage.py runserver Test the webserver by pointing your browser at http://127.0.0.1:8000 or http://localhost:8000 . The webserver is independent of runpipeline and you can use the website while the pipeline commands are running. Running a pipeline run via the web server \u00b6 It is possible to launch the processing of a pipeline run by using the relevant option on the pipeline run detail page. This uses DjangoQ to schedule and process the runs and a cluster needs to be set up in order for the runs to process: Check the Q_CLUSTER options in ./webinterface/settings.py . Refer to the DjangoQ docs if you are unsure on the meaning of any parameters. Launch the cluster using the following command, making sure you are in the pipeline environment: ( pipeline_env ) $ ./manage.py qcluster If the pipeline is updated then the qcluster also needs to be be restarted. A warning that if you submit jobs before the cluster is set up, or is taken down, then these jobs will begin immediately once the cluster is back online.","title":"Web App"},{"location":"adminusage/app/#web-app","text":"This section describes how to run the pipeline Django web app/server.","title":"Web App"},{"location":"adminusage/app/#starting-the-pipeline-web-app","text":"Make sure you installed and compiled correctly the frontend assets see guide Start the Django development web server: ( pipeline_env ) $ ./manage.py runserver Test the webserver by pointing your browser at http://127.0.0.1:8000 or http://localhost:8000 . The webserver is independent of runpipeline and you can use the website while the pipeline commands are running.","title":"Starting the Pipeline Web App"},{"location":"adminusage/app/#running-a-pipeline-run-via-the-web-server","text":"It is possible to launch the processing of a pipeline run by using the relevant option on the pipeline run detail page. This uses DjangoQ to schedule and process the runs and a cluster needs to be set up in order for the runs to process: Check the Q_CLUSTER options in ./webinterface/settings.py . Refer to the DjangoQ docs if you are unsure on the meaning of any parameters. Launch the cluster using the following command, making sure you are in the pipeline environment: ( pipeline_env ) $ ./manage.py qcluster If the pipeline is updated then the qcluster also needs to be be restarted. A warning that if you submit jobs before the cluster is set up, or is taken down, then these jobs will begin immediately once the cluster is back online.","title":"Running a pipeline run via the web server"},{"location":"adminusage/cli/","text":"Command Line Interface (CLI) \u00b6 This section describes the commands available to the administrators of the pipelines. Pipeline Usage \u00b6 All the pipeline commands are run using the Django global ./manage.py <command> interface. Therefore you need to activate the Python environment. You can have a look at the available commands for the pipeline app: (pipeline_env)$ ./manage.py help Output: ... [vast_pipeline] clearpiperun createmeasarrow debugrun ingestimages initingest initpiperun restorepiperun runpipeline ... There are 8 commands, described in detail below. clearpiperun \u00b6 Resetting a pipeline run can be done using the clearpiperun command. This will delete all images and related objects such as sources associated with that pipeline run. Images that have been used in other pipeline runs will not be deleted. ./manage.py clearpiperun --help usage: manage.py clearpiperun [-h] [--keep-parquet] [--remove-all] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperuns [piperuns ...] Delete a pipeline run and all related images, sources, etc. Will not delete objects if they are also related to another pipeline run. positional arguments: piperuns Name or path of pipeline run(s) to delete. Pass \"clearall\" to delete all the runs. optional arguments: -h, --help show this help message and exit --keep-parquet Flag to keep the pipeline run(s) parquet files. Will also apply to arrow files if present. --remove-all Flag to remove all the content of the pipeline run(s) folder. --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. Example usage: (pipeline_env)$ ./manage.py clearpiperun path/to/my_pipe_run # or (pipeline_env)$ ./manage.py clearpiperun my_pipe_run Tip Further information on clearing a specific run, or resetting the database, can be found in the Contributing and Developing section. createmeasarrow \u00b6 This command allows for the creation of the measurements.arrow and measurement_pairs.arrow files after a run has been successfully completed. See Arrow Files for more information. ./manage.py createmeasarrow --help usage: manage.py createmeasarrow [-h] [--overwrite] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperun Create `measurements.arrow` and `measurement_pairs.arrow` files for a completed pipeline run. positional arguments: piperun Path or name of the pipeline run. optional arguments: -h, --help show this help message and exit --overwrite Overwrite previous 'measurements.arrow' file. --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the co Example usage: ./manage.py createmeasarrow docs_example_run 2021-03-30 10:48:40,952 createmeasarrow INFO Creating measurements arrow file for 'docs_example_run'. 2021-03-30 10:48:40,952 utils INFO Creating measurements.arrow for run docs_example_run. 2021-03-30 10:48:41,829 createmeasarrow INFO Creating measurement pairs arrow file for 'docs_example_run'. 2021-03-30 10:48:41,829 utils INFO Creating measurement_pairs.arrow for run docs_example_run. debugrun \u00b6 The debugrun command is used to print out a summary of the pipeline run to the terminal. A single pipeline run can be entered as an argument or all can be entered to print the statistics of all the pipeline runs in the database. ./manage.py debugrun --help usage: manage.py debugrun [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperuns [piperuns ...] Print out total metrics such as nr of measurements for runs positional arguments: piperuns Name or path of pipeline run(s) to debug.Pass \"all\" to print summary data of all the runs. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. Example usage: ./manage.py debugrun docs_example_run * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Printing summary data of pipeline run \"docs_example_run\" Nr of images: 14 Nr of measurements: 4312 Nr of forced measurements: 2156 Nr of sources: 557 Nr of association: 3276 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ingestimages \u00b6 This command runs the first part of the pipeline only. It ingests/adds a set of images, and their measurements, to the database. It requires an image ingestion configuration file as input. A template ingest configuration file can be generated with the initingest command (below). (pipeline_env)$ ./manage.py ingestimages --help Output: usage: manage.py ingestimages [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] image_ingest_config Ingest/add a set of images to the database positional arguments: image_ingest_config Image ingestion configuration filename/path. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. General usage: (pipeline_env)$ ./manage.py ingestimages ./ingest_config.yml Output: 2021-06-25 03:08:44,313 loading INFO Reading image epoch01.fits ... 2021-06-25 03:08:44,348 utils INFO Adding new frequency band: 888 2021-06-25 03:08:44,390 utils INFO Created sky region 150.001, -30.001 2021-06-25 03:08:44,441 loading INFO Processed measurements dataframe of shape: (4, 40) 2021-06-25 03:08:44,452 loading INFO Bulk created #4 Measurement 2021-06-25 03:08:44,504 loading INFO Reading image epoch02.fits ... ... 2021-06-25 03:08:44,731 loading INFO Reading image epoch04.fits ... 2021-06-25 03:08:44,771 utils INFO Created sky region 150.021, -30.017 2021-06-25 03:08:44,805 loading INFO Processed measurements dataframe of shape: (5, 40) 2021-06-25 03:08:44,810 loading INFO Bulk created #5 Measurement 2021-06-25 03:08:44,819 loading INFO Total images upload/loading time: 0.97 seconds initingest \u00b6 This command generates a template configuration file for use with the ingestimages command. (pipeline_env)$ ./manage.py initingest --help Output: usage: manage.py initingest [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] config_file_name Create a template image ingestion configuration file positional arguments: config_file_name Filename to write template ingest configuration to. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. General usage: (pipeline_env)$ ./manage.py initingest ingest_config.yml Output: Writing template to: ingest_config.yml Then modify ingest_config.yml to suit your needs. initpiperun \u00b6 In order to process the images in the pipeline, you must create/initialise a pipeline run first. The pipeline run creation is done using the initpiperun django command, which requires a pipeline run folder. The command creates a folder with the pipeline run name under the settings PROJECT_WORKING_DIR defined in settings . (pipeline_env)$ ./manage.py initpiperun --help Output: usage: manage.py initpiperun [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] runname Create the pipeline run folder structure to run a pipeline instance positional arguments: runname Name of the pipeline run. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. The command yields the following folder structure: (pipeline_env)$ ./manage.py initpiperun my_pipe_run Output: 2020-02-27 23:04:33,344 initpiperun INFO creating pipeline run folder 2020-02-27 23:04:33,344 initpiperun INFO copying default config in pipeline run folder 2020-02-27 23:04:33,344 initpiperun INFO pipeline run initialisation successful! Please modify the \"config.yaml\" restorepiperun \u00b6 Details on the add images feature can be found here . It allows for a pipeline run that has had an image added to the run to be restored to the state it was in before the image addition was made. By default the command will ask for confirmation that the run is to be restored (the option --no-confirm skips this). ./manage.py restorepiperun --help usage: manage.py restorepiperun [-h] [--no-confirm] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperuns [piperuns ...] Restore a pipeline run to the previous person after image add mode has been used. positional arguments: piperuns Name or path of pipeline run(s) to restore. optional arguments: -h, --help show this help message and exit --no-confirm Flag to skip the confirmation stage and proceed to restore the pipeline run. --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. (pipeline_env)$ ./manage.py restorepiperun path/to/my_pipe_run # or (pipeline_env)$ ./manage.py restorepiperun my_pipe_run Example usage: (pipeline_env) $ ./manage.py restorepiperun docs_example_run 2021-04-02 21:24:20,497 restorepiperun INFO Will restore the run to the following config: run: path: /Users/obrienan/sandbox/vast-pipeline-dirs/pipeline-runs/docs_example_run suppress_astropy_warnings: yes inputs: image: 1: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout.fits 2: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout.fits 3: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout.fits 4: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout.fits 5: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout.fits 6: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout.fits 7: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout.fits selavy: 1: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout.components.txt 2: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout.components.txt 3: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout.components.txt 4: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout.components.txt 5: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout.components.txt 6: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout.components.txt 7: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout.components.txt noise: 1: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout_rms.fits 2: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout_rms.fits 3: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout_rms.fits 4: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout_rms.fits 5: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout_rms.fits 6: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout_rms.fits 7: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout_rms.fits source_monitoring: monitor: no min_sigma: 3.0 edge_buffer_scale: 1.2 cluster_threshold: 3.0 allow_nan: no source_association: method: basic radius: 10.0 deruiter_radius: 5.68 deruiter_beamwidth_limit: 1.5 parallel: no epoch_duplicate_radius: 2.5 new_sources: min_sigma: 5.0 measurements: source_finder: selavy flux_fractional_error: 0.0 condon_errors: yes selavy_local_rms_fill_value: 0.2 write_arrow_files: no ra_uncertainty: 1.0 dec_uncertainty: 1.0 variability: source_aggregate_pair_metrics_min_abs_vs: 4.3 Would you like to restore the run ? (y/n): y 2021-04-02 21:24:28,685 restorepiperun INFO Restoring 'docs_example_run' from backup parquet files. 2021-04-02 21:24:29,602 restorepiperun INFO Deleting new sources and associated objects to restore run Total objects deleted: 433 2021-04-02 21:24:29,624 restorepiperun INFO Restoring metrics for 461 sources. 2021-04-02 21:24:29,663 restorepiperun INFO Removing 7 images from the run. 2021-04-02 21:24:29,754 restorepiperun INFO Deleting associations to restore run. Total objects deleted: 846 2021-04-02 21:24:29,979 restorepiperun INFO Deleting measurement pairs to restore run. Total objects deleted: 4212 2021-04-02 21:24:29,981 restorepiperun INFO Restoring run metrics. 2021-04-02 21:24:29,990 restorepiperun INFO Restoring parquet files and removing .bak files. 2021-04-02 21:24:29,995 restorepiperun INFO Restore complete. runpipeline \u00b6 The pipeline is run using runpipeline django command. (pipeline_env)$ ./manage.py runpipeline --help Output: usage: manage.py runpipeline [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperun Process the pipeline for a list of images and Selavy catalogs positional arguments: piperun Path or name of the pipeline run. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. General usage: (pipeline_env)$ ./manage.py runpipeline path/to/my_pipe_run","title":"CLI"},{"location":"adminusage/cli/#command-line-interface-cli","text":"This section describes the commands available to the administrators of the pipelines.","title":"Command Line Interface (CLI)"},{"location":"adminusage/cli/#pipeline-usage","text":"All the pipeline commands are run using the Django global ./manage.py <command> interface. Therefore you need to activate the Python environment. You can have a look at the available commands for the pipeline app: (pipeline_env)$ ./manage.py help Output: ... [vast_pipeline] clearpiperun createmeasarrow debugrun ingestimages initingest initpiperun restorepiperun runpipeline ... There are 8 commands, described in detail below.","title":"Pipeline Usage"},{"location":"adminusage/cli/#clearpiperun","text":"Resetting a pipeline run can be done using the clearpiperun command. This will delete all images and related objects such as sources associated with that pipeline run. Images that have been used in other pipeline runs will not be deleted. ./manage.py clearpiperun --help usage: manage.py clearpiperun [-h] [--keep-parquet] [--remove-all] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperuns [piperuns ...] Delete a pipeline run and all related images, sources, etc. Will not delete objects if they are also related to another pipeline run. positional arguments: piperuns Name or path of pipeline run(s) to delete. Pass \"clearall\" to delete all the runs. optional arguments: -h, --help show this help message and exit --keep-parquet Flag to keep the pipeline run(s) parquet files. Will also apply to arrow files if present. --remove-all Flag to remove all the content of the pipeline run(s) folder. --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. Example usage: (pipeline_env)$ ./manage.py clearpiperun path/to/my_pipe_run # or (pipeline_env)$ ./manage.py clearpiperun my_pipe_run Tip Further information on clearing a specific run, or resetting the database, can be found in the Contributing and Developing section.","title":"clearpiperun"},{"location":"adminusage/cli/#createmeasarrow","text":"This command allows for the creation of the measurements.arrow and measurement_pairs.arrow files after a run has been successfully completed. See Arrow Files for more information. ./manage.py createmeasarrow --help usage: manage.py createmeasarrow [-h] [--overwrite] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperun Create `measurements.arrow` and `measurement_pairs.arrow` files for a completed pipeline run. positional arguments: piperun Path or name of the pipeline run. optional arguments: -h, --help show this help message and exit --overwrite Overwrite previous 'measurements.arrow' file. --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the co Example usage: ./manage.py createmeasarrow docs_example_run 2021-03-30 10:48:40,952 createmeasarrow INFO Creating measurements arrow file for 'docs_example_run'. 2021-03-30 10:48:40,952 utils INFO Creating measurements.arrow for run docs_example_run. 2021-03-30 10:48:41,829 createmeasarrow INFO Creating measurement pairs arrow file for 'docs_example_run'. 2021-03-30 10:48:41,829 utils INFO Creating measurement_pairs.arrow for run docs_example_run.","title":"createmeasarrow"},{"location":"adminusage/cli/#debugrun","text":"The debugrun command is used to print out a summary of the pipeline run to the terminal. A single pipeline run can be entered as an argument or all can be entered to print the statistics of all the pipeline runs in the database. ./manage.py debugrun --help usage: manage.py debugrun [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperuns [piperuns ...] Print out total metrics such as nr of measurements for runs positional arguments: piperuns Name or path of pipeline run(s) to debug.Pass \"all\" to print summary data of all the runs. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. Example usage: ./manage.py debugrun docs_example_run * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Printing summary data of pipeline run \"docs_example_run\" Nr of images: 14 Nr of measurements: 4312 Nr of forced measurements: 2156 Nr of sources: 557 Nr of association: 3276 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *","title":"debugrun"},{"location":"adminusage/cli/#ingestimages","text":"This command runs the first part of the pipeline only. It ingests/adds a set of images, and their measurements, to the database. It requires an image ingestion configuration file as input. A template ingest configuration file can be generated with the initingest command (below). (pipeline_env)$ ./manage.py ingestimages --help Output: usage: manage.py ingestimages [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] image_ingest_config Ingest/add a set of images to the database positional arguments: image_ingest_config Image ingestion configuration filename/path. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. General usage: (pipeline_env)$ ./manage.py ingestimages ./ingest_config.yml Output: 2021-06-25 03:08:44,313 loading INFO Reading image epoch01.fits ... 2021-06-25 03:08:44,348 utils INFO Adding new frequency band: 888 2021-06-25 03:08:44,390 utils INFO Created sky region 150.001, -30.001 2021-06-25 03:08:44,441 loading INFO Processed measurements dataframe of shape: (4, 40) 2021-06-25 03:08:44,452 loading INFO Bulk created #4 Measurement 2021-06-25 03:08:44,504 loading INFO Reading image epoch02.fits ... ... 2021-06-25 03:08:44,731 loading INFO Reading image epoch04.fits ... 2021-06-25 03:08:44,771 utils INFO Created sky region 150.021, -30.017 2021-06-25 03:08:44,805 loading INFO Processed measurements dataframe of shape: (5, 40) 2021-06-25 03:08:44,810 loading INFO Bulk created #5 Measurement 2021-06-25 03:08:44,819 loading INFO Total images upload/loading time: 0.97 seconds","title":"ingestimages"},{"location":"adminusage/cli/#initingest","text":"This command generates a template configuration file for use with the ingestimages command. (pipeline_env)$ ./manage.py initingest --help Output: usage: manage.py initingest [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] config_file_name Create a template image ingestion configuration file positional arguments: config_file_name Filename to write template ingest configuration to. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. General usage: (pipeline_env)$ ./manage.py initingest ingest_config.yml Output: Writing template to: ingest_config.yml Then modify ingest_config.yml to suit your needs.","title":"initingest"},{"location":"adminusage/cli/#initpiperun","text":"In order to process the images in the pipeline, you must create/initialise a pipeline run first. The pipeline run creation is done using the initpiperun django command, which requires a pipeline run folder. The command creates a folder with the pipeline run name under the settings PROJECT_WORKING_DIR defined in settings . (pipeline_env)$ ./manage.py initpiperun --help Output: usage: manage.py initpiperun [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] runname Create the pipeline run folder structure to run a pipeline instance positional arguments: runname Name of the pipeline run. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. The command yields the following folder structure: (pipeline_env)$ ./manage.py initpiperun my_pipe_run Output: 2020-02-27 23:04:33,344 initpiperun INFO creating pipeline run folder 2020-02-27 23:04:33,344 initpiperun INFO copying default config in pipeline run folder 2020-02-27 23:04:33,344 initpiperun INFO pipeline run initialisation successful! Please modify the \"config.yaml\"","title":"initpiperun"},{"location":"adminusage/cli/#restorepiperun","text":"Details on the add images feature can be found here . It allows for a pipeline run that has had an image added to the run to be restored to the state it was in before the image addition was made. By default the command will ask for confirmation that the run is to be restored (the option --no-confirm skips this). ./manage.py restorepiperun --help usage: manage.py restorepiperun [-h] [--no-confirm] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperuns [piperuns ...] Restore a pipeline run to the previous person after image add mode has been used. positional arguments: piperuns Name or path of pipeline run(s) to restore. optional arguments: -h, --help show this help message and exit --no-confirm Flag to skip the confirmation stage and proceed to restore the pipeline run. --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. (pipeline_env)$ ./manage.py restorepiperun path/to/my_pipe_run # or (pipeline_env)$ ./manage.py restorepiperun my_pipe_run Example usage: (pipeline_env) $ ./manage.py restorepiperun docs_example_run 2021-04-02 21:24:20,497 restorepiperun INFO Will restore the run to the following config: run: path: /Users/obrienan/sandbox/vast-pipeline-dirs/pipeline-runs/docs_example_run suppress_astropy_warnings: yes inputs: image: 1: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout.fits 2: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout.fits 3: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout.fits 4: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout.fits 5: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout.fits 6: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout.fits 7: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout.fits selavy: 1: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout.components.txt 2: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout.components.txt 3: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout.components.txt 4: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout.components.txt 5: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout.components.txt 6: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout.components.txt 7: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout.components.txt noise: 1: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout_rms.fits 2: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout_rms.fits 3: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout_rms.fits 4: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout_rms.fits 5: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout_rms.fits 6: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout_rms.fits 7: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout_rms.fits source_monitoring: monitor: no min_sigma: 3.0 edge_buffer_scale: 1.2 cluster_threshold: 3.0 allow_nan: no source_association: method: basic radius: 10.0 deruiter_radius: 5.68 deruiter_beamwidth_limit: 1.5 parallel: no epoch_duplicate_radius: 2.5 new_sources: min_sigma: 5.0 measurements: source_finder: selavy flux_fractional_error: 0.0 condon_errors: yes selavy_local_rms_fill_value: 0.2 write_arrow_files: no ra_uncertainty: 1.0 dec_uncertainty: 1.0 variability: source_aggregate_pair_metrics_min_abs_vs: 4.3 Would you like to restore the run ? (y/n): y 2021-04-02 21:24:28,685 restorepiperun INFO Restoring 'docs_example_run' from backup parquet files. 2021-04-02 21:24:29,602 restorepiperun INFO Deleting new sources and associated objects to restore run Total objects deleted: 433 2021-04-02 21:24:29,624 restorepiperun INFO Restoring metrics for 461 sources. 2021-04-02 21:24:29,663 restorepiperun INFO Removing 7 images from the run. 2021-04-02 21:24:29,754 restorepiperun INFO Deleting associations to restore run. Total objects deleted: 846 2021-04-02 21:24:29,979 restorepiperun INFO Deleting measurement pairs to restore run. Total objects deleted: 4212 2021-04-02 21:24:29,981 restorepiperun INFO Restoring run metrics. 2021-04-02 21:24:29,990 restorepiperun INFO Restoring parquet files and removing .bak files. 2021-04-02 21:24:29,995 restorepiperun INFO Restore complete.","title":"restorepiperun"},{"location":"adminusage/cli/#runpipeline","text":"The pipeline is run using runpipeline django command. (pipeline_env)$ ./manage.py runpipeline --help Output: usage: manage.py runpipeline [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperun Process the pipeline for a list of images and Selavy catalogs positional arguments: piperun Path or name of the pipeline run. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. General usage: (pipeline_env)$ ./manage.py runpipeline path/to/my_pipe_run","title":"runpipeline"},{"location":"adminusage/ingestimages/","text":"Standalone Image Ingestion \u00b6 Administrators have the added option of ingesting new images via the command line, without running the full pipeline. Note Currently, when non-admin users run the pipeline, any images listed in the configuration that do not exist in the database will be processed and uploaded to the database. Hence, it is not a requirement to ingest images using this process before a run is processed that uses new images. The relevant command is ingestimages , which requires and ingest configuration file as input. The configuration file must contain the images to be listed, as well as a subset of options from the main run configuration file. In particular, along with the images to be ingested, the configuration requires the following options: measurements.condon_errors Boolean. Calculate the Condon errors of the extractions when read in from the source extraction file. If False then the errors directly from the source finder output are used. Recommended to set to True for selavy extractions. Defaults to True . measurements.selavy_local_rms_fill_value Float. Value to substitute for the local_rms parameter in selavy extractions if a 0.0 value is found. Unit is mJy. Defaults to 0.2 . measurements.ra_uncertainty Float. Defines an uncertainty error to the RA that will be added in quadrature to the existing source extraction error. Used to represent a systematic positional error. Unit is arcseconds. Defaults to 1.0. measurements.dec_uncertainty Float. Defines an uncertainty error to the Dec that will be added in quadrature to the existing source extraction error. Used to represent systematic positional error. Unit is arcseconds. Defaults to 1.0. ingest_config.yaml # This file is used for ingest only mode # It specifies the images to be processed and the relevant # settings to be used. inputs : # NOTE: all the inputs must match with each other, i.e. the catalogue for the first # input image (inputs.image[0]) must be the first input catalogue (inputs.selavy[0]) # and so on. image : # list input images here, e.g. (note the leading hyphens) glob : ./data/epoch0?.fits selavy : # list input selavy catalogues here, as above with the images glob : ./data/epoch0?.selavy.components.txt noise : # list input noise (rms) images here, as above with the images glob : ./data/epoch0?.noiseMap.fits # Required only if source_monitoring.monitor is true, otherwise optional. If not providing # background images, remove the entire background section below. # background: # list input background images here, as above with the images measurements : # Replace the selavy errors with Condon (1997) errors. condon_errors : True # Sometimes the local rms for a source is reported as 0 by selavy. # Choose a value to use for the local rms in these cases in mJy/beam. selavy_local_rms_fill_value : 0.2 # The positional uncertainty of a measurement is in reality the fitting errors and the # astrometric uncertainty of the image/survey/instrument combined in quadrature. # These two parameters are the astrometric uncertainty in RA/Dec and they may be different. ra_uncertainty : 1 # arcsec dec_uncertainty : 1 # arcsec","title":"Standalone Image Ingestion"},{"location":"adminusage/ingestimages/#standalone-image-ingestion","text":"Administrators have the added option of ingesting new images via the command line, without running the full pipeline. Note Currently, when non-admin users run the pipeline, any images listed in the configuration that do not exist in the database will be processed and uploaded to the database. Hence, it is not a requirement to ingest images using this process before a run is processed that uses new images. The relevant command is ingestimages , which requires and ingest configuration file as input. The configuration file must contain the images to be listed, as well as a subset of options from the main run configuration file. In particular, along with the images to be ingested, the configuration requires the following options: measurements.condon_errors Boolean. Calculate the Condon errors of the extractions when read in from the source extraction file. If False then the errors directly from the source finder output are used. Recommended to set to True for selavy extractions. Defaults to True . measurements.selavy_local_rms_fill_value Float. Value to substitute for the local_rms parameter in selavy extractions if a 0.0 value is found. Unit is mJy. Defaults to 0.2 . measurements.ra_uncertainty Float. Defines an uncertainty error to the RA that will be added in quadrature to the existing source extraction error. Used to represent a systematic positional error. Unit is arcseconds. Defaults to 1.0. measurements.dec_uncertainty Float. Defines an uncertainty error to the Dec that will be added in quadrature to the existing source extraction error. Used to represent systematic positional error. Unit is arcseconds. Defaults to 1.0. ingest_config.yaml # This file is used for ingest only mode # It specifies the images to be processed and the relevant # settings to be used. inputs : # NOTE: all the inputs must match with each other, i.e. the catalogue for the first # input image (inputs.image[0]) must be the first input catalogue (inputs.selavy[0]) # and so on. image : # list input images here, e.g. (note the leading hyphens) glob : ./data/epoch0?.fits selavy : # list input selavy catalogues here, as above with the images glob : ./data/epoch0?.selavy.components.txt noise : # list input noise (rms) images here, as above with the images glob : ./data/epoch0?.noiseMap.fits # Required only if source_monitoring.monitor is true, otherwise optional. If not providing # background images, remove the entire background section below. # background: # list input background images here, as above with the images measurements : # Replace the selavy errors with Condon (1997) errors. condon_errors : True # Sometimes the local rms for a source is reported as 0 by selavy. # Choose a value to use for the local rms in these cases in mJy/beam. selavy_local_rms_fill_value : 0.2 # The positional uncertainty of a measurement is in reality the fitting errors and the # astrometric uncertainty of the image/survey/instrument combined in quadrature. # These two parameters are the astrometric uncertainty in RA/Dec and they may be different. ra_uncertainty : 1 # arcsec dec_uncertainty : 1 # arcsec","title":"Standalone Image Ingestion"},{"location":"architecture/database/","text":"Database Schema \u00b6 This section describes the relationships between the objects/tables stored in the database. Django Web App Schema \u00b6 The following figure shows a detailed schematics of the schema and relationships as well as tables parameters of the Django App. Pipeline Detailed Schema \u00b6 A focussed view of the pipeline schema is shown below: Important points \u00b6 Some of the key points of the above relationship diagram are: each image object is indipendent from the others and can belong to multiple pipeline runs to avoid duplication. An image can belong to multiple pipeline run objects and a run object can have multiple images. If a user want to upload an image object with different characteristic (i.e. using a custom source extraction tool), is free to do so but the image name need to be unique . So we suggest to assign a custom name to your image files. Each image is linked to a set of source measurement objects by means of a foreign key. Therefore those objects can belong to multiple source objects. A source object can have multiple measurements and a measurements can belong to multiple source objects. The pipeline schema has been mainly designed to allow for completely disjoint run objects so that each users can run their own processing with their specific settings, defined in the configuration file.","title":"Database Schema"},{"location":"architecture/database/#database-schema","text":"This section describes the relationships between the objects/tables stored in the database.","title":"Database Schema"},{"location":"architecture/database/#django-web-app-schema","text":"The following figure shows a detailed schematics of the schema and relationships as well as tables parameters of the Django App.","title":"Django Web App Schema"},{"location":"architecture/database/#pipeline-detailed-schema","text":"A focussed view of the pipeline schema is shown below:","title":"Pipeline Detailed Schema"},{"location":"architecture/database/#important-points","text":"Some of the key points of the above relationship diagram are: each image object is indipendent from the others and can belong to multiple pipeline runs to avoid duplication. An image can belong to multiple pipeline run objects and a run object can have multiple images. If a user want to upload an image object with different characteristic (i.e. using a custom source extraction tool), is free to do so but the image name need to be unique . So we suggest to assign a custom name to your image files. Each image is linked to a set of source measurement objects by means of a foreign key. Therefore those objects can belong to multiple source objects. A source object can have multiple measurements and a measurements can belong to multiple source objects. The pipeline schema has been mainly designed to allow for completely disjoint run objects so that each users can run their own processing with their specific settings, defined in the configuration file.","title":"Important points"},{"location":"architecture/intro/","text":"VAST Pipeline Architecture \u00b6 The pipeline is essentially a Django app in which the pipeline is run as a Django admin command. The main structure of the pipeline is described in this schematics: Design Philosophy \u00b6 We design the pipeline in order to make it easy to use but at the same time powerful and fast. We decided to use the familiar Pandas Dataframe structure to wrap all the data manipulations, including the association operations, in the back-end. The Python community, as well as the research and scientific communities (including the astro-physicists) are very familiar with Pandas, and they should be able to understand, mantain and develop the code base. Usually in the \"Big Data\" world the commond tools adopted by the industry and research are Apache Hadoop and Spark . We decided to use Dask which is similar to Spark in same ways, but it integrates well with Pandas Dataframe and its syntax is quite similar to Pandas. Further it provides scalability by means of clustering and integrating with HPC (High Performance Comptuing) stacks. The pipeline code itself and the web app are integrated into one code base, for the sake of simplicity, easy to develop using one central repository. The user can still run the pipeline via CLI (Command Line Interface), using Django Admin Commands , as well as thorugh the web app itself. The integration avoid duplication in code, especially on regards the declaration of the schema in the ORM (Object Relational Mapping), and add user and permission management on the underlyng data, through the in-built functionality of Django framework. The front-end is built in simple HTML, CSS and Javascript using a freely available Bootstrap 4 template. The developers know best practices in the web development are focusing mostly on single page applications using framework such as ReactJS and AngularJS . The choice of using just the basic web stack (HTML + CSS + JS) was driven by the fact that future developers do not need to learn modern web frameworks such as React and Angular, but the fundamental web programming which is still the core of those tools. Technology Stack \u00b6 Back-End \u00b6 Astropy 4+ Astroquery 0.4+ Bokeh 2+ Dask 2+ Django 3+ Django Rest Framework Rest Framework Datatables Django Q Python Social Auth - Django Django Crispy Forms Django Tagulous Pandas 1+ Python 3.7+ Pyarrow 0.17+ Postgres 10+ Q3C Vaex 3+ Front-End \u00b6 Aladin Lite Bokeh Bootstrap 4 DataTables D3 Celestial Jquery JS9 ParticleJS PrismJS SB Admin 2 template Additional \u00b6 Docker node 12+ npm 6+ gulp 4+ GitHub Actions","title":"Architecture Overview"},{"location":"architecture/intro/#vast-pipeline-architecture","text":"The pipeline is essentially a Django app in which the pipeline is run as a Django admin command. The main structure of the pipeline is described in this schematics:","title":"VAST Pipeline Architecture"},{"location":"architecture/intro/#design-philosophy","text":"We design the pipeline in order to make it easy to use but at the same time powerful and fast. We decided to use the familiar Pandas Dataframe structure to wrap all the data manipulations, including the association operations, in the back-end. The Python community, as well as the research and scientific communities (including the astro-physicists) are very familiar with Pandas, and they should be able to understand, mantain and develop the code base. Usually in the \"Big Data\" world the commond tools adopted by the industry and research are Apache Hadoop and Spark . We decided to use Dask which is similar to Spark in same ways, but it integrates well with Pandas Dataframe and its syntax is quite similar to Pandas. Further it provides scalability by means of clustering and integrating with HPC (High Performance Comptuing) stacks. The pipeline code itself and the web app are integrated into one code base, for the sake of simplicity, easy to develop using one central repository. The user can still run the pipeline via CLI (Command Line Interface), using Django Admin Commands , as well as thorugh the web app itself. The integration avoid duplication in code, especially on regards the declaration of the schema in the ORM (Object Relational Mapping), and add user and permission management on the underlyng data, through the in-built functionality of Django framework. The front-end is built in simple HTML, CSS and Javascript using a freely available Bootstrap 4 template. The developers know best practices in the web development are focusing mostly on single page applications using framework such as ReactJS and AngularJS . The choice of using just the basic web stack (HTML + CSS + JS) was driven by the fact that future developers do not need to learn modern web frameworks such as React and Angular, but the fundamental web programming which is still the core of those tools.","title":"Design Philosophy"},{"location":"architecture/intro/#technology-stack","text":"","title":"Technology Stack"},{"location":"architecture/intro/#back-end","text":"Astropy 4+ Astroquery 0.4+ Bokeh 2+ Dask 2+ Django 3+ Django Rest Framework Rest Framework Datatables Django Q Python Social Auth - Django Django Crispy Forms Django Tagulous Pandas 1+ Python 3.7+ Pyarrow 0.17+ Postgres 10+ Q3C Vaex 3+","title":"Back-End"},{"location":"architecture/intro/#front-end","text":"Aladin Lite Bokeh Bootstrap 4 DataTables D3 Celestial Jquery JS9 ParticleJS PrismJS SB Admin 2 template","title":"Front-End"},{"location":"architecture/intro/#additional","text":"Docker node 12+ npm 6+ gulp 4+ GitHub Actions","title":"Additional"},{"location":"design/association/","text":"Source Association \u00b6 This page details the association stage of a pipeline run. There are three association methods available which are summarised in the table below, and detailed in the following sections. Tip For complex fields and large surveys the De Ruiter method is recommended. Method Fixed Assoc. Radius Astropy function Possible Relation Types Basic Yes match_coordinates_sky one-to-many Advanced Yes search_around_sky many-to-many, many-to-one, one-to-many de Ruiter (TraP) No search_around_sky many-to-many, many-to-one, one-to-many General Association Notes \u00b6 Terminology \u00b6 During association, measurements are associated into unique sources . Association Process \u00b6 By default, association is performed on an image-by-image basis, ordered by the observational date. The only time this isn't the case is when Epoch Based Association is used. Note Epoch Based Association is not an association method, rather it changes how the measurements are handled when passed to one of the three methods for association. Weighted Average Coordinates \u00b6 After every iteration of each association method, the average RA and Dec, weighted by the positional uncertainty, are calculated for each source. These weighted averages are then used as the base catalogue for the next association iteration. In other words, as the measurements are associated, new measurements are associated against the weighted average of the sources identified to that point in the process. Sources positions are reported using the weighted averages. Association Methods \u00b6 Tip For a better understanding on the underlying process, see this page in the astropy documentation for examples on matching catalogues. Basic \u00b6 The most basic association method uses the astropy match_coordinates_sky function which: Associates measurements using only the nearest neighbour for each source when comparing catalogues. Uses a fixed association radius as a threshold for a 'match'. Only one-to-many relations are possible. Advanced \u00b6 This method uses the same process as Basic , however the astropy function search_around_sky is used instead. This means: All possible matches between the two catalogues are found, rather than only the nearest neighbour. A fixed association radius is still applied as the threshold. All types of relations are possible. de Ruiter \u00b6 The de Ruiter method is a translation of the association method used by the LOFAR Transients Pipeline (TraP) , which uses the de Ruiter radius in order to define associations. The search_around_sky astropy method is still used, but the threshold for a potential match is first limited by a beamwidth limit value which is defined in the pipeline run configuration file ( source_association.deruiter_beamwidth_limit ), such that the initial threshold separation distance is set to \\[ \\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,img}}}{2}, \\] where \\(\\theta_{\\text{bmaj,img}}\\) is the major axis of the restoring beam of the image being associated. Then, the de Ruiter radius is calculated for all potential matches which is defined as \\[ r_{i,j} = \\sqrt{ \\frac{ (\\alpha_{i} - \\alpha_{j})^{2}((\\delta_{i} + \\delta_{j})/2)}{\\sigma^{2}_{\\alpha_{i}} + \\sigma^{2}_{\\alpha_{j}}} \\\\+ \\frac{(\\delta_{i} + \\delta_{j})^{2}}{\\sigma^{2}_{\\delta_{i}} + \\sigma^{2}_{\\delta_{j}}} } \\] where \\(\\alpha_{n}\\) is the right ascension of source n, \\(\\delta_{n}\\) is its declination, and \\(\\sigma_{y}\\) represents the error on the quantity y. Matches are then identified by applying a threshold maximum value to the de Ruiter radius which is defined by the user in the pipeline run configuration file ( source_association.deruiter_radius ). All relation types are possible using this method. Epoch Based Association Note When the de Ruiter association method is used with epoch based assocation, the beamwidth limit is applied to the maximum bmaj value out of all the images included in the epoch. See the de Ruiter and Epoch Based Association section below for further details. Relations \u00b6 Situations can arise where a source is associated with more than one source in the catalogue being cross-matched (or vice versa). Internally these types of associations are called: many-to-many one-to-many many-to-one a good explanation of these situations is presented in the TraP documentation here . The VAST Pipeline follows the TraP methods in handling these types of associations, which is also detailed in the linked documentation. In short: many-to-many associations are reduced to one-to-one or one-to-many associations. one-to-many and many-to-one associations create \"forked\" unique sources. I.e. an individual datapoint can belong to two different sources. The VAST Pipeline reports the one-to-many and many-to-one associations by relating sources. A source may have one or more relations which signifies the the source could be associated with more than one other source. This often happens for complex sources with many closely packed components. A read-through of the TraP documentation is highly encouraged on this point as it contains an excellent description. Relations False Variability \u00b6 The VAST Pipeline builds associations only using the component information. What this means is that, while the island information from the selavy source finder is stored, it is not considered during the association stage. Because of this, the relation process detailed above has the potential to cause sources to appear variable, when in reality it is not the case. For an example consider the source below: In the 3rd, 7th, and 8th measurement (EPOCH02, EPOCH09, and EPOCH12), the source is detected as an island with two Gaussian components, as opposed to the one component in all other epochs. The source lightcurve shows how the flux has reduced by approximately 50% in these three epochs, which makes the source appear variable. The pipeline provides information for each source that allows for these kind of situations to be swiftly identified: the number number of measurements that contain siblings, and the number of relations. These are the columns n_sibl and n_rel , respectively, in the pipeline sources output file (refer to the Column Descriptions section). If these values are not 0 for a source then care must be taken when analysing variability. For the example source above, the values are n_sibl = 3 and n_rel = 1 . The missing flux can be seen in the lightcurve of the related source: Epoch Based Association \u00b6 The pipeline is able to associate inputs on an epoch basis. What this means is that, for example, all VAST Pilot Epoch 1 measurements are grouped together and are associated with grouped together Epoch 2 measurements, and so on. In doing this, duplicate measurements from within the same epoch are cut with the measurement kept being that which is closest to the centre of its respective image. The separation distance that defines a duplicate is defined in the pipeline run configuration file ( source_association.epoch_duplicate_radius ). The mode is activated by entering the images to be processed under an extra heading in the .yaml configuration file as demonstrated below. The heading acts as the epoch 'key', hence be sure to use a string that can be ordered as the heading to maintain the correct epoch order. config.yaml inputs : image : epoch01 : - /full/path/to/image1.fits - /full/path/to/image2.fits epoch02 : - /full/path/to/image3.fits The lightcurves below show the difference between 'regular' association (top) and 'epoch based' association (lower) for a source. For large surveys where transient and variablity searches on the epoch timescale is required, using this mode can greatly speed up the association stage. Warning Epoch based association does eliminate the full time resolution of your data! The base time resolution will be between the defined epochs. de Ruiter and Epoch Based Association \u00b6 During the standard de Ruiter assoication , an initial on sky separation cut is made of \\(\\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,img}}}{2}\\) , where beamwidth limit is a value entered by the user and \\(\\theta_{\\text{bmaj,img}}\\) is the major component size of the restoring beam of the image being associated. When using epoch based association, an epoch that contains more than one image will have multiple values of \\(\\theta_{\\text{bmaj,img}}\\) to apply to the combined measurements. In this case, the maximum major axis value of all the images, \\(\\theta_{\\text{bmaj,max}}\\) , is used. Hence, the initial de Ruiter association step threshold becomes \\[ \\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,max}}}{2}. \\] Parallel Association \u00b6 When parallel association is used, the images to process are analysed and grouped into distinct patches of the sky that do not overlap. These distinct regions are then processed through the source association in parallel. It is recommended to use parallel association when your dataset covers three or more distinct patches of sky.","title":"Source Association"},{"location":"design/association/#source-association","text":"This page details the association stage of a pipeline run. There are three association methods available which are summarised in the table below, and detailed in the following sections. Tip For complex fields and large surveys the De Ruiter method is recommended. Method Fixed Assoc. Radius Astropy function Possible Relation Types Basic Yes match_coordinates_sky one-to-many Advanced Yes search_around_sky many-to-many, many-to-one, one-to-many de Ruiter (TraP) No search_around_sky many-to-many, many-to-one, one-to-many","title":"Source Association"},{"location":"design/association/#general-association-notes","text":"","title":"General Association Notes"},{"location":"design/association/#terminology","text":"During association, measurements are associated into unique sources .","title":"Terminology"},{"location":"design/association/#association-process","text":"By default, association is performed on an image-by-image basis, ordered by the observational date. The only time this isn't the case is when Epoch Based Association is used. Note Epoch Based Association is not an association method, rather it changes how the measurements are handled when passed to one of the three methods for association.","title":"Association Process"},{"location":"design/association/#weighted-average-coordinates","text":"After every iteration of each association method, the average RA and Dec, weighted by the positional uncertainty, are calculated for each source. These weighted averages are then used as the base catalogue for the next association iteration. In other words, as the measurements are associated, new measurements are associated against the weighted average of the sources identified to that point in the process. Sources positions are reported using the weighted averages.","title":"Weighted Average Coordinates"},{"location":"design/association/#association-methods","text":"Tip For a better understanding on the underlying process, see this page in the astropy documentation for examples on matching catalogues.","title":"Association Methods"},{"location":"design/association/#basic","text":"The most basic association method uses the astropy match_coordinates_sky function which: Associates measurements using only the nearest neighbour for each source when comparing catalogues. Uses a fixed association radius as a threshold for a 'match'. Only one-to-many relations are possible.","title":"Basic"},{"location":"design/association/#advanced","text":"This method uses the same process as Basic , however the astropy function search_around_sky is used instead. This means: All possible matches between the two catalogues are found, rather than only the nearest neighbour. A fixed association radius is still applied as the threshold. All types of relations are possible.","title":"Advanced"},{"location":"design/association/#de-ruiter","text":"The de Ruiter method is a translation of the association method used by the LOFAR Transients Pipeline (TraP) , which uses the de Ruiter radius in order to define associations. The search_around_sky astropy method is still used, but the threshold for a potential match is first limited by a beamwidth limit value which is defined in the pipeline run configuration file ( source_association.deruiter_beamwidth_limit ), such that the initial threshold separation distance is set to \\[ \\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,img}}}{2}, \\] where \\(\\theta_{\\text{bmaj,img}}\\) is the major axis of the restoring beam of the image being associated. Then, the de Ruiter radius is calculated for all potential matches which is defined as \\[ r_{i,j} = \\sqrt{ \\frac{ (\\alpha_{i} - \\alpha_{j})^{2}((\\delta_{i} + \\delta_{j})/2)}{\\sigma^{2}_{\\alpha_{i}} + \\sigma^{2}_{\\alpha_{j}}} \\\\+ \\frac{(\\delta_{i} + \\delta_{j})^{2}}{\\sigma^{2}_{\\delta_{i}} + \\sigma^{2}_{\\delta_{j}}} } \\] where \\(\\alpha_{n}\\) is the right ascension of source n, \\(\\delta_{n}\\) is its declination, and \\(\\sigma_{y}\\) represents the error on the quantity y. Matches are then identified by applying a threshold maximum value to the de Ruiter radius which is defined by the user in the pipeline run configuration file ( source_association.deruiter_radius ). All relation types are possible using this method. Epoch Based Association Note When the de Ruiter association method is used with epoch based assocation, the beamwidth limit is applied to the maximum bmaj value out of all the images included in the epoch. See the de Ruiter and Epoch Based Association section below for further details.","title":"de Ruiter"},{"location":"design/association/#relations","text":"Situations can arise where a source is associated with more than one source in the catalogue being cross-matched (or vice versa). Internally these types of associations are called: many-to-many one-to-many many-to-one a good explanation of these situations is presented in the TraP documentation here . The VAST Pipeline follows the TraP methods in handling these types of associations, which is also detailed in the linked documentation. In short: many-to-many associations are reduced to one-to-one or one-to-many associations. one-to-many and many-to-one associations create \"forked\" unique sources. I.e. an individual datapoint can belong to two different sources. The VAST Pipeline reports the one-to-many and many-to-one associations by relating sources. A source may have one or more relations which signifies the the source could be associated with more than one other source. This often happens for complex sources with many closely packed components. A read-through of the TraP documentation is highly encouraged on this point as it contains an excellent description.","title":"Relations"},{"location":"design/association/#relations-false-variability","text":"The VAST Pipeline builds associations only using the component information. What this means is that, while the island information from the selavy source finder is stored, it is not considered during the association stage. Because of this, the relation process detailed above has the potential to cause sources to appear variable, when in reality it is not the case. For an example consider the source below: In the 3rd, 7th, and 8th measurement (EPOCH02, EPOCH09, and EPOCH12), the source is detected as an island with two Gaussian components, as opposed to the one component in all other epochs. The source lightcurve shows how the flux has reduced by approximately 50% in these three epochs, which makes the source appear variable. The pipeline provides information for each source that allows for these kind of situations to be swiftly identified: the number number of measurements that contain siblings, and the number of relations. These are the columns n_sibl and n_rel , respectively, in the pipeline sources output file (refer to the Column Descriptions section). If these values are not 0 for a source then care must be taken when analysing variability. For the example source above, the values are n_sibl = 3 and n_rel = 1 . The missing flux can be seen in the lightcurve of the related source:","title":"Relations False Variability"},{"location":"design/association/#epoch-based-association","text":"The pipeline is able to associate inputs on an epoch basis. What this means is that, for example, all VAST Pilot Epoch 1 measurements are grouped together and are associated with grouped together Epoch 2 measurements, and so on. In doing this, duplicate measurements from within the same epoch are cut with the measurement kept being that which is closest to the centre of its respective image. The separation distance that defines a duplicate is defined in the pipeline run configuration file ( source_association.epoch_duplicate_radius ). The mode is activated by entering the images to be processed under an extra heading in the .yaml configuration file as demonstrated below. The heading acts as the epoch 'key', hence be sure to use a string that can be ordered as the heading to maintain the correct epoch order. config.yaml inputs : image : epoch01 : - /full/path/to/image1.fits - /full/path/to/image2.fits epoch02 : - /full/path/to/image3.fits The lightcurves below show the difference between 'regular' association (top) and 'epoch based' association (lower) for a source. For large surveys where transient and variablity searches on the epoch timescale is required, using this mode can greatly speed up the association stage. Warning Epoch based association does eliminate the full time resolution of your data! The base time resolution will be between the defined epochs.","title":"Epoch Based Association"},{"location":"design/association/#de-ruiter-and-epoch-based-association","text":"During the standard de Ruiter assoication , an initial on sky separation cut is made of \\(\\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,img}}}{2}\\) , where beamwidth limit is a value entered by the user and \\(\\theta_{\\text{bmaj,img}}\\) is the major component size of the restoring beam of the image being associated. When using epoch based association, an epoch that contains more than one image will have multiple values of \\(\\theta_{\\text{bmaj,img}}\\) to apply to the combined measurements. In this case, the maximum major axis value of all the images, \\(\\theta_{\\text{bmaj,max}}\\) , is used. Hence, the initial de Ruiter association step threshold becomes \\[ \\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,max}}}{2}. \\]","title":"de Ruiter and Epoch Based Association"},{"location":"design/association/#parallel-association","text":"When parallel association is used, the images to process are analysed and grouped into distinct patches of the sky that do not overlap. These distinct regions are then processed through the source association in parallel. It is recommended to use parallel association when your dataset covers three or more distinct patches of sky.","title":"Parallel Association"},{"location":"design/imageingest/","text":"Image & Selavy Catalogue Ingest \u00b6 This page details the stage of the pipeline that ingests the images to be processed. When the pipeline encounters an image for the first time (in any pipeline run), the image and accompanying selavy catalogue are uploaded to the pipeline database. The portion of a pipeline log file below shows the messages for the ingestion of three images. Note Once an image is uploaded then that image is available for all other runs to use without having to re-upload. 2021-03-11-12-48-21_log.txt 2021-03-11 12:59:49,751 loading INFO Reading image VAST_0127-73A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:49,756 utils INFO Adding new frequency band: 887 2021-03-11 12:59:49,771 utils INFO Created sky region 21.838, -73.121 2021-03-11 12:59:49,775 utils INFO Adding new-test-data to sky region 21.838, -73.121 2021-03-11 12:59:50,100 loading INFO Processed measurements dataframe of shape: (203, 40) 2021-03-11 12:59:50,273 loading INFO Bulk created #203 Measurement 2021-03-11 12:59:50,334 loading INFO Reading image VAST_2118+00A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:50,345 utils INFO Created sky region 322.439, -3.987 2021-03-11 12:59:50,347 utils INFO Adding new-test-data to sky region 322.439, -3.987 2021-03-11 12:59:50,577 loading INFO Processed measurements dataframe of shape: (148, 40) 2021-03-11 12:59:50,708 loading INFO Bulk created #148 Measurement 2021-03-11 12:59:50,736 loading INFO Reading image VAST_2118-06A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:50,749 utils INFO Created sky region 322.439, -4.487 2021-03-11 12:59:50,752 utils INFO Adding new-test-data to sky region 322.439, -4.487 2021-03-11 12:59:50,977 loading INFO Processed measurements dataframe of shape: (159, 40) 2021-03-11 12:59:51,111 loading INFO Bulk created #159 Measurement Ingest Steps Summary \u00b6 The FITS file is opened and read (the header is used to obtain metadata) along with the selavy component catalogue text file. The selavy component file is cleaned for erroneous components along with the calculation of extra measurements metrics such as signal-to-noise ratio , compactness and positional uncertainties. Also, optionally, flux errors are recalculated using the Condon (1997) method. See the Selavy Measurements Processing section below for further details. Median, minimum and maximum root-mean-square (RMS) values are read from the accompanying RMS image provided by the user and these values are attached to the image. The image is also attached to a sky region and a frequency band based on its properties (see Sky Region and Frequency Band ). The cleaned measurements (selavy components) are saved to a parquet file for repeated easy access. The overall image, band and sky region information for the pipeline run are written to a parquet file. See Ingest Steps Details for further details on the steps. Uniqueness \u00b6 The image uniqueness is defined by the filename. If you wish to upload a different version of the same image, e.g. a version where different Selavy settings were used in the source extraction, then you would have to make sure the image filename was different to the previously ingested image. Ingest Steps Details \u00b6 Selavy Measurements Processing \u00b6 Cleaning \u00b6 The selavy measurements are checked for erroneous values that could cause issues with the source association. Any sources that are found to have the following properties are removed: Sources that have a peak or integrated flux value of 0. Sources that have a bmaj or bmin value of 0. Sources that have a bmaj or bmin value less than half of the respective values of the image restoring beam. In addition, components are also checked for zero values that can be corrected, where the correction values to apply are defined in either the user or overall pipeline configuration files. The field names of these zero checks are defined in the table below. Field Name Correct with Location flux_int_err FLUX_DEFAULT_MIN_ERROR settings.py flux_peak_err FLUX_DEFAULT_MIN_ERROR settings.py ra_err POS_DEFAULT_MIN_ERROR settings.py dec_err POS_DEFAULT_MIN_ERROR settings.py local_rms measurements.selavy_local_rms_fill_value config.yaml Note settings.py refers to the pipeline configuration file webinterface/settings.py which is configured by the system administrator and cannot be modified by regular users. config.yaml refers to a pipeline run configuration file which is set by the user. Condon (1997) Flux & Positional Errors \u00b6 If selected in the pipeline run configuration file, the flux and positional errors are recalculated using the Condon (1997) method. The following errors are replaced with those that are recalculated: flux_peak_err flux_int_err err_bmaj err_bmin err_pa ra_err dec_err Positional Errors (de Ruiter method) \u00b6 Firstly, the systematic astrometry error from the user pipeline run configuration file ( measurements.ra_uncertainty and measurements.dec_uncertainty ) are applied to the measurement. These values are saved as ew_sys_err and ns_sys_err . Warning Currently the systematic errors applied at the pipeline run stage are then permanently fixed to the measurements, meaning that all subsequent runs using these measurements will use the fixed astrometic error. It is recommended to leave the values to the default value of 1.0. In order to apply the TraP de Ruiter association method, some extra positional error values are calculated. Firstly the ra_err and dec_err are used to estimate the largest angular uncertainty of the measurement which is recorded as the error_radius . It is estimated by finding the largest angular separation between the measurement coordinate and every coordinate combination of \\(ra \\pm \\delta ra\\) and \\(dec \\pm \\delta dec\\) . The final uncertainties are then defined as the hypotenuse values of ew_sys_err / ns_sys_err and the error_radius . These are defined as the uncertainty_ew and uncertainty_ns , respectively. The weights of the errors are defined as \\(\\frac{1}{\\text{uncertainty_x}^{2}}\\) where x is either ew or ns . Other Metrics \u00b6 The table below defines extra metrics that are added to the measurements. Field Name Description time The image datetime applied to the measurement. snr \\(\\frac{\\text{flux_peak}}{\\text{local_rms}}\\) . compactness \\(\\frac{\\text{flux_int}}{\\text{flux_peak}}\\) . flux_int_isl_ratio \\(\\frac{\\text{flux_int}}{\\text{total_island_int_flux}}\\) . flux_peak_isl_ratio \\(\\frac{\\text{flux_peak}}{\\text{total_island_peak_flux}}\\) . Sky Region \u00b6 The pipeline defines sky regions that are used to easily find images that cover the same region of the sky. A sky region is defined by: The central coodinate. The width in both ra and dec (the physical_bmaj and physical_bmin values are used here, see Uploaded Image Information ). An extraction radius ( xtr_radius ; fov_bmin is used here, again see Uploaded Image Information ). Hence, images that cover the exact same patch of sky will be assigned to the same sky region. Frequency Band \u00b6 The image is associated to a frequency object in the pipeline that represents the observational frequency information of the image. The frequency and the bandwidth are recorded. Image RMS Values \u00b6 The median, minimum and maximum values are calculated directly from the RMS map supplied by the user as a required input. This is achieved by loading the data from the FITS file and using the respective numpy operations on the data array to obtain the values. FITS Headers Used \u00b6 The table below defines which header fields are used to read the image information. Header Field Used For DATE-OBS The date and time of the observation. TIMESYS The timezone of the date and time. DURATION Duration of the observation in seconds. STOKES Stokes parameter of the image. TELESCOP Telescope name. BMAJ Major axis size of the restoring beam. BMIN Minor axis size of the restoring beam. BPA Position angle of the restoring beam. NAXIS1 Size of the image RA axis in pixels. NAXIS2 Size of the image Dec axis in pixels. CTYPE3(or 4) Check if equal to FREQ to use for frequency information. CRVAL3(or 4) Central frequency. CDELT3(or 4) Bandwidth. RESTFREQ and RESTBW can also be used as fallback options for frequency detection. The pixel scales are obtained with astropy.wcs.utils.proj_plane_pixel_scales . Uploaded Image Information \u00b6 The table below defines what is defined and uploaded using the meta data (FITS header) and other inputs. Field Name Default Description measurements_path n/a The system path to the corresponding selavy components file (saved as a parquet file by the pipeline) polarisation I The polarisation of the image (currently only Stokes I is supported). name n/a The name of the image which is taken from the filename. path n/a The system path to the image FITS file. noise_path '' The system path to the related noise image FITS file. background_path '' The system path to the related background image FITS file. datetime n/a Observational datetime of the image. jd n/a Observational datetime of the image in Julian days format. duration 0 Duration of the observation (if found in header). Seconds. ra n/a The Right Ascension of the image pointing centre. Degrees. dec n/a The Declination of the image pointing centre. Degrees. fov_bmaj n/a The estimated major axis field-of-view value - the radius_pixels multiplied by the major axis pixel size. Degrees. fov_bmin n/a The estimated minor axis field-of-view value - the radius_pixels multiplied by the minor axis pixel size. Degrees. physical_bmaj n/a The actual major axis on-sky size - the number of pixels on the major axis multiplied by the major axis pixel size. Degrees. physical_bmin n/a The actual minor axis on-sky size - the number of pixels on the minor axis multiplied by the minor axis pixel size. Degrees. radius_pixels n/a Estimated 'diameter' of the useable image area. Pixels. beam_bmaj n/a The size of the major axis of the image restoring beam. Degrees. beam_bmin n/a The size of the minor axis of the image restoring beam. Degrees. beam_bpa n/a The position angle of the image restoring beam. Degrees East of North. rms_median n/a The median RMS value from the RMS map. mJy/beam. rms_min n/a The minimum RMS value from the RMS map (pixel value). mJy/beam. rms_max n/a The maximum RMS value from the RMS map (pixel value). mJy/beam.","title":"Image & Selavy Catalogue Ingest"},{"location":"design/imageingest/#image-selavy-catalogue-ingest","text":"This page details the stage of the pipeline that ingests the images to be processed. When the pipeline encounters an image for the first time (in any pipeline run), the image and accompanying selavy catalogue are uploaded to the pipeline database. The portion of a pipeline log file below shows the messages for the ingestion of three images. Note Once an image is uploaded then that image is available for all other runs to use without having to re-upload. 2021-03-11-12-48-21_log.txt 2021-03-11 12:59:49,751 loading INFO Reading image VAST_0127-73A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:49,756 utils INFO Adding new frequency band: 887 2021-03-11 12:59:49,771 utils INFO Created sky region 21.838, -73.121 2021-03-11 12:59:49,775 utils INFO Adding new-test-data to sky region 21.838, -73.121 2021-03-11 12:59:50,100 loading INFO Processed measurements dataframe of shape: (203, 40) 2021-03-11 12:59:50,273 loading INFO Bulk created #203 Measurement 2021-03-11 12:59:50,334 loading INFO Reading image VAST_2118+00A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:50,345 utils INFO Created sky region 322.439, -3.987 2021-03-11 12:59:50,347 utils INFO Adding new-test-data to sky region 322.439, -3.987 2021-03-11 12:59:50,577 loading INFO Processed measurements dataframe of shape: (148, 40) 2021-03-11 12:59:50,708 loading INFO Bulk created #148 Measurement 2021-03-11 12:59:50,736 loading INFO Reading image VAST_2118-06A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:50,749 utils INFO Created sky region 322.439, -4.487 2021-03-11 12:59:50,752 utils INFO Adding new-test-data to sky region 322.439, -4.487 2021-03-11 12:59:50,977 loading INFO Processed measurements dataframe of shape: (159, 40) 2021-03-11 12:59:51,111 loading INFO Bulk created #159 Measurement","title":"Image &amp; Selavy Catalogue Ingest"},{"location":"design/imageingest/#ingest-steps-summary","text":"The FITS file is opened and read (the header is used to obtain metadata) along with the selavy component catalogue text file. The selavy component file is cleaned for erroneous components along with the calculation of extra measurements metrics such as signal-to-noise ratio , compactness and positional uncertainties. Also, optionally, flux errors are recalculated using the Condon (1997) method. See the Selavy Measurements Processing section below for further details. Median, minimum and maximum root-mean-square (RMS) values are read from the accompanying RMS image provided by the user and these values are attached to the image. The image is also attached to a sky region and a frequency band based on its properties (see Sky Region and Frequency Band ). The cleaned measurements (selavy components) are saved to a parquet file for repeated easy access. The overall image, band and sky region information for the pipeline run are written to a parquet file. See Ingest Steps Details for further details on the steps.","title":"Ingest Steps Summary"},{"location":"design/imageingest/#uniqueness","text":"The image uniqueness is defined by the filename. If you wish to upload a different version of the same image, e.g. a version where different Selavy settings were used in the source extraction, then you would have to make sure the image filename was different to the previously ingested image.","title":"Uniqueness"},{"location":"design/imageingest/#ingest-steps-details","text":"","title":"Ingest Steps Details"},{"location":"design/imageingest/#selavy-measurements-processing","text":"","title":"Selavy Measurements Processing"},{"location":"design/imageingest/#cleaning","text":"The selavy measurements are checked for erroneous values that could cause issues with the source association. Any sources that are found to have the following properties are removed: Sources that have a peak or integrated flux value of 0. Sources that have a bmaj or bmin value of 0. Sources that have a bmaj or bmin value less than half of the respective values of the image restoring beam. In addition, components are also checked for zero values that can be corrected, where the correction values to apply are defined in either the user or overall pipeline configuration files. The field names of these zero checks are defined in the table below. Field Name Correct with Location flux_int_err FLUX_DEFAULT_MIN_ERROR settings.py flux_peak_err FLUX_DEFAULT_MIN_ERROR settings.py ra_err POS_DEFAULT_MIN_ERROR settings.py dec_err POS_DEFAULT_MIN_ERROR settings.py local_rms measurements.selavy_local_rms_fill_value config.yaml Note settings.py refers to the pipeline configuration file webinterface/settings.py which is configured by the system administrator and cannot be modified by regular users. config.yaml refers to a pipeline run configuration file which is set by the user.","title":"Cleaning"},{"location":"design/imageingest/#condon-1997-flux-positional-errors","text":"If selected in the pipeline run configuration file, the flux and positional errors are recalculated using the Condon (1997) method. The following errors are replaced with those that are recalculated: flux_peak_err flux_int_err err_bmaj err_bmin err_pa ra_err dec_err","title":"Condon (1997) Flux &amp; Positional Errors"},{"location":"design/imageingest/#positional-errors-de-ruiter-method","text":"Firstly, the systematic astrometry error from the user pipeline run configuration file ( measurements.ra_uncertainty and measurements.dec_uncertainty ) are applied to the measurement. These values are saved as ew_sys_err and ns_sys_err . Warning Currently the systematic errors applied at the pipeline run stage are then permanently fixed to the measurements, meaning that all subsequent runs using these measurements will use the fixed astrometic error. It is recommended to leave the values to the default value of 1.0. In order to apply the TraP de Ruiter association method, some extra positional error values are calculated. Firstly the ra_err and dec_err are used to estimate the largest angular uncertainty of the measurement which is recorded as the error_radius . It is estimated by finding the largest angular separation between the measurement coordinate and every coordinate combination of \\(ra \\pm \\delta ra\\) and \\(dec \\pm \\delta dec\\) . The final uncertainties are then defined as the hypotenuse values of ew_sys_err / ns_sys_err and the error_radius . These are defined as the uncertainty_ew and uncertainty_ns , respectively. The weights of the errors are defined as \\(\\frac{1}{\\text{uncertainty_x}^{2}}\\) where x is either ew or ns .","title":"Positional Errors (de Ruiter method)"},{"location":"design/imageingest/#other-metrics","text":"The table below defines extra metrics that are added to the measurements. Field Name Description time The image datetime applied to the measurement. snr \\(\\frac{\\text{flux_peak}}{\\text{local_rms}}\\) . compactness \\(\\frac{\\text{flux_int}}{\\text{flux_peak}}\\) . flux_int_isl_ratio \\(\\frac{\\text{flux_int}}{\\text{total_island_int_flux}}\\) . flux_peak_isl_ratio \\(\\frac{\\text{flux_peak}}{\\text{total_island_peak_flux}}\\) .","title":"Other Metrics"},{"location":"design/imageingest/#sky-region","text":"The pipeline defines sky regions that are used to easily find images that cover the same region of the sky. A sky region is defined by: The central coodinate. The width in both ra and dec (the physical_bmaj and physical_bmin values are used here, see Uploaded Image Information ). An extraction radius ( xtr_radius ; fov_bmin is used here, again see Uploaded Image Information ). Hence, images that cover the exact same patch of sky will be assigned to the same sky region.","title":"Sky Region"},{"location":"design/imageingest/#frequency-band","text":"The image is associated to a frequency object in the pipeline that represents the observational frequency information of the image. The frequency and the bandwidth are recorded.","title":"Frequency Band"},{"location":"design/imageingest/#image-rms-values","text":"The median, minimum and maximum values are calculated directly from the RMS map supplied by the user as a required input. This is achieved by loading the data from the FITS file and using the respective numpy operations on the data array to obtain the values.","title":"Image RMS Values"},{"location":"design/imageingest/#fits-headers-used","text":"The table below defines which header fields are used to read the image information. Header Field Used For DATE-OBS The date and time of the observation. TIMESYS The timezone of the date and time. DURATION Duration of the observation in seconds. STOKES Stokes parameter of the image. TELESCOP Telescope name. BMAJ Major axis size of the restoring beam. BMIN Minor axis size of the restoring beam. BPA Position angle of the restoring beam. NAXIS1 Size of the image RA axis in pixels. NAXIS2 Size of the image Dec axis in pixels. CTYPE3(or 4) Check if equal to FREQ to use for frequency information. CRVAL3(or 4) Central frequency. CDELT3(or 4) Bandwidth. RESTFREQ and RESTBW can also be used as fallback options for frequency detection. The pixel scales are obtained with astropy.wcs.utils.proj_plane_pixel_scales .","title":"FITS Headers Used"},{"location":"design/imageingest/#uploaded-image-information","text":"The table below defines what is defined and uploaded using the meta data (FITS header) and other inputs. Field Name Default Description measurements_path n/a The system path to the corresponding selavy components file (saved as a parquet file by the pipeline) polarisation I The polarisation of the image (currently only Stokes I is supported). name n/a The name of the image which is taken from the filename. path n/a The system path to the image FITS file. noise_path '' The system path to the related noise image FITS file. background_path '' The system path to the related background image FITS file. datetime n/a Observational datetime of the image. jd n/a Observational datetime of the image in Julian days format. duration 0 Duration of the observation (if found in header). Seconds. ra n/a The Right Ascension of the image pointing centre. Degrees. dec n/a The Declination of the image pointing centre. Degrees. fov_bmaj n/a The estimated major axis field-of-view value - the radius_pixels multiplied by the major axis pixel size. Degrees. fov_bmin n/a The estimated minor axis field-of-view value - the radius_pixels multiplied by the minor axis pixel size. Degrees. physical_bmaj n/a The actual major axis on-sky size - the number of pixels on the major axis multiplied by the major axis pixel size. Degrees. physical_bmin n/a The actual minor axis on-sky size - the number of pixels on the minor axis multiplied by the minor axis pixel size. Degrees. radius_pixels n/a Estimated 'diameter' of the useable image area. Pixels. beam_bmaj n/a The size of the major axis of the image restoring beam. Degrees. beam_bmin n/a The size of the minor axis of the image restoring beam. Degrees. beam_bpa n/a The position angle of the image restoring beam. Degrees East of North. rms_median n/a The median RMS value from the RMS map. mJy/beam. rms_min n/a The minimum RMS value from the RMS map (pixel value). mJy/beam. rms_max n/a The maximum RMS value from the RMS map (pixel value). mJy/beam.","title":"Uploaded Image Information"},{"location":"design/monitor/","text":"Forced Measurements \u00b6 This page details the forced measurements obtained by the pipeline. Definition \u00b6 When source_monitoring.monitor: true is set in the pipeline run configuration file, any sources that have non-detections in their lightcurve will have these measurements 'filled in' by performing forced measurements . This means that the flux at the source's current average position in the non-detection image is forcefully measured by fitting a Gaussian with the same shape as the respective image restoring beam. Forced measurements are labelled in the measurements table and parquet files by the column forced . Note Forced measurements are local to a pipeline run - they will not appear in any other pipeline run. Warning Forced measurements are not performed within 3 beamwidths of the image edge. Minimum Sigma Filter \u00b6 Before forced measurements are processed, a minimum sigma check is made to make sure that the forced measurements would provide useful information. For example, a dataset may contain an image that has significantly less sensitivity than the other images. In this case a faint source in the more sensitive images will not be expected to be seen in the less sensitive image. To avoid unnecessary computation, this source is not forcefully measured. The check is performed like that which is made in the New Sources process where the signal-to-noise ratio is calculated using the rms \\(_{min}\\) of the image it is to be extracted from. Hence, for a forced measurement to take place the following condition must be met: \\[ \\frac{f_{peak,det}}{\\text{rms}_{min,i}} > \\text{source_monitoring.min_sigma}, \\] where \\(i\\) is the image for which the measurement is to be forcefully measured. source_monitoring.min_sigma is able to be controlled by the user in the pipeline run configuration file. By default the value is set to 3.0. Tip Setting source_monitoring.min_sigma: 0.0 will ensure that all forced measurements are performed regardless of signal-to-noise. Configuration File Options \u00b6 The following options are present in the pipeline run configuration file to users along with their defaults: config.yaml source_monitoring : monitor : false min_sigma : 3.0 edge_buffer_scale : 1.2 cluster_threshold : 3.0 allow_nan : false source_monitoring.monitor turns forced measurements on ( true ) or off ( false ). source_monitoring.min_sigma controls the the minimum sigma check threshold as explained in Minimum Sigma Filter . source_monitoring.edge_buffer_scale controls the size of the buffer from the image edge where forced measurements are not performed ( source_monitoring.edge_buffer_scale \\(\\times 3\\theta_{beam}\\) ). An error can sometimes occur that increasing this value can solve. source_monitoring.cluster_threshold is directly passed to the forced photometry package used by the pipeline. It defines the multiple of major_axes to use for identifying clusters. source_monitoring.allow_nan is directly passed to the forced photometry package used by the pipeline. It defines whether NaN values are allowed to be present in the extraction area in the rms or background maps, i.e. true would mean that NaN values are allowed. Software - forced_phot \u00b6 The software used to perform the forced measurements, forced_phot , was written by David Kaplan and can be found on the VAST GitHub here .","title":"Forced Measurements"},{"location":"design/monitor/#forced-measurements","text":"This page details the forced measurements obtained by the pipeline.","title":"Forced Measurements"},{"location":"design/monitor/#definition","text":"When source_monitoring.monitor: true is set in the pipeline run configuration file, any sources that have non-detections in their lightcurve will have these measurements 'filled in' by performing forced measurements . This means that the flux at the source's current average position in the non-detection image is forcefully measured by fitting a Gaussian with the same shape as the respective image restoring beam. Forced measurements are labelled in the measurements table and parquet files by the column forced . Note Forced measurements are local to a pipeline run - they will not appear in any other pipeline run. Warning Forced measurements are not performed within 3 beamwidths of the image edge.","title":"Definition"},{"location":"design/monitor/#minimum-sigma-filter","text":"Before forced measurements are processed, a minimum sigma check is made to make sure that the forced measurements would provide useful information. For example, a dataset may contain an image that has significantly less sensitivity than the other images. In this case a faint source in the more sensitive images will not be expected to be seen in the less sensitive image. To avoid unnecessary computation, this source is not forcefully measured. The check is performed like that which is made in the New Sources process where the signal-to-noise ratio is calculated using the rms \\(_{min}\\) of the image it is to be extracted from. Hence, for a forced measurement to take place the following condition must be met: \\[ \\frac{f_{peak,det}}{\\text{rms}_{min,i}} > \\text{source_monitoring.min_sigma}, \\] where \\(i\\) is the image for which the measurement is to be forcefully measured. source_monitoring.min_sigma is able to be controlled by the user in the pipeline run configuration file. By default the value is set to 3.0. Tip Setting source_monitoring.min_sigma: 0.0 will ensure that all forced measurements are performed regardless of signal-to-noise.","title":"Minimum Sigma Filter"},{"location":"design/monitor/#configuration-file-options","text":"The following options are present in the pipeline run configuration file to users along with their defaults: config.yaml source_monitoring : monitor : false min_sigma : 3.0 edge_buffer_scale : 1.2 cluster_threshold : 3.0 allow_nan : false source_monitoring.monitor turns forced measurements on ( true ) or off ( false ). source_monitoring.min_sigma controls the the minimum sigma check threshold as explained in Minimum Sigma Filter . source_monitoring.edge_buffer_scale controls the size of the buffer from the image edge where forced measurements are not performed ( source_monitoring.edge_buffer_scale \\(\\times 3\\theta_{beam}\\) ). An error can sometimes occur that increasing this value can solve. source_monitoring.cluster_threshold is directly passed to the forced photometry package used by the pipeline. It defines the multiple of major_axes to use for identifying clusters. source_monitoring.allow_nan is directly passed to the forced photometry package used by the pipeline. It defines whether NaN values are allowed to be present in the extraction area in the rms or background maps, i.e. true would mean that NaN values are allowed.","title":"Configuration File Options"},{"location":"design/monitor/#software-forced_phot","text":"The software used to perform the forced measurements, forced_phot , was written by David Kaplan and can be found on the VAST GitHub here .","title":"Software - forced_phot"},{"location":"design/newsources/","text":"New Sources \u00b6 This page details the new source analysis performed by the pipeline. Definition \u00b6 A new source is defined as a source that is detected during the pipeline run that was not detected in any previous epoch of the location of the source, and has a peak flux such that it should have been detected. Note Remember that pipeline runs are self-contained - i.e. a run does not have any knowledge of another run, hence new sources are local to their pipeline run. New Source Process \u00b6 The pipeline identifies new sources by using the following steps: Sources are found that have 'incomplete' light curves, i.e. there are epochs in the pipeline run of the source location where the source is not detected. The would-be 'ideal' coverage is then calculated to determine which images contain the source location but have a non-detection. Remove sources where the epoch of the first detection is also the earliest possible detection epoch. For the remaining sources a general rms check is made to answer the question of should this source be expected to be detected at all in the previous epochs. This is done by taking the minimum \\(\\text{rms}_{min}\\) of all the non-detection images and making sure that \\[ \\frac{f_{peak,det}}{\\text{minimum rms}_{min}} > \\text{new_sources.min_sigma}, \\] where \\(f_{peak,det}\\) is the peak flux density of the first detection of the source. The default value of new_sources.min_sigma is 5.0, but the parameter can be controlled by the user in the pipeline run configuration file. The sources that meet the above criteria are marked as a new source . The new source high sigma value is calculated for all new sources. New Source High Sigma \u00b6 In the process detailed above, the rms check is made against the minimum rms of the previous non-detection images. This might not be an accurate representation of the rms at the source's actual location in the image, for example, the rms might be high at the source location such that a detection of the source wouldn't be expected at the \\(5\\sigma\\) level. To account for this the new source high sigma value is calculated for all new sources. For each non-detection image for a source, the true signal-to-noise ratio the source would have in the non-detection image is calculated, i.e. \\[ \\text{new source true sigma}_i = \\frac{f_{peak,det}}{\\text{rms}_{location,i}} \\] where \\(\\text{rms}_{location,i}\\) is the rms at the source location for each non-detection image, \\(i\\) . The new source high sigma is then equal to the maximum \\(\\text{rms}_{location,i}\\) . The value can be used to filter those new sources which would be borderline detections, or not expected to be detected at all, at the actual location in the previous images. This allows users to concentrate on the significant new sources. Viewing New Sources \u00b6 New sources are marked as new on the website interface (see Source Pages ) and in the source parquet file output there is a boolean column named new .","title":"New Sources"},{"location":"design/newsources/#new-sources","text":"This page details the new source analysis performed by the pipeline.","title":"New Sources"},{"location":"design/newsources/#definition","text":"A new source is defined as a source that is detected during the pipeline run that was not detected in any previous epoch of the location of the source, and has a peak flux such that it should have been detected. Note Remember that pipeline runs are self-contained - i.e. a run does not have any knowledge of another run, hence new sources are local to their pipeline run.","title":"Definition"},{"location":"design/newsources/#new-source-process","text":"The pipeline identifies new sources by using the following steps: Sources are found that have 'incomplete' light curves, i.e. there are epochs in the pipeline run of the source location where the source is not detected. The would-be 'ideal' coverage is then calculated to determine which images contain the source location but have a non-detection. Remove sources where the epoch of the first detection is also the earliest possible detection epoch. For the remaining sources a general rms check is made to answer the question of should this source be expected to be detected at all in the previous epochs. This is done by taking the minimum \\(\\text{rms}_{min}\\) of all the non-detection images and making sure that \\[ \\frac{f_{peak,det}}{\\text{minimum rms}_{min}} > \\text{new_sources.min_sigma}, \\] where \\(f_{peak,det}\\) is the peak flux density of the first detection of the source. The default value of new_sources.min_sigma is 5.0, but the parameter can be controlled by the user in the pipeline run configuration file. The sources that meet the above criteria are marked as a new source . The new source high sigma value is calculated for all new sources.","title":"New Source Process"},{"location":"design/newsources/#new-source-high-sigma","text":"In the process detailed above, the rms check is made against the minimum rms of the previous non-detection images. This might not be an accurate representation of the rms at the source's actual location in the image, for example, the rms might be high at the source location such that a detection of the source wouldn't be expected at the \\(5\\sigma\\) level. To account for this the new source high sigma value is calculated for all new sources. For each non-detection image for a source, the true signal-to-noise ratio the source would have in the non-detection image is calculated, i.e. \\[ \\text{new source true sigma}_i = \\frac{f_{peak,det}}{\\text{rms}_{location,i}} \\] where \\(\\text{rms}_{location,i}\\) is the rms at the source location for each non-detection image, \\(i\\) . The new source high sigma is then equal to the maximum \\(\\text{rms}_{location,i}\\) . The value can be used to filter those new sources which would be borderline detections, or not expected to be detected at all, at the actual location in the previous images. This allows users to concentrate on the significant new sources.","title":"New Source High Sigma"},{"location":"design/newsources/#viewing-new-sources","text":"New sources are marked as new on the website interface (see Source Pages ) and in the source parquet file output there is a boolean column named new .","title":"Viewing New Sources"},{"location":"design/overview/","text":"Pipeline Steps Overview \u00b6 This page gives an overview of the processing steps of a pipeline run. Each section contains a link to a feature page that contains more details. Terminology \u00b6 Run A single pipeline dataset defined by a configuration file. Image A FITS image that is being processed as part of a pipeline run. It also has related inputs of the selavy source catalogue, and the noise and background images also produced by selavy. Measurement An extracted measurement read from the selavy source catalogue from an associated image. The only measurements produced by the pipeline are forced measurements which are performed when monitoring is used. Source A group of measurements that have been identified as the same astrophysical source by a pipeline association method. Pipeline Processing Steps \u00b6 Note Each pipeline run is self-aware only, which means that each run does not draw on the results of other runs. However, since images and their measurements don't change, subsequent runs that use any image that was ingested as part of a previous run will not be ingested again. 1. Image & Selavy Catalogue Ingest \u00b6 Full details: Image & Selavy Catalogue Ingest . The first stage of the pipeline is to read and ingest to the database the input data that has been provided in the configuration file. This includes determing statistics about the image footprint and properties, and also importing and cleaning the associated measurements from the selavy file. The errors on the measurements can also be recalculated at this stage based upon the Condon (1997) method. Image uniqueness is determined by the filename, and once the image is ingested, it is available for other pipeline runs to use without having to re-ingest. 2. Source Association \u00b6 Full details: Source Association . Once all images and measurments have been ingested the source association step is performed, where measurements over time are associated to a unique astrophysical source. Images are arranged chronologically and association is performed on an image by image basis, or as a grouped \"epoch\" if epoch based association is used. The association is performed as per the settings entered in the run configuration file. 3. Ideal Coverage & New Source Analysis \u00b6 Full details: New Sources . With the measurements associated the sources are analysed to check for non-detections over time and whether the source should have been seen in any non-detection images. The ideal coverage calculation is also used to determine any sources that should be marked as new , i.e. a source that has appeared over time that was not detected in the first image of its location on the sky. The non-detections are then passed to the forced monitoring step. 4. Monitoring Forced Measurements \u00b6 Full details: Forced Measurements . This step is optional. The non-detections which form gaps in the lightcurves of each source are filled in by forcefully extracting a flux measurement at the location of the source. 5. Source Statistics Calculation \u00b6 Full details: Source Statistics . Statistics are calculated for each source such as the weighted average sky position, average flux values, variability metrics (including two-epoch pair metrics) and various counts. 6. Database Upload \u00b6 All the results from the pipeline run are uploaded to the database. Specifically at the end of the run the following is written to the database: Sources and their statistics. Relations between sources. Associations. For large runs this can be a substantial component of the pipeline run time. Bulk upload statements will be seen in the pipeline run log file such as these shown below: 2021-03-11-12-48-21_log.txt 2021-03-11 13:00:04,893 loading INFO Bulk created #557 Source 2021-03-11 13:00:04,910 loading INFO Populate \"related\" field of sources... 2021-03-11 13:00:04,919 loading INFO Bulk created #29 RelatedSource 2021-03-11 13:00:04,943 loading INFO Upload associations... 2021-03-11 13:00:05,650 loading INFO Bulk created #3276 Association","title":"Pipeline Steps Overview"},{"location":"design/overview/#pipeline-steps-overview","text":"This page gives an overview of the processing steps of a pipeline run. Each section contains a link to a feature page that contains more details.","title":"Pipeline Steps Overview"},{"location":"design/overview/#terminology","text":"Run A single pipeline dataset defined by a configuration file. Image A FITS image that is being processed as part of a pipeline run. It also has related inputs of the selavy source catalogue, and the noise and background images also produced by selavy. Measurement An extracted measurement read from the selavy source catalogue from an associated image. The only measurements produced by the pipeline are forced measurements which are performed when monitoring is used. Source A group of measurements that have been identified as the same astrophysical source by a pipeline association method.","title":"Terminology"},{"location":"design/overview/#pipeline-processing-steps","text":"Note Each pipeline run is self-aware only, which means that each run does not draw on the results of other runs. However, since images and their measurements don't change, subsequent runs that use any image that was ingested as part of a previous run will not be ingested again.","title":"Pipeline Processing Steps"},{"location":"design/overview/#1-image-selavy-catalogue-ingest","text":"Full details: Image & Selavy Catalogue Ingest . The first stage of the pipeline is to read and ingest to the database the input data that has been provided in the configuration file. This includes determing statistics about the image footprint and properties, and also importing and cleaning the associated measurements from the selavy file. The errors on the measurements can also be recalculated at this stage based upon the Condon (1997) method. Image uniqueness is determined by the filename, and once the image is ingested, it is available for other pipeline runs to use without having to re-ingest.","title":"1. Image &amp; Selavy Catalogue Ingest"},{"location":"design/overview/#2-source-association","text":"Full details: Source Association . Once all images and measurments have been ingested the source association step is performed, where measurements over time are associated to a unique astrophysical source. Images are arranged chronologically and association is performed on an image by image basis, or as a grouped \"epoch\" if epoch based association is used. The association is performed as per the settings entered in the run configuration file.","title":"2. Source Association"},{"location":"design/overview/#3-ideal-coverage-new-source-analysis","text":"Full details: New Sources . With the measurements associated the sources are analysed to check for non-detections over time and whether the source should have been seen in any non-detection images. The ideal coverage calculation is also used to determine any sources that should be marked as new , i.e. a source that has appeared over time that was not detected in the first image of its location on the sky. The non-detections are then passed to the forced monitoring step.","title":"3. Ideal Coverage &amp; New Source Analysis"},{"location":"design/overview/#4-monitoring-forced-measurements","text":"Full details: Forced Measurements . This step is optional. The non-detections which form gaps in the lightcurves of each source are filled in by forcefully extracting a flux measurement at the location of the source.","title":"4. Monitoring Forced Measurements"},{"location":"design/overview/#5-source-statistics-calculation","text":"Full details: Source Statistics . Statistics are calculated for each source such as the weighted average sky position, average flux values, variability metrics (including two-epoch pair metrics) and various counts.","title":"5. Source Statistics Calculation"},{"location":"design/overview/#6-database-upload","text":"All the results from the pipeline run are uploaded to the database. Specifically at the end of the run the following is written to the database: Sources and their statistics. Relations between sources. Associations. For large runs this can be a substantial component of the pipeline run time. Bulk upload statements will be seen in the pipeline run log file such as these shown below: 2021-03-11-12-48-21_log.txt 2021-03-11 13:00:04,893 loading INFO Bulk created #557 Source 2021-03-11 13:00:04,910 loading INFO Populate \"related\" field of sources... 2021-03-11 13:00:04,919 loading INFO Bulk created #29 RelatedSource 2021-03-11 13:00:04,943 loading INFO Upload associations... 2021-03-11 13:00:05,650 loading INFO Bulk created #3276 Association","title":"6. Database Upload"},{"location":"design/sourcestats/","text":"Source Statistics \u00b6 This page details the source statistics that are calculated by the pipeline. Overview \u00b6 The table below provides a summary of all the statistic and counts provided by the pipeline. See the Variability Statistics section for the table containing the variability metrics. Note Remember that all source statistics and counts are calculated from the individual measurements that are associated with the source. Parameter Includes Forced Meas. Description wavg_ra No The weighted average of the Right Ascension, degrees. wavg_dec No The weighted average of the Declination, degrees. wavg_uncertainty_ew No The weighted average uncertainty in the east-west (RA) direction, degrees. wavg_uncertainty_ns No The weighted average uncertainty in the north-south (Dec) direction, degrees. avg_flux_int Yes The average integrated flux, mJy. max_flux_int Yes The maximum integrated flux value, mJy. min_flux_int Yes The minimum integrated flux value, mJy. avg_flux_peak Yes The average peak flux, mJy/beam. max_flux_peak Yes The maximum peak flux value, mJy/beam. min_flux_peak Yes The minimum peak flux value, mJy/beam. min_flux_int_isl_ratio Yes The minimum integrated flux value island ratio (int_flux / total_isl_int_flux). min_flux_peak_isl_ratio Yes The minimum peak flux value island ratio (peak_flux / total_isl_peak_flux). avg_compactness No The average compactness of the source (compactness is defined by int_flux / peak_flux). min_snr No The minimum signal-to-noise ratio of the source. max_snr No The maximum signal-to-noise ratio of the source. n_neighbour_dist n/a On sky separation distance to the nearest neighbour within the same run, degrees (arcmin on webserver). new_high_sigma n/a The largest sigma value a new source would have if it was placed at its location in the previous images it was not detected in. See New Sources for more information. New sources only. n_meas Yes The total number of measurements associated to the source. Named Total Datapoints on the webserver. n_meas_sel No The total number of selavy measurements associated to the source. Named Selavy Datapoints on the webserver. n_meas_forced Yes The total number of forced measurements associated to the source. Named Forced Datapoints on the webserver. n_rel n/a The total number of relations the source has. See Source Association . Named Relations on the webserver. n_sibl n/a The total number measurements that has a sibling. On the webserver tables this is firstly presented as a boolean column of if the source contains measurements that have a sibling. Variability Statistics \u00b6 Below is a table describing the variability metrics of the source. See the following sections for further explanation of these metrics. Parameter Includes Forced Meas. Description v_int Yes The \\(V\\) metric for the integrated flux. v_peak Yes The \\(V\\) metric for the peak flux. eta_int Yes The \\(\\eta\\) metric for the integrated flux. eta_peak Yes The \\(\\eta\\) metric for the peak flux. vs_abs_significant_max_int Yes The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. m_abs_significant_max_int Yes The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. vs_abs_significant_max_peak Yes The \\(\\mid V_s \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. m_abs_significant_max_peak Yes The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. V and \u03b7 Metrics \u00b6 The \\(V\\) and \\(\\eta\\) metrics are the same as those used by the LOFAR Transients Pipeline (TraP) , for a complete description please refer to Swinbank et al. (2015) . In the VAST Pipeline, the metrics are calculated twice, for both the integrated and peak fluxes. \\(V\\) is the proportional flux variability of the source and is given by the ratio of the sample standard deviation ( \\(s\\) ) and mean of the flux, \\(I\\) : \\[ V = \\frac{s}{\\overline{I}} = \\frac{1}{\\overline{I}} \\sqrt{\\frac{N}{N - 1}\\left(\\overline{I^{2}}-\\overline{I}^{2}\\right)}. \\] The \\(\\eta\\) value is the significance of the variability, based on \\(\\chi^{2}\\) statistics, and is given by: \\[ \\eta = \\frac{N}{N - 1}\\left(\\overline{wI^{2}} - \\frac{\\overline{wI}^{2}}{\\overline{w}}\\right) \\] where \\(w\\) is the uncertainty ( \\(e\\) ) in \\(I\\) of a measurement, and is given by \\(w=\\frac{1}{e}\\) . Two-Epoch Metrics \u00b6 Alternative variability metrics, \\(V_s\\) and \\(m\\) , are also calculated which we refer to as the 'two-epoch metrics'. They are calculated for each unique pair of measurements assoicated with the source, with the most significant pair of values attached to the source (see section below). Please refer to Mooley et al. (2016) for further details. Note All the two-epoch pair \\(V_s\\) and \\(m\\) values for a run are saved in the output file measurement_pairs.parquet for offline analysis. \\(V_s\\) is a statistic to compare the flux densities of a source between two-epochs and is given by: \\[ V_s = \\frac{\\Delta S}{\\sigma} = \\frac{S_1 - S_2}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{2}^{2}}} \\] where \\(S\\) is the flux and \\(\\sigma\\) is the associated error. This metric is known to follow a Student-t distribution. Typically, in the literature, a source is defined as variable if this parameter is beyond the 95% confidence interval, i.e.: \\[ \\mid V_s \\mid \\geq 4.3. \\] \\(m\\) is a moduluation index variable given by: \\[ m = \\frac{\\Delta S}{\\overline{S}} \\] where \\(\\overline{S}\\) is the mean of the flux densities \\(S_1\\) and \\(S_2\\) . Typically, in the literature, the threshold for this value for a source to be considered variable is: \\[ \\mid m \\mid \\gt 0.26, \\] which equates to a variability of 30%. However the user is free to set their own level to define variablity. Significant Source Values \u00b6 The \\(V_s\\) and \\(m\\) metrics of the 'maximum signficant pair' is attached to the source. The maximum significant pair is determind by selecting the most significant \\(\\mid m \\mid\\) value given a minimum \\(V_s\\) threshold which is defined in the pipeline configuration file variability.source_aggregate_pair_metrics_min_abs_vs : config.yaml variability : # Only measurement pairs where the Vs metric exceeds this value are selected for the # aggregate pair metrics that are stored in Source objects. source_aggregate_pair_metrics_min_abs_vs : 4.3 By default this value is set to 4.3. For example, if a source with three associatied measurements gave the following pair metrics: Pair \\(\\mid V_s \\mid\\) \\(\\mid m \\mid\\) A-B 4.5 0.1 B-C 2.5 0.05 A-C 4.3 0.4 then the A-C pair metrics are attached to the source as the most significant. This can be used to quickly determine significant two-epoch variability for a source. If there are no pair values above the minimum \\(V_s\\) threshold then these values attached to the source will be 0. The measurement_pairs.parquet file can be used to manually explore the measurement pairs if one wishes to lower the threshold.","title":"Source Statistics"},{"location":"design/sourcestats/#source-statistics","text":"This page details the source statistics that are calculated by the pipeline.","title":"Source Statistics"},{"location":"design/sourcestats/#overview","text":"The table below provides a summary of all the statistic and counts provided by the pipeline. See the Variability Statistics section for the table containing the variability metrics. Note Remember that all source statistics and counts are calculated from the individual measurements that are associated with the source. Parameter Includes Forced Meas. Description wavg_ra No The weighted average of the Right Ascension, degrees. wavg_dec No The weighted average of the Declination, degrees. wavg_uncertainty_ew No The weighted average uncertainty in the east-west (RA) direction, degrees. wavg_uncertainty_ns No The weighted average uncertainty in the north-south (Dec) direction, degrees. avg_flux_int Yes The average integrated flux, mJy. max_flux_int Yes The maximum integrated flux value, mJy. min_flux_int Yes The minimum integrated flux value, mJy. avg_flux_peak Yes The average peak flux, mJy/beam. max_flux_peak Yes The maximum peak flux value, mJy/beam. min_flux_peak Yes The minimum peak flux value, mJy/beam. min_flux_int_isl_ratio Yes The minimum integrated flux value island ratio (int_flux / total_isl_int_flux). min_flux_peak_isl_ratio Yes The minimum peak flux value island ratio (peak_flux / total_isl_peak_flux). avg_compactness No The average compactness of the source (compactness is defined by int_flux / peak_flux). min_snr No The minimum signal-to-noise ratio of the source. max_snr No The maximum signal-to-noise ratio of the source. n_neighbour_dist n/a On sky separation distance to the nearest neighbour within the same run, degrees (arcmin on webserver). new_high_sigma n/a The largest sigma value a new source would have if it was placed at its location in the previous images it was not detected in. See New Sources for more information. New sources only. n_meas Yes The total number of measurements associated to the source. Named Total Datapoints on the webserver. n_meas_sel No The total number of selavy measurements associated to the source. Named Selavy Datapoints on the webserver. n_meas_forced Yes The total number of forced measurements associated to the source. Named Forced Datapoints on the webserver. n_rel n/a The total number of relations the source has. See Source Association . Named Relations on the webserver. n_sibl n/a The total number measurements that has a sibling. On the webserver tables this is firstly presented as a boolean column of if the source contains measurements that have a sibling.","title":"Overview"},{"location":"design/sourcestats/#variability-statistics","text":"Below is a table describing the variability metrics of the source. See the following sections for further explanation of these metrics. Parameter Includes Forced Meas. Description v_int Yes The \\(V\\) metric for the integrated flux. v_peak Yes The \\(V\\) metric for the peak flux. eta_int Yes The \\(\\eta\\) metric for the integrated flux. eta_peak Yes The \\(\\eta\\) metric for the peak flux. vs_abs_significant_max_int Yes The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. m_abs_significant_max_int Yes The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. vs_abs_significant_max_peak Yes The \\(\\mid V_s \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. m_abs_significant_max_peak Yes The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair.","title":"Variability Statistics"},{"location":"design/sourcestats/#v-and-metrics","text":"The \\(V\\) and \\(\\eta\\) metrics are the same as those used by the LOFAR Transients Pipeline (TraP) , for a complete description please refer to Swinbank et al. (2015) . In the VAST Pipeline, the metrics are calculated twice, for both the integrated and peak fluxes. \\(V\\) is the proportional flux variability of the source and is given by the ratio of the sample standard deviation ( \\(s\\) ) and mean of the flux, \\(I\\) : \\[ V = \\frac{s}{\\overline{I}} = \\frac{1}{\\overline{I}} \\sqrt{\\frac{N}{N - 1}\\left(\\overline{I^{2}}-\\overline{I}^{2}\\right)}. \\] The \\(\\eta\\) value is the significance of the variability, based on \\(\\chi^{2}\\) statistics, and is given by: \\[ \\eta = \\frac{N}{N - 1}\\left(\\overline{wI^{2}} - \\frac{\\overline{wI}^{2}}{\\overline{w}}\\right) \\] where \\(w\\) is the uncertainty ( \\(e\\) ) in \\(I\\) of a measurement, and is given by \\(w=\\frac{1}{e}\\) .","title":"V and \u03b7 Metrics"},{"location":"design/sourcestats/#two-epoch-metrics","text":"Alternative variability metrics, \\(V_s\\) and \\(m\\) , are also calculated which we refer to as the 'two-epoch metrics'. They are calculated for each unique pair of measurements assoicated with the source, with the most significant pair of values attached to the source (see section below). Please refer to Mooley et al. (2016) for further details. Note All the two-epoch pair \\(V_s\\) and \\(m\\) values for a run are saved in the output file measurement_pairs.parquet for offline analysis. \\(V_s\\) is a statistic to compare the flux densities of a source between two-epochs and is given by: \\[ V_s = \\frac{\\Delta S}{\\sigma} = \\frac{S_1 - S_2}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{2}^{2}}} \\] where \\(S\\) is the flux and \\(\\sigma\\) is the associated error. This metric is known to follow a Student-t distribution. Typically, in the literature, a source is defined as variable if this parameter is beyond the 95% confidence interval, i.e.: \\[ \\mid V_s \\mid \\geq 4.3. \\] \\(m\\) is a moduluation index variable given by: \\[ m = \\frac{\\Delta S}{\\overline{S}} \\] where \\(\\overline{S}\\) is the mean of the flux densities \\(S_1\\) and \\(S_2\\) . Typically, in the literature, the threshold for this value for a source to be considered variable is: \\[ \\mid m \\mid \\gt 0.26, \\] which equates to a variability of 30%. However the user is free to set their own level to define variablity.","title":"Two-Epoch Metrics"},{"location":"design/sourcestats/#significant-source-values","text":"The \\(V_s\\) and \\(m\\) metrics of the 'maximum signficant pair' is attached to the source. The maximum significant pair is determind by selecting the most significant \\(\\mid m \\mid\\) value given a minimum \\(V_s\\) threshold which is defined in the pipeline configuration file variability.source_aggregate_pair_metrics_min_abs_vs : config.yaml variability : # Only measurement pairs where the Vs metric exceeds this value are selected for the # aggregate pair metrics that are stored in Source objects. source_aggregate_pair_metrics_min_abs_vs : 4.3 By default this value is set to 4.3. For example, if a source with three associatied measurements gave the following pair metrics: Pair \\(\\mid V_s \\mid\\) \\(\\mid m \\mid\\) A-B 4.5 0.1 B-C 2.5 0.05 A-C 4.3 0.4 then the A-C pair metrics are attached to the source as the most significant. This can be used to quickly determine significant two-epoch variability for a source. If there are no pair values above the minimum \\(V_s\\) threshold then these values attached to the source will be 0. The measurement_pairs.parquet file can be used to manually explore the measurement pairs if one wishes to lower the threshold.","title":"Significant Source Values"},{"location":"developing/docsdev/","text":"Development Guidelines for Documentation \u00b6 The pipeline documentation has been developed using the python package mkdocs and the material theme . It is published as a static website using GitHub pages. Documentation development server \u00b6 Note Remember to install the development dependencies which include the modules required for the documentation. This section describes how to set up a development server to live reload your changes to the pipeline documentation. The main code of the documentation is located in the docs directory. In order to keep the repository, CHANGELOG.md , LICENSE.txt and CODE_OF_CONDUCT.md on the root path, relative soft links have been created under the docs folder: adminusage architecture changelog.md -> ../CHANGELOG.md code_of_conduct.md -> ../CODE_OF_CONDUCT.md design developing exploringwebsite faqs.md gen_doc_stubs.py gettingstarted img index.md license.md -> ../LICENSE.txt outputs reference theme using Start the development server: (pipeline_env) $ mkdocs serve Files in the docs directory are then 'watched' such that any changes will cause the mkdocs server to reload the new content. The structure of the site (see nav section) and the settings are in the mkdocs.yml in the root of the repository. Plugins and Customisations \u00b6 mkdocs and the material theme have a lot of customisations and plugins available. The more involved plugins or customisations that are in use for this documentation are detailed in the following sections. Custom theme directory \u00b6 The theme directory contains various overrides to the standard material template. This is enabled by the following line in the mkdocs.yml file: mkdocs.yml theme : custom_dir : docs/theme Custom javascript and css files are also stored in this directory and are enabled in the following mkdocs.yml file: mkdocs.yml extra_css : - theme/css/extra.css extra_javascript : - theme/js/extra.js - https://polyfill.io/v3/polyfill.min.js?features=es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js The contents of the theme directory are shown below: docs/theme \u251c\u2500\u2500 assets \u2502 \u2514\u2500\u2500 stylesheets \u2502 \u2514\u2500\u2500 overrides.css \u251c\u2500\u2500 css \u2502 \u251c\u2500\u2500 extra.css \u2502 \u2514\u2500\u2500 lightgallery.min.css \u251c\u2500\u2500 fonts \u2502 \u251c\u2500\u2500 lg.svg \u2502 \u251c\u2500\u2500 lg.ttf \u2502 \u2514\u2500\u2500 lg.woff \u251c\u2500\u2500 home.html \u251c\u2500\u2500 img \u2502 \u2514\u2500\u2500 loading.gif \u251c\u2500\u2500 js \u2502 \u251c\u2500\u2500 extra.js \u2502 \u251c\u2500\u2500 lg-zoom.js \u2502 \u2514\u2500\u2500 lightgallery.min.js \u2514\u2500\u2500 main.html Full documentation on customising the mkdocs-material theme can be found in the documentation . main.html contains edits to the main.html as described in the documentation linked to above. The other files shown are used for the custom homepage and the Lightgallery. Custom Homepage \u00b6 The homepage is overwritten by a custom stylised HTML page. This is achieved using the following files: docs \u251c\u2500\u2500 index.md \u251c\u2500\u2500 theme \u2502 \u251c\u2500\u2500 assets \u2502 \u2502 \u2514\u2500\u2500 stylesheets \u2502 \u2502 \u2514\u2500\u2500 overrides.css \u2502 \u251c\u2500\u2500 home.html \u2502 \u2514\u2500\u2500 main.html The index.md file in the main docs directory should only contain the following. index.md --- title: VAST Pipeline template: home.html --- Creation and Last Updated Dates \u00b6 Each page displays a date of creation and also a \"last updated\" date. This is done by using the plugin mkdocs-git-revision-date-localized-plugin . The following options are used: mkdocs.yml plugins : - git-revision-date-localized : fallback_to_build_date : true enable_creation_date : true Dates and Code Reference Pages The option fallback_to_build_date: true is required for the code reference pages that are auto generated by the mkdocs-gen-files plugin (see the Python Docstrings and Source Code section below). These pages will show the build date rather than the \"last updated\" date. During the build process the following warning will be seen, which is expected and ok to ignore: WARNING - [git-revision-date-localized-plugin] Unable to find a git directory and/or git is not installed. Option 'fallback_to_build_date' set to 'true': Falling back to build date Lightgallery \u00b6 Images are displayed in the documentation using the lightgallery-markdown plugin. As described in the repository linked to above, certain assets from lightgallery.js are copied over to make the extension work, the files that apply here are: docs/theme \u251c\u2500\u2500 css \u2502 \u2514\u2500\u2500 lightgallery.min.css \u251c\u2500\u2500 fonts \u2502 \u251c\u2500\u2500 lg.svg \u2502 \u251c\u2500\u2500 lg.ttf \u2502 \u2514\u2500\u2500 lg.woff \u251c\u2500\u2500 img \u2502 \u2514\u2500\u2500 loading.gif \u251c\u2500\u2500 js \u2502 \u251c\u2500\u2500 extra.js \u2502 \u251c\u2500\u2500 lg-zoom.js \u2502 \u2514\u2500\u2500 lightgallery.min.js \u2514\u2500\u2500 main.html lg-zoom.js is also copied over to activate the zoom feature on the gallery images. Note The javascript to select the lightgallery class objects and run them through the lightgallery function is placed in theme/js/extra.js such that the plugin works with instant navigation . MathJax \u00b6 MathJax is enabled as described in the mkdocs-material documentation . Python Docstrings and Source Code \u00b6 Python docstrings and source code are rendered using the mkdocstrings plugin. Docstrings Format All docstrings must be done using the Google format . The markdown files for the docstrings are automatically generated using mkdocs-gen-files in the file docs/gen_doc_stubs.py . It is activated by the following lines in the mkdocs.yml file: mkdocs.yml plugins : - search - gen-files : scripts : - docs/gen_doc_stubs.py The markdown files are programmatically generated, this means that the markdown files don't actually appear, but are virtually part of the site build. The files must still be referenced in the nav: section of mkdocs.yml , for example: mkdocs.yml - nav : - Code Reference : - vast_pipeline : - image : - main.py : reference/image/main.md - utils.py : reference/image/utils.md The files are virtually created relative to the doc path that is stated in gen_doc_stubs.py . For example, using the vast_pipeline as the base for the python files and reference as the doc path, the markdown file for the docstrings in vast_pipeline/image/main.py is created at reference/image/main.md . Please refer to the mkdocstrings documentation for full details of the options available to declare in mkdocs.yml . Deployment to GitHub pages \u00b6 Automatic deployment to GitHub pages is set up using GitHub actions and workflows. See source code under the .github folder.","title":"Documentation"},{"location":"developing/docsdev/#development-guidelines-for-documentation","text":"The pipeline documentation has been developed using the python package mkdocs and the material theme . It is published as a static website using GitHub pages.","title":"Development Guidelines for Documentation"},{"location":"developing/docsdev/#documentation-development-server","text":"Note Remember to install the development dependencies which include the modules required for the documentation. This section describes how to set up a development server to live reload your changes to the pipeline documentation. The main code of the documentation is located in the docs directory. In order to keep the repository, CHANGELOG.md , LICENSE.txt and CODE_OF_CONDUCT.md on the root path, relative soft links have been created under the docs folder: adminusage architecture changelog.md -> ../CHANGELOG.md code_of_conduct.md -> ../CODE_OF_CONDUCT.md design developing exploringwebsite faqs.md gen_doc_stubs.py gettingstarted img index.md license.md -> ../LICENSE.txt outputs reference theme using Start the development server: (pipeline_env) $ mkdocs serve Files in the docs directory are then 'watched' such that any changes will cause the mkdocs server to reload the new content. The structure of the site (see nav section) and the settings are in the mkdocs.yml in the root of the repository.","title":"Documentation development server"},{"location":"developing/docsdev/#plugins-and-customisations","text":"mkdocs and the material theme have a lot of customisations and plugins available. The more involved plugins or customisations that are in use for this documentation are detailed in the following sections.","title":"Plugins and Customisations"},{"location":"developing/docsdev/#custom-theme-directory","text":"The theme directory contains various overrides to the standard material template. This is enabled by the following line in the mkdocs.yml file: mkdocs.yml theme : custom_dir : docs/theme Custom javascript and css files are also stored in this directory and are enabled in the following mkdocs.yml file: mkdocs.yml extra_css : - theme/css/extra.css extra_javascript : - theme/js/extra.js - https://polyfill.io/v3/polyfill.min.js?features=es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js The contents of the theme directory are shown below: docs/theme \u251c\u2500\u2500 assets \u2502 \u2514\u2500\u2500 stylesheets \u2502 \u2514\u2500\u2500 overrides.css \u251c\u2500\u2500 css \u2502 \u251c\u2500\u2500 extra.css \u2502 \u2514\u2500\u2500 lightgallery.min.css \u251c\u2500\u2500 fonts \u2502 \u251c\u2500\u2500 lg.svg \u2502 \u251c\u2500\u2500 lg.ttf \u2502 \u2514\u2500\u2500 lg.woff \u251c\u2500\u2500 home.html \u251c\u2500\u2500 img \u2502 \u2514\u2500\u2500 loading.gif \u251c\u2500\u2500 js \u2502 \u251c\u2500\u2500 extra.js \u2502 \u251c\u2500\u2500 lg-zoom.js \u2502 \u2514\u2500\u2500 lightgallery.min.js \u2514\u2500\u2500 main.html Full documentation on customising the mkdocs-material theme can be found in the documentation . main.html contains edits to the main.html as described in the documentation linked to above. The other files shown are used for the custom homepage and the Lightgallery.","title":"Custom theme directory"},{"location":"developing/docsdev/#custom-homepage","text":"The homepage is overwritten by a custom stylised HTML page. This is achieved using the following files: docs \u251c\u2500\u2500 index.md \u251c\u2500\u2500 theme \u2502 \u251c\u2500\u2500 assets \u2502 \u2502 \u2514\u2500\u2500 stylesheets \u2502 \u2502 \u2514\u2500\u2500 overrides.css \u2502 \u251c\u2500\u2500 home.html \u2502 \u2514\u2500\u2500 main.html The index.md file in the main docs directory should only contain the following. index.md --- title: VAST Pipeline template: home.html ---","title":"Custom Homepage"},{"location":"developing/docsdev/#creation-and-last-updated-dates","text":"Each page displays a date of creation and also a \"last updated\" date. This is done by using the plugin mkdocs-git-revision-date-localized-plugin . The following options are used: mkdocs.yml plugins : - git-revision-date-localized : fallback_to_build_date : true enable_creation_date : true Dates and Code Reference Pages The option fallback_to_build_date: true is required for the code reference pages that are auto generated by the mkdocs-gen-files plugin (see the Python Docstrings and Source Code section below). These pages will show the build date rather than the \"last updated\" date. During the build process the following warning will be seen, which is expected and ok to ignore: WARNING - [git-revision-date-localized-plugin] Unable to find a git directory and/or git is not installed. Option 'fallback_to_build_date' set to 'true': Falling back to build date","title":"Creation and Last Updated Dates"},{"location":"developing/docsdev/#lightgallery","text":"Images are displayed in the documentation using the lightgallery-markdown plugin. As described in the repository linked to above, certain assets from lightgallery.js are copied over to make the extension work, the files that apply here are: docs/theme \u251c\u2500\u2500 css \u2502 \u2514\u2500\u2500 lightgallery.min.css \u251c\u2500\u2500 fonts \u2502 \u251c\u2500\u2500 lg.svg \u2502 \u251c\u2500\u2500 lg.ttf \u2502 \u2514\u2500\u2500 lg.woff \u251c\u2500\u2500 img \u2502 \u2514\u2500\u2500 loading.gif \u251c\u2500\u2500 js \u2502 \u251c\u2500\u2500 extra.js \u2502 \u251c\u2500\u2500 lg-zoom.js \u2502 \u2514\u2500\u2500 lightgallery.min.js \u2514\u2500\u2500 main.html lg-zoom.js is also copied over to activate the zoom feature on the gallery images. Note The javascript to select the lightgallery class objects and run them through the lightgallery function is placed in theme/js/extra.js such that the plugin works with instant navigation .","title":"Lightgallery"},{"location":"developing/docsdev/#mathjax","text":"MathJax is enabled as described in the mkdocs-material documentation .","title":"MathJax"},{"location":"developing/docsdev/#python-docstrings-and-source-code","text":"Python docstrings and source code are rendered using the mkdocstrings plugin. Docstrings Format All docstrings must be done using the Google format . The markdown files for the docstrings are automatically generated using mkdocs-gen-files in the file docs/gen_doc_stubs.py . It is activated by the following lines in the mkdocs.yml file: mkdocs.yml plugins : - search - gen-files : scripts : - docs/gen_doc_stubs.py The markdown files are programmatically generated, this means that the markdown files don't actually appear, but are virtually part of the site build. The files must still be referenced in the nav: section of mkdocs.yml , for example: mkdocs.yml - nav : - Code Reference : - vast_pipeline : - image : - main.py : reference/image/main.md - utils.py : reference/image/utils.md The files are virtually created relative to the doc path that is stated in gen_doc_stubs.py . For example, using the vast_pipeline as the base for the python files and reference as the doc path, the markdown file for the docstrings in vast_pipeline/image/main.py is created at reference/image/main.md . Please refer to the mkdocstrings documentation for full details of the options available to declare in mkdocs.yml .","title":"Python Docstrings and Source Code"},{"location":"developing/docsdev/#deployment-to-github-pages","text":"Automatic deployment to GitHub pages is set up using GitHub actions and workflows. See source code under the .github folder.","title":"Deployment to GitHub pages"},{"location":"developing/github/","text":"GitHub Platform Guidelines \u00b6 This section explains how to interact with GitHub platform for opening issues, starting discussions, creating pull requests (PR), and some notes how to make a release of the pipeline if you are a maintainer of the code base. The VAST team uses the \"git flow\" branching model which we briefly summarise here. More detail can be found here . There are two main branches, both with infinite lifetimes (they are never deleted): master for stable, production-ready code that has been released, and dev for the latest reviewed updates for the next release. Other branches for bug fixes and new features are created as needed, branching off and merging back into dev . An exception to this is for critical patches for a released version called a \"hotfix\". These are branched off master and merged back into both master and dev . Branches are also created for each new release candidate, which are branched off dev and merged into master and dev when completed. See the Releases section below for more information. Issues \u00b6 An issue can be created by anyone with access to the repository. Users are encouraged to create issues for problems they encounter while using the pipeline or to request a new feature be added to the software. Issues are created by clicking the \"New issue\" button near the top-right of the issues page . When creating a new issue, please consider the following: Search for a similar issue before opening a new one by using the search box near the top of the issues page. When opening a new issue, please specify the issue type (e.g. bug, feature, etc.) and provide a detailed description with use cases when appropriate. Discussions \u00b6 GitHub repositories also have a discussions page which serves as a collaborative forum to discuss ideas and ask questions. Users are encouraged to ask general questions, or start a conversation about potential new features to the software by creating a new discussion thread on the discussions page . Note that new software features may also be requested by creating an issue, but a discussion thread is more appropriate if the details of the new feature are still yet to be determined or require wider discussion \u2013 issues can be created from discussions once a consensus is reached. Pull Requests \u00b6 Pull requests are created when a developer wishes to merge changes they have made in a branch into another branch. They enable others to review the changes and make comments. While issues typically describe in detail a specific problem or proposed feature, pull requests contain a detailed description and the required code changes for a solution to a problem or implementation of a new feature. Opening a PR \u00b6 First consider ... Search existing issues for similar problems or feature proposals. Opening an issue to describe the problem or feature before creating a PR. This will help separate problems from solutions. Steps to issue a pull request: Create a new issue on GitHub, giving it a succinct title and describe the problem. GitHub will assign an ID e.g. #123 . Create a new branch off the dev branch and name the branch based on the issue title, e.g. fix-123-some-problem (keep it short please). Make your changes. Run the test suite locally with python manage.py test vast_pipeline . See the complete guide on the test for more details. Run the webserver and check the functionality. This is important as the test suite does not currently check the web UI. Commit the changes to your branch, push it to GitHub, and open a PR for the branch. Update the CHANGELOG.md file by adding the link to your PR and briefly describing the changes. An example of the change log format can be found here Assign the review to one or more reviewers if none are assigned by default. Warning PRs not branched off dev will be rejected !. Reviewing a PR \u00b6 The guidelines to dealing with reviews and conversations on GitHub are essentially: Be nice with the review and do not offend the author of the PR: Nobody is a perfect developer or born so! The reviewers will in general mark the conversation as \"resolved\" (e.g. he/she is satisfied with the answer from the PR author). The PR author will re-request the review by clicking on the on the top right corner and might ping the reviewer on a comment if necessary with @github_name . When the PR is approved by at least one reviewer you might want to merge it to dev (you should have that privileges), unless you want to make sure that such PR is reviewed by another reviewer (e.g. you are doing big changes or important changes or you want to make sure that other person is aware/updated about the changes in that PR). Releases \u00b6 In to order to make a release, please follow these steps: Make sure that all new feature and bug fix PRs that should be part of the new release have been merged to dev . Checkout the dev branch and update it with git pull . Ensure that there are no uncommitted changes. Create a new branch off dev , naming it release-vX.Y.Z where X.Y.Z is the new version. Typically, patch version increments for bug fixes, minor version increments for new features that do not break backward compatibility with previous versions (i.e. no database schema changes), and major version increments for large changes or for changes that would break backward compatibility. Bump the version number of the Python package using Poetry, i.e. poetry version X.Y.Z . This will update the version number in pyproject.toml . Update the version in package.json and vast_pipeline/_version.py to match the new version number, then run npm install to update the package-lock.json file. Update the \"announcement bar\" in the documentation to refer to the new release. This can be found in docs/theme/main.html at line 37. Update the CHANGELOG.md by making a copy of the \"Unreleased\" heading at the top, and renaming the second one to the new version. Include a link to the release - it won't exist yet, so just follow the format of the others. After this there should be an \"Unreleased\" heading at the top, immediately followed by another heading with the new version number, which is followed by all the existing changes. Commit all the changes made above to the new branch and push it to GitHub. Open a PR to merge the new branch into master . Note that the default target branch is dev so you will need to change this to master when creating the PR. Once the PR has been reviewed and approved, merge the branch into master . This can only be done by administrators of the repository. Tag the merge commit on master with the version, i.e. git tag vX.Y.Z , then push the tag to GitHub. Warning If you merged the release branch into master with the GitHub web UI, you will need to sync that merge to your local copy and checkout master before creating the tag. You cannot create tags with the GitHub web UI. Push the tag to GitHub, i.e. git push origin vX.Y.Z . Merge the release branch into dev , resolving any conflicts. Append \"dev\" to the version numbers in pyproject.toml , package.json and vast_pipeline/_version.py , then run npm install to update package-lock.json , and commit the changes to dev . This can either be done as a new commit, or while resolving merge conflicts in the previous step, if appropriate. Create a new release on GitHub that points to the tagged commit on master.","title":"GitHub Platform"},{"location":"developing/github/#github-platform-guidelines","text":"This section explains how to interact with GitHub platform for opening issues, starting discussions, creating pull requests (PR), and some notes how to make a release of the pipeline if you are a maintainer of the code base. The VAST team uses the \"git flow\" branching model which we briefly summarise here. More detail can be found here . There are two main branches, both with infinite lifetimes (they are never deleted): master for stable, production-ready code that has been released, and dev for the latest reviewed updates for the next release. Other branches for bug fixes and new features are created as needed, branching off and merging back into dev . An exception to this is for critical patches for a released version called a \"hotfix\". These are branched off master and merged back into both master and dev . Branches are also created for each new release candidate, which are branched off dev and merged into master and dev when completed. See the Releases section below for more information.","title":"GitHub Platform Guidelines"},{"location":"developing/github/#issues","text":"An issue can be created by anyone with access to the repository. Users are encouraged to create issues for problems they encounter while using the pipeline or to request a new feature be added to the software. Issues are created by clicking the \"New issue\" button near the top-right of the issues page . When creating a new issue, please consider the following: Search for a similar issue before opening a new one by using the search box near the top of the issues page. When opening a new issue, please specify the issue type (e.g. bug, feature, etc.) and provide a detailed description with use cases when appropriate.","title":"Issues"},{"location":"developing/github/#discussions","text":"GitHub repositories also have a discussions page which serves as a collaborative forum to discuss ideas and ask questions. Users are encouraged to ask general questions, or start a conversation about potential new features to the software by creating a new discussion thread on the discussions page . Note that new software features may also be requested by creating an issue, but a discussion thread is more appropriate if the details of the new feature are still yet to be determined or require wider discussion \u2013 issues can be created from discussions once a consensus is reached.","title":"Discussions"},{"location":"developing/github/#pull-requests","text":"Pull requests are created when a developer wishes to merge changes they have made in a branch into another branch. They enable others to review the changes and make comments. While issues typically describe in detail a specific problem or proposed feature, pull requests contain a detailed description and the required code changes for a solution to a problem or implementation of a new feature.","title":"Pull Requests"},{"location":"developing/github/#opening-a-pr","text":"First consider ... Search existing issues for similar problems or feature proposals. Opening an issue to describe the problem or feature before creating a PR. This will help separate problems from solutions. Steps to issue a pull request: Create a new issue on GitHub, giving it a succinct title and describe the problem. GitHub will assign an ID e.g. #123 . Create a new branch off the dev branch and name the branch based on the issue title, e.g. fix-123-some-problem (keep it short please). Make your changes. Run the test suite locally with python manage.py test vast_pipeline . See the complete guide on the test for more details. Run the webserver and check the functionality. This is important as the test suite does not currently check the web UI. Commit the changes to your branch, push it to GitHub, and open a PR for the branch. Update the CHANGELOG.md file by adding the link to your PR and briefly describing the changes. An example of the change log format can be found here Assign the review to one or more reviewers if none are assigned by default. Warning PRs not branched off dev will be rejected !.","title":"Opening a PR"},{"location":"developing/github/#reviewing-a-pr","text":"The guidelines to dealing with reviews and conversations on GitHub are essentially: Be nice with the review and do not offend the author of the PR: Nobody is a perfect developer or born so! The reviewers will in general mark the conversation as \"resolved\" (e.g. he/she is satisfied with the answer from the PR author). The PR author will re-request the review by clicking on the on the top right corner and might ping the reviewer on a comment if necessary with @github_name . When the PR is approved by at least one reviewer you might want to merge it to dev (you should have that privileges), unless you want to make sure that such PR is reviewed by another reviewer (e.g. you are doing big changes or important changes or you want to make sure that other person is aware/updated about the changes in that PR).","title":"Reviewing a PR"},{"location":"developing/github/#releases","text":"In to order to make a release, please follow these steps: Make sure that all new feature and bug fix PRs that should be part of the new release have been merged to dev . Checkout the dev branch and update it with git pull . Ensure that there are no uncommitted changes. Create a new branch off dev , naming it release-vX.Y.Z where X.Y.Z is the new version. Typically, patch version increments for bug fixes, minor version increments for new features that do not break backward compatibility with previous versions (i.e. no database schema changes), and major version increments for large changes or for changes that would break backward compatibility. Bump the version number of the Python package using Poetry, i.e. poetry version X.Y.Z . This will update the version number in pyproject.toml . Update the version in package.json and vast_pipeline/_version.py to match the new version number, then run npm install to update the package-lock.json file. Update the \"announcement bar\" in the documentation to refer to the new release. This can be found in docs/theme/main.html at line 37. Update the CHANGELOG.md by making a copy of the \"Unreleased\" heading at the top, and renaming the second one to the new version. Include a link to the release - it won't exist yet, so just follow the format of the others. After this there should be an \"Unreleased\" heading at the top, immediately followed by another heading with the new version number, which is followed by all the existing changes. Commit all the changes made above to the new branch and push it to GitHub. Open a PR to merge the new branch into master . Note that the default target branch is dev so you will need to change this to master when creating the PR. Once the PR has been reviewed and approved, merge the branch into master . This can only be done by administrators of the repository. Tag the merge commit on master with the version, i.e. git tag vX.Y.Z , then push the tag to GitHub. Warning If you merged the release branch into master with the GitHub web UI, you will need to sync that merge to your local copy and checkout master before creating the tag. You cannot create tags with the GitHub web UI. Push the tag to GitHub, i.e. git push origin vX.Y.Z . Merge the release branch into dev , resolving any conflicts. Append \"dev\" to the version numbers in pyproject.toml , package.json and vast_pipeline/_version.py , then run npm install to update package-lock.json , and commit the changes to dev . This can either be done as a new commit, or while resolving merge conflicts in the previous step, if appropriate. Create a new release on GitHub that points to the tagged commit on master.","title":"Releases"},{"location":"developing/intro/","text":"Contributing and Developing Guidelines \u00b6 This section explains how to contribute to the project code base and collaborate on GitHub platform. Please use the section navigator to left to visit pages for specific details on areas of development. Terminology \u00b6 These below is the terminology used to identify pipeline objects. Pipeline run (or \"Run\") -> Pipeline run instance, also referred as run, p_run, piperun, pipe_run, ... in the code Measurement -> the extracted measurement from the source finder of a single astrophysical source from an image, referred in the code as measurement(s), meas, ... Source -> A collection of single measurements for the same astrophysical source, referred as src, source, ... in the code Docstrings \u00b6 All docstrings must be done using the Google format to ensure compatibility with the documentation.","title":"Development Guidlines"},{"location":"developing/intro/#contributing-and-developing-guidelines","text":"This section explains how to contribute to the project code base and collaborate on GitHub platform. Please use the section navigator to left to visit pages for specific details on areas of development.","title":"Contributing and Developing Guidelines"},{"location":"developing/intro/#terminology","text":"These below is the terminology used to identify pipeline objects. Pipeline run (or \"Run\") -> Pipeline run instance, also referred as run, p_run, piperun, pipe_run, ... in the code Measurement -> the extracted measurement from the source finder of a single astrophysical source from an image, referred in the code as measurement(s), meas, ... Source -> A collection of single measurements for the same astrophysical source, referred as src, source, ... in the code","title":"Terminology"},{"location":"developing/intro/#docstrings","text":"All docstrings must be done using the Google format to ensure compatibility with the documentation.","title":"Docstrings"},{"location":"developing/localdevenv/","text":"Pipeline Local Development Environment Guidelines \u00b6 This section describes how to set up a local development environment more in details. Back End \u00b6 Installation \u00b6 The installation instructions are the same as the ones describes in the Getting Started section with one key difference. Rather than installing the Python dependencies with pip, you will need to install and use Poetry . After installing Poetry, running the command below will install the pipeline dependencies defined in poetry.lock into a virtual environment. The main difference between using Poetry and pip is that pip will only install the dependencies necessary for using the pipeline, whereas Poetry will also install development dependencies required for contributing (e.g. tools to build the documentation). poetry install Note Poetry will automatically create a virtual environment if it detects that your shell isn't currently using one. This should be fine for most users. If you prefer to use an alternative virtual environment manager (e.g. Miniconda), you can prevent Poetry from creating virtual environments . However, even if you are using something like Miniconda, allowing Poetry to manage the virtualenv (the default behaviour) is fine. The development team only uses this option during our automated testing since our test runner machines only contain a single Python environment so using virtualenvs is redundant. Changes to the data models \u00b6 When changes are made to the data models defined in vast_pipeline/models.py , the database schema needs to be updated to reflect those changes. Django can handle this for you by generating migration files that contain the necessary code to update the schema. Migration files are generated with the Django management command python manage.py makemigrations and applied to the database with python manage.py migrate . Depending on the nature of the changes, this may break backward compatibility, i.e. runs created with previous versions of the pipeline may not be compatible with your changes. Database migrations must be committed to source control so that others can pull in your model changes. They can sometimes be complex and require additional attention. If you have any difficulty with migrations, please contact the VAST development team for help. More information can be found in the Django documentation on migrations . Removing/Clearing Data \u00b6 The following sub-sections show how to completely drop every data in the database and how to remove only the data related to one or more pipeline runs. Reset the database \u00b6 Make sure you installed the requirements dev.txt . And django_extensions is in EXTRA_APPS in your setting configuration file .env (e.g. EXTRA_APPS=django_extensions,another_app,... ). ( pipeline_env ) $ ./manage.py reset_db && ./manage.py migrate # use the following for no confirmation prompt ( pipeline_env ) $ ./manage.py reset_db --noinput && ./manage.py migrate Clearing Run Data \u00b6 It is sometimes convenient to remove the data belonging to one or more pipeline runs while developing the code base. This is particularly useful to save time by not having to re-upload the image data along with the measurements. The data related to the pipeline are the Sources, Associations, Forced extractions entries in database and the parquet files in the respective folder. By default the command will keep the run folder with the config and the log files. ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run To clear more than one run: ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run1 my-pipe-run2 path/to/my-pipe-run3 The command accept both a path or a name of the pipeline run(s). To remove all the runs, issue: ( pipeline_env ) $ ./manage.py clearpiperun clearall The command to keep the parquet files is: ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run --keep-parquet The remove completely the pipeline folder ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run --remove-all FrontEnd Assets Management and Guidelines \u00b6 This guide explain the installation, compilation and development of the front end assets (HTML, CSS, JS and relative modules). We make use of a node installation with npm and gulp tasks to build the front end assets. Installation of node packages \u00b6 After installing a node version and npm , install the node modules using from the base folder ( vast-pipeline ): npm ci Npm will install the node packages in a node_modules folder under the main root. ... \u251c\u2500\u2500 node_modules ... For installing future additional dependencies you can run npm install --save my-package or npm install --save-dev my-dev-package (to save a development module), and after that commit both package.json and package-lock.json files. For details about the installed packages and npm scripts see package.json . FrontEnd Tasks with gulp \u00b6 Using gulp and npm scripts you can: Install dependencies under the ./static/vendor folder. Building (e.g. minify/uglify) CSS and/or Javascript files. Run a development server that \"hot-reload\" your web page when any HTML, CSS or Javascript file is modified. The command to list all the gulp \"tasks\" and sub-tasks is (you might need gulp-cli installed globally, i.e. npm i --global gulp-cli , more info here ): gulp --tasks Output: [11:55:30] Tasks for ~/PATH/TO/REPO/vast-pipeline/gulpfile.js [11:55:30] \u251c\u2500\u2500 clean [11:55:30] \u251c\u2500\u252c js9 [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u251c\u2500\u2500 css [11:55:30] \u251c\u2500\u2500 js [11:55:30] \u251c\u2500\u252c vendor [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u251c\u2500\u252c build [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u251c\u2500\u252c watch [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 watchFiles [11:55:30] \u2502 \u2514\u2500\u2500 browserSync [11:55:30] \u251c\u2500\u252c default [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u2514\u2500\u2500 debug Alternatively you can run gulp from the installed version in the node_modules folder with: ./node_modules/.bin/gulp --tasks For further details about tasks, see gulpfile . 1. Install Dependencies under vendor Folder \u00b6 Install the dependencies under the ./static/vendor folder, with: npm run vendor Or, using global gulp-cli : gulp vendor As seen in the tasks diagram above, the vendor task run the module task in parallel with the js9 tasks. JS9 has many task as these run with manual command that involve make/make install and then writing configuration to js9prefs.js file. You can run manually the installation of JS9 with gulp js9 . 2. Building CSS and Javascript files \u00b6 npm run build # or npm start # or gulp build # or gulp default # or gulp will run the vendor task and minify both CSS and Javascript files. By default, when no other tasks is specified, gulp runs the build task. You can run single tasks with: gulp css to run just the minification of the CSS files. 3. Run Development Server \u00b6 Start your normal Django server with ( NOTE : do not change the default port!): ( pipeline_env ) $: ./manage.py runserver In another terminal run: npm run watch # or gulp watch The latter will open your dev server, that will auto reload and apply your latest changes in any CSS, Javascript and/or HTML files. As pointed out in the gulp task tree above the watch task run both the vendor and build tasks. 4. Debug Task \u00b6 This task is for debugging the paths used in the others task, but also serve as a place holder to debug commands. npm run debug # or gulp debug 5. Clean Task \u00b6 This task delete the vendor folder ( /static/vendor ) along with all the files. npm run clean # or gulp clean FrontEnd assets for Production \u00b6 In order to compile the frontend assets for production, activate the Python virtual environment, then run: ( pipeline_env ) $ npm run js9staticprod && ./manage.py collectstatic -c This command will collect all static assets (Javascript and CSS files) and copy them to STATIC_ROOT path in setting.py, so make sure you have permission to write to that. STATIC_ROOT is assigned to ./staticfiles by default, otherwise assigned to the path you defined in your .env file. The js9staticprod gulp task is necessary if you specify a STATIC_URL and a BASE_URL different than the default, for example if you need to prefix the site / with a base url because you are running another webserver (e.g. another web server is running on https://my-astro-platform.com/ so you want to run the pipeline on the same server/domain https://my-astro-platform.com/pipeline , so you need to set BASE_URL='/pipeline/' and STATIC_URL=/pipeline-static/ in settings.py ). We recommend to run this in any case! Then you can move that folder to where it can be served by the production static files server ( Ningx or Apache are usually good choices, in case refer to the Django documentation ).","title":"Local Development Environment"},{"location":"developing/localdevenv/#pipeline-local-development-environment-guidelines","text":"This section describes how to set up a local development environment more in details.","title":"Pipeline Local Development Environment Guidelines"},{"location":"developing/localdevenv/#back-end","text":"","title":"Back End"},{"location":"developing/localdevenv/#installation","text":"The installation instructions are the same as the ones describes in the Getting Started section with one key difference. Rather than installing the Python dependencies with pip, you will need to install and use Poetry . After installing Poetry, running the command below will install the pipeline dependencies defined in poetry.lock into a virtual environment. The main difference between using Poetry and pip is that pip will only install the dependencies necessary for using the pipeline, whereas Poetry will also install development dependencies required for contributing (e.g. tools to build the documentation). poetry install Note Poetry will automatically create a virtual environment if it detects that your shell isn't currently using one. This should be fine for most users. If you prefer to use an alternative virtual environment manager (e.g. Miniconda), you can prevent Poetry from creating virtual environments . However, even if you are using something like Miniconda, allowing Poetry to manage the virtualenv (the default behaviour) is fine. The development team only uses this option during our automated testing since our test runner machines only contain a single Python environment so using virtualenvs is redundant.","title":"Installation"},{"location":"developing/localdevenv/#changes-to-the-data-models","text":"When changes are made to the data models defined in vast_pipeline/models.py , the database schema needs to be updated to reflect those changes. Django can handle this for you by generating migration files that contain the necessary code to update the schema. Migration files are generated with the Django management command python manage.py makemigrations and applied to the database with python manage.py migrate . Depending on the nature of the changes, this may break backward compatibility, i.e. runs created with previous versions of the pipeline may not be compatible with your changes. Database migrations must be committed to source control so that others can pull in your model changes. They can sometimes be complex and require additional attention. If you have any difficulty with migrations, please contact the VAST development team for help. More information can be found in the Django documentation on migrations .","title":"Changes to the data models"},{"location":"developing/localdevenv/#removingclearing-data","text":"The following sub-sections show how to completely drop every data in the database and how to remove only the data related to one or more pipeline runs.","title":"Removing/Clearing Data"},{"location":"developing/localdevenv/#reset-the-database","text":"Make sure you installed the requirements dev.txt . And django_extensions is in EXTRA_APPS in your setting configuration file .env (e.g. EXTRA_APPS=django_extensions,another_app,... ). ( pipeline_env ) $ ./manage.py reset_db && ./manage.py migrate # use the following for no confirmation prompt ( pipeline_env ) $ ./manage.py reset_db --noinput && ./manage.py migrate","title":"Reset the database"},{"location":"developing/localdevenv/#clearing-run-data","text":"It is sometimes convenient to remove the data belonging to one or more pipeline runs while developing the code base. This is particularly useful to save time by not having to re-upload the image data along with the measurements. The data related to the pipeline are the Sources, Associations, Forced extractions entries in database and the parquet files in the respective folder. By default the command will keep the run folder with the config and the log files. ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run To clear more than one run: ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run1 my-pipe-run2 path/to/my-pipe-run3 The command accept both a path or a name of the pipeline run(s). To remove all the runs, issue: ( pipeline_env ) $ ./manage.py clearpiperun clearall The command to keep the parquet files is: ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run --keep-parquet The remove completely the pipeline folder ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run --remove-all","title":"Clearing Run Data"},{"location":"developing/localdevenv/#frontend-assets-management-and-guidelines","text":"This guide explain the installation, compilation and development of the front end assets (HTML, CSS, JS and relative modules). We make use of a node installation with npm and gulp tasks to build the front end assets.","title":"FrontEnd Assets Management and Guidelines"},{"location":"developing/localdevenv/#installation-of-node-packages","text":"After installing a node version and npm , install the node modules using from the base folder ( vast-pipeline ): npm ci Npm will install the node packages in a node_modules folder under the main root. ... \u251c\u2500\u2500 node_modules ... For installing future additional dependencies you can run npm install --save my-package or npm install --save-dev my-dev-package (to save a development module), and after that commit both package.json and package-lock.json files. For details about the installed packages and npm scripts see package.json .","title":"Installation of node packages"},{"location":"developing/localdevenv/#frontend-tasks-with-gulp","text":"Using gulp and npm scripts you can: Install dependencies under the ./static/vendor folder. Building (e.g. minify/uglify) CSS and/or Javascript files. Run a development server that \"hot-reload\" your web page when any HTML, CSS or Javascript file is modified. The command to list all the gulp \"tasks\" and sub-tasks is (you might need gulp-cli installed globally, i.e. npm i --global gulp-cli , more info here ): gulp --tasks Output: [11:55:30] Tasks for ~/PATH/TO/REPO/vast-pipeline/gulpfile.js [11:55:30] \u251c\u2500\u2500 clean [11:55:30] \u251c\u2500\u252c js9 [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u251c\u2500\u2500 css [11:55:30] \u251c\u2500\u2500 js [11:55:30] \u251c\u2500\u252c vendor [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u251c\u2500\u252c build [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u251c\u2500\u252c watch [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 watchFiles [11:55:30] \u2502 \u2514\u2500\u2500 browserSync [11:55:30] \u251c\u2500\u252c default [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u2514\u2500\u2500 debug Alternatively you can run gulp from the installed version in the node_modules folder with: ./node_modules/.bin/gulp --tasks For further details about tasks, see gulpfile .","title":"FrontEnd Tasks with gulp"},{"location":"developing/localdevenv/#1-install-dependencies-under-vendor-folder","text":"Install the dependencies under the ./static/vendor folder, with: npm run vendor Or, using global gulp-cli : gulp vendor As seen in the tasks diagram above, the vendor task run the module task in parallel with the js9 tasks. JS9 has many task as these run with manual command that involve make/make install and then writing configuration to js9prefs.js file. You can run manually the installation of JS9 with gulp js9 .","title":"1. Install Dependencies under vendor Folder"},{"location":"developing/localdevenv/#2-building-css-and-javascript-files","text":"npm run build # or npm start # or gulp build # or gulp default # or gulp will run the vendor task and minify both CSS and Javascript files. By default, when no other tasks is specified, gulp runs the build task. You can run single tasks with: gulp css to run just the minification of the CSS files.","title":"2. Building CSS and Javascript files"},{"location":"developing/localdevenv/#3-run-development-server","text":"Start your normal Django server with ( NOTE : do not change the default port!): ( pipeline_env ) $: ./manage.py runserver In another terminal run: npm run watch # or gulp watch The latter will open your dev server, that will auto reload and apply your latest changes in any CSS, Javascript and/or HTML files. As pointed out in the gulp task tree above the watch task run both the vendor and build tasks.","title":"3. Run Development Server"},{"location":"developing/localdevenv/#4-debug-task","text":"This task is for debugging the paths used in the others task, but also serve as a place holder to debug commands. npm run debug # or gulp debug","title":"4. Debug Task"},{"location":"developing/localdevenv/#5-clean-task","text":"This task delete the vendor folder ( /static/vendor ) along with all the files. npm run clean # or gulp clean","title":"5. Clean Task"},{"location":"developing/localdevenv/#frontend-assets-for-production","text":"In order to compile the frontend assets for production, activate the Python virtual environment, then run: ( pipeline_env ) $ npm run js9staticprod && ./manage.py collectstatic -c This command will collect all static assets (Javascript and CSS files) and copy them to STATIC_ROOT path in setting.py, so make sure you have permission to write to that. STATIC_ROOT is assigned to ./staticfiles by default, otherwise assigned to the path you defined in your .env file. The js9staticprod gulp task is necessary if you specify a STATIC_URL and a BASE_URL different than the default, for example if you need to prefix the site / with a base url because you are running another webserver (e.g. another web server is running on https://my-astro-platform.com/ so you want to run the pipeline on the same server/domain https://my-astro-platform.com/pipeline , so you need to set BASE_URL='/pipeline/' and STATIC_URL=/pipeline-static/ in settings.py ). We recommend to run this in any case! Then you can move that folder to where it can be served by the production static files server ( Ningx or Apache are usually good choices, in case refer to the Django documentation ).","title":"FrontEnd assets for Production"},{"location":"developing/profiling/","text":"Benchmarks \u00b6 Initial Profiling \u00b6 These profiling tests were run with the pipeline codebase correspondent to commit 373c2ce (some commits after the first release). Running on 12GB of data with 464MB peak memory usage takes 4 mins: performance: ~80% final_operations , ~10% get_src_skyregion_merged_df . final_operations calls other functions, out of these the largest is 50% in make_upload_sources which spends about 20% of time on utils <method 'execute' of 'psycopg2.extensions.cursor' objects> . The get_src_skyregion_merged_df time sink is in threading 15% of time is on wait ( final_operations spends some time on threading as well, hence 15% > 10%). memory: 40% pyarrow parquet write_table , rest is mostly fragmented, some more pyarrow and some pandas Running on 3MB of data with peak memory usage 176MB, takes 1.5s: performance: ~30% goes to pipeline (about 9% of this is pickle ), ~11% goes to read , rest goes to django I think memory: 30% memory is spent on django , 20% is spent on astropy/coordinates/matrix_utilities , 10% on importing other modules, rest is fragmented quite small Note that I didn't include the generation of the images/*/measurements.parquet or other files in these profiles. Database Update Operations \u00b6 Delete ( Model.objects.all().delete() ) and reupload ( bulk_upload ) (in seconds) columns\\rows 10 3 10 4 10 5 4 0.15 1.24 12.95 8 0.26 1.64 19.11 12 0.31 2.18 21.49 Per cell, 10 3 rows is slower than 10 4 and 10 5 rows, possibly due to overhead. Best to avoid uploading 10 3 rows each bulk_create call. Django bulk_update columns\\rows 10 3 10 4 10 5 4 3.39 na na 8 4.38 na na 12 5.50 na na I don't think there's any point testing 10 4 or 10 5 rows, it's obviously the worst performing function, and I've already had to force quit the terminal twice because keyboard interrupt didn't work. SQL join as ( SQL_update in vast_pipeline.pipeline.loading ) columns\\rows 10 3 10 4 10 5 4 0.016 0.11 3.08 8 0.019 0.32 4.31 12 0.027 0.38 5.39 10 5 is slower per cell than 10 4 and 10 3 , not sure why. Recommend updating 10 4 rows each time. This timing info does vary a bit on randomness. Sometimes the SQL join as takes as long as 1 second to complete 10 3 rows, I'm not sure what's causing this.","title":"Benchmarks"},{"location":"developing/profiling/#benchmarks","text":"","title":"Benchmarks"},{"location":"developing/profiling/#initial-profiling","text":"These profiling tests were run with the pipeline codebase correspondent to commit 373c2ce (some commits after the first release). Running on 12GB of data with 464MB peak memory usage takes 4 mins: performance: ~80% final_operations , ~10% get_src_skyregion_merged_df . final_operations calls other functions, out of these the largest is 50% in make_upload_sources which spends about 20% of time on utils <method 'execute' of 'psycopg2.extensions.cursor' objects> . The get_src_skyregion_merged_df time sink is in threading 15% of time is on wait ( final_operations spends some time on threading as well, hence 15% > 10%). memory: 40% pyarrow parquet write_table , rest is mostly fragmented, some more pyarrow and some pandas Running on 3MB of data with peak memory usage 176MB, takes 1.5s: performance: ~30% goes to pipeline (about 9% of this is pickle ), ~11% goes to read , rest goes to django I think memory: 30% memory is spent on django , 20% is spent on astropy/coordinates/matrix_utilities , 10% on importing other modules, rest is fragmented quite small Note that I didn't include the generation of the images/*/measurements.parquet or other files in these profiles.","title":"Initial Profiling"},{"location":"developing/profiling/#database-update-operations","text":"Delete ( Model.objects.all().delete() ) and reupload ( bulk_upload ) (in seconds) columns\\rows 10 3 10 4 10 5 4 0.15 1.24 12.95 8 0.26 1.64 19.11 12 0.31 2.18 21.49 Per cell, 10 3 rows is slower than 10 4 and 10 5 rows, possibly due to overhead. Best to avoid uploading 10 3 rows each bulk_create call. Django bulk_update columns\\rows 10 3 10 4 10 5 4 3.39 na na 8 4.38 na na 12 5.50 na na I don't think there's any point testing 10 4 or 10 5 rows, it's obviously the worst performing function, and I've already had to force quit the terminal twice because keyboard interrupt didn't work. SQL join as ( SQL_update in vast_pipeline.pipeline.loading ) columns\\rows 10 3 10 4 10 5 4 0.016 0.11 3.08 8 0.019 0.32 4.31 12 0.027 0.38 5.39 10 5 is slower per cell than 10 4 and 10 3 , not sure why. Recommend updating 10 4 rows each time. This timing info does vary a bit on randomness. Sometimes the SQL join as takes as long as 1 second to complete 10 3 rows, I'm not sure what's causing this.","title":"Database Update Operations"},{"location":"developing/tests/","text":"Tests Guidelines \u00b6 This section describes how to run the test suite of the VAST Pipeline. General Tests \u00b6 Test are found under the folder tests . Have a look and feel free to include new tests. Run the tests with the following: To run all tests: ( pipeline_env ) $ ./manage.py test To run one test file or class, use: ( pipeline_env ) $ ./manage.py test <path.to.test> for example, to run the test class CheckRunConfigValidationTest located in test_pipelineconfig.py , use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_pipelineconfig.CheckRunConfigValidationTest to run the tests located in test_webserver.py , use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_webserver Regression Tests \u00b6 Regression tests located in test_regression require the use of the VAST_2118-06A field test dataset which is not a part of the repository. This data is downloadable from cloudstor . You can use the script located in tests/regression-data/ : cd vast_pipeline/tests/regression-data/ && ./download.sh to download the VAST_2118-06A field test dataset into the regression-data folder. Or manually by clicking the button below: Download data for test and place the VAST_2118-06A field test dataset into the regression-data folder. These regression tests are skipped if the dataset is not present. All tests should be run before pushing to master. Running all the tests takes a few minutes, so it is not recommended to run them for every change. If you have made a minor change and would like to only run unit tests, skipping regression tests, use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_pipeline Note If changes are made to the default config keys, these changes need to be propagated to the test config files.","title":"Tests"},{"location":"developing/tests/#tests-guidelines","text":"This section describes how to run the test suite of the VAST Pipeline.","title":"Tests Guidelines"},{"location":"developing/tests/#general-tests","text":"Test are found under the folder tests . Have a look and feel free to include new tests. Run the tests with the following: To run all tests: ( pipeline_env ) $ ./manage.py test To run one test file or class, use: ( pipeline_env ) $ ./manage.py test <path.to.test> for example, to run the test class CheckRunConfigValidationTest located in test_pipelineconfig.py , use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_pipelineconfig.CheckRunConfigValidationTest to run the tests located in test_webserver.py , use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_webserver","title":"General Tests"},{"location":"developing/tests/#regression-tests","text":"Regression tests located in test_regression require the use of the VAST_2118-06A field test dataset which is not a part of the repository. This data is downloadable from cloudstor . You can use the script located in tests/regression-data/ : cd vast_pipeline/tests/regression-data/ && ./download.sh to download the VAST_2118-06A field test dataset into the regression-data folder. Or manually by clicking the button below: Download data for test and place the VAST_2118-06A field test dataset into the regression-data folder. These regression tests are skipped if the dataset is not present. All tests should be run before pushing to master. Running all the tests takes a few minutes, so it is not recommended to run them for every change. If you have made a minor change and would like to only run unit tests, skipping regression tests, use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_pipeline Note If changes are made to the default config keys, these changes need to be propagated to the test config files.","title":"Regression Tests"},{"location":"exploringwebsite/admintools/","text":"Website Admin Tools \u00b6 Accessing the Admin Tools \u00b6 Users designated as administrators of the pipeline instance being explored (controlled by GitHub team membership) will be able to see the admin button at the top of the navbar as shown below. Clicking this button and then selecting the Django open will take the user to the Django admin backend interface shown below. Authentification and Authorization \u00b6 This section allows for the management of the user accounts and groups. Here users can be made admins and details such as email address updated. Django Q Tasks \u00b6 This area allows for the management of the Django Q processing queue. Admins are able to cancel scheduled tasks, view failed tasks or schedule new tasks. Python Social Auth \u00b6 The area for managing aspects of the authentification system that allows users to log in via GitHub. VAST_PIPELINE \u00b6 Admins are able to interact with the pipeline results data that has been uploaded from pipeline runs. This includes editing and removing objects or fields in the data as well as tags and comments.","title":"Web App Admin Tools"},{"location":"exploringwebsite/admintools/#website-admin-tools","text":"","title":"Website Admin Tools"},{"location":"exploringwebsite/admintools/#accessing-the-admin-tools","text":"Users designated as administrators of the pipeline instance being explored (controlled by GitHub team membership) will be able to see the admin button at the top of the navbar as shown below. Clicking this button and then selecting the Django open will take the user to the Django admin backend interface shown below.","title":"Accessing the Admin Tools"},{"location":"exploringwebsite/admintools/#authentification-and-authorization","text":"This section allows for the management of the user accounts and groups. Here users can be made admins and details such as email address updated.","title":"Authentification and Authorization"},{"location":"exploringwebsite/admintools/#django-q-tasks","text":"This area allows for the management of the Django Q processing queue. Admins are able to cancel scheduled tasks, view failed tasks or schedule new tasks.","title":"Django Q Tasks"},{"location":"exploringwebsite/admintools/#python-social-auth","text":"The area for managing aspects of the authentification system that allows users to log in via GitHub.","title":"Python Social Auth"},{"location":"exploringwebsite/admintools/#vast_pipeline","text":"Admins are able to interact with the pipeline results data that has been uploaded from pipeline runs. This includes editing and removing objects or fields in the data as well as tags and comments.","title":"VAST_PIPELINE"},{"location":"exploringwebsite/datatables/","text":"DataTables \u00b6 Much of the data is presented using tables that share consistent functionality across the website. An example of a table is shown below, note the interactive features across the top of the table, these are explained after the screenshot. Show 10 entries : A selectable limiter of how many rows to display at once (maximum 100). Column visibility : Enables the user to hide and show columns columns. In the screenshot below the compactness column is hidden by deselecting it in the presented list. CSV : Will download a CSV file of the data currently shown on screen. Excel : Will download an Excel file of the data currently shown on screen. Warning Note the statement currently shown on screen - only this data will be downloaded to the CSV and Excel files. All the records are not able to be downloaded in this manner - for this it is recommened to interact with the output parquet files . Search : A search bar for the user to filter the table to the row they require. The search will take into account all appropriate columns when searching.","title":"DataTables"},{"location":"exploringwebsite/datatables/#datatables","text":"Much of the data is presented using tables that share consistent functionality across the website. An example of a table is shown below, note the interactive features across the top of the table, these are explained after the screenshot. Show 10 entries : A selectable limiter of how many rows to display at once (maximum 100). Column visibility : Enables the user to hide and show columns columns. In the screenshot below the compactness column is hidden by deselecting it in the presented list. CSV : Will download a CSV file of the data currently shown on screen. Excel : Will download an Excel file of the data currently shown on screen. Warning Note the statement currently shown on screen - only this data will be downloaded to the CSV and Excel files. All the records are not able to be downloaded in this manner - for this it is recommened to interact with the output parquet files . Search : A search bar for the user to filter the table to the row they require. The search will take into account all appropriate columns when searching.","title":"DataTables"},{"location":"exploringwebsite/imagepages/","text":"Image Pages \u00b6 This page details the website pages for information on the images. List of Images \u00b6 Shown on this page is a list of images that have been ingested into the pipeline database from all pipeline runs, along with their statistics. From this page the full detail page of a specific image can be accessed by clicking on the image name. Explanation of the table options can be found in the DataTables section . Image Detail Page \u00b6 This page presents all the information about the selected image. Previous & Next Buttons \u00b6 These buttons do the following: Previous : Navigates to the previous image by id value. Next : Navigates to the next image by id value. Details \u00b6 A text representation of details of the image. Aladin Lite Viewer \u00b6 Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the image central coordinates of the image. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS. The red square shows the footprint of the image sky region on the sky. User Comments \u00b6 Users are able to read and post comments on an image using this form. Measurements Table \u00b6 This table displays all the Selavy measurements that were ingested with the image (no forced measurements appear here as they are run specific). The measurement detail page can be reached by clicking the measurement name. Pipeline Runs Table \u00b6 This table displays all the pipeline runs that use the current image. The pipeline detail page can be reached by clicking the run name.","title":"Image Pages"},{"location":"exploringwebsite/imagepages/#image-pages","text":"This page details the website pages for information on the images.","title":"Image Pages"},{"location":"exploringwebsite/imagepages/#list-of-images","text":"Shown on this page is a list of images that have been ingested into the pipeline database from all pipeline runs, along with their statistics. From this page the full detail page of a specific image can be accessed by clicking on the image name. Explanation of the table options can be found in the DataTables section .","title":"List of Images"},{"location":"exploringwebsite/imagepages/#image-detail-page","text":"This page presents all the information about the selected image.","title":"Image Detail Page"},{"location":"exploringwebsite/imagepages/#previous-next-buttons","text":"These buttons do the following: Previous : Navigates to the previous image by id value. Next : Navigates to the next image by id value.","title":"Previous &amp; Next Buttons"},{"location":"exploringwebsite/imagepages/#details","text":"A text representation of details of the image.","title":"Details"},{"location":"exploringwebsite/imagepages/#aladin-lite-viewer","text":"Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the image central coordinates of the image. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS. The red square shows the footprint of the image sky region on the sky.","title":"Aladin Lite Viewer"},{"location":"exploringwebsite/imagepages/#user-comments","text":"Users are able to read and post comments on an image using this form.","title":"User Comments"},{"location":"exploringwebsite/imagepages/#measurements-table","text":"This table displays all the Selavy measurements that were ingested with the image (no forced measurements appear here as they are run specific). The measurement detail page can be reached by clicking the measurement name.","title":"Measurements Table"},{"location":"exploringwebsite/imagepages/#pipeline-runs-table","text":"This table displays all the pipeline runs that use the current image. The pipeline detail page can be reached by clicking the run name.","title":"Pipeline Runs Table"},{"location":"exploringwebsite/measurementpages/","text":"Measurement Pages \u00b6 This page details the website pages for information on the measurements. List of Measurements \u00b6 A list of measurements that have been ingested into the pipeline database from all pipeline runs, along with their statistics, is shown on this page. From this page the full detail page of a specific measurement can be accessed by clicking on the name of the measurement. Explanation of the table options can be found in the DataTables section . Measurement Detail Page \u00b6 This page presents all the information about the selected measurement, including a postage stamp cutout of the component. SIMBAD, NED, Previous & Next Buttons \u00b6 These buttons do the following: SIMBAD : Performs a cone search on SIMBAD with a radius of 10 arcmin centered on the measurement location. NED : Performs a cone search on NED with a radius of 10 arcmin centered on the measurement location. Previous : Navigates to the previous measurement by id value. Next : Navigates to the next measurement by id value. Details \u00b6 A text representation of details of the measurement. Aladin Lite Viewer \u00b6 Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the location of the measurement. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS. JS9 Viewer \u00b6 JS9 website . The right panel contains a JS9 viewer showing the postage stamp FITS image of the measurement loaded from its respective image FITS file. Note If the image data is removed from its location when the pipeline run was processed the JS9 viewer will no longer work. User Comments \u00b6 Users are able to read and post comments on a measurement using this form. Sources Table \u00b6 The sources table shows all the sources, from all pipeline runs, that the measurement is associated to. Explanation of the table options can be found on the overview page here . Siblings Table \u00b6 The siblings table displays all other measurements that are a sibling of the current measurement, i.e., the measurements belong to the same island (as determined by Selavy ).","title":"Measurement Pages"},{"location":"exploringwebsite/measurementpages/#measurement-pages","text":"This page details the website pages for information on the measurements.","title":"Measurement Pages"},{"location":"exploringwebsite/measurementpages/#list-of-measurements","text":"A list of measurements that have been ingested into the pipeline database from all pipeline runs, along with their statistics, is shown on this page. From this page the full detail page of a specific measurement can be accessed by clicking on the name of the measurement. Explanation of the table options can be found in the DataTables section .","title":"List of Measurements"},{"location":"exploringwebsite/measurementpages/#measurement-detail-page","text":"This page presents all the information about the selected measurement, including a postage stamp cutout of the component.","title":"Measurement Detail Page"},{"location":"exploringwebsite/measurementpages/#simbad-ned-previous-next-buttons","text":"These buttons do the following: SIMBAD : Performs a cone search on SIMBAD with a radius of 10 arcmin centered on the measurement location. NED : Performs a cone search on NED with a radius of 10 arcmin centered on the measurement location. Previous : Navigates to the previous measurement by id value. Next : Navigates to the next measurement by id value.","title":"SIMBAD, NED, Previous &amp; Next Buttons"},{"location":"exploringwebsite/measurementpages/#details","text":"A text representation of details of the measurement.","title":"Details"},{"location":"exploringwebsite/measurementpages/#aladin-lite-viewer","text":"Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the location of the measurement. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS.","title":"Aladin Lite Viewer"},{"location":"exploringwebsite/measurementpages/#js9-viewer","text":"JS9 website . The right panel contains a JS9 viewer showing the postage stamp FITS image of the measurement loaded from its respective image FITS file. Note If the image data is removed from its location when the pipeline run was processed the JS9 viewer will no longer work.","title":"JS9 Viewer"},{"location":"exploringwebsite/measurementpages/#user-comments","text":"Users are able to read and post comments on a measurement using this form.","title":"User Comments"},{"location":"exploringwebsite/measurementpages/#sources-table","text":"The sources table shows all the sources, from all pipeline runs, that the measurement is associated to. Explanation of the table options can be found on the overview page here .","title":"Sources Table"},{"location":"exploringwebsite/measurementpages/#siblings-table","text":"The siblings table displays all other measurements that are a sibling of the current measurement, i.e., the measurements belong to the same island (as determined by Selavy ).","title":"Siblings Table"},{"location":"exploringwebsite/runpages/","text":"Pipeline Run Pages \u00b6 This page details the website pages for information on the pipeline runs. List of Pipeline Runs \u00b6 A list of the pipeline runs that have been processed or initialised are presented on this page along with basic statistics, including the run status. From this page the full detail page of a specific pipeline run can be accessed by clicking on the name of the pipeline run. Explanation of the table options can be found in the DataTables section . Run Status Description Completed The run has successfully finished processing. Deleting The pipeline run is currently being deleted. Error The run has encountered an error during processing and has stopped. Initialised The run has been created but not yet run. Queued The run has been sent to the scheduler for running but has not started yet. Restoring The pipeline run is currently being restored. Running The run is currently processing. Pipeline Run Detail Page \u00b6 This page presents all the information about the pipeline run, including options to edit the configuration file and to schedule the run for processing, restore the run, delete the run and generate the arrow measurement files. Action Buttons \u00b6 For admins and creators of runs there are four action buttons available: Generate Arrow Files A process to generate the arrow measurement files. See Generating Arrow Files . Delete Run Delete the pipeline run. See Deleting a Run . Restore Run A process to restore the run to the previous successful state. See Restoring a Run . Add Images or Re-Process Run/Process Run Process the pipeline run. See Processing a Run . Summary Cards \u00b6 The cards at the top of the page give a summary of the total numbers of: Images in the pipeline run. Measurements in the pipeline run. Sources in the pipeline run. New sources in the pipeline run. Clicking on the total number of images or measurements will navigate the user to the Image and Measurements tables on this page, where as the source cards will take the user to the Sources Query page. Warning When sent to the source query page, the user should make sure to click submit on the search. Details \u00b6 A text representation of details of the pipeline run. Run Sky Regions \u00b6 A sky map showing the area of sky covered by the images associated with the pipeline run. Configuration File \u00b6 Here the pipeline run configuration file can be viewed, edited and validated. Editing the Configuration File \u00b6 To edit the configuration file first select the Toggle on/off Config Edit option, that is shown in the screenshot to the right. This will enter edit mode on the configuration file as denoted by the --Edit Mode-- message shown in the screenshot below. Warning Do not toggle off edit mode without first selecting Wrtie Current Config otherwise changes will be lost. When all changes are applied, select the Write Current Config to save the changes. Validating the Configuration File \u00b6 From the configuration file menu select the Validate Config option. A feedback modal will then appear with feedback stating whether the configuration validation was successful or failed. The feedback may take a moment to appear as the check is performed. User Comments \u00b6 Users are able to read and post comments on a pipeline run using this form. Log Files \u00b6 There are three log files available, which are present depending on the actions performed. All logs are timestamped with the run time, and by default the most recent log is shown. A dropdown menu of available log files to view is available at the right hand side of the header as shown in the examples below. If there are no logs to show for the respective task then the log window will display No logs to show and the dropdown menu will appear empty. Run Log File \u00b6 The full log file of the pipeline run process. Restore Log File \u00b6 The log file of the restore run action. Generate Arrow Files Log File \u00b6 The log file of the generate arrow files action. Image and Measurements Tables \u00b6 Two tables are on the pipeline run detail page displaying the images and measurements (including forced measurements) that are part of the pipeline run.","title":"Pipeline Run Pages"},{"location":"exploringwebsite/runpages/#pipeline-run-pages","text":"This page details the website pages for information on the pipeline runs.","title":"Pipeline Run Pages"},{"location":"exploringwebsite/runpages/#list-of-pipeline-runs","text":"A list of the pipeline runs that have been processed or initialised are presented on this page along with basic statistics, including the run status. From this page the full detail page of a specific pipeline run can be accessed by clicking on the name of the pipeline run. Explanation of the table options can be found in the DataTables section . Run Status Description Completed The run has successfully finished processing. Deleting The pipeline run is currently being deleted. Error The run has encountered an error during processing and has stopped. Initialised The run has been created but not yet run. Queued The run has been sent to the scheduler for running but has not started yet. Restoring The pipeline run is currently being restored. Running The run is currently processing.","title":"List of Pipeline Runs"},{"location":"exploringwebsite/runpages/#pipeline-run-detail-page","text":"This page presents all the information about the pipeline run, including options to edit the configuration file and to schedule the run for processing, restore the run, delete the run and generate the arrow measurement files.","title":"Pipeline Run Detail Page"},{"location":"exploringwebsite/runpages/#action-buttons","text":"For admins and creators of runs there are four action buttons available: Generate Arrow Files A process to generate the arrow measurement files. See Generating Arrow Files . Delete Run Delete the pipeline run. See Deleting a Run . Restore Run A process to restore the run to the previous successful state. See Restoring a Run . Add Images or Re-Process Run/Process Run Process the pipeline run. See Processing a Run .","title":"Action Buttons"},{"location":"exploringwebsite/runpages/#summary-cards","text":"The cards at the top of the page give a summary of the total numbers of: Images in the pipeline run. Measurements in the pipeline run. Sources in the pipeline run. New sources in the pipeline run. Clicking on the total number of images or measurements will navigate the user to the Image and Measurements tables on this page, where as the source cards will take the user to the Sources Query page. Warning When sent to the source query page, the user should make sure to click submit on the search.","title":"Summary Cards"},{"location":"exploringwebsite/runpages/#details","text":"A text representation of details of the pipeline run.","title":"Details"},{"location":"exploringwebsite/runpages/#run-sky-regions","text":"A sky map showing the area of sky covered by the images associated with the pipeline run.","title":"Run Sky Regions"},{"location":"exploringwebsite/runpages/#configuration-file","text":"Here the pipeline run configuration file can be viewed, edited and validated.","title":"Configuration File"},{"location":"exploringwebsite/runpages/#editing-the-configuration-file","text":"To edit the configuration file first select the Toggle on/off Config Edit option, that is shown in the screenshot to the right. This will enter edit mode on the configuration file as denoted by the --Edit Mode-- message shown in the screenshot below. Warning Do not toggle off edit mode without first selecting Wrtie Current Config otherwise changes will be lost. When all changes are applied, select the Write Current Config to save the changes.","title":"Editing the Configuration File"},{"location":"exploringwebsite/runpages/#validating-the-configuration-file","text":"From the configuration file menu select the Validate Config option. A feedback modal will then appear with feedback stating whether the configuration validation was successful or failed. The feedback may take a moment to appear as the check is performed.","title":"Validating the Configuration File"},{"location":"exploringwebsite/runpages/#user-comments","text":"Users are able to read and post comments on a pipeline run using this form.","title":"User Comments"},{"location":"exploringwebsite/runpages/#log-files","text":"There are three log files available, which are present depending on the actions performed. All logs are timestamped with the run time, and by default the most recent log is shown. A dropdown menu of available log files to view is available at the right hand side of the header as shown in the examples below. If there are no logs to show for the respective task then the log window will display No logs to show and the dropdown menu will appear empty.","title":"Log Files"},{"location":"exploringwebsite/runpages/#run-log-file","text":"The full log file of the pipeline run process.","title":"Run Log File"},{"location":"exploringwebsite/runpages/#restore-log-file","text":"The log file of the restore run action.","title":"Restore Log File"},{"location":"exploringwebsite/runpages/#generate-arrow-files-log-file","text":"The log file of the generate arrow files action.","title":"Generate Arrow Files Log File"},{"location":"exploringwebsite/runpages/#image-and-measurements-tables","text":"Two tables are on the pipeline run detail page displaying the images and measurements (including forced measurements) that are part of the pipeline run.","title":"Image and Measurements Tables"},{"location":"exploringwebsite/sourceanalysis/","text":"Source \u03b7-V Analysis \u00b6 This page details the interactive analysis tool of the \u03b7 and V metrics for a selection of sources. Further Reading Descriptions of the \u03b7 and V metrics can be found on the source statistics page . For a detailed overview of the method, please refer to Rowlinson et al., 2019 . The analysis can be performed on the results of a source query . Tip: Sensible Querying! To get the most out of the tool is advised to design a query that will eliminate as many erroneous results as possible. For example, making sure sources are isolated and have no siblings or relations. The VAST Tools package should be used for more advanced queries. Accessing the Analysis Page \u00b6 Once a query has been performed on the source query page the Go to \u03b7-V analysis button will be active as highlighted in the image below. Click this button and the analysis page will open. Warning: Bad Sources Bad sources for analysis will be automatically removed. These include sources that only have one datapoint or where the \u03b7 and/or V are equal to 0 or have failed. \u03b7-V Plot Description \u00b6 Shown on the left side of the page is the the log-log plot of the peak flux \u03b7 and V metrics of the sources from the query. The distributions of the metrics are fitted with a Gaussian, and a sigma cut is displayed on the plot that by default is set to a value of \\(3\\sigma\\) . The highlighted region represents the area of plot that is beyond both thresholds, and this is where transient sources will be found. The colour of the points represent how many detection datapoints the source contains. Plot Options \u00b6 At the bottom of the plot are options to change both the flux type and the multiplication factor of the sigma cut. Once the new settings are entered click the Apply button and the plot will reload with the new options. Viewing Source Light Curves & Information \u00b6 Hovering over a source on the plot will show an information window that displays the source name, id and the \u03b7 and V values. Clicking on the source will load the light curve, source information and external crossmatching search results into the panels on the right side of the page. Selecting another source will dynamically update these panels without having to leave the page. The source information panel contains a link to go to the full source detail page and the ability to favourite a source directly from this page. Displaying High Source Counts \u00b6 When querying large pipeline runs it is possible that a query will return tens of thousands of results. Plotting such a high number of sources is very intensive and would take a significant amount of time to render. To solve this, when a high number of sources are requested to be plotted, all the sources outside of the threshold transient area are plotted as a static image that represents the distribution of the sources. These sources are not interactive. By default the threshold is set to 20,000 datapoints and is configurable by the administrator . Any sources that fall within the transient threshold region are plotted as normal and are interactive as with the standard plot. Warning: Setting Low Thresholds Setting low thresholds with high source counts could cause a significant amount of candidates to be plotted with the normal method. This may result in the plot taking up to a few minutes to load within the browser. Note: Colour Bar The colour bar only applies to the interactive datapoints.","title":"Source \u03b7-V Analysis"},{"location":"exploringwebsite/sourceanalysis/#source-v-analysis","text":"This page details the interactive analysis tool of the \u03b7 and V metrics for a selection of sources. Further Reading Descriptions of the \u03b7 and V metrics can be found on the source statistics page . For a detailed overview of the method, please refer to Rowlinson et al., 2019 . The analysis can be performed on the results of a source query . Tip: Sensible Querying! To get the most out of the tool is advised to design a query that will eliminate as many erroneous results as possible. For example, making sure sources are isolated and have no siblings or relations. The VAST Tools package should be used for more advanced queries.","title":"Source \u03b7-V Analysis"},{"location":"exploringwebsite/sourceanalysis/#accessing-the-analysis-page","text":"Once a query has been performed on the source query page the Go to \u03b7-V analysis button will be active as highlighted in the image below. Click this button and the analysis page will open. Warning: Bad Sources Bad sources for analysis will be automatically removed. These include sources that only have one datapoint or where the \u03b7 and/or V are equal to 0 or have failed.","title":"Accessing the Analysis Page"},{"location":"exploringwebsite/sourceanalysis/#-v-plot-description","text":"Shown on the left side of the page is the the log-log plot of the peak flux \u03b7 and V metrics of the sources from the query. The distributions of the metrics are fitted with a Gaussian, and a sigma cut is displayed on the plot that by default is set to a value of \\(3\\sigma\\) . The highlighted region represents the area of plot that is beyond both thresholds, and this is where transient sources will be found. The colour of the points represent how many detection datapoints the source contains.","title":"\u03b7-V Plot Description"},{"location":"exploringwebsite/sourceanalysis/#plot-options","text":"At the bottom of the plot are options to change both the flux type and the multiplication factor of the sigma cut. Once the new settings are entered click the Apply button and the plot will reload with the new options.","title":"Plot Options"},{"location":"exploringwebsite/sourceanalysis/#viewing-source-light-curves-information","text":"Hovering over a source on the plot will show an information window that displays the source name, id and the \u03b7 and V values. Clicking on the source will load the light curve, source information and external crossmatching search results into the panels on the right side of the page. Selecting another source will dynamically update these panels without having to leave the page. The source information panel contains a link to go to the full source detail page and the ability to favourite a source directly from this page.","title":"Viewing Source Light Curves &amp; Information"},{"location":"exploringwebsite/sourceanalysis/#displaying-high-source-counts","text":"When querying large pipeline runs it is possible that a query will return tens of thousands of results. Plotting such a high number of sources is very intensive and would take a significant amount of time to render. To solve this, when a high number of sources are requested to be plotted, all the sources outside of the threshold transient area are plotted as a static image that represents the distribution of the sources. These sources are not interactive. By default the threshold is set to 20,000 datapoints and is configurable by the administrator . Any sources that fall within the transient threshold region are plotted as normal and are interactive as with the standard plot. Warning: Setting Low Thresholds Setting low thresholds with high source counts could cause a significant amount of candidates to be plotted with the normal method. This may result in the plot taking up to a few minutes to load within the browser. Note: Colour Bar The colour bar only applies to the interactive datapoints.","title":"Displaying High Source Counts"},{"location":"exploringwebsite/sourcedetail/","text":"Source Detail \u00b6 This page presents all the information about the selected source, including a light curve and cutouts of all the measurements that are associated to the source. Star, SIMBAD, NED, Previous & Next Buttons \u00b6 These buttons do the following: Star : Adds the source to the user's favourites, see Source Tags and Favourites . SIMBAD : Performs a cone search on SIMBAD with a radius of 10 arcmin centered on the source location. NED : Performs a cone search on NED with a radius of 10 arcmin centered on the source location. Previous : Navigates to the previous source that was returned in the source query. Next : Navigates to the next source that was returned in the source query Details \u00b6 A text representation of details of the measurement. User Comments & Tags \u00b6 Users are able to read and post comments on a measurement using this form, in addition to adding and removing tags, see Source Tags and Favourites . Aladin Lite Viewer \u00b6 Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the location of the source. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS. Light Curve \u00b6 The light curve of the source is shown. The peak or integrated flux can be selected by using the radio selection buttons. Hovering over the data points on the light curve will show an information panel that contains the date of the measurement, the flux and the measurement number. It also contains a thumbnail image preview of the respective measurement. Two-epoch Node Graph \u00b6 The node graph is a visual representation of what two-epoch pairings have significant variability metric values. If an epoch pairing is significant then they are joined by a line on the graph. Hovering over the line will display the pair metrics for the selected flux type (peak or integrated) and highlight the epoch pairing on the light curve plot. External Search Results Table \u00b6 This table shows the result of a query to the SIMBAD, NED, and TNS services for astronomical sources within 1 arcmin of the source location. Along with the name and coordinate of the matches, the on-sky separation between the source is shown along with the source type. JS9 Viewer Postage Stamps \u00b6 JS9 website . The JS9 viewer is used to show the postage stamp FITS images of the measurements that are associated with the source, loaded from their respective image FITS files. Note If the image data is removed from its location when the pipeline run was processed the JS9 viewer will no longer work. Source Measurements Table \u00b6 This table displays the measurements that are associated with the source. The detail page for the measurement can be reached by clicking the name of the respective measurement. Related Sources Table \u00b6 This table displays the sources that are a relation of the source in question. For further information refer to the Relations section in the association documentation.","title":"Source Detail"},{"location":"exploringwebsite/sourcedetail/#source-detail","text":"This page presents all the information about the selected source, including a light curve and cutouts of all the measurements that are associated to the source.","title":"Source Detail"},{"location":"exploringwebsite/sourcedetail/#star-simbad-ned-previous-next-buttons","text":"These buttons do the following: Star : Adds the source to the user's favourites, see Source Tags and Favourites . SIMBAD : Performs a cone search on SIMBAD with a radius of 10 arcmin centered on the source location. NED : Performs a cone search on NED with a radius of 10 arcmin centered on the source location. Previous : Navigates to the previous source that was returned in the source query. Next : Navigates to the next source that was returned in the source query","title":"Star, SIMBAD, NED, Previous &amp; Next Buttons"},{"location":"exploringwebsite/sourcedetail/#details","text":"A text representation of details of the measurement.","title":"Details"},{"location":"exploringwebsite/sourcedetail/#user-comments-tags","text":"Users are able to read and post comments on a measurement using this form, in addition to adding and removing tags, see Source Tags and Favourites .","title":"User Comments &amp; Tags"},{"location":"exploringwebsite/sourcedetail/#aladin-lite-viewer","text":"Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the location of the source. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS.","title":"Aladin Lite Viewer"},{"location":"exploringwebsite/sourcedetail/#light-curve","text":"The light curve of the source is shown. The peak or integrated flux can be selected by using the radio selection buttons. Hovering over the data points on the light curve will show an information panel that contains the date of the measurement, the flux and the measurement number. It also contains a thumbnail image preview of the respective measurement.","title":"Light Curve"},{"location":"exploringwebsite/sourcedetail/#two-epoch-node-graph","text":"The node graph is a visual representation of what two-epoch pairings have significant variability metric values. If an epoch pairing is significant then they are joined by a line on the graph. Hovering over the line will display the pair metrics for the selected flux type (peak or integrated) and highlight the epoch pairing on the light curve plot.","title":"Two-epoch Node Graph"},{"location":"exploringwebsite/sourcedetail/#external-search-results-table","text":"This table shows the result of a query to the SIMBAD, NED, and TNS services for astronomical sources within 1 arcmin of the source location. Along with the name and coordinate of the matches, the on-sky separation between the source is shown along with the source type.","title":"External Search Results Table"},{"location":"exploringwebsite/sourcedetail/#js9-viewer-postage-stamps","text":"JS9 website . The JS9 viewer is used to show the postage stamp FITS images of the measurements that are associated with the source, loaded from their respective image FITS files. Note If the image data is removed from its location when the pipeline run was processed the JS9 viewer will no longer work.","title":"JS9 Viewer Postage Stamps"},{"location":"exploringwebsite/sourcedetail/#source-measurements-table","text":"This table displays the measurements that are associated with the source. The detail page for the measurement can be reached by clicking the name of the respective measurement.","title":"Source Measurements Table"},{"location":"exploringwebsite/sourcedetail/#related-sources-table","text":"This table displays the sources that are a relation of the source in question. For further information refer to the Relations section in the association documentation.","title":"Related Sources Table"},{"location":"exploringwebsite/sourcequery/","text":"Source Query \u00b6 This page details the Source Query interface. Query Options \u00b6 Users can filter and query the sources currently in the database by using the form located on this page. The form is submitted by clicking the blue button, the red button will reset the form by removing all entered values. Once the form is submitted the results are dynamically updated in the results table below the form (i.e. on the same page). The following sections provide further details on the form. Data Source \u00b6 Here a specific pipeline run can be selected from a dropdown list to filter sources to only those from the chosen run. By default sources from all runs are shown. Note Only successfully completed pipeline runs are available to select. This rule also applies when all is selected. Cone Search \u00b6 Users can choose whether to input their coordinates directly or use the object name resolver to attempt to fetch the coordinates. Manual Input \u00b6 The format of the coordinates should be in a standard format that is recognised by astropy, for example: 21 29 45.29 -04 29 11.9 21:29:45.29 -04:29:11.9 322.4387083 -4.4866389 Galactic coordinates can also be entered by selecting Galactic from the dropdown menu that is set to ICRS by default. Feedback will be given immediately whether the coordinates are valid, as shown in the screenshots below: Once the coordinates have been entered the radius value must also be specified as shown in the screenshot above. Use the dropdown menu to change the radius unit to be arcsec , arcmin or deg . Name Resolver \u00b6 To use the name resolver, the name of the source should be entered into the Object Name field (e.g. PSR J2129-04 ), select the name resolver service and then click the Resolve button. The coordinates will then be automatically filled on a successful match. if no match is found then this will be communicated by the form as below: Table Filters \u00b6 This section of the form allows the user to place thresholds and selections on specific metrics of the sources. Please refer to the Source Statistics page for details on the different metrics. There are also tooltips located on the form to offer explanations. The following options are not standard source metrics: Include and Exclude Tags \u00b6 Users can attach tags to sources (see Source Tags and Favourites ) and here tags can be selected to include or exclude in the source search. Source Selection \u00b6 Here specific sources can be searched for by entering the source names, or source database id values, in a comma-separated list. For example: J011816.05-730747.77,J011816.05-730747.77,J213221.21-040900.42 1031,1280,52 are valid entries to this search field. Use the dropdown menu to declare whether name (default) or id values are being searched. Results Table \u00b6 Located directly below the form is the results table which is dynamically updated once the form is submitted. The full detail page of a specific source can be accessed by clicking on the source name in the table. Explanation of the table options can be found in the DataTables section . The Go to \u03b7-V analysis button will launch the \u03b7-V analysis page for the sources contained in the query results. Please refer to the Source \u03b7-V Analysis Page section for the full details of this feature. Note The Go to \u03b7-V analysis button is only active after a query has been performed.","title":"Source Query"},{"location":"exploringwebsite/sourcequery/#source-query","text":"This page details the Source Query interface.","title":"Source Query"},{"location":"exploringwebsite/sourcequery/#query-options","text":"Users can filter and query the sources currently in the database by using the form located on this page. The form is submitted by clicking the blue button, the red button will reset the form by removing all entered values. Once the form is submitted the results are dynamically updated in the results table below the form (i.e. on the same page). The following sections provide further details on the form.","title":"Query Options"},{"location":"exploringwebsite/sourcequery/#data-source","text":"Here a specific pipeline run can be selected from a dropdown list to filter sources to only those from the chosen run. By default sources from all runs are shown. Note Only successfully completed pipeline runs are available to select. This rule also applies when all is selected.","title":"Data Source"},{"location":"exploringwebsite/sourcequery/#cone-search","text":"Users can choose whether to input their coordinates directly or use the object name resolver to attempt to fetch the coordinates.","title":"Cone Search"},{"location":"exploringwebsite/sourcequery/#manual-input","text":"The format of the coordinates should be in a standard format that is recognised by astropy, for example: 21 29 45.29 -04 29 11.9 21:29:45.29 -04:29:11.9 322.4387083 -4.4866389 Galactic coordinates can also be entered by selecting Galactic from the dropdown menu that is set to ICRS by default. Feedback will be given immediately whether the coordinates are valid, as shown in the screenshots below: Once the coordinates have been entered the radius value must also be specified as shown in the screenshot above. Use the dropdown menu to change the radius unit to be arcsec , arcmin or deg .","title":"Manual Input"},{"location":"exploringwebsite/sourcequery/#name-resolver","text":"To use the name resolver, the name of the source should be entered into the Object Name field (e.g. PSR J2129-04 ), select the name resolver service and then click the Resolve button. The coordinates will then be automatically filled on a successful match. if no match is found then this will be communicated by the form as below:","title":"Name Resolver"},{"location":"exploringwebsite/sourcequery/#table-filters","text":"This section of the form allows the user to place thresholds and selections on specific metrics of the sources. Please refer to the Source Statistics page for details on the different metrics. There are also tooltips located on the form to offer explanations. The following options are not standard source metrics:","title":"Table Filters"},{"location":"exploringwebsite/sourcequery/#include-and-exclude-tags","text":"Users can attach tags to sources (see Source Tags and Favourites ) and here tags can be selected to include or exclude in the source search.","title":"Include and Exclude Tags"},{"location":"exploringwebsite/sourcequery/#source-selection","text":"Here specific sources can be searched for by entering the source names, or source database id values, in a comma-separated list. For example: J011816.05-730747.77,J011816.05-730747.77,J213221.21-040900.42 1031,1280,52 are valid entries to this search field. Use the dropdown menu to declare whether name (default) or id values are being searched.","title":"Source Selection"},{"location":"exploringwebsite/sourcequery/#results-table","text":"Located directly below the form is the results table which is dynamically updated once the form is submitted. The full detail page of a specific source can be accessed by clicking on the source name in the table. Explanation of the table options can be found in the DataTables section . The Go to \u03b7-V analysis button will launch the \u03b7-V analysis page for the sources contained in the query results. Please refer to the Source \u03b7-V Analysis Page section for the full details of this feature. Note The Go to \u03b7-V analysis button is only active after a query has been performed.","title":"Results Table"},{"location":"exploringwebsite/sourcetagsfavs/","text":"Source Tags and Favourites \u00b6 Users are able to save a source as a favourite for later reference, in addition to adding tags to sources that can be used in source queries. Adding a Source to Favourites \u00b6 A source can be added to a user's favourites by: Selecting the 'star' button at the top of the source detail page as shown below. A modal window will open to confirm the saving of the source as a favourite. An optional comment can be entered. Select Add to Favourites and a confirmation alert will be shown to signify the source has been added successfully. Viewing Favourite Sources \u00b6 A user can access their favourite sources by selecting the Favourite Sources option from the menu when clicking on their username at the top right-hand corner of the page. The user will then be navigated to their favourite sources table as shown below. Adding a Tag \u00b6 Follow these steps to add a tag to a source: Type the tag to be added into the field tag field. If the tag has already been used it will appear in the dropdown text options and can be selected by clicking the text. To add a new tag, enter the complete text of the new tag and again click the text in the dropdown text. After clicking the text the tag will then show as a bordered tag in the input field. Finally, click the submit button (a comment is optional) and the tag will be saved as shown below. A comment will appear stating the addition of the tag. Removing a Tag \u00b6 Click the x on the tag to remove it and then click the Submit button to save the removal.","title":"Source Tags & Favourites"},{"location":"exploringwebsite/sourcetagsfavs/#source-tags-and-favourites","text":"Users are able to save a source as a favourite for later reference, in addition to adding tags to sources that can be used in source queries.","title":"Source Tags and Favourites"},{"location":"exploringwebsite/sourcetagsfavs/#adding-a-source-to-favourites","text":"A source can be added to a user's favourites by: Selecting the 'star' button at the top of the source detail page as shown below. A modal window will open to confirm the saving of the source as a favourite. An optional comment can be entered. Select Add to Favourites and a confirmation alert will be shown to signify the source has been added successfully.","title":"Adding a Source to Favourites"},{"location":"exploringwebsite/sourcetagsfavs/#viewing-favourite-sources","text":"A user can access their favourite sources by selecting the Favourite Sources option from the menu when clicking on their username at the top right-hand corner of the page. The user will then be navigated to their favourite sources table as shown below.","title":"Viewing Favourite Sources"},{"location":"exploringwebsite/sourcetagsfavs/#adding-a-tag","text":"Follow these steps to add a tag to a source: Type the tag to be added into the field tag field. If the tag has already been used it will appear in the dropdown text options and can be selected by clicking the text. To add a new tag, enter the complete text of the new tag and again click the text in the dropdown text. After clicking the text the tag will then show as a bordered tag in the input field. Finally, click the submit button (a comment is optional) and the tag will be saved as shown below. A comment will appear stating the addition of the tag.","title":"Adding a Tag"},{"location":"exploringwebsite/sourcetagsfavs/#removing-a-tag","text":"Click the x on the tag to remove it and then click the Submit button to save the removal.","title":"Removing a Tag"},{"location":"exploringwebsite/websiteoverview/","text":"Website Overview \u00b6 This page gives an overview of the pipeline website, with links to main pages on features where appropriate. Refer to Accessing the Pipeline for details on how to access the pipeline instance that is hosted by the VAST collaboration. For admins, refer to the following pages for details of the configuration and set up of the web server: Configuration , Deployment and Web App Admin Usage . Homepage \u00b6 The homepage provides a summary of the data currently held in the pipeline instance that has been processed. The four cards at the top of the homepage provide total values for the amount of pipeline runs, images, measurements and sources that are stored in the database. Clicking any of them will take you to the respective overview page for the data type. Note The totals presented on the homepage are totals for all pipeline runs combined! Also displayed is a sky region map that shows all the areas of the sky that have had successful and completed pipeline runs performed. Navbar \u00b6 The navbar, shown to the right, acts as the main method in which to navigate around the website. The following sections link to the respective documentation pages explainging the features of each link. Note The admin button on the navbar is only seen when the user is designated as an administrator. Tip The navbar can be collapsed by pressing the menu (or hamburger) button next to it at the top of the page. Admin \u00b6 See the website admin tools page. Allows for admins to manage users, Django Q schedules and the data itself. Pipeline Runs \u00b6 See the Pipeline Run Pages doc. Navigates the user to the list of pipeline runs available, which in turn link to the detail page for each respective run. Sources Query \u00b6 See the Source Query section. Takes the user to the source query page, where users can search for sources by defining a set of thresholds and feature requirements. From the results users can also access the detail page for individual sources. Measurements \u00b6 See the Measurement Pages section. Navigates the user to the measurements page that features a table containing all the measurements currently held in the database. From here users can also access the detail page for individual measurements. Images \u00b6 See the Image Pages section. Takes the user to the images page that features a table containing all the images currently held in the database. From here users can also access the detail page for individual images. External Links \u00b6 Documentation : Links to this documentation website. Pipeline Repository : A link to the GitHub pipeline repository. Raise an Issue : A link to open a new issue on the GitHub repository. Start a Discussion : A link to open a new discussion on the GitHub repository. VAST Links GitHub : A link to the VAST organisation GitHub page. JupyterHub : Links to the VAST hosted JupyterHub instance which includes access to the pipeline results and vast-tools . Website : Links to the VAST collaboration website. Wiki : Links to the VAST Wiki which is hosted on GitHub.","title":"Website Overview"},{"location":"exploringwebsite/websiteoverview/#website-overview","text":"This page gives an overview of the pipeline website, with links to main pages on features where appropriate. Refer to Accessing the Pipeline for details on how to access the pipeline instance that is hosted by the VAST collaboration. For admins, refer to the following pages for details of the configuration and set up of the web server: Configuration , Deployment and Web App Admin Usage .","title":"Website Overview"},{"location":"exploringwebsite/websiteoverview/#homepage","text":"The homepage provides a summary of the data currently held in the pipeline instance that has been processed. The four cards at the top of the homepage provide total values for the amount of pipeline runs, images, measurements and sources that are stored in the database. Clicking any of them will take you to the respective overview page for the data type. Note The totals presented on the homepage are totals for all pipeline runs combined! Also displayed is a sky region map that shows all the areas of the sky that have had successful and completed pipeline runs performed.","title":"Homepage"},{"location":"exploringwebsite/websiteoverview/#navbar","text":"The navbar, shown to the right, acts as the main method in which to navigate around the website. The following sections link to the respective documentation pages explainging the features of each link. Note The admin button on the navbar is only seen when the user is designated as an administrator. Tip The navbar can be collapsed by pressing the menu (or hamburger) button next to it at the top of the page.","title":"Navbar"},{"location":"exploringwebsite/websiteoverview/#admin","text":"See the website admin tools page. Allows for admins to manage users, Django Q schedules and the data itself.","title":"Admin"},{"location":"exploringwebsite/websiteoverview/#pipeline-runs","text":"See the Pipeline Run Pages doc. Navigates the user to the list of pipeline runs available, which in turn link to the detail page for each respective run.","title":"Pipeline Runs"},{"location":"exploringwebsite/websiteoverview/#sources-query","text":"See the Source Query section. Takes the user to the source query page, where users can search for sources by defining a set of thresholds and feature requirements. From the results users can also access the detail page for individual sources.","title":"Sources Query"},{"location":"exploringwebsite/websiteoverview/#measurements","text":"See the Measurement Pages section. Navigates the user to the measurements page that features a table containing all the measurements currently held in the database. From here users can also access the detail page for individual measurements.","title":"Measurements"},{"location":"exploringwebsite/websiteoverview/#images","text":"See the Image Pages section. Takes the user to the images page that features a table containing all the images currently held in the database. From here users can also access the detail page for individual images.","title":"Images"},{"location":"exploringwebsite/websiteoverview/#external-links","text":"Documentation : Links to this documentation website. Pipeline Repository : A link to the GitHub pipeline repository. Raise an Issue : A link to open a new issue on the GitHub repository. Start a Discussion : A link to open a new discussion on the GitHub repository. VAST Links GitHub : A link to the VAST organisation GitHub page. JupyterHub : Links to the VAST hosted JupyterHub instance which includes access to the pipeline results and vast-tools . Website : Links to the VAST collaboration website. Wiki : Links to the VAST Wiki which is hosted on GitHub.","title":"External Links"},{"location":"gettingstarted/configuration/","text":"Configuration \u00b6 This section describe how to configure your VAST Pipeline installation. Pipeline Configuration \u00b6 The following instructions, will get you started in setting up the database and pipeline configuration. Note The commands given in this section, unless otherwise stated, assume that the current directory is the pipeline root and that your pipeline Python environment has been activated. Create a database for the pipeline. If you followed the installation process, you will have a PostgreSQL Docker container running on your system. Use the provided script init-tools/init-db.py script to create a new database for the pipeline. As a security precaution, this script will also create a new database user and set the pipeline database owner to this new user. The initialization script requires several input parameters. For usage information, run with the --help option: python init-tools/init-db.py --help usage: init-db.py [-h] host port admin-username admin-password username password database-name Initialize a PostgreSQL database for VAST Pipeline use. Creates a new superuser and creates a new database owned by the new superuser. positional arguments: host database host port database port admin-username database administrator username admin-password database administrator password username username for the new user/role to create for the VAST Pipeline password password for the new user/role to create for the VAST Pipeline database-name name of the new database to create for the VAST Pipeline optional arguments: -h, --help show this help message and exit Fill in the parameters as appropriate for your configuration. If you followed the installation instructions, these would be the details for your PostgreSQL Docker container. Following from the same example in the installation section: python init-tools/init-db.py localhost 55002 postgres <password> vast <vast-user-password> vastdb Info Where <password> is the superuser password that was passed to docker run , and <vast-user-password> is a new password of your choice for the new vast database user. You may change the values for the username and database-name, the above is just an example. If everything went well the output should be: Creating new user/role vast ... Creating new database vastdb ... Done! Copy the setting configuration file template and modify it with your desired settings. Please refer to the .env File section on this page for further details about the settings that are set in this file along with their defaults. cp webinterface/.env.template webinterface/.env Set the database connection settings in the webinterface/.env file by modifying DATABASE_URL (for URL syntax see this link ). For example: .env DATABASE_URL=psql://vast:<vast-user-password>@localhost:55002/vastdb Note The connection details are the same that you setup during the installation . The database/user names must not contain any spaces or dashes, so use the underscore if you want, e.g. this_is_my_db_name . Create the pipeline database tables. The createcachetable command creates the cache tables required by DjangoQ. python manage.py migrate python manage.py createcachetable Create the pipeline data directories. The pipeline has several directories that can be configured in webinterface/.env : PIPELINE_WORKING_DIR : location to store various pipeline output files. RAW_IMAGE_DIR : default location that the pipeline will search for input images and catalogues to ingest during a pipeline run. Data inputs can also be defined as absolute paths in a pipeline run configuration file, so this setting only affects relative paths in the pipeline run configuration. HOME_DATA_DIR : a path relative to a user's home directory to search for additional input images and catalogues. Intended for multi-user server deployments and unlikely to be useful for local installations. See below for some examples. HOME_DATA_ROOT : path to the location of user home directories. Used together with HOME_DATA_DIR . If not supplied, the pipeline will search for the user's home directory using the default OS location. See below for some examples. .env \u2013 User data configuration examples In the following examples, assume that the user's name is foo . # HOME_DATA_ROOT = Uncomment to set a custom path to user data dirs HOME_DATA_DIR=vast-pipeline-extra-data Using the above settings, the pipeline will search for additional input data in the user's home directory as resolved by the OS. e.g. on an Ubuntu system, this would be /home/foo/vast-pipeline-extra-data . HOME_DATA_ROOT=/data/home HOME_DATA_DIR=vast-pipeline-extra-data Using the above settings, the pipeline will search for additional input data in /data/home/foo/vast-pipeline-extra-data . While the default values for these settings are relative to the pipeline codebase root (i.e. within the repo), we recommend creating these directories outside of the repo and updating the webinterface/.env file appropriately with absolute paths. For example, assuming you wish to create these directories in /data/vast-pipeline : mkdir -p /data/vast-pipeline mkdir /data/vast-pipeline/pipeline-runs mkdir /data/vast-pipeline/raw-images and update the webinterface/.env file with: .env PIPELINE_WORKING_DIR=/data/vast-pipeline/pipeline-runs RAW_IMAGE_DIR=/data/vast-pipeline/raw-images .env File \u00b6 The .env file contains various top-level settings that apply to Django, authentication and the running of the pipeline itself. Shown below is the .env.template file which is provided to be able to copy in step 3 above. .env.template # Django DEBUG=True SECRET_KEY=FillMeUPWithSomeComplicatedString # see https://django-environ.readthedocs.io/en/latest/#tips DATABASE_URL=psql://FILLMYUSER:FILLMYPASSWORD@FILLMYHOST:FILLMYPORT/FILLMYDBNAME # BASE_URL = this for append a base url in a production deployment STATIC_ROOT=./staticfiles/ STATIC_URL=/static/ # STATICFILES_DIRS = uncomment and fill to use # EXTRA_APPS = uncomment and fill to use # EXTRA_MIDDLEWARE = uncomment and fill to use ALLOWED_HOSTS=localhost # Github Authentication GITHUB_AUTH_TYPE='org' SOCIAL_AUTH_GITHUB_KEY=fillMeUp SOCIAL_AUTH_GITHUB_SECRET=fillMeUp SOCIAL_AUTH_GITHUB_ORG_NAME=fillMeUp SOCIAL_AUTH_GITHUB_ADMIN_TEAM=fillMeUp # External APIs # TNS_API_KEY = uncomment and fill to use # TNS_USER_AGENT = uncomment and fill to use # Pipeline PIPELINE_WORKING_DIR=pipeline-runs FLUX_DEFAULT_MIN_ERROR=0.001 POS_DEFAULT_MIN_ERROR=0.01 RAW_IMAGE_DIR=raw-images HOME_DATA_DIR=vast-pipeline-extra-data # HOME_DATA_ROOT = Uncomment to set a custom path to user data dirs # PIPELINE_MAINTAINANCE_MESSAGE = Uncomment and fill to show MAX_PIPELINE_RUNS=3 MAX_PIPERUN_IMAGES=200 # Q_CLUSTER_TIMEOUT = 86400 # Q_CLUSTER_RETRY = 86402 # Q_CLUSTER_MAX_ATTEMPTS = 1 ETA_V_DATASHADER_THRESHOLD=20000 The available settings are grouped into 4 distinct categories: Django \u00b6 These settings are standard Django settings that are commonly set in the settings.py file of Django projects. Please see this page in the Django documentation for explanations on their meaning. Multiple entries for settings such as EXTRA_APPS or EXTRA_MIDDLEWARE can be entered as comma-separated strings like the following example: .env EXTRA_APPS=django_extensions,debug_toolbar GitHub Authentication \u00b6 The settings in this section control the GitHub organization authentication method. Please refer to the Python Social Auth documentation for descriptions of the required settings. Note By default the pipeline is set up for authentication using GitHub organizations. Note that switching to teams will require changes to settings.py . Please refer to the instructions in the Python Social Auth documentation . External APIs \u00b6 The pipeline website interface supports querying some external APIs, e.g. SIMBAD, NED, VizieR, TNS. Some of these APIs require authentication which are described below. Transient Name Server (TNS) \u00b6 If you wish to enable TNS cone search results on the source detail page , you must first obtain an API key for TNS. Create a TNS account at https://www.wis-tns.org/user/register if you do not already have one. Once logged in, create a bot by navigating to https://www.wis-tns.org/bots and clicking \"Add bot\" near the top of the table. Fill in the create bot form. Ensure that you select \"Create new API key\". Securely store the API key and paste it into your pipeline webinterface/.env file under TNS_API_KEY . Warning Do not lose the API key! It is not possible to retrieve it again past this point. Navigate to your account page on TNS and copy the User-Agent specification. Paste it into your pipeline webinterface/.env file under TNS_USER_AGENT . webinterface/.env TNS_USER_AGENT='tns_marker{\"tns_id\": 0000, \"type\": \"user\", \"name\": \"your_username\"}' Pipeline \u00b6 These settings apply to various aspects of the VAST pipeline itself. The table below provides descriptions of each setting. Setting Default Value Description PIPELINE_WORKING_DIR pipeline-runs The name of the working directory where pipeline run directories are created. The pipeline location acts as the root directory. FLUX_DEFAULT_MIN_ERROR 0.001 In the event a measurement is ingested with a flux error of 0 from Selavy, the error is replaced with this default value (mJy). POS_DEFAULT_MIN_ERROR 0.01 In the event a measurement is ingested with an positional error of 0 from Selavy, the error is replaced with this default value (arcsec). RAW_IMAGE_DIR raw-images Directory where the majority of raw ASKAP FITS images are expected to be stored. This directory is scanned to provide user with an image list when configuration a job using the website interface. HOME_DATA_DIR vast-pipeline-extra-data Directory relative to the user's home directory that contains additional input images and catalogues. Safe to ignore if you don't intend to use this functionality. HOME_DATA_ROOT Disabled Path to directory that contains user's home directories. Enable by uncommenting and setting the desired path. If left disabled (commented), the pipeline will assume the OS default home directory location. PIPELINE_MAINTAINANCE_MESSAGE Disabled The message to display at the top of the webserver. See image below this table for an example. Comment out the setting to disable. MAX_PIPELINE_RUNS 3 The allowed maximum number of concurrent pipeline runs. MAX_PIPERUN_IMAGES 200 The allowed maximum number of images in a single pipeline run (non-admins). Q_CLUSTER_TIMEOUT 86400 Number of seconds a Django-Q cluster worker may spend on a task before it is terminated. See the Django-Q documentation . Q_CLUSTER_RETRY 86402 Number of seconds a Django-Q broker will wait for a cluster to finish a task before it's presented again. See the Django-Q documentation . Q_CLUSTER_MAX_ATTEMPTS 1 Number of times a failed task is retried. See the Django-Q documentation . ETA_V_DATASHADER_THRESHOLD 20000 The number of datapoints above which the eta-V plot uses datashader to plot the non-threshold distribution. Maintenance Message Example \u00b6 .env PIPELINE_MAINTAINANCE_MESSAGE=This website is subject to rapid changes which may result in data loss and may go offline with minimal warning. Please be mindful of usage. Authentication \u00b6 The pipeline supports two authentication methods: GitHub Organizations, intended to multi-user server deployments; and local Django administrator. For a single-user local installation, we recommend creating a Django superuser account. GitHub Organizations \u00b6 Please refer to the Python Social Auth documentation for a complete description on this authentication method and how to set up the GitHub app used for authentication. All settings are entered into the .env file as detailed in the above section . Django superuser \u00b6 Create a Django superuser account with the following command and follow the interactive prompts. python manage.py createsuperuser This account can be used to log into the Django admin panel once the webserver is running (see Starting the Pipeline Web App ) by navigating to https://localhost:8000/pipe-admin/ . Once logged in, you will land on the Django admin page. Navigate back to the pipeline homepage http://localhost:8000/ and you should be authenticated. Data Exploration via Django Web Server \u00b6 You can start the web app/server via the instructions provided in Starting the Pipeline Web App .","title":"Configuration"},{"location":"gettingstarted/configuration/#configuration","text":"This section describe how to configure your VAST Pipeline installation.","title":"Configuration"},{"location":"gettingstarted/configuration/#pipeline-configuration","text":"The following instructions, will get you started in setting up the database and pipeline configuration. Note The commands given in this section, unless otherwise stated, assume that the current directory is the pipeline root and that your pipeline Python environment has been activated. Create a database for the pipeline. If you followed the installation process, you will have a PostgreSQL Docker container running on your system. Use the provided script init-tools/init-db.py script to create a new database for the pipeline. As a security precaution, this script will also create a new database user and set the pipeline database owner to this new user. The initialization script requires several input parameters. For usage information, run with the --help option: python init-tools/init-db.py --help usage: init-db.py [-h] host port admin-username admin-password username password database-name Initialize a PostgreSQL database for VAST Pipeline use. Creates a new superuser and creates a new database owned by the new superuser. positional arguments: host database host port database port admin-username database administrator username admin-password database administrator password username username for the new user/role to create for the VAST Pipeline password password for the new user/role to create for the VAST Pipeline database-name name of the new database to create for the VAST Pipeline optional arguments: -h, --help show this help message and exit Fill in the parameters as appropriate for your configuration. If you followed the installation instructions, these would be the details for your PostgreSQL Docker container. Following from the same example in the installation section: python init-tools/init-db.py localhost 55002 postgres <password> vast <vast-user-password> vastdb Info Where <password> is the superuser password that was passed to docker run , and <vast-user-password> is a new password of your choice for the new vast database user. You may change the values for the username and database-name, the above is just an example. If everything went well the output should be: Creating new user/role vast ... Creating new database vastdb ... Done! Copy the setting configuration file template and modify it with your desired settings. Please refer to the .env File section on this page for further details about the settings that are set in this file along with their defaults. cp webinterface/.env.template webinterface/.env Set the database connection settings in the webinterface/.env file by modifying DATABASE_URL (for URL syntax see this link ). For example: .env DATABASE_URL=psql://vast:<vast-user-password>@localhost:55002/vastdb Note The connection details are the same that you setup during the installation . The database/user names must not contain any spaces or dashes, so use the underscore if you want, e.g. this_is_my_db_name . Create the pipeline database tables. The createcachetable command creates the cache tables required by DjangoQ. python manage.py migrate python manage.py createcachetable Create the pipeline data directories. The pipeline has several directories that can be configured in webinterface/.env : PIPELINE_WORKING_DIR : location to store various pipeline output files. RAW_IMAGE_DIR : default location that the pipeline will search for input images and catalogues to ingest during a pipeline run. Data inputs can also be defined as absolute paths in a pipeline run configuration file, so this setting only affects relative paths in the pipeline run configuration. HOME_DATA_DIR : a path relative to a user's home directory to search for additional input images and catalogues. Intended for multi-user server deployments and unlikely to be useful for local installations. See below for some examples. HOME_DATA_ROOT : path to the location of user home directories. Used together with HOME_DATA_DIR . If not supplied, the pipeline will search for the user's home directory using the default OS location. See below for some examples. .env \u2013 User data configuration examples In the following examples, assume that the user's name is foo . # HOME_DATA_ROOT = Uncomment to set a custom path to user data dirs HOME_DATA_DIR=vast-pipeline-extra-data Using the above settings, the pipeline will search for additional input data in the user's home directory as resolved by the OS. e.g. on an Ubuntu system, this would be /home/foo/vast-pipeline-extra-data . HOME_DATA_ROOT=/data/home HOME_DATA_DIR=vast-pipeline-extra-data Using the above settings, the pipeline will search for additional input data in /data/home/foo/vast-pipeline-extra-data . While the default values for these settings are relative to the pipeline codebase root (i.e. within the repo), we recommend creating these directories outside of the repo and updating the webinterface/.env file appropriately with absolute paths. For example, assuming you wish to create these directories in /data/vast-pipeline : mkdir -p /data/vast-pipeline mkdir /data/vast-pipeline/pipeline-runs mkdir /data/vast-pipeline/raw-images and update the webinterface/.env file with: .env PIPELINE_WORKING_DIR=/data/vast-pipeline/pipeline-runs RAW_IMAGE_DIR=/data/vast-pipeline/raw-images","title":"Pipeline Configuration"},{"location":"gettingstarted/configuration/#env-file","text":"The .env file contains various top-level settings that apply to Django, authentication and the running of the pipeline itself. Shown below is the .env.template file which is provided to be able to copy in step 3 above. .env.template # Django DEBUG=True SECRET_KEY=FillMeUPWithSomeComplicatedString # see https://django-environ.readthedocs.io/en/latest/#tips DATABASE_URL=psql://FILLMYUSER:FILLMYPASSWORD@FILLMYHOST:FILLMYPORT/FILLMYDBNAME # BASE_URL = this for append a base url in a production deployment STATIC_ROOT=./staticfiles/ STATIC_URL=/static/ # STATICFILES_DIRS = uncomment and fill to use # EXTRA_APPS = uncomment and fill to use # EXTRA_MIDDLEWARE = uncomment and fill to use ALLOWED_HOSTS=localhost # Github Authentication GITHUB_AUTH_TYPE='org' SOCIAL_AUTH_GITHUB_KEY=fillMeUp SOCIAL_AUTH_GITHUB_SECRET=fillMeUp SOCIAL_AUTH_GITHUB_ORG_NAME=fillMeUp SOCIAL_AUTH_GITHUB_ADMIN_TEAM=fillMeUp # External APIs # TNS_API_KEY = uncomment and fill to use # TNS_USER_AGENT = uncomment and fill to use # Pipeline PIPELINE_WORKING_DIR=pipeline-runs FLUX_DEFAULT_MIN_ERROR=0.001 POS_DEFAULT_MIN_ERROR=0.01 RAW_IMAGE_DIR=raw-images HOME_DATA_DIR=vast-pipeline-extra-data # HOME_DATA_ROOT = Uncomment to set a custom path to user data dirs # PIPELINE_MAINTAINANCE_MESSAGE = Uncomment and fill to show MAX_PIPELINE_RUNS=3 MAX_PIPERUN_IMAGES=200 # Q_CLUSTER_TIMEOUT = 86400 # Q_CLUSTER_RETRY = 86402 # Q_CLUSTER_MAX_ATTEMPTS = 1 ETA_V_DATASHADER_THRESHOLD=20000 The available settings are grouped into 4 distinct categories:","title":".env File"},{"location":"gettingstarted/configuration/#django","text":"These settings are standard Django settings that are commonly set in the settings.py file of Django projects. Please see this page in the Django documentation for explanations on their meaning. Multiple entries for settings such as EXTRA_APPS or EXTRA_MIDDLEWARE can be entered as comma-separated strings like the following example: .env EXTRA_APPS=django_extensions,debug_toolbar","title":"Django"},{"location":"gettingstarted/configuration/#github-authentication","text":"The settings in this section control the GitHub organization authentication method. Please refer to the Python Social Auth documentation for descriptions of the required settings. Note By default the pipeline is set up for authentication using GitHub organizations. Note that switching to teams will require changes to settings.py . Please refer to the instructions in the Python Social Auth documentation .","title":"GitHub Authentication"},{"location":"gettingstarted/configuration/#external-apis","text":"The pipeline website interface supports querying some external APIs, e.g. SIMBAD, NED, VizieR, TNS. Some of these APIs require authentication which are described below.","title":"External APIs"},{"location":"gettingstarted/configuration/#transient-name-server-tns","text":"If you wish to enable TNS cone search results on the source detail page , you must first obtain an API key for TNS. Create a TNS account at https://www.wis-tns.org/user/register if you do not already have one. Once logged in, create a bot by navigating to https://www.wis-tns.org/bots and clicking \"Add bot\" near the top of the table. Fill in the create bot form. Ensure that you select \"Create new API key\". Securely store the API key and paste it into your pipeline webinterface/.env file under TNS_API_KEY . Warning Do not lose the API key! It is not possible to retrieve it again past this point. Navigate to your account page on TNS and copy the User-Agent specification. Paste it into your pipeline webinterface/.env file under TNS_USER_AGENT . webinterface/.env TNS_USER_AGENT='tns_marker{\"tns_id\": 0000, \"type\": \"user\", \"name\": \"your_username\"}'","title":"Transient Name Server (TNS)"},{"location":"gettingstarted/configuration/#pipeline","text":"These settings apply to various aspects of the VAST pipeline itself. The table below provides descriptions of each setting. Setting Default Value Description PIPELINE_WORKING_DIR pipeline-runs The name of the working directory where pipeline run directories are created. The pipeline location acts as the root directory. FLUX_DEFAULT_MIN_ERROR 0.001 In the event a measurement is ingested with a flux error of 0 from Selavy, the error is replaced with this default value (mJy). POS_DEFAULT_MIN_ERROR 0.01 In the event a measurement is ingested with an positional error of 0 from Selavy, the error is replaced with this default value (arcsec). RAW_IMAGE_DIR raw-images Directory where the majority of raw ASKAP FITS images are expected to be stored. This directory is scanned to provide user with an image list when configuration a job using the website interface. HOME_DATA_DIR vast-pipeline-extra-data Directory relative to the user's home directory that contains additional input images and catalogues. Safe to ignore if you don't intend to use this functionality. HOME_DATA_ROOT Disabled Path to directory that contains user's home directories. Enable by uncommenting and setting the desired path. If left disabled (commented), the pipeline will assume the OS default home directory location. PIPELINE_MAINTAINANCE_MESSAGE Disabled The message to display at the top of the webserver. See image below this table for an example. Comment out the setting to disable. MAX_PIPELINE_RUNS 3 The allowed maximum number of concurrent pipeline runs. MAX_PIPERUN_IMAGES 200 The allowed maximum number of images in a single pipeline run (non-admins). Q_CLUSTER_TIMEOUT 86400 Number of seconds a Django-Q cluster worker may spend on a task before it is terminated. See the Django-Q documentation . Q_CLUSTER_RETRY 86402 Number of seconds a Django-Q broker will wait for a cluster to finish a task before it's presented again. See the Django-Q documentation . Q_CLUSTER_MAX_ATTEMPTS 1 Number of times a failed task is retried. See the Django-Q documentation . ETA_V_DATASHADER_THRESHOLD 20000 The number of datapoints above which the eta-V plot uses datashader to plot the non-threshold distribution.","title":"Pipeline"},{"location":"gettingstarted/configuration/#maintenance-message-example","text":".env PIPELINE_MAINTAINANCE_MESSAGE=This website is subject to rapid changes which may result in data loss and may go offline with minimal warning. Please be mindful of usage.","title":"Maintenance Message Example"},{"location":"gettingstarted/configuration/#authentication","text":"The pipeline supports two authentication methods: GitHub Organizations, intended to multi-user server deployments; and local Django administrator. For a single-user local installation, we recommend creating a Django superuser account.","title":"Authentication"},{"location":"gettingstarted/configuration/#github-organizations","text":"Please refer to the Python Social Auth documentation for a complete description on this authentication method and how to set up the GitHub app used for authentication. All settings are entered into the .env file as detailed in the above section .","title":"GitHub Organizations"},{"location":"gettingstarted/configuration/#django-superuser","text":"Create a Django superuser account with the following command and follow the interactive prompts. python manage.py createsuperuser This account can be used to log into the Django admin panel once the webserver is running (see Starting the Pipeline Web App ) by navigating to https://localhost:8000/pipe-admin/ . Once logged in, you will land on the Django admin page. Navigate back to the pipeline homepage http://localhost:8000/ and you should be authenticated.","title":"Django superuser"},{"location":"gettingstarted/configuration/#data-exploration-via-django-web-server","text":"You can start the web app/server via the instructions provided in Starting the Pipeline Web App .","title":"Data Exploration via Django Web Server"},{"location":"gettingstarted/deployment/","text":"Deployment \u00b6 Production System \u00b6 This section describes a simple deployment without using Docker containers, assuming the use of WhiteNoise to serve the static files. It is possible to serve the static files using other methods (e.g. Nginx). And in the future it is possible to upgrade the deployment stack using Docker container and Docker compose (we foresee 3 main containers: Django, Dask and Traefik/Nginx). We recommend in any case reading Django deployment documentation for general knowledge. Note We assume deployment to a UNIX server . The following steps describes how to set up the Django side of the production deployment, and can be of reference for a future Dockerization. They assumed you have SSH access to your remote server and have sudo priviledges. Web App Deployment \u00b6 Clone the repo in a suitable path, e.g. /opt/ . $ cd /opt && sudo git clone https://github.com/askap-vast/vast-pipeline Follow the Installation Instructions . We recommend installing the Python virtual environment under the pipeline folder. $ cd /opt/vast-pipeline && virtualenv -p python3 pipeline_env Configure your .env files with all the right settings. Check that your server is running fine by changing DEBUG = True in the .env file. Run Django deployment checklist command to see what are you missing. It is possible that some options are turned off, as implemented in the reverse proxy or load balancer of your server (e.g. SECURE_SSL_REDIRECT = False or not set, assumes your reverse proxy redirect HTTP to HTTPS). ( pipeline_env ) $ ./manage.py check --deploy Build up the static and fix url in JS9: ( pipeline_env ) $ cd /opt/vast-pipeline && npm ci && npm start \\ && npm run js9staticprod && ./manage.py collectstatic -c --noinput Set up a unit/systemd file as recommended in Gunicorn docs (feel free to use the socket or an IP and port). An example of command to write in the file is (assuming a virtual environment is installed in venv under the main pipeline folder): ExecStart = /opt/vast-pipeline/venv/bin/gunicorn -w 3 -k gevent \\ --worker-connections = 1000 --timeout 120 --limit-request-line 6500 \\ -b 127 .0.0.1:8000 webinterface.wsgi NOTE : (for future development) the --limit-request-line parameter needs to be adjusted for the actual request length as that might change if more parameters are added to the query. Finalise the installation of the unit file. Some good instructions on where to put, link and install the unit file are described in the Jupyter Hub docs Extra Service(s) Deployment \u00b6 Django Q \u00b6 In order to run a pipeline run from the Web App, the Django Q process needs to be started and managed as a service by the OS. In order to do so we recommend building a unit/systemd file to manage the Django Q process, in a similar way of the gunicorn process (following the Jupyter Hub docs ): ... WorkingDirectory = /opt/vast-pipeline ExecStart = /opt/vast-pipeline/venv/bin/python manage.py qcluster ... Tip In the examples above, the Python virtual enviroment used by the pipeline is installed in the venv folder under the cloned repository. Security \u00b6 By default the settings file has some security parameters that are set when you run the web app in production ( DEBUG = False ), but you can read more in the Django documentation or in this blog post in which they explain how to get an A+ rating for your web site.","title":"Deployment"},{"location":"gettingstarted/deployment/#deployment","text":"","title":"Deployment"},{"location":"gettingstarted/deployment/#production-system","text":"This section describes a simple deployment without using Docker containers, assuming the use of WhiteNoise to serve the static files. It is possible to serve the static files using other methods (e.g. Nginx). And in the future it is possible to upgrade the deployment stack using Docker container and Docker compose (we foresee 3 main containers: Django, Dask and Traefik/Nginx). We recommend in any case reading Django deployment documentation for general knowledge. Note We assume deployment to a UNIX server . The following steps describes how to set up the Django side of the production deployment, and can be of reference for a future Dockerization. They assumed you have SSH access to your remote server and have sudo priviledges.","title":"Production System"},{"location":"gettingstarted/deployment/#web-app-deployment","text":"Clone the repo in a suitable path, e.g. /opt/ . $ cd /opt && sudo git clone https://github.com/askap-vast/vast-pipeline Follow the Installation Instructions . We recommend installing the Python virtual environment under the pipeline folder. $ cd /opt/vast-pipeline && virtualenv -p python3 pipeline_env Configure your .env files with all the right settings. Check that your server is running fine by changing DEBUG = True in the .env file. Run Django deployment checklist command to see what are you missing. It is possible that some options are turned off, as implemented in the reverse proxy or load balancer of your server (e.g. SECURE_SSL_REDIRECT = False or not set, assumes your reverse proxy redirect HTTP to HTTPS). ( pipeline_env ) $ ./manage.py check --deploy Build up the static and fix url in JS9: ( pipeline_env ) $ cd /opt/vast-pipeline && npm ci && npm start \\ && npm run js9staticprod && ./manage.py collectstatic -c --noinput Set up a unit/systemd file as recommended in Gunicorn docs (feel free to use the socket or an IP and port). An example of command to write in the file is (assuming a virtual environment is installed in venv under the main pipeline folder): ExecStart = /opt/vast-pipeline/venv/bin/gunicorn -w 3 -k gevent \\ --worker-connections = 1000 --timeout 120 --limit-request-line 6500 \\ -b 127 .0.0.1:8000 webinterface.wsgi NOTE : (for future development) the --limit-request-line parameter needs to be adjusted for the actual request length as that might change if more parameters are added to the query. Finalise the installation of the unit file. Some good instructions on where to put, link and install the unit file are described in the Jupyter Hub docs","title":"Web App Deployment"},{"location":"gettingstarted/deployment/#extra-services-deployment","text":"","title":"Extra Service(s) Deployment"},{"location":"gettingstarted/deployment/#django-q","text":"In order to run a pipeline run from the Web App, the Django Q process needs to be started and managed as a service by the OS. In order to do so we recommend building a unit/systemd file to manage the Django Q process, in a similar way of the gunicorn process (following the Jupyter Hub docs ): ... WorkingDirectory = /opt/vast-pipeline ExecStart = /opt/vast-pipeline/venv/bin/python manage.py qcluster ... Tip In the examples above, the Python virtual enviroment used by the pipeline is installed in the venv folder under the cloned repository.","title":"Django Q"},{"location":"gettingstarted/deployment/#security","text":"By default the settings file has some security parameters that are set when you run the web app in production ( DEBUG = False ), but you can read more in the Django documentation or in this blog post in which they explain how to get an A+ rating for your web site.","title":"Security"},{"location":"gettingstarted/installation/","text":"Installation \u00b6 This document provides instructions on installing the VAST Pipeline for local use. The VAST Pipeline consists of 3 main components that require installation: a PostgreSQL database, a Django application, a front-end website. The instructions have been tested on Debian/Ubuntu and macOS. PostgreSQL \u00b6 We recommend using a Docker container for the database rather than installing the database system-wide. Steps: Install Docker. Refer to the official documentation , and for Ubuntu users to this . Remember to add your user account to the docker group official docs , by running: sudo groupadd docker sudo usermod -aG docker $USER Create a PostgreSQL container. The VAST Pipeline requires a PostgreSQL database with the Q3C plugin to enable special indexing on coordinates and fast cone-search queries. We have prepared a Docker image based on the latest PostgreSQL image that includes Q3C . Start a container using this image by running the command below, replacing <container-name> with a name of your choice (e.g. vast-pipeline-db) and <password> with a password of your choice which will be set for the default postgres database superuser account. docker run --name <container-name> --env POSTGRES_PASSWORD=<password> --publish-all --detach ghcr.io/marxide/postgres-q3c:latest The --publish-all option will make the PostgreSQL server port 5432 in the container accessible on a random available port on your system (the host). The --detach option instructs Docker to start the container in the background rather than taking over your current shell. Verify that the container is running and note the host port that 5432/tcp is published on by running docker ps , e.g. in the example below, the host port is 55002 . docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8ff553add2ed ghcr.io/marxide/postgres-q3c:latest \"docker-entrypoint.s\u2026\" 4 seconds ago Up 3 seconds 0.0.0.0:55002->5432/tcp vast-pipeline-db The database server should now be running in a container on your machine. Tip To stop the database server, simply stop the container with the following command docker stop <container-name or container-id> You can start an existing stopped container with the following command docker start <container-name or container-id> Note that docker run and docker start are not the same. docker run will create and start a container from an image; docker start will start an existing stopped container. If you have previously created a VAST Pipeline database container and you wish to reuse it, you want to use docker start . You will likely need to restart the container after a system reboot. Python Environment \u00b6 We strongly recommend installing the VAST Pipeline in an isolated virtual environment (e.g. using Miniconda , Virtualenv , or venv ). This will keep the rather complex set of dependencies separated from the system-wide Python installation. Create a new Python environment using your chosen virtual environment manager and activate it. For example, Miniconda users should run the following command, replacing <environment-name> with an appropriate name (e.g. pipeline-env): conda create --name <environment-name> python=3.8 conda activate <environment-name> Note All further installation instructions will assume you have activated your new virtual environment. Your environment manager will usually prepend the virtual environment name to the shell prompt, e.g. (pipeline-env) $ ... Clone the pipeline repository https://github.com/askap-vast/vast-pipeline and change into the repo directory. git clone https://github.com/askap-vast/vast-pipeline.git cd vast-pipeline Warning Do not change the the repo folder name, e.g. git clone https://github.com/askap-vast/vast-pipeline.git my-pipeline-local-dev (Optional) Checkout the version you want to install. Currently, the repo will have cloned the latest code from the master branch. If you require a specific version, checkout the appropriate version tag into a new branch e.g. for version 0.2.0 git checkout -b <new-branch-name> 0.2.0 Install non-Python dependencies. Some of the Python dependencies required by the pipeline depend on some non-Python libraries. These can also be installed by Miniconda, otherwise they are best installed using an appropriate package manager for your operating system e.g. apt for Debian/Ubuntu, dnf for RHEL 8/CentOS 8, Homebrew for macOS. The dependencies are: Miniconda libpq graphviz Both are available on the conda-forge channel. They are also specified in the environment file requirements/environment.yml which can be used to install the required packages into an activated conda environment with the following command conda env update -f requirements/environment.yml Debian/Ubuntu libpq-dev libgraphviz-dev RHEL/CentOS libpq-devel graphviz-devel CentOS users You may need to enable the PowerTools repository to install graphviz-devel . dnf install dnf-plugins-core dnf config-manager --set-enabled powertools Homebrew libpq graphviz Install the pipeline and it's Python dependencies. pip install . Warning Don't forget the . at the end of the above command. It instructs pip that the root directory of the package to install is the current directory. Tip If you are intending to deploy an instance of the pipeline onto a server, you may also want to install the recommended production extras with pip install .[prod] . However, note that these are recommendations only and there are other alternative packages that may work just as well. Tip If you intend to contribute to development of the pipeline, you will need the Python dependency management tool Poetry . See the development guidelines . Front-End Assets Quickstart \u00b6 In order to install and compile the front-end website assets (modules like js9 and bootstrap, as well as minification of JS and CSS files) you need a recent version of NodeJS installed. Installation of NodeJS \u00b6 If you are using Miniconda and installed the requirements/environment.yml file as shown above, then NodeJS is already installed. Otherwise, we recommend following the instructions on the NodeJS downloads page for your OS (there are many installation options). Setting up the front-end assets \u00b6 In order to set up the front end assets, run: npm ci && npm start Note Ensure you are still in the root of the repo before running the command above. The npm ci command (\"clean install\") will remove all previous node modules and install all the dependencies from scratch. The npm start command will run the default gulp \"task\" which, among other things, compiles Sass into CSS, minifies CSS and JS files, and copies these files into the static/vendor folder. For more details of compilation of frontend assets (e.g. single tasks), and front-end developement set up read the Front End Developing Guidelines . Bug When npm start or npm run start was run in a Ubuntu 20.04 LTS (containerised environment), for some unknown reasons, both commands failed with the following error. [12:48:19] 'js9Make' errored after 7.67 ms [12:48:19] Error: spawn make ENOENT at Process.ChildProcess._handle.onexit (internal/child_process.js:267:19) at onErrorNT (internal/child_process.js:469:16) at processTicksAndRejections (internal/process/task_queues.js:84:21) [12:48:19] 'default' errored after 2.63 s npm ERR! code ELIFECYCLE npm ERR! errno 1 npm ERR! vast-pipeline@99.99.99-dev start: `gulp default` npm ERR! Exit status 1 npm ERR! npm ERR! Failed at the vast-pipeline@99.99.99-dev start script. npm ERR! This is probably not a problem with npm. There is likely additional logging output above. npm ERR! A complete log of this run can be found in: npm ERR! /home/vast/.npm/_logs/2020-10-06T01_48_19_215Z-debug.log The way around for this issue is unorthodox. The following steps were followed to overcome the issue: cd node_modules/js9/ ./configure make make install cd ~/vast-pipeline/ ## (to comeback to the root folder of the project) npm install That somehow solved the issue mentioned above. Done! Now go to Vast Pipeline Configuration file to see how to initialize and run the pipeline. Otherwise if you intend on developing the repo open the Contributing and Developing Guidelines file for instructions on how to contribute to the repo.","title":"Installation"},{"location":"gettingstarted/installation/#installation","text":"This document provides instructions on installing the VAST Pipeline for local use. The VAST Pipeline consists of 3 main components that require installation: a PostgreSQL database, a Django application, a front-end website. The instructions have been tested on Debian/Ubuntu and macOS.","title":"Installation"},{"location":"gettingstarted/installation/#postgresql","text":"We recommend using a Docker container for the database rather than installing the database system-wide. Steps: Install Docker. Refer to the official documentation , and for Ubuntu users to this . Remember to add your user account to the docker group official docs , by running: sudo groupadd docker sudo usermod -aG docker $USER Create a PostgreSQL container. The VAST Pipeline requires a PostgreSQL database with the Q3C plugin to enable special indexing on coordinates and fast cone-search queries. We have prepared a Docker image based on the latest PostgreSQL image that includes Q3C . Start a container using this image by running the command below, replacing <container-name> with a name of your choice (e.g. vast-pipeline-db) and <password> with a password of your choice which will be set for the default postgres database superuser account. docker run --name <container-name> --env POSTGRES_PASSWORD=<password> --publish-all --detach ghcr.io/marxide/postgres-q3c:latest The --publish-all option will make the PostgreSQL server port 5432 in the container accessible on a random available port on your system (the host). The --detach option instructs Docker to start the container in the background rather than taking over your current shell. Verify that the container is running and note the host port that 5432/tcp is published on by running docker ps , e.g. in the example below, the host port is 55002 . docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8ff553add2ed ghcr.io/marxide/postgres-q3c:latest \"docker-entrypoint.s\u2026\" 4 seconds ago Up 3 seconds 0.0.0.0:55002->5432/tcp vast-pipeline-db The database server should now be running in a container on your machine. Tip To stop the database server, simply stop the container with the following command docker stop <container-name or container-id> You can start an existing stopped container with the following command docker start <container-name or container-id> Note that docker run and docker start are not the same. docker run will create and start a container from an image; docker start will start an existing stopped container. If you have previously created a VAST Pipeline database container and you wish to reuse it, you want to use docker start . You will likely need to restart the container after a system reboot.","title":"PostgreSQL"},{"location":"gettingstarted/installation/#python-environment","text":"We strongly recommend installing the VAST Pipeline in an isolated virtual environment (e.g. using Miniconda , Virtualenv , or venv ). This will keep the rather complex set of dependencies separated from the system-wide Python installation. Create a new Python environment using your chosen virtual environment manager and activate it. For example, Miniconda users should run the following command, replacing <environment-name> with an appropriate name (e.g. pipeline-env): conda create --name <environment-name> python=3.8 conda activate <environment-name> Note All further installation instructions will assume you have activated your new virtual environment. Your environment manager will usually prepend the virtual environment name to the shell prompt, e.g. (pipeline-env) $ ... Clone the pipeline repository https://github.com/askap-vast/vast-pipeline and change into the repo directory. git clone https://github.com/askap-vast/vast-pipeline.git cd vast-pipeline Warning Do not change the the repo folder name, e.g. git clone https://github.com/askap-vast/vast-pipeline.git my-pipeline-local-dev (Optional) Checkout the version you want to install. Currently, the repo will have cloned the latest code from the master branch. If you require a specific version, checkout the appropriate version tag into a new branch e.g. for version 0.2.0 git checkout -b <new-branch-name> 0.2.0 Install non-Python dependencies. Some of the Python dependencies required by the pipeline depend on some non-Python libraries. These can also be installed by Miniconda, otherwise they are best installed using an appropriate package manager for your operating system e.g. apt for Debian/Ubuntu, dnf for RHEL 8/CentOS 8, Homebrew for macOS. The dependencies are: Miniconda libpq graphviz Both are available on the conda-forge channel. They are also specified in the environment file requirements/environment.yml which can be used to install the required packages into an activated conda environment with the following command conda env update -f requirements/environment.yml Debian/Ubuntu libpq-dev libgraphviz-dev RHEL/CentOS libpq-devel graphviz-devel CentOS users You may need to enable the PowerTools repository to install graphviz-devel . dnf install dnf-plugins-core dnf config-manager --set-enabled powertools Homebrew libpq graphviz Install the pipeline and it's Python dependencies. pip install . Warning Don't forget the . at the end of the above command. It instructs pip that the root directory of the package to install is the current directory. Tip If you are intending to deploy an instance of the pipeline onto a server, you may also want to install the recommended production extras with pip install .[prod] . However, note that these are recommendations only and there are other alternative packages that may work just as well. Tip If you intend to contribute to development of the pipeline, you will need the Python dependency management tool Poetry . See the development guidelines .","title":"Python Environment"},{"location":"gettingstarted/installation/#front-end-assets-quickstart","text":"In order to install and compile the front-end website assets (modules like js9 and bootstrap, as well as minification of JS and CSS files) you need a recent version of NodeJS installed.","title":"Front-End Assets Quickstart"},{"location":"gettingstarted/installation/#installation-of-nodejs","text":"If you are using Miniconda and installed the requirements/environment.yml file as shown above, then NodeJS is already installed. Otherwise, we recommend following the instructions on the NodeJS downloads page for your OS (there are many installation options).","title":"Installation of NodeJS"},{"location":"gettingstarted/installation/#setting-up-the-front-end-assets","text":"In order to set up the front end assets, run: npm ci && npm start Note Ensure you are still in the root of the repo before running the command above. The npm ci command (\"clean install\") will remove all previous node modules and install all the dependencies from scratch. The npm start command will run the default gulp \"task\" which, among other things, compiles Sass into CSS, minifies CSS and JS files, and copies these files into the static/vendor folder. For more details of compilation of frontend assets (e.g. single tasks), and front-end developement set up read the Front End Developing Guidelines . Bug When npm start or npm run start was run in a Ubuntu 20.04 LTS (containerised environment), for some unknown reasons, both commands failed with the following error. [12:48:19] 'js9Make' errored after 7.67 ms [12:48:19] Error: spawn make ENOENT at Process.ChildProcess._handle.onexit (internal/child_process.js:267:19) at onErrorNT (internal/child_process.js:469:16) at processTicksAndRejections (internal/process/task_queues.js:84:21) [12:48:19] 'default' errored after 2.63 s npm ERR! code ELIFECYCLE npm ERR! errno 1 npm ERR! vast-pipeline@99.99.99-dev start: `gulp default` npm ERR! Exit status 1 npm ERR! npm ERR! Failed at the vast-pipeline@99.99.99-dev start script. npm ERR! This is probably not a problem with npm. There is likely additional logging output above. npm ERR! A complete log of this run can be found in: npm ERR! /home/vast/.npm/_logs/2020-10-06T01_48_19_215Z-debug.log The way around for this issue is unorthodox. The following steps were followed to overcome the issue: cd node_modules/js9/ ./configure make make install cd ~/vast-pipeline/ ## (to comeback to the root folder of the project) npm install That somehow solved the issue mentioned above. Done! Now go to Vast Pipeline Configuration file to see how to initialize and run the pipeline. Otherwise if you intend on developing the repo open the Contributing and Developing Guidelines file for instructions on how to contribute to the repo.","title":"Setting up the front-end assets"},{"location":"outputs/coldesc/","text":"Column Descriptions \u00b6 This page details the columns contained in each output file. associations \u00b6 Column Unit Description source_id n/a The database id of the source for the association. meas_id n/a The database id of the measurement for the association. d2d arcsec The on-sky separation of the measurement to the source at the iteration stage the association was created. dr n/a The de Ruiter radius of the measurement to the source at the iteration stage the association was created. Will be 0 if de Ruiter assocation is not being used. bands \u00b6 Column Unit Description id n/a The database id of the band. name n/a The string name of the band, equal to the frequency value. frequency MHz The band central frequency. bandwidth MHz The bandwidth of the frequency band, will be 0 if not known. images \u00b6 Column Unit Description id n/a The database id of the image. band_id n/a The database id of the associated band. skyreg_id n/a The database id of the associated sky region. measurements_path n/a The system path to the measurements parquet file. polarisation n/a The polarisation of the image. name n/a The name of the image, taken from the filename. path n/a The system path to the image FITS file. noise_path n/a The system path to the associated noise image FITS file. background_path n/a The system path to the associated background image FITS file. datetime n/a The date and time of the observation, read from the FITS header. jd days The date and time of the observation in Julian Days. duration s The duration of the observation taken from the FITS header, if available. ra deg The central Right Ascension coordinate of the image. dec deg The central Declination coordinate of the image. fov_bmaj deg The estimated major axis field-of-view value - the radius_pixels multipled by the major axis pixel size. fov_bmin deg The estimated minor axis field-of-view value - the radius_pixels multipled by the minor axis pixel size. physical_bmaj deg The actual major axis on-sky size - the number of pixels on the major axis multiplied by the major axis pixel size. physical_bmin deg The actual minor axis on-sky size - the number of pixels on the minor axis multiplied by the minor axis pixel size. radius_pixels pixels Estimated 'diameter' of the useable image area. beam_bmaj deg The size of the major axis of the image restoring beam. beam_bmin deg The size of the minor axis of the image restoring beam. beam_bpa deg The position angle of the image restoring beam. rms_median mJy/beam The median RMS value derrived from the RMS map. rms_min mJy/beam The minimum RMS value derrived from the RMS map (pixel value). rms_max mJy/beam The maximum RMS value derrived from the RMS map (pixel value). measurements \u00b6 Tip Some columns are the same as that defined in the Selavy source finder output . Column Unit Description island_id n/a The Selavy assigned island_id. component_id n/a The Selavy assigned component_id. local_rms mJy The rms value at the location of the measurement. ra deg The right ascension coordinate of the measurement. ra_err deg The error of the right ascension coordinate of the measurement. dec deg The declination coordinate of the measurement. dec_err deg The error of the declination coordinate of the measurement. flux_peak mJy/beam The measured peak flux of the component. flux_peak_err mJy/beam The error of the measured peak flux of the component. flux_int mJy The measured integrated flux of the component. flux_int_err mJy The error of the measured integrated flux of the component. bmaj arcsec The major axis size of the fitted Gaussian (FWHM). err_bmaj deg The error of the major axis size of the fitted Gaussian (FWHM). bmin arcsec The minor axis size of the fitted Gaussian (FWHM). err_bmin deg The error of the minor axis size of the fitted Gaussian (FWHM). pa deg The position angle of the fitted Gaussian (FWHM). err_pa deg The error of the position angle of the fitted Gaussian (FWHM). psf_bmaj arcsec The Selavy deconvolved size of the major axis of the fitted Gaussian. psf_bmin arcsec The Selavy deconvolved size of the minor axis of the fitted Gaussian. psf_pa deg The Selavy deconvolved position angle of the fitted Gaussian. flag_c4 n/a Selavy flag denoting whether the component is considered formally bad (doesn't meet chi-squared criterion). chi_squared_fit n/a The Selavy quality of the fit. spectral_index n/a The fitted Selavy spectral index of the component. spectral_index_from_TT n/a Selavy flag to denote if the spectral index has been derived from the Taylor-term images ( True ). has_siblings n/a Selavy flag to denote whether the component is one of many fitted to the same island. image_id n/a The database id of the image the measurement is from. time n/a The date and time of observation the measurement is from (obtained from the image). name n/a The string name of the measurement. snr n/a The signal-to-noise ratio of the measurement. compactness n/a The compactness of the measurement ( flux_int / flux_peak ). ew_sys_err deg The systematic right ascension error assigned to the measurement. ns_sys_err deg The systematic declination error assigned to the measurement. error_radius deg The pipeline estimated error radius of the measurement. uncertainty_ew deg Total RA positional error of the measurement. uncertainty_ns deg Total Dec positional error of the measurement. weight_ew deg \\(^{-1}\\) The weight of the RA error (1/e). weight_ns deg \\(^{-1}\\) The weight of the Dec error (1/e). forced n/a Flag to denote whether the measurement is produced from the forced fitting procedure ( True ). flux_int_isl_ratio n/a The ratio of the measurements integrated flux to the total island integrated flux. flux_peak_isl_ratio n/a The ratio of the measurements peak flux to the total island peak flux. id n/a The database id of the measurement. measurement_pairs \u00b6 Column Unit Description meas_id_a n/a The database id of measurement a of the pair. meas_id_b n/a The database id of measurement b of the pair. flux_int_a mJy The integrated flux of measurement a of the pair. flux_int_err_a mJy The error of the integrated flux of measurement a of the pair. flux_peak_a mJy/beam The peak flux of measurement a of the pair. flux_peak_err_a mJy/beam The error of the peak flux of measurement a of the pair. image_name_a n/a The image name of measurement a of the pair. flux_int_b mJy The integrated flux of measurement b of the pair. flux_int_err_b mJy The error of the integrated flux of measurement b of the pair. flux_peak_b mJy/beam The peak flux of measurement b of the pair. flux_peak_err_b mJy/beam The error of the peak flux of measurement b of the pair. image_name_b n/a The image name of measurement b of the pair. vs_peak n/a The pair \\(V_s\\) value using the peak flux. vs_int n/a The pair \\(V_s\\) value using the integrated flux. m_peak n/a The pair \\(m\\) value using the peak flux. m_int n/a The pair \\(m\\) value using the integrated flux. source_id n/a The database id of the source the pair is associated to. relations \u00b6 Column Unit Description from_source_id n/a The database id of the first source in the relation pair. to_source_id n/a The database id of the second source in the relation pair. skyregions \u00b6 Column Unit Description id n/a The database id of the sky region. centre_ra deg The right ascension value of the sky region central coordinate. centre_dec deg The declination value of the sky region central coordinate. width_ra deg The width of the area covered by the sky region. width_dec deg The height of the area covered by the sky region. xtr_radius deg The hypotenuse radius of the sky region. x rad The central cartesian x coordinate of the sky region. y rad The central cartesian y coordinate of the sky region. z rad The central cartesian z coordinate of the sky region. sources \u00b6 Note The index column of the sources parquet is set to the database id of the source. Column Unit Description n_meas n/a The total number of measurements associated to the source (selavy and forced). n_meas_sel n/a The total number of selavy measurements associated to the source. n_meas_forced n/a The total number of forced measurements associated to the source. n_sibl n/a The total number of measurements that have a has_sibling value of True . n_rel n/a The total number of relations the source has. wavg_ra deg The weighted average of the Right Ascension of the measurements, that acts as the source position. wavg_dec deg The weighted average of the Declination of the measurements, that acts as the source position. wavg_uncertainty_ew deg The error of the weighted average right ascension value. wavg_uncertainty_ns deg The error of the weighted average declination value. new n/a Flag to signify the source is classed as a new source ( True ). new_high_sigma n/a The largest sigma value a new source would have if it was placed at its location in the previous images it was not detected in. See New Sources for more information. Set to 0 for non-new sources. n_neighbour_dist deg The on-sky separation to the nearest source with in the same pipeline run. avg_compactness n/a The average compactness value of the associated measurements. min_snr n/a The minimum signal-to-noise ratio of the associated measurements. max_snr n/a The maximum signal-to-noise ratio of the associated measurements. avg_flux_int mJy The average integrated flux value of the measurements associated to the source (inc. forced measurements). max_flux_int mJy The maximum integrated flux value of the measurements associated to the source (inc. forced measurements). min_flux_int mJy The minimum integrated flux value of the measurements associated to the source (inc. forced measurements). avg_flux_peak mJy/beam The average peak flux value of the measurements associated to the source (inc. forced measurements). max_flux_peak mJy/beam The maximum peak flux value of the measurements associated to the source (inc. forced measurements). min_flux_peak mJy/beam The minimum peak flux value of the measurements associated to the source (inc. forced measurements). min_flux_peak_isl_ratio n/a The minimum ratio of the peak flux to the total island peak flux of the measurements associated to the source. min_flux_int_isl_ratio n/a The minimum ratio of the integrated flux to the total island integrated flux of the measurements associated to the source. v_int n/a The calculated variability \\(V\\) metric using the integrated flux values. See variability statistics . v_peak n/a The calculated variability \\(V\\) metric using the peak flux values. See variability statistics . eta_int n/a The calculated variability \\(\\eta\\) metric using the integrated flux. See variability statistics . eta_peak n/a The calculated variability \\(\\eta\\) metric using the peak flux. See variability statistics . vs_abs_significant_max_peak n/a The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. See variability statistics . m_abs_significant_max_peak n/a The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. See variability statistics . vs_abs_significant_max_int n/a The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. See variability statistics . m_abs_significant_max_int n/a The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. See variability statistics .","title":"Column Descriptions"},{"location":"outputs/coldesc/#column-descriptions","text":"This page details the columns contained in each output file.","title":"Column Descriptions"},{"location":"outputs/coldesc/#associations","text":"Column Unit Description source_id n/a The database id of the source for the association. meas_id n/a The database id of the measurement for the association. d2d arcsec The on-sky separation of the measurement to the source at the iteration stage the association was created. dr n/a The de Ruiter radius of the measurement to the source at the iteration stage the association was created. Will be 0 if de Ruiter assocation is not being used.","title":"associations"},{"location":"outputs/coldesc/#bands","text":"Column Unit Description id n/a The database id of the band. name n/a The string name of the band, equal to the frequency value. frequency MHz The band central frequency. bandwidth MHz The bandwidth of the frequency band, will be 0 if not known.","title":"bands"},{"location":"outputs/coldesc/#images","text":"Column Unit Description id n/a The database id of the image. band_id n/a The database id of the associated band. skyreg_id n/a The database id of the associated sky region. measurements_path n/a The system path to the measurements parquet file. polarisation n/a The polarisation of the image. name n/a The name of the image, taken from the filename. path n/a The system path to the image FITS file. noise_path n/a The system path to the associated noise image FITS file. background_path n/a The system path to the associated background image FITS file. datetime n/a The date and time of the observation, read from the FITS header. jd days The date and time of the observation in Julian Days. duration s The duration of the observation taken from the FITS header, if available. ra deg The central Right Ascension coordinate of the image. dec deg The central Declination coordinate of the image. fov_bmaj deg The estimated major axis field-of-view value - the radius_pixels multipled by the major axis pixel size. fov_bmin deg The estimated minor axis field-of-view value - the radius_pixels multipled by the minor axis pixel size. physical_bmaj deg The actual major axis on-sky size - the number of pixels on the major axis multiplied by the major axis pixel size. physical_bmin deg The actual minor axis on-sky size - the number of pixels on the minor axis multiplied by the minor axis pixel size. radius_pixels pixels Estimated 'diameter' of the useable image area. beam_bmaj deg The size of the major axis of the image restoring beam. beam_bmin deg The size of the minor axis of the image restoring beam. beam_bpa deg The position angle of the image restoring beam. rms_median mJy/beam The median RMS value derrived from the RMS map. rms_min mJy/beam The minimum RMS value derrived from the RMS map (pixel value). rms_max mJy/beam The maximum RMS value derrived from the RMS map (pixel value).","title":"images"},{"location":"outputs/coldesc/#measurements","text":"Tip Some columns are the same as that defined in the Selavy source finder output . Column Unit Description island_id n/a The Selavy assigned island_id. component_id n/a The Selavy assigned component_id. local_rms mJy The rms value at the location of the measurement. ra deg The right ascension coordinate of the measurement. ra_err deg The error of the right ascension coordinate of the measurement. dec deg The declination coordinate of the measurement. dec_err deg The error of the declination coordinate of the measurement. flux_peak mJy/beam The measured peak flux of the component. flux_peak_err mJy/beam The error of the measured peak flux of the component. flux_int mJy The measured integrated flux of the component. flux_int_err mJy The error of the measured integrated flux of the component. bmaj arcsec The major axis size of the fitted Gaussian (FWHM). err_bmaj deg The error of the major axis size of the fitted Gaussian (FWHM). bmin arcsec The minor axis size of the fitted Gaussian (FWHM). err_bmin deg The error of the minor axis size of the fitted Gaussian (FWHM). pa deg The position angle of the fitted Gaussian (FWHM). err_pa deg The error of the position angle of the fitted Gaussian (FWHM). psf_bmaj arcsec The Selavy deconvolved size of the major axis of the fitted Gaussian. psf_bmin arcsec The Selavy deconvolved size of the minor axis of the fitted Gaussian. psf_pa deg The Selavy deconvolved position angle of the fitted Gaussian. flag_c4 n/a Selavy flag denoting whether the component is considered formally bad (doesn't meet chi-squared criterion). chi_squared_fit n/a The Selavy quality of the fit. spectral_index n/a The fitted Selavy spectral index of the component. spectral_index_from_TT n/a Selavy flag to denote if the spectral index has been derived from the Taylor-term images ( True ). has_siblings n/a Selavy flag to denote whether the component is one of many fitted to the same island. image_id n/a The database id of the image the measurement is from. time n/a The date and time of observation the measurement is from (obtained from the image). name n/a The string name of the measurement. snr n/a The signal-to-noise ratio of the measurement. compactness n/a The compactness of the measurement ( flux_int / flux_peak ). ew_sys_err deg The systematic right ascension error assigned to the measurement. ns_sys_err deg The systematic declination error assigned to the measurement. error_radius deg The pipeline estimated error radius of the measurement. uncertainty_ew deg Total RA positional error of the measurement. uncertainty_ns deg Total Dec positional error of the measurement. weight_ew deg \\(^{-1}\\) The weight of the RA error (1/e). weight_ns deg \\(^{-1}\\) The weight of the Dec error (1/e). forced n/a Flag to denote whether the measurement is produced from the forced fitting procedure ( True ). flux_int_isl_ratio n/a The ratio of the measurements integrated flux to the total island integrated flux. flux_peak_isl_ratio n/a The ratio of the measurements peak flux to the total island peak flux. id n/a The database id of the measurement.","title":"measurements"},{"location":"outputs/coldesc/#measurement_pairs","text":"Column Unit Description meas_id_a n/a The database id of measurement a of the pair. meas_id_b n/a The database id of measurement b of the pair. flux_int_a mJy The integrated flux of measurement a of the pair. flux_int_err_a mJy The error of the integrated flux of measurement a of the pair. flux_peak_a mJy/beam The peak flux of measurement a of the pair. flux_peak_err_a mJy/beam The error of the peak flux of measurement a of the pair. image_name_a n/a The image name of measurement a of the pair. flux_int_b mJy The integrated flux of measurement b of the pair. flux_int_err_b mJy The error of the integrated flux of measurement b of the pair. flux_peak_b mJy/beam The peak flux of measurement b of the pair. flux_peak_err_b mJy/beam The error of the peak flux of measurement b of the pair. image_name_b n/a The image name of measurement b of the pair. vs_peak n/a The pair \\(V_s\\) value using the peak flux. vs_int n/a The pair \\(V_s\\) value using the integrated flux. m_peak n/a The pair \\(m\\) value using the peak flux. m_int n/a The pair \\(m\\) value using the integrated flux. source_id n/a The database id of the source the pair is associated to.","title":"measurement_pairs"},{"location":"outputs/coldesc/#relations","text":"Column Unit Description from_source_id n/a The database id of the first source in the relation pair. to_source_id n/a The database id of the second source in the relation pair.","title":"relations"},{"location":"outputs/coldesc/#skyregions","text":"Column Unit Description id n/a The database id of the sky region. centre_ra deg The right ascension value of the sky region central coordinate. centre_dec deg The declination value of the sky region central coordinate. width_ra deg The width of the area covered by the sky region. width_dec deg The height of the area covered by the sky region. xtr_radius deg The hypotenuse radius of the sky region. x rad The central cartesian x coordinate of the sky region. y rad The central cartesian y coordinate of the sky region. z rad The central cartesian z coordinate of the sky region.","title":"skyregions"},{"location":"outputs/coldesc/#sources","text":"Note The index column of the sources parquet is set to the database id of the source. Column Unit Description n_meas n/a The total number of measurements associated to the source (selavy and forced). n_meas_sel n/a The total number of selavy measurements associated to the source. n_meas_forced n/a The total number of forced measurements associated to the source. n_sibl n/a The total number of measurements that have a has_sibling value of True . n_rel n/a The total number of relations the source has. wavg_ra deg The weighted average of the Right Ascension of the measurements, that acts as the source position. wavg_dec deg The weighted average of the Declination of the measurements, that acts as the source position. wavg_uncertainty_ew deg The error of the weighted average right ascension value. wavg_uncertainty_ns deg The error of the weighted average declination value. new n/a Flag to signify the source is classed as a new source ( True ). new_high_sigma n/a The largest sigma value a new source would have if it was placed at its location in the previous images it was not detected in. See New Sources for more information. Set to 0 for non-new sources. n_neighbour_dist deg The on-sky separation to the nearest source with in the same pipeline run. avg_compactness n/a The average compactness value of the associated measurements. min_snr n/a The minimum signal-to-noise ratio of the associated measurements. max_snr n/a The maximum signal-to-noise ratio of the associated measurements. avg_flux_int mJy The average integrated flux value of the measurements associated to the source (inc. forced measurements). max_flux_int mJy The maximum integrated flux value of the measurements associated to the source (inc. forced measurements). min_flux_int mJy The minimum integrated flux value of the measurements associated to the source (inc. forced measurements). avg_flux_peak mJy/beam The average peak flux value of the measurements associated to the source (inc. forced measurements). max_flux_peak mJy/beam The maximum peak flux value of the measurements associated to the source (inc. forced measurements). min_flux_peak mJy/beam The minimum peak flux value of the measurements associated to the source (inc. forced measurements). min_flux_peak_isl_ratio n/a The minimum ratio of the peak flux to the total island peak flux of the measurements associated to the source. min_flux_int_isl_ratio n/a The minimum ratio of the integrated flux to the total island integrated flux of the measurements associated to the source. v_int n/a The calculated variability \\(V\\) metric using the integrated flux values. See variability statistics . v_peak n/a The calculated variability \\(V\\) metric using the peak flux values. See variability statistics . eta_int n/a The calculated variability \\(\\eta\\) metric using the integrated flux. See variability statistics . eta_peak n/a The calculated variability \\(\\eta\\) metric using the peak flux. See variability statistics . vs_abs_significant_max_peak n/a The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. See variability statistics . m_abs_significant_max_peak n/a The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. See variability statistics . vs_abs_significant_max_int n/a The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. See variability statistics . m_abs_significant_max_int n/a The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. See variability statistics .","title":"sources"},{"location":"outputs/outputs/","text":"Outputs Overview \u00b6 This page gives details on the output files that the pipeline writes to disk. Pipeline Run Output Overview \u00b6 The output for a pipeline run will be located in the pipeline working directory, which is defined at the pipeline configuration stage (see Pipeline Configuration ). A sub-directory will exist for each pipeline run that contains the output products for the run. Note If you do not administrate your system or do not have access to a vast-tools notebook interface, please contact your system admin to confirm the working directory and how to best access the files. The pipeline uses the Apache Parquet file format to write results to disk. Details on how to read these files can be found below in Reading the Outputs . Below is the output structure for a pipeline run named new-test-data when the pipeline run option measurements.write_arrow_files has been set to True and the working directory is named pipeline-runs (see File Details for descriptions): pipeline-runs \u251c\u2500\u2500 new-test-data \u2502 \u251c\u2500\u2500 associations.parquet \u2502 \u251c\u2500\u2500 bands.parquet \u2502 \u251c\u2500\u2500 config.yaml \u2502 \u251c\u2500\u2500 config_prev.yaml \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH02_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH03x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH02_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH03x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH12_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 images.parquet \u2502 \u251c\u2500\u2500 YYYY-MM-DD-HH-MM-SS_log.txt \u2502 \u251c\u2500\u2500 measurements.arrow \u2502 \u251c\u2500\u2500 measurement_pairs.arrow \u2502 \u251c\u2500\u2500 measurement_pairs.parquet \u2502 \u251c\u2500\u2500 relations.parquet \u2502 \u251c\u2500\u2500 skyregions.parquet \u2502 \u2514\u2500\u2500 sources.parquet Arrow Files \u00b6 Large pipeline runs (hundreds of images) mean that to read the measurements, hundreds of parquet files need to be read in, and can contain millions of rows. This can be slow using libraries such as pandas, and also consumes a lot of system memory. A solution to this is to save all the measurements associated with the pipeline run into one single file in the Apache Arrow format. The library vaex is able to open .arrow files in an out-of-core context so the memory footprint is hugely reduced along with the reading of the file being very fast. The two-epoch measurement pairs are also saved to arrow format due to the same reasons. See Reading with vaex for further details on using vaex . Note At the time of development vaex could not open parquets in an out-of-core context. This will be reviewed in the future if such functionality is added to vaex . To enable the arrow files to be produced, the option measurements.write_arrow_files is required to be set to True in the pipeline run config. Alternatively, the arrow files can be generated after the completion of the run, see the Generating Arrow Files page for full details. Image Data \u00b6 The data for the images ingested into the pipeline is also stored in the pipeline working directory under the subdirectory images : pipeline-runs \u251c\u2500\u2500 images \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH02_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH03x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH02_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH03x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u2514\u2500\u2500 VAST_2118-06A_EPOCH12_I_cutout_fits \u2502 \u2514\u2500\u2500 measurements.parquet Here, for each image, the selavy measurements that have been ingested are stored in the parquet format under a subdirectory of the respective image name. File Details \u00b6 File Description associations.parquet Contains the association information between sources and measurements. bands.parquet Contains the information of the bands associated with the pipeline run. config.yaml The pipeline run configuration file. config_prev.yaml The previous pipeline run configuration file used by the add image mode. forced_measurements*.parquet Multiple files that contain the forced measurements extracted from the respective image denoted in the filename. images.parquet Contains the information of the images processed in the pipeline run. YYYY-MM-DD-HH-MM-SS_log.txt The log file of the pipeline run. It is timestamped with the date and time of the run start. measurements.arrow An Apache Arrow format file containing all the measurements associated with the pipeline run (see Arrow Files ). measurement_pairs.arrow An Apache Arrow format file containing all the measurement pair metrics (see Arrow Files ). measurement_pairs.parquet Contains all the measurement pairs metrics. relations.parquet Contains the relation information between sources. skyregions.parquet Contains the sky region information of the pipeline run. sources.parquet Contains all the sources resulting from teh pipeline run.","title":"Outputs Overview"},{"location":"outputs/outputs/#outputs-overview","text":"This page gives details on the output files that the pipeline writes to disk.","title":"Outputs Overview"},{"location":"outputs/outputs/#pipeline-run-output-overview","text":"The output for a pipeline run will be located in the pipeline working directory, which is defined at the pipeline configuration stage (see Pipeline Configuration ). A sub-directory will exist for each pipeline run that contains the output products for the run. Note If you do not administrate your system or do not have access to a vast-tools notebook interface, please contact your system admin to confirm the working directory and how to best access the files. The pipeline uses the Apache Parquet file format to write results to disk. Details on how to read these files can be found below in Reading the Outputs . Below is the output structure for a pipeline run named new-test-data when the pipeline run option measurements.write_arrow_files has been set to True and the working directory is named pipeline-runs (see File Details for descriptions): pipeline-runs \u251c\u2500\u2500 new-test-data \u2502 \u251c\u2500\u2500 associations.parquet \u2502 \u251c\u2500\u2500 bands.parquet \u2502 \u251c\u2500\u2500 config.yaml \u2502 \u251c\u2500\u2500 config_prev.yaml \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH02_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH03x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH02_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH03x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH12_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 images.parquet \u2502 \u251c\u2500\u2500 YYYY-MM-DD-HH-MM-SS_log.txt \u2502 \u251c\u2500\u2500 measurements.arrow \u2502 \u251c\u2500\u2500 measurement_pairs.arrow \u2502 \u251c\u2500\u2500 measurement_pairs.parquet \u2502 \u251c\u2500\u2500 relations.parquet \u2502 \u251c\u2500\u2500 skyregions.parquet \u2502 \u2514\u2500\u2500 sources.parquet","title":"Pipeline Run Output Overview"},{"location":"outputs/outputs/#arrow-files","text":"Large pipeline runs (hundreds of images) mean that to read the measurements, hundreds of parquet files need to be read in, and can contain millions of rows. This can be slow using libraries such as pandas, and also consumes a lot of system memory. A solution to this is to save all the measurements associated with the pipeline run into one single file in the Apache Arrow format. The library vaex is able to open .arrow files in an out-of-core context so the memory footprint is hugely reduced along with the reading of the file being very fast. The two-epoch measurement pairs are also saved to arrow format due to the same reasons. See Reading with vaex for further details on using vaex . Note At the time of development vaex could not open parquets in an out-of-core context. This will be reviewed in the future if such functionality is added to vaex . To enable the arrow files to be produced, the option measurements.write_arrow_files is required to be set to True in the pipeline run config. Alternatively, the arrow files can be generated after the completion of the run, see the Generating Arrow Files page for full details.","title":"Arrow Files"},{"location":"outputs/outputs/#image-data","text":"The data for the images ingested into the pipeline is also stored in the pipeline working directory under the subdirectory images : pipeline-runs \u251c\u2500\u2500 images \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH02_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH03x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH02_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH03x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u2514\u2500\u2500 VAST_2118-06A_EPOCH12_I_cutout_fits \u2502 \u2514\u2500\u2500 measurements.parquet Here, for each image, the selavy measurements that have been ingested are stored in the parquet format under a subdirectory of the respective image name.","title":"Image Data"},{"location":"outputs/outputs/#file-details","text":"File Description associations.parquet Contains the association information between sources and measurements. bands.parquet Contains the information of the bands associated with the pipeline run. config.yaml The pipeline run configuration file. config_prev.yaml The previous pipeline run configuration file used by the add image mode. forced_measurements*.parquet Multiple files that contain the forced measurements extracted from the respective image denoted in the filename. images.parquet Contains the information of the images processed in the pipeline run. YYYY-MM-DD-HH-MM-SS_log.txt The log file of the pipeline run. It is timestamped with the date and time of the run start. measurements.arrow An Apache Arrow format file containing all the measurements associated with the pipeline run (see Arrow Files ). measurement_pairs.arrow An Apache Arrow format file containing all the measurement pair metrics (see Arrow Files ). measurement_pairs.parquet Contains all the measurement pairs metrics. relations.parquet Contains the relation information between sources. skyregions.parquet Contains the sky region information of the pipeline run. sources.parquet Contains all the sources resulting from teh pipeline run.","title":"File Details"},{"location":"outputs/usingoutputs/","text":"Using the Outputs \u00b6 This page gives details on how to open and use the pipeline output files. It is recommended to use pandas or vaex to read the pipeline results from the parquet files. See the sections below for more information on using each library. Note It is also possible to use Dask to read the parquets in an out-of-core context but the general performance can sometimes be poor with many parquet files. vaex is the preferred out-of-core method. Tip Be sure to look at vast-tools , a ready-made library for exploring pipeline results! Reading with pandas \u00b6 pandas documentation . Warning pyarrow will be required to open parquets with pandas . We recommend using this instead of fastparquet . To open a parquet using pandas use the read_parquet method: import pandas as pd sources = pd . read_parquet ( 'pipeline-runs/new-test-data/sources.parquet' ) sources . head () n_meas_forced n_meas ... vs_abs_significant_max_int m_abs_significant_max_int id ... 1 0 3 ... 55.050146 0.191083 2 1 3 ... 29.367098 0.525999 3 0 3 ... 4.388447 0.199877 4 1 3 ... 20.000058 1.047998 5 0 3 ... 0.000000 0.000000 [ 5 rows x 31 columns ] To read multiple parquets at once using pandas a loop must be used: import glob import pandas as pd files = glob . glob ( \"pipeline-runs/images/*/measurements.parquet\" ) data = [ pd . read_parquet ( f ) for f in files ] measurements = pd . concat ( data , ignore_index = True ) Tip If you don't require all the columns you can specify which columes to read using the columns variable. sources = pd . read_parquet ( 'pipeline-runs/new-test-data/sources.parquet' , columns = [ 'id' , 'n_meas' ]) Reading with vaex \u00b6 vaex documentation . Warning vaex is a young project so bugs may be expected along with frequent updates. It has currently been tested with version 3.0.0 . Version 4.0.0 promises opening parquet files in an out-of-core context. Warning Some pipeline parquet format files do not open with vaex 3.0.0. arrow format files should open successfully. A parquet, or arrow file, can be opened using the open() method: import vaex measurements = vaex . open ( 'pipeline-runs/new-test-data/measurements.arrow' ) measurements . head () # source island_id component_id local_rms ra ra_err dec dec_err flux_peak flux_peak_err flux_int flux_int_err bmaj err_bmaj bmin err_bmin pa err_pa psf_bmaj psf_bmin psf_pa flag_c4 chi_squared_fit spectral_index spectral_index_from_TT has_siblings image_id time name snr compactness ew_sys_err ns_sys_err error_radius uncertainty_ew uncertainty_ns weight_ew weight_ns forced flux_int_isl_ratio flux_peak_isl_ratio id 0 730 SB00004_island_1 SB00004_component_1a 0.463596 321.902 4.42819e-06 - 4.20097 2.2637e-06 307.991 0.4742 425.175 1.04728 20.95 1.08838e-05 12.28 4.32359e-06 108.19 0.00160065 15.58 0 - 73.86 False 3516.59 - 99 True True 2 2019 - 08 - 27 13 : 38 : 38.810000000 VAST_2118 + 00 A_SB00004_component_1a 664.352 1.38048 0.000277778 0.000277778 5.05099e-06 0.000277824 0.000277824 1.29557e+07 1.29557e+07 False 0.651019 0.709182 204 1 730 SB00009_island_1 SB00009_component_1a 0.463422 321.902 3.52062e-06 - 4.20103 2.45968e-06 318.544 0.472982 349.471 0.87264 21.4 8.50698e-06 12.78 5.46908e-06 107.02 0.0020224 5.72 0 46.03 True 15427.1 - 99 True True 3 2019 - 08 - 27 18 : 52 : 00.556000000 VAST_2118 - 06 A_SB00009_component_1a 687.374 1.09709 0.000277778 0.000277778 4.26887e-06 0.000277811 0.000277811 1.29569e+07 1.29569e+07 False 0.721399 0.7483 352 2 730 SB00006_island_1 SB00006_component_1a 0.627357 321.901 4.58076e-06 - 4.20086 2.92861e-06 310.503 0.662503 421.137 1.4171 17.05 1.07873e-05 12.42 6.89559e-06 90.71 0.00438684 10.51 4.24 - 82.52 False 4483.74 - 99 True True 5 2019 - 10 - 29 10 : 28 : 07.911000000 VAST_2118 + 00 A_SB00006_component_1a 494.938 1.35631 0.000277778 0.000277778 5.46682e-06 0.000277832 0.000277832 1.2955e+07 1.2955e+07 False 0.677562 0.721427 670 3 730 SB00011_island_1 SB00011_component_1a 0.627496 321.901 3.92144e-06 - 4.20087 3.21715e-06 310.998 0.643049 350.901 1.21083 17.06 9.23494e-06 12.43 7.57501e-06 91.19 0.00481863 6.43 0 27.62 False 4405.81 - 99 True True 4 2019 - 10 - 29 13 : 39 : 33.996000000 VAST_2118 - 06 A_SB00011_component_1a 495.618 1.12831 0.000277778 0.000277778 5.05099e-06 0.000277824 0.000277824 1.29557e+07 1.29557e+07 False 0.677576 0.721816 511 4 730 SB00005_island_1 SB00005_component_1a 0.346147 321.901 1.83032e-06 - 4.20051 1.71797e-06 299.072 0.342783 288.462 0.579893 14.13 4.37963e-06 12.14 3.97013e-06 65.14 0.00546325 2.86 0 6.42 False 2661.49 - 99 True True 7 2019 - 10 - 30 09 : 10 : 04.340000000 VAST_2118 + 00 A_SB00005_component_1a 864.004 0.964524 0.000277778 0.000277778 2.69987e-06 0.000277791 0.000277791 1.29588e+07 1.29588e+07 False 0.659887 0.719556 976 5 730 SB00010_island_1 SB00010_component_1a 0.347692 321.901 1.97328e-06 - 4.20052 1.75466e-06 300.969 0.360198 353.643 0.695754 14.16 4.76862e-06 12.16 3.99059e-06 65.77 0.00546515 6.12 3.95 49.18 False 2412.18 - 99 True True 6 2019 - 10 - 30 10 : 11 : 56.913000000 VAST_2118 - 06 A_SB00010_component_1a 865.62 1.17502 0.000277778 0.000277778 2.56132e-06 0.00027779 0.00027779 1.29589e+07 1.29589e+07 False 0.664605 0.723765 816 6 730 SB00007_island_1 SB00007_component_1a 0.387605 321.901 2.03947e-06 - 4.20032 1.77854e-06 317.014 0.392106 332.662 0.701451 14.56 4.96382e-06 11.54 3.99573e-06 64.78 0.00375775 4.81 0 24.08 False 1486.99 - 99 True True 10 2020 - 01 - 11 05 : 27 : 24.605000000 VAST_2118 + 00 A_SB00007_component_1a 817.88 1.04936 0.000277778 0.000277778 2.83165e-06 0.000277792 0.000277792 1.29587e+07 1.29587e+07 False 0.666924 0.716339 1493 7 730 SB00012_island_1 SB00012_component_1a 0.391978 321.901 2.12442e-06 - 4.20032 1.81129e-06 318.042 0.404457 365.987 0.770202 14.57 5.18203e-06 11.53 4.0454e-06 65.33 0.00378202 5.75 3.63 46.53 False 1328.58 - 99 True True 9 2020 - 01 - 11 05 : 40 : 11.007000000 VAST_2118 - 06 A_SB00012_component_1a 811.376 1.15075 0.000277778 0.000277778 2.69987e-06 0.000277791 0.000277791 1.29588e+07 1.29588e+07 False 0.667317 0.717746 1330 8 730 SB00008_island_1 SB00008_component_1a 0.432726 321.901 2.98443e-06 - 4.20052 2.32201e-06 293.737 0.436863 309.072 0.784657 18.35 7.14713e-06 12.12 5.31097e-06 105.78 0.00261377 4.82 0 18.62 False 2448.82 - 99 True True 13 2020 - 01 - 12 05 : 23 : 07.478000000 VAST_2118 + 00 A_SB00008_component_1a 678.807 1.05221 0.000277778 0.000277778 3.81819e-06 0.000277804 0.000277804 1.29576e+07 1.29576e+07 False 0.63994 0.725001 1889 9 730 SB00013_island_1 SB00013_component_1a 0.437279 321.901 3.14407e-06 - 4.20052 2.36161e-06 294.141 0.451346 340.92 0.864347 18.38 7.55055e-06 12.12 5.36009e-06 106.18 0.00262701 6.01 4 51.55 False 2368.93 - 99 True True 12 2020 - 01 - 12 05 : 36 : 03.834000000 VAST_2118 - 06 A_SB00013_component_1a 672.663 1.15903 0.000277778 0.000277778 4.00455e-06 0.000277807 0.000277807 1.29573e+07 1.29573e+07 False 0.640807 0.72508 1740 Multiple parquet files can be opened at once using the open_many() method: import glob import vaex files = glob . glob ( \"pipeline-runs/images/*/measurements.parquet\" ) measurements = vaex . open_many ( files ) Tip You can convert a vaex dataframe to pandas by using the to_pandas_df() method: import vaex sources = vaex . open ( 'pipeline-runs/new-test-data/sources.parquet' ) sources = sources . to_pandas_df () Linking the Results \u00b6 The table below shows what parameters act as keys to link data from the different results tables. Tip If loading the measurements via the .arrow file, then the measurements already have the source column in-place. Tip The images.parquet file contains the column measurements_path which can be used to get the filepaths for all the selavy parquet files. Data Column Linked to Column associations.parquet meas_id measurements.parquet , forced_measurements*.parquet id associations.parquet source_id sources.parquet id (index column) measurements.parquet , forced_measurements*.parquet image_id images.parquet id images.parquet band_id bands.parquet id images.parquet skyreg_id skyregions.parquet id measurement_pairs.parquet meas_id_a , meas_id_b measurements.parquet , forced_measurements*.parquet id measurement_pairs.parquet source_id sources.parquet id (index column) relations.parquet from_source_id , to_source_id sources.parquet id (index column) vast-tools \u00b6 Link to the vast-tools documentation . VAST has developed a python library called vast-tools that makes the exploration of results from the pipeline simple and efficient, in addition to being designed to be used in a Jupyter Notebook environment. Full details can be found in the documentation linked above, which also includes example notebooks of how to interact with the data.","title":"Using the Outputs"},{"location":"outputs/usingoutputs/#using-the-outputs","text":"This page gives details on how to open and use the pipeline output files. It is recommended to use pandas or vaex to read the pipeline results from the parquet files. See the sections below for more information on using each library. Note It is also possible to use Dask to read the parquets in an out-of-core context but the general performance can sometimes be poor with many parquet files. vaex is the preferred out-of-core method. Tip Be sure to look at vast-tools , a ready-made library for exploring pipeline results!","title":"Using the Outputs"},{"location":"outputs/usingoutputs/#reading-with-pandas","text":"pandas documentation . Warning pyarrow will be required to open parquets with pandas . We recommend using this instead of fastparquet . To open a parquet using pandas use the read_parquet method: import pandas as pd sources = pd . read_parquet ( 'pipeline-runs/new-test-data/sources.parquet' ) sources . head () n_meas_forced n_meas ... vs_abs_significant_max_int m_abs_significant_max_int id ... 1 0 3 ... 55.050146 0.191083 2 1 3 ... 29.367098 0.525999 3 0 3 ... 4.388447 0.199877 4 1 3 ... 20.000058 1.047998 5 0 3 ... 0.000000 0.000000 [ 5 rows x 31 columns ] To read multiple parquets at once using pandas a loop must be used: import glob import pandas as pd files = glob . glob ( \"pipeline-runs/images/*/measurements.parquet\" ) data = [ pd . read_parquet ( f ) for f in files ] measurements = pd . concat ( data , ignore_index = True ) Tip If you don't require all the columns you can specify which columes to read using the columns variable. sources = pd . read_parquet ( 'pipeline-runs/new-test-data/sources.parquet' , columns = [ 'id' , 'n_meas' ])","title":"Reading with pandas"},{"location":"outputs/usingoutputs/#reading-with-vaex","text":"vaex documentation . Warning vaex is a young project so bugs may be expected along with frequent updates. It has currently been tested with version 3.0.0 . Version 4.0.0 promises opening parquet files in an out-of-core context. Warning Some pipeline parquet format files do not open with vaex 3.0.0. arrow format files should open successfully. A parquet, or arrow file, can be opened using the open() method: import vaex measurements = vaex . open ( 'pipeline-runs/new-test-data/measurements.arrow' ) measurements . head () # source island_id component_id local_rms ra ra_err dec dec_err flux_peak flux_peak_err flux_int flux_int_err bmaj err_bmaj bmin err_bmin pa err_pa psf_bmaj psf_bmin psf_pa flag_c4 chi_squared_fit spectral_index spectral_index_from_TT has_siblings image_id time name snr compactness ew_sys_err ns_sys_err error_radius uncertainty_ew uncertainty_ns weight_ew weight_ns forced flux_int_isl_ratio flux_peak_isl_ratio id 0 730 SB00004_island_1 SB00004_component_1a 0.463596 321.902 4.42819e-06 - 4.20097 2.2637e-06 307.991 0.4742 425.175 1.04728 20.95 1.08838e-05 12.28 4.32359e-06 108.19 0.00160065 15.58 0 - 73.86 False 3516.59 - 99 True True 2 2019 - 08 - 27 13 : 38 : 38.810000000 VAST_2118 + 00 A_SB00004_component_1a 664.352 1.38048 0.000277778 0.000277778 5.05099e-06 0.000277824 0.000277824 1.29557e+07 1.29557e+07 False 0.651019 0.709182 204 1 730 SB00009_island_1 SB00009_component_1a 0.463422 321.902 3.52062e-06 - 4.20103 2.45968e-06 318.544 0.472982 349.471 0.87264 21.4 8.50698e-06 12.78 5.46908e-06 107.02 0.0020224 5.72 0 46.03 True 15427.1 - 99 True True 3 2019 - 08 - 27 18 : 52 : 00.556000000 VAST_2118 - 06 A_SB00009_component_1a 687.374 1.09709 0.000277778 0.000277778 4.26887e-06 0.000277811 0.000277811 1.29569e+07 1.29569e+07 False 0.721399 0.7483 352 2 730 SB00006_island_1 SB00006_component_1a 0.627357 321.901 4.58076e-06 - 4.20086 2.92861e-06 310.503 0.662503 421.137 1.4171 17.05 1.07873e-05 12.42 6.89559e-06 90.71 0.00438684 10.51 4.24 - 82.52 False 4483.74 - 99 True True 5 2019 - 10 - 29 10 : 28 : 07.911000000 VAST_2118 + 00 A_SB00006_component_1a 494.938 1.35631 0.000277778 0.000277778 5.46682e-06 0.000277832 0.000277832 1.2955e+07 1.2955e+07 False 0.677562 0.721427 670 3 730 SB00011_island_1 SB00011_component_1a 0.627496 321.901 3.92144e-06 - 4.20087 3.21715e-06 310.998 0.643049 350.901 1.21083 17.06 9.23494e-06 12.43 7.57501e-06 91.19 0.00481863 6.43 0 27.62 False 4405.81 - 99 True True 4 2019 - 10 - 29 13 : 39 : 33.996000000 VAST_2118 - 06 A_SB00011_component_1a 495.618 1.12831 0.000277778 0.000277778 5.05099e-06 0.000277824 0.000277824 1.29557e+07 1.29557e+07 False 0.677576 0.721816 511 4 730 SB00005_island_1 SB00005_component_1a 0.346147 321.901 1.83032e-06 - 4.20051 1.71797e-06 299.072 0.342783 288.462 0.579893 14.13 4.37963e-06 12.14 3.97013e-06 65.14 0.00546325 2.86 0 6.42 False 2661.49 - 99 True True 7 2019 - 10 - 30 09 : 10 : 04.340000000 VAST_2118 + 00 A_SB00005_component_1a 864.004 0.964524 0.000277778 0.000277778 2.69987e-06 0.000277791 0.000277791 1.29588e+07 1.29588e+07 False 0.659887 0.719556 976 5 730 SB00010_island_1 SB00010_component_1a 0.347692 321.901 1.97328e-06 - 4.20052 1.75466e-06 300.969 0.360198 353.643 0.695754 14.16 4.76862e-06 12.16 3.99059e-06 65.77 0.00546515 6.12 3.95 49.18 False 2412.18 - 99 True True 6 2019 - 10 - 30 10 : 11 : 56.913000000 VAST_2118 - 06 A_SB00010_component_1a 865.62 1.17502 0.000277778 0.000277778 2.56132e-06 0.00027779 0.00027779 1.29589e+07 1.29589e+07 False 0.664605 0.723765 816 6 730 SB00007_island_1 SB00007_component_1a 0.387605 321.901 2.03947e-06 - 4.20032 1.77854e-06 317.014 0.392106 332.662 0.701451 14.56 4.96382e-06 11.54 3.99573e-06 64.78 0.00375775 4.81 0 24.08 False 1486.99 - 99 True True 10 2020 - 01 - 11 05 : 27 : 24.605000000 VAST_2118 + 00 A_SB00007_component_1a 817.88 1.04936 0.000277778 0.000277778 2.83165e-06 0.000277792 0.000277792 1.29587e+07 1.29587e+07 False 0.666924 0.716339 1493 7 730 SB00012_island_1 SB00012_component_1a 0.391978 321.901 2.12442e-06 - 4.20032 1.81129e-06 318.042 0.404457 365.987 0.770202 14.57 5.18203e-06 11.53 4.0454e-06 65.33 0.00378202 5.75 3.63 46.53 False 1328.58 - 99 True True 9 2020 - 01 - 11 05 : 40 : 11.007000000 VAST_2118 - 06 A_SB00012_component_1a 811.376 1.15075 0.000277778 0.000277778 2.69987e-06 0.000277791 0.000277791 1.29588e+07 1.29588e+07 False 0.667317 0.717746 1330 8 730 SB00008_island_1 SB00008_component_1a 0.432726 321.901 2.98443e-06 - 4.20052 2.32201e-06 293.737 0.436863 309.072 0.784657 18.35 7.14713e-06 12.12 5.31097e-06 105.78 0.00261377 4.82 0 18.62 False 2448.82 - 99 True True 13 2020 - 01 - 12 05 : 23 : 07.478000000 VAST_2118 + 00 A_SB00008_component_1a 678.807 1.05221 0.000277778 0.000277778 3.81819e-06 0.000277804 0.000277804 1.29576e+07 1.29576e+07 False 0.63994 0.725001 1889 9 730 SB00013_island_1 SB00013_component_1a 0.437279 321.901 3.14407e-06 - 4.20052 2.36161e-06 294.141 0.451346 340.92 0.864347 18.38 7.55055e-06 12.12 5.36009e-06 106.18 0.00262701 6.01 4 51.55 False 2368.93 - 99 True True 12 2020 - 01 - 12 05 : 36 : 03.834000000 VAST_2118 - 06 A_SB00013_component_1a 672.663 1.15903 0.000277778 0.000277778 4.00455e-06 0.000277807 0.000277807 1.29573e+07 1.29573e+07 False 0.640807 0.72508 1740 Multiple parquet files can be opened at once using the open_many() method: import glob import vaex files = glob . glob ( \"pipeline-runs/images/*/measurements.parquet\" ) measurements = vaex . open_many ( files ) Tip You can convert a vaex dataframe to pandas by using the to_pandas_df() method: import vaex sources = vaex . open ( 'pipeline-runs/new-test-data/sources.parquet' ) sources = sources . to_pandas_df ()","title":"Reading with vaex"},{"location":"outputs/usingoutputs/#linking-the-results","text":"The table below shows what parameters act as keys to link data from the different results tables. Tip If loading the measurements via the .arrow file, then the measurements already have the source column in-place. Tip The images.parquet file contains the column measurements_path which can be used to get the filepaths for all the selavy parquet files. Data Column Linked to Column associations.parquet meas_id measurements.parquet , forced_measurements*.parquet id associations.parquet source_id sources.parquet id (index column) measurements.parquet , forced_measurements*.parquet image_id images.parquet id images.parquet band_id bands.parquet id images.parquet skyreg_id skyregions.parquet id measurement_pairs.parquet meas_id_a , meas_id_b measurements.parquet , forced_measurements*.parquet id measurement_pairs.parquet source_id sources.parquet id (index column) relations.parquet from_source_id , to_source_id sources.parquet id (index column)","title":"Linking the Results"},{"location":"outputs/usingoutputs/#vast-tools","text":"Link to the vast-tools documentation . VAST has developed a python library called vast-tools that makes the exploration of results from the pipeline simple and efficient, in addition to being designed to be used in a Jupyter Notebook environment. Full details can be found in the documentation linked above, which also includes example notebooks of how to interact with the data.","title":"vast-tools"},{"location":"reference/admin/","text":"This module contains the admin classes that are registered with the Django Admin site. ImageAdmin \u00b6 The ImageAdmin class. MeasurementAdmin \u00b6 The MeasurementAdmin class. RunAdmin \u00b6 The RunAdmin class. SkyRegionAdmin \u00b6 The SkyRegionAdmin class. SourceAdmin \u00b6 The SourceAdmin class. SourceFavAdmin \u00b6 The SourceFavAdmin class.","title":"admin.py"},{"location":"reference/admin/#vast_pipeline.admin.ImageAdmin","text":"The ImageAdmin class.","title":"ImageAdmin"},{"location":"reference/admin/#vast_pipeline.admin.MeasurementAdmin","text":"The MeasurementAdmin class.","title":"MeasurementAdmin"},{"location":"reference/admin/#vast_pipeline.admin.RunAdmin","text":"The RunAdmin class.","title":"RunAdmin"},{"location":"reference/admin/#vast_pipeline.admin.SkyRegionAdmin","text":"The SkyRegionAdmin class.","title":"SkyRegionAdmin"},{"location":"reference/admin/#vast_pipeline.admin.SourceAdmin","text":"The SourceAdmin class.","title":"SourceAdmin"},{"location":"reference/admin/#vast_pipeline.admin.SourceFavAdmin","text":"The SourceFavAdmin class.","title":"SourceFavAdmin"},{"location":"reference/apps/","text":"PipelineConfig \u00b6 Class representing the configuration for the vast_pipeline app. ready ( self ) \u00b6 Initialization tasks performed as soon as the app registry is populated. See https://docs.djangoproject.com/en/3.1/ref/applications/#django.apps.AppConfig.ready . Source code in vast_pipeline/apps.py def ready ( self ) -> None : \"\"\"Initialization tasks performed as soon as the app registry is populated. See <https://docs.djangoproject.com/en/3.1/ref/applications/#django.apps.AppConfig.ready>. \"\"\" # import the signals to register them with the application import vast_pipeline.signals # noqa: F401","title":"apps.py"},{"location":"reference/apps/#vast_pipeline.apps.PipelineConfig","text":"Class representing the configuration for the vast_pipeline app.","title":"PipelineConfig"},{"location":"reference/apps/#vast_pipeline.apps.PipelineConfig.ready","text":"Initialization tasks performed as soon as the app registry is populated. See https://docs.djangoproject.com/en/3.1/ref/applications/#django.apps.AppConfig.ready . Source code in vast_pipeline/apps.py def ready ( self ) -> None : \"\"\"Initialization tasks performed as soon as the app registry is populated. See <https://docs.djangoproject.com/en/3.1/ref/applications/#django.apps.AppConfig.ready>. \"\"\" # import the signals to register them with the application import vast_pipeline.signals # noqa: F401","title":"ready()"},{"location":"reference/context_processors/","text":"maintainance_banner ( request ) \u00b6 Generates the maintainance banner for the web server if a message has been set in the Django settings. Parameters: Name Type Description Default request HttpRequest The web server request. required Returns: Type Description Dict[str, Optional[str]] Dictionary representing the JSON object with the maintainance message. Source code in vast_pipeline/context_processors.py def maintainance_banner ( request : HttpRequest ) -> Dict [ str , Optional [ str ]]: \"\"\" Generates the maintainance banner for the web server if a message has been set in the Django settings. Args: request (HttpRequest): The web server request. Returns: Dictionary representing the JSON object with the maintainance message. \"\"\" if settings . PIPELINE_MAINTAINANCE_MESSAGE : return { \"maintainance_message\" : settings . PIPELINE_MAINTAINANCE_MESSAGE } return { \"maintainance_message\" : None } pipeline_version ( request ) \u00b6 Adds the pipeline version to the template context. Parameters: Name Type Description Default request HttpRequest The web server request. required Returns: Type Description Dict[str, Optional[str]] Dict[str, str]: key-value pairs to add to the template context. Source code in vast_pipeline/context_processors.py def pipeline_version ( request : HttpRequest ) -> Dict [ str , Optional [ str ]]: \"\"\"Adds the pipeline version to the template context. Args: request (HttpRequest): The web server request. Returns: Dict[str, str]: key-value pairs to add to the template context. \"\"\" url : Optional [ str ] = None if not __version__ . endswith ( \"dev\" ): url = f \"https://github.com/askap-vast/vast-pipeline/releases/tag/v { __version__ } \" return { \"pipeline_version\" : __version__ , \"pipeline_version_url\" : url , }","title":"context_processors.py"},{"location":"reference/context_processors/#vast_pipeline.context_processors.maintainance_banner","text":"Generates the maintainance banner for the web server if a message has been set in the Django settings. Parameters: Name Type Description Default request HttpRequest The web server request. required Returns: Type Description Dict[str, Optional[str]] Dictionary representing the JSON object with the maintainance message. Source code in vast_pipeline/context_processors.py def maintainance_banner ( request : HttpRequest ) -> Dict [ str , Optional [ str ]]: \"\"\" Generates the maintainance banner for the web server if a message has been set in the Django settings. Args: request (HttpRequest): The web server request. Returns: Dictionary representing the JSON object with the maintainance message. \"\"\" if settings . PIPELINE_MAINTAINANCE_MESSAGE : return { \"maintainance_message\" : settings . PIPELINE_MAINTAINANCE_MESSAGE } return { \"maintainance_message\" : None }","title":"maintainance_banner()"},{"location":"reference/context_processors/#vast_pipeline.context_processors.pipeline_version","text":"Adds the pipeline version to the template context. Parameters: Name Type Description Default request HttpRequest The web server request. required Returns: Type Description Dict[str, Optional[str]] Dict[str, str]: key-value pairs to add to the template context. Source code in vast_pipeline/context_processors.py def pipeline_version ( request : HttpRequest ) -> Dict [ str , Optional [ str ]]: \"\"\"Adds the pipeline version to the template context. Args: request (HttpRequest): The web server request. Returns: Dict[str, str]: key-value pairs to add to the template context. \"\"\" url : Optional [ str ] = None if not __version__ . endswith ( \"dev\" ): url = f \"https://github.com/askap-vast/vast-pipeline/releases/tag/v { __version__ } \" return { \"pipeline_version\" : __version__ , \"pipeline_version_url\" : url , }","title":"pipeline_version()"},{"location":"reference/converters/","text":"AngleConverter \u00b6 Accept any valid input value for an astropy.coordinates.Angle and ensure the returned value is a float in decimal degrees. The unit should be included in the input value. to_python ( self , value ) \u00b6 Return the decimal degrees from the coordinate input as an Angle object. Parameters: Name Type Description Default value The value of the angle input. required Returns: Type Description float The angle returned as an Angle object. Source code in vast_pipeline/converters.py def to_python ( self , value ) -> float : \"\"\" Return the decimal degrees from the coordinate input as an Angle object. Args: value: The value of the angle input. Returns: The angle returned as an Angle object. \"\"\" return Angle ( value ) . deg to_url ( self , value ) \u00b6 Return the string format of an Angle object from the coordinate input. Parameters: Name Type Description Default value The value of the angle input. required Returns: Type Description str The string representation of the Angle object created from the input. Source code in vast_pipeline/converters.py def to_url ( self , value ) -> str : \"\"\" Return the string format of an Angle object from the coordinate input. Args: value: The value of the angle input. Returns: The string representation of the Angle object created from the input. \"\"\" return value . to_string () DeclinationConverter \u00b6 Accept both decimal and sexigesimal representations of Dec and ensure the returned value is a float in decimal degrees. The input units are always assumed to be degrees. to_python ( self , value ) \u00b6 Return the decimal degrees from the coordinate input as a python float object. Parameters: Name Type Description Default value str The value of the declination input. required Returns: Type Description float The decimal degrees value. Source code in vast_pipeline/converters.py def to_python ( self , value : str ) -> float : \"\"\" Return the decimal degrees from the coordinate input as a python float object. Args: value: The value of the declination input. Returns: The decimal degrees value. \"\"\" return Latitude ( value , unit = \"deg\" ) . deg to_url ( self , value ) \u00b6 Return the decimal degrees from the coordinate input in a URL format. Parameters: Name Type Description Default value str The value of the declination input. required Returns: Type Description str The decimal degrees value as a string. Source code in vast_pipeline/converters.py def to_url ( self , value : str ) -> str : \"\"\" Return the decimal degrees from the coordinate input in a URL format. Args: value: The value of the declination input. Returns: The decimal degrees value as a string. \"\"\" return value . to_string ( unit = \"deg\" , decimal = True ) RightAscensionConverter \u00b6 Accept both decimal and sexigesimal representations of RA and ensure the returned value is a float in decimal degrees. If the input is in sexigesimal format, assume it is in units of hourangle. to_python ( self , value ) \u00b6 Return the decimal degrees from the coordinate input as a python float object. Parameters: Name Type Description Default value str The value of the RA input. required Returns: Type Description float The decimal degrees value. Source code in vast_pipeline/converters.py def to_python ( self , value : str ) -> float : \"\"\" Return the decimal degrees from the coordinate input as a python float object. Args: value: The value of the RA input. Returns: The decimal degrees value. \"\"\" unit = \"hourangle\" if \":\" in value else \"deg\" return Longitude ( value , unit = unit ) . deg to_url ( self , value ) \u00b6 Return the decimal degrees from the coordinate input in a URL format. Parameters: Name Type Description Default value str The value of the RA input. required Returns: Type Description str The decimal degrees value as a string. Source code in vast_pipeline/converters.py def to_url ( self , value : str ) -> str : \"\"\" Return the decimal degrees from the coordinate input in a URL format. Args: value: The value of the RA input. Returns: The decimal degrees value as a string. \"\"\" return value . to_string ( unit = \"deg\" , decimal = True )","title":"converters.py"},{"location":"reference/converters/#vast_pipeline.converters.AngleConverter","text":"Accept any valid input value for an astropy.coordinates.Angle and ensure the returned value is a float in decimal degrees. The unit should be included in the input value.","title":"AngleConverter"},{"location":"reference/converters/#vast_pipeline.converters.AngleConverter.to_python","text":"Return the decimal degrees from the coordinate input as an Angle object. Parameters: Name Type Description Default value The value of the angle input. required Returns: Type Description float The angle returned as an Angle object. Source code in vast_pipeline/converters.py def to_python ( self , value ) -> float : \"\"\" Return the decimal degrees from the coordinate input as an Angle object. Args: value: The value of the angle input. Returns: The angle returned as an Angle object. \"\"\" return Angle ( value ) . deg","title":"to_python()"},{"location":"reference/converters/#vast_pipeline.converters.AngleConverter.to_url","text":"Return the string format of an Angle object from the coordinate input. Parameters: Name Type Description Default value The value of the angle input. required Returns: Type Description str The string representation of the Angle object created from the input. Source code in vast_pipeline/converters.py def to_url ( self , value ) -> str : \"\"\" Return the string format of an Angle object from the coordinate input. Args: value: The value of the angle input. Returns: The string representation of the Angle object created from the input. \"\"\" return value . to_string ()","title":"to_url()"},{"location":"reference/converters/#vast_pipeline.converters.DeclinationConverter","text":"Accept both decimal and sexigesimal representations of Dec and ensure the returned value is a float in decimal degrees. The input units are always assumed to be degrees.","title":"DeclinationConverter"},{"location":"reference/converters/#vast_pipeline.converters.DeclinationConverter.to_python","text":"Return the decimal degrees from the coordinate input as a python float object. Parameters: Name Type Description Default value str The value of the declination input. required Returns: Type Description float The decimal degrees value. Source code in vast_pipeline/converters.py def to_python ( self , value : str ) -> float : \"\"\" Return the decimal degrees from the coordinate input as a python float object. Args: value: The value of the declination input. Returns: The decimal degrees value. \"\"\" return Latitude ( value , unit = \"deg\" ) . deg","title":"to_python()"},{"location":"reference/converters/#vast_pipeline.converters.DeclinationConverter.to_url","text":"Return the decimal degrees from the coordinate input in a URL format. Parameters: Name Type Description Default value str The value of the declination input. required Returns: Type Description str The decimal degrees value as a string. Source code in vast_pipeline/converters.py def to_url ( self , value : str ) -> str : \"\"\" Return the decimal degrees from the coordinate input in a URL format. Args: value: The value of the declination input. Returns: The decimal degrees value as a string. \"\"\" return value . to_string ( unit = \"deg\" , decimal = True )","title":"to_url()"},{"location":"reference/converters/#vast_pipeline.converters.RightAscensionConverter","text":"Accept both decimal and sexigesimal representations of RA and ensure the returned value is a float in decimal degrees. If the input is in sexigesimal format, assume it is in units of hourangle.","title":"RightAscensionConverter"},{"location":"reference/converters/#vast_pipeline.converters.RightAscensionConverter.to_python","text":"Return the decimal degrees from the coordinate input as a python float object. Parameters: Name Type Description Default value str The value of the RA input. required Returns: Type Description float The decimal degrees value. Source code in vast_pipeline/converters.py def to_python ( self , value : str ) -> float : \"\"\" Return the decimal degrees from the coordinate input as a python float object. Args: value: The value of the RA input. Returns: The decimal degrees value. \"\"\" unit = \"hourangle\" if \":\" in value else \"deg\" return Longitude ( value , unit = unit ) . deg","title":"to_python()"},{"location":"reference/converters/#vast_pipeline.converters.RightAscensionConverter.to_url","text":"Return the decimal degrees from the coordinate input in a URL format. Parameters: Name Type Description Default value str The value of the RA input. required Returns: Type Description str The decimal degrees value as a string. Source code in vast_pipeline/converters.py def to_url ( self , value : str ) -> str : \"\"\" Return the decimal degrees from the coordinate input in a URL format. Args: value: The value of the RA input. Returns: The decimal degrees value as a string. \"\"\" return value . to_string ( unit = \"deg\" , decimal = True )","title":"to_url()"},{"location":"reference/forms/","text":"CommentForm \u00b6 The form used for users to leave comments on objects. Meta \u00b6 model \u00b6 The model object for a comment. get_avatar_url ( self ) \u00b6 Get the URL for the user's avatar from GitHub. If the user has no associated GitHub account (e.g. a Django superuser), return the URL to the default user avatar. Returns: Type Description str The avatar URL. Source code in vast_pipeline/forms.py def get_avatar_url ( self ) -> str : \"\"\"Get the URL for the user's avatar from GitHub. If the user has no associated GitHub account (e.g. a Django superuser), return the URL to the default user avatar. Returns: The avatar URL. \"\"\" social = UserSocialAuth . get_social_auth_for_user ( self . author ) . first () if social and \"avatar_url\" in social . extra_data : return social . extra_data [ \"avatar_url\" ] else : return static ( \"img/user-32.png\" ) __init__ ( self , * args , ** kwargs ) special \u00b6 Initialise a CommentForm. Returns: Type Description None None. Source code in vast_pipeline/forms.py def __init__ ( self , * args , ** kwargs ) -> None : \"\"\" Initialise a CommentForm. Returns: None. \"\"\" super () . __init__ ( * args , ** kwargs ) self . helper = FormHelper () self . helper . layout = Layout ( Field ( \"comment\" , rows = 2 ), ) self . helper . add_input ( Submit ( \"submit\" , \"Submit\" )) PipelineRunForm \u00b6 Class for the form used in the creation of a new pipeline run through the webserver. TagWithCommentsForm \u00b6 Class to combined tags with the CommentsForm. __init__ ( self , * args , ** kwargs ) special \u00b6 Initialise a TagWithCommentsForm. Returns: Type Description None None. Source code in vast_pipeline/forms.py def __init__ ( self , * args , ** kwargs ) -> None : \"\"\" Initialise a TagWithCommentsForm. Returns: None. \"\"\" super () . __init__ ( * args , ** kwargs ) self . helper = FormHelper () self . helper . layout = Layout ( Field ( \"tags\" ), Field ( \"comment\" , rows = 2 , placeholder = ( \"Optional. If changing the tags, you should provide justification here.\" ), ), Submit ( \"submit\" , \"Submit\" , css_class = \"btn-block\" ), )","title":"forms.py"},{"location":"reference/forms/#vast_pipeline.forms.CommentForm","text":"The form used for users to leave comments on objects.","title":"CommentForm"},{"location":"reference/forms/#vast_pipeline.forms.CommentForm.Meta","text":"","title":"Meta"},{"location":"reference/forms/#vast_pipeline.forms.CommentForm.Meta.model","text":"The model object for a comment.","title":"model"},{"location":"reference/forms/#vast_pipeline.forms.CommentForm.__init__","text":"Initialise a CommentForm. Returns: Type Description None None. Source code in vast_pipeline/forms.py def __init__ ( self , * args , ** kwargs ) -> None : \"\"\" Initialise a CommentForm. Returns: None. \"\"\" super () . __init__ ( * args , ** kwargs ) self . helper = FormHelper () self . helper . layout = Layout ( Field ( \"comment\" , rows = 2 ), ) self . helper . add_input ( Submit ( \"submit\" , \"Submit\" ))","title":"__init__()"},{"location":"reference/forms/#vast_pipeline.forms.PipelineRunForm","text":"Class for the form used in the creation of a new pipeline run through the webserver.","title":"PipelineRunForm"},{"location":"reference/forms/#vast_pipeline.forms.TagWithCommentsForm","text":"Class to combined tags with the CommentsForm.","title":"TagWithCommentsForm"},{"location":"reference/forms/#vast_pipeline.forms.TagWithCommentsForm.__init__","text":"Initialise a TagWithCommentsForm. Returns: Type Description None None. Source code in vast_pipeline/forms.py def __init__ ( self , * args , ** kwargs ) -> None : \"\"\" Initialise a TagWithCommentsForm. Returns: None. \"\"\" super () . __init__ ( * args , ** kwargs ) self . helper = FormHelper () self . helper . layout = Layout ( Field ( \"tags\" ), Field ( \"comment\" , rows = 2 , placeholder = ( \"Optional. If changing the tags, you should provide justification here.\" ), ), Submit ( \"submit\" , \"Submit\" , css_class = \"btn-block\" ), )","title":"__init__()"},{"location":"reference/models/","text":"Association \u00b6 model association between sources and measurements based on some parameters Band \u00b6 A band on the frequency spectrum used for imaging. Each image is associated with one band. Comment \u00b6 The model object for a comment. get_avatar_url ( self ) \u00b6 Get the URL for the user's avatar from GitHub. If the user has no associated GitHub account (e.g. a Django superuser), return the URL to the default user avatar. Returns: Type Description str The avatar URL. Source code in vast_pipeline/models.py def get_avatar_url ( self ) -> str : \"\"\"Get the URL for the user's avatar from GitHub. If the user has no associated GitHub account (e.g. a Django superuser), return the URL to the default user avatar. Returns: The avatar URL. \"\"\" social = UserSocialAuth . get_social_auth_for_user ( self . author ) . first () if social and \"avatar_url\" in social . extra_data : return social . extra_data [ \"avatar_url\" ] else : return static ( \"img/user-32.png\" ) CommentableModel \u00b6 A class to provide a commentable model. Image \u00b6 An image is a 2D radio image from a cube Measurement \u00b6 A Measurement is an object in the sky that has been detected at least once. Essentially a source single measurement in time. MeasurementPair dataclass \u00b6 MeasurementPair(source_id: int, measurement_a_id: int, measurement_b_id: int, vs_peak: float, m_peak: float, vs_int: float, m_int: float) MeasurementQuerySet \u00b6 cone_search ( self , ra , dec , radius_deg ) \u00b6 Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Parameters: Name Type Description Default ra float The right ascension value of the cone search central coordinate. required dec float The declination value of the cone search central coordinate. required radius_deg float The radius over which to perform the cone search. required Returns: Type Description QuerySet Measurements found withing the cone search area. Source code in vast_pipeline/models.py def cone_search ( self , ra : float , dec : float , radius_deg : float ) -> models . QuerySet : \"\"\" Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Args: ra: The right ascension value of the cone search central coordinate. dec: The declination value of the cone search central coordinate. radius_deg: The radius over which to perform the cone search. Returns: Measurements found withing the cone search area. \"\"\" return ( self . extra ( select = { \"distance\" : \"q3c_dist(ra, dec, %s , %s ) * 3600\" }, select_params = [ ra , dec ], where = [ \"q3c_radial_query(ra, dec, %s , %s , %s )\" ], params = [ ra , dec , radius_deg ], ) . order_by ( \"distance\" ) ) RelatedSource \u00b6 Association table for the many to many Source relationship with itself Django doc https://docs.djangoproject.com/en/3.1/ref/models/fields/#django.db.models.ManyToManyField.through Run \u00b6 A Run is essentially a pipeline run/processing istance over a set of images save ( self , * args , ** kwargs ) \u00b6 Save the current instance. Override this in a subclass if you want to control the saving process. The 'force_insert' and 'force_update' parameters can be used to insist that the \"save\" must be an SQL insert or update (or equivalent for non-SQL backends), respectively. Normally, they should not be set. Source code in vast_pipeline/models.py def save ( self , * args , ** kwargs ): # enforce the full model validation on save self . full_clean () super ( Run , self ) . save ( * args , ** kwargs ) RunQuerySet \u00b6 check_max_runs ( self , max_runs = 5 ) \u00b6 Check if number of running pipeline runs is above threshold. Parameters: Name Type Description Default max_runs int The maximum number of processing runs allowed. 5 Returns: Type Description int The count of the current pipeline runs with a status of RUN . Source code in vast_pipeline/models.py def check_max_runs ( self , max_runs : int = 5 ) -> int : \"\"\" Check if number of running pipeline runs is above threshold. Args: max_runs: The maximum number of processing runs allowed. Returns: The count of the current pipeline runs with a status of `RUN`. \"\"\" return self . filter ( status = 'RUN' ) . count () >= max_runs SkyRegion \u00b6 SkyRegion(id, centre_ra, centre_dec, width_ra, width_dec, xtr_radius, x, y, z) Source \u00b6 Source(id, run, name, new, wavg_ra, wavg_dec, wavg_uncertainty_ew, wavg_uncertainty_ns, avg_flux_int, avg_flux_peak, max_flux_peak, min_flux_peak, max_flux_int, min_flux_int, min_flux_int_isl_ratio, min_flux_peak_isl_ratio, avg_compactness, min_snr, max_snr, v_int, v_peak, eta_int, eta_peak, new_high_sigma, n_neighbour_dist, vs_abs_significant_max_int, m_abs_significant_max_int, vs_abs_significant_max_peak, m_abs_significant_max_peak, n_meas, n_meas_sel, n_meas_forced, n_rel, n_sibl) SourceFav \u00b6 SourceFav(id, user, source, comment) SourceQuerySet \u00b6 cone_search ( self , ra , dec , radius_deg ) \u00b6 Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Parameters: Name Type Description Default ra float The right ascension value of the cone search central coordinate. required dec float The declination value of the cone search central coordinate. required radius_deg float The radius over which to perform the cone search. required Returns: Type Description QuerySet Sources found withing the cone search area. Source code in vast_pipeline/models.py def cone_search ( self , ra : float , dec : float , radius_deg : float ) -> models . QuerySet : \"\"\" Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Args: ra: The right ascension value of the cone search central coordinate. dec: The declination value of the cone search central coordinate. radius_deg: The radius over which to perform the cone search. Returns: Sources found withing the cone search area. \"\"\" return ( self . extra ( select = { \"distance\" : \"q3c_dist(wavg_ra, wavg_dec, %s , %s ) * 3600\" }, select_params = [ ra , dec ], where = [ \"q3c_radial_query(wavg_ra, wavg_dec, %s , %s , %s )\" ], params = [ ra , dec , radius_deg ], ) . order_by ( \"distance\" ) )","title":"models.py"},{"location":"reference/models/#vast_pipeline.models.Association","text":"model association between sources and measurements based on some parameters","title":"Association"},{"location":"reference/models/#vast_pipeline.models.Band","text":"A band on the frequency spectrum used for imaging. Each image is associated with one band.","title":"Band"},{"location":"reference/models/#vast_pipeline.models.Comment","text":"The model object for a comment.","title":"Comment"},{"location":"reference/models/#vast_pipeline.models.Comment.get_avatar_url","text":"Get the URL for the user's avatar from GitHub. If the user has no associated GitHub account (e.g. a Django superuser), return the URL to the default user avatar. Returns: Type Description str The avatar URL. Source code in vast_pipeline/models.py def get_avatar_url ( self ) -> str : \"\"\"Get the URL for the user's avatar from GitHub. If the user has no associated GitHub account (e.g. a Django superuser), return the URL to the default user avatar. Returns: The avatar URL. \"\"\" social = UserSocialAuth . get_social_auth_for_user ( self . author ) . first () if social and \"avatar_url\" in social . extra_data : return social . extra_data [ \"avatar_url\" ] else : return static ( \"img/user-32.png\" )","title":"get_avatar_url()"},{"location":"reference/models/#vast_pipeline.models.CommentableModel","text":"A class to provide a commentable model.","title":"CommentableModel"},{"location":"reference/models/#vast_pipeline.models.Image","text":"An image is a 2D radio image from a cube","title":"Image"},{"location":"reference/models/#vast_pipeline.models.Measurement","text":"A Measurement is an object in the sky that has been detected at least once. Essentially a source single measurement in time.","title":"Measurement"},{"location":"reference/models/#vast_pipeline.models.MeasurementPair","text":"MeasurementPair(source_id: int, measurement_a_id: int, measurement_b_id: int, vs_peak: float, m_peak: float, vs_int: float, m_int: float)","title":"MeasurementPair"},{"location":"reference/models/#vast_pipeline.models.MeasurementQuerySet","text":"","title":"MeasurementQuerySet"},{"location":"reference/models/#vast_pipeline.models.MeasurementQuerySet.cone_search","text":"Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Parameters: Name Type Description Default ra float The right ascension value of the cone search central coordinate. required dec float The declination value of the cone search central coordinate. required radius_deg float The radius over which to perform the cone search. required Returns: Type Description QuerySet Measurements found withing the cone search area. Source code in vast_pipeline/models.py def cone_search ( self , ra : float , dec : float , radius_deg : float ) -> models . QuerySet : \"\"\" Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Args: ra: The right ascension value of the cone search central coordinate. dec: The declination value of the cone search central coordinate. radius_deg: The radius over which to perform the cone search. Returns: Measurements found withing the cone search area. \"\"\" return ( self . extra ( select = { \"distance\" : \"q3c_dist(ra, dec, %s , %s ) * 3600\" }, select_params = [ ra , dec ], where = [ \"q3c_radial_query(ra, dec, %s , %s , %s )\" ], params = [ ra , dec , radius_deg ], ) . order_by ( \"distance\" ) )","title":"cone_search()"},{"location":"reference/models/#vast_pipeline.models.RelatedSource","text":"Association table for the many to many Source relationship with itself Django doc https://docs.djangoproject.com/en/3.1/ref/models/fields/#django.db.models.ManyToManyField.through","title":"RelatedSource"},{"location":"reference/models/#vast_pipeline.models.Run","text":"A Run is essentially a pipeline run/processing istance over a set of images","title":"Run"},{"location":"reference/models/#vast_pipeline.models.Run.save","text":"Save the current instance. Override this in a subclass if you want to control the saving process. The 'force_insert' and 'force_update' parameters can be used to insist that the \"save\" must be an SQL insert or update (or equivalent for non-SQL backends), respectively. Normally, they should not be set. Source code in vast_pipeline/models.py def save ( self , * args , ** kwargs ): # enforce the full model validation on save self . full_clean () super ( Run , self ) . save ( * args , ** kwargs )","title":"save()"},{"location":"reference/models/#vast_pipeline.models.RunQuerySet","text":"","title":"RunQuerySet"},{"location":"reference/models/#vast_pipeline.models.RunQuerySet.check_max_runs","text":"Check if number of running pipeline runs is above threshold. Parameters: Name Type Description Default max_runs int The maximum number of processing runs allowed. 5 Returns: Type Description int The count of the current pipeline runs with a status of RUN . Source code in vast_pipeline/models.py def check_max_runs ( self , max_runs : int = 5 ) -> int : \"\"\" Check if number of running pipeline runs is above threshold. Args: max_runs: The maximum number of processing runs allowed. Returns: The count of the current pipeline runs with a status of `RUN`. \"\"\" return self . filter ( status = 'RUN' ) . count () >= max_runs","title":"check_max_runs()"},{"location":"reference/models/#vast_pipeline.models.SkyRegion","text":"SkyRegion(id, centre_ra, centre_dec, width_ra, width_dec, xtr_radius, x, y, z)","title":"SkyRegion"},{"location":"reference/models/#vast_pipeline.models.Source","text":"Source(id, run, name, new, wavg_ra, wavg_dec, wavg_uncertainty_ew, wavg_uncertainty_ns, avg_flux_int, avg_flux_peak, max_flux_peak, min_flux_peak, max_flux_int, min_flux_int, min_flux_int_isl_ratio, min_flux_peak_isl_ratio, avg_compactness, min_snr, max_snr, v_int, v_peak, eta_int, eta_peak, new_high_sigma, n_neighbour_dist, vs_abs_significant_max_int, m_abs_significant_max_int, vs_abs_significant_max_peak, m_abs_significant_max_peak, n_meas, n_meas_sel, n_meas_forced, n_rel, n_sibl)","title":"Source"},{"location":"reference/models/#vast_pipeline.models.SourceFav","text":"SourceFav(id, user, source, comment)","title":"SourceFav"},{"location":"reference/models/#vast_pipeline.models.SourceQuerySet","text":"","title":"SourceQuerySet"},{"location":"reference/models/#vast_pipeline.models.SourceQuerySet.cone_search","text":"Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Parameters: Name Type Description Default ra float The right ascension value of the cone search central coordinate. required dec float The declination value of the cone search central coordinate. required radius_deg float The radius over which to perform the cone search. required Returns: Type Description QuerySet Sources found withing the cone search area. Source code in vast_pipeline/models.py def cone_search ( self , ra : float , dec : float , radius_deg : float ) -> models . QuerySet : \"\"\" Return all the Sources withing radius_deg of (ra,dec). Returns a QuerySet of Sources, ordered by distance from (ra,dec) ascending. Args: ra: The right ascension value of the cone search central coordinate. dec: The declination value of the cone search central coordinate. radius_deg: The radius over which to perform the cone search. Returns: Sources found withing the cone search area. \"\"\" return ( self . extra ( select = { \"distance\" : \"q3c_dist(wavg_ra, wavg_dec, %s , %s ) * 3600\" }, select_params = [ ra , dec ], where = [ \"q3c_radial_query(wavg_ra, wavg_dec, %s , %s , %s )\" ], params = [ ra , dec , radius_deg ], ) . order_by ( \"distance\" ) )","title":"cone_search()"},{"location":"reference/plots/","text":"Contains plotting code used by the web server. fit_eta_v ( df , use_peak_flux = False ) \u00b6 Fits the eta and v distributions with Gaussians. Used from within the 'run_eta_v_analysis' method. Parameters: Name Type Description Default df DataFrame DataFrame containing the sources from the pipeline run. A pandas.core.frame.DataFrame instance. required use_peak_flux bool Use peak fluxes for the analysis instead of integrated fluxes, defaults to 'False'. False Tuple containing the eta_fit_mean, eta_fit_sigma, v_fit_mean and the v_fit_sigma. Source code in vast_pipeline/plots.py def fit_eta_v ( df : pd . DataFrame , use_peak_flux : bool = False ) -> Tuple [ float , float , float , float ]: \"\"\" Fits the eta and v distributions with Gaussians. Used from within the 'run_eta_v_analysis' method. Args: df: DataFrame containing the sources from the pipeline run. A `pandas.core.frame.DataFrame` instance. use_peak_flux: Use peak fluxes for the analysis instead of integrated fluxes, defaults to 'False'. Returns: Tuple containing the eta_fit_mean, eta_fit_sigma, v_fit_mean and the v_fit_sigma. \"\"\" if use_peak_flux : eta_label = 'eta_peak' v_label = 'v_peak' else : eta_label = 'eta_int' v_label = 'v_int' eta_log = np . log10 ( df [ eta_label ]) v_log = np . log10 ( df [ v_label ]) eta_log_clipped = sigma_clip ( eta_log , masked = False , stdfunc = mad_std , sigma = 3 ) v_log_clipped = sigma_clip ( v_log , masked = False , stdfunc = mad_std , sigma = 3 ) eta_fit_mean , eta_fit_sigma = norm . fit ( eta_log_clipped ) v_fit_mean , v_fit_sigma = norm . fit ( v_log_clipped ) return ( eta_fit_mean , eta_fit_sigma , v_fit_mean , v_fit_sigma ) plot_eta_v_bokeh ( source , eta_sigma , v_sigma , use_peak_flux = True ) \u00b6 Adapted from code written by Andrew O'Brien. Produces the eta, V candidates plot (see Rowlinson et al., 2018, https://ui.adsabs.harvard.edu/abs/2019A%26C....27..111R/abstract). Returns a bokeh version. Parameters: Name Type Description Default source Source The source model object containing the result of the query. required eta_sigma float The log10 eta_cutoff from the analysis. required v_sigma float The log10 v_cutoff from the analysis. required use_peak_flux bool Use peak fluxes for the analysis instead of integrated fluxes, defaults to 'True'. True Returns: Type Description <function gridplot at 0x7f87a2fbcf70> Bokeh grid object containing figure. Source code in vast_pipeline/plots.py def plot_eta_v_bokeh ( source : Source , eta_sigma : float , v_sigma : float , use_peak_flux : bool = True ) -> gridplot : \"\"\" Adapted from code written by Andrew O'Brien. Produces the eta, V candidates plot (see Rowlinson et al., 2018, https://ui.adsabs.harvard.edu/abs/2019A%26C....27..111R/abstract). Returns a bokeh version. Args: source: The source model object containing the result of the query. eta_sigma: The log10 eta_cutoff from the analysis. v_sigma: The log10 v_cutoff from the analysis. use_peak_flux: Use peak fluxes for the analysis instead of integrated fluxes, defaults to 'True'. Returns: Bokeh grid object containing figure. \"\"\" df = pd . DataFrame ( source . values ( \"id\" , \"name\" , \"eta_peak\" , \"eta_int\" , \"v_peak\" , \"v_int\" , \"n_meas_sel\" )) ( eta_fit_mean , eta_fit_sigma , v_fit_mean , v_fit_sigma ) = fit_eta_v ( df , use_peak_flux = use_peak_flux ) eta_cutoff_log10 = eta_fit_mean + eta_sigma * eta_fit_sigma v_cutoff_log10 = v_fit_mean + v_sigma * v_fit_sigma eta_cutoff = 10 ** eta_cutoff_log10 v_cutoff = 10 ** v_cutoff_log10 # generate fitted curve data for plotting eta_x = np . linspace ( norm . ppf ( 0.001 , loc = eta_fit_mean , scale = eta_fit_sigma ), norm . ppf ( 0.999 , loc = eta_fit_mean , scale = eta_fit_sigma ), ) eta_y = norm . pdf ( eta_x , loc = eta_fit_mean , scale = eta_fit_sigma ) v_x = np . linspace ( norm . ppf ( 0.001 , loc = v_fit_mean , scale = v_fit_sigma ), norm . ppf ( 0.999 , loc = v_fit_mean , scale = v_fit_sigma ), ) v_y = norm . pdf ( v_x , loc = v_fit_mean , scale = v_fit_sigma ) if use_peak_flux : x_label = 'eta_peak' y_label = 'v_peak' title = 'Peak Flux' else : x_label = 'eta_int' y_label = 'v_int' title = \"Int. Flux\" # PLOTTING NOTE! # Datashader does not play nice with setting the axis to log-log, in fact # it just doesn't work as of writing. # See https://github.com/holoviz/holoviews/issues/2195. # So this is why the actual log10 values are plotted instead on a linear # axis. This could be revisited if the probelm with datashader and # holoviews is resolved. for i in [ x_label , y_label ]: df [ f \" { i } _log10\" ] = np . log10 ( df [ i ]) PLOT_WIDTH = 700 PLOT_HEIGHT = PLOT_WIDTH x_axis_label = \"log \\u03B7 \" y_axis_label = \"log V\" cmap = linear_cmap ( \"n_meas_sel\" , cc . kb , df [ \"n_meas_sel\" ] . min (), df [ \"n_meas_sel\" ] . max (), ) cb_title = \"Number of Selavy Measurements\" if df . shape [ 0 ] > settings . ETA_V_DATASHADER_THRESHOLD : hv . extension ( 'bokeh' ) # create dfs for bokeh and datashader mask = (( df [ x_label ] >= eta_cutoff ) & ( df [ y_label ] >= v_cutoff )) bokeh_df = df . loc [ mask ] ds_df = df . loc [ ~ mask ] # create datashader version first points = spread ( datashade ( hv . Points ( ds_df [[ f \" { x_label } _log10\" , f \" { y_label } _log10\" ]]), cmap = \"Blues\" ), px = 1 , shape = 'square' ) . opts ( height = PLOT_HEIGHT , width = PLOT_WIDTH ) fig = hv . render ( points ) fig . xaxis . axis_label = x_axis_label fig . yaxis . axis_label = y_axis_label fig . aspect_scale = 1 fig . sizing_mode = 'stretch_width' fig . output_backend = \"webgl\" # update the y axis default range if bokeh_df . shape [ 0 ] > 0 : fig . y_range . end = bokeh_df [ f ' { y_label } _log10' ] . max () + 0.2 cb_title += \" (interactive points only)\" else : bokeh_df = df fig = figure ( output_backend = \"webgl\" , plot_width = PLOT_WIDTH , plot_height = PLOT_HEIGHT , aspect_scale = 1 , x_axis_label = x_axis_label , y_axis_label = y_axis_label , sizing_mode = \"stretch_width\" , ) # activate scroll wheel zoom by default fig . toolbar . active_scroll = fig . select_one ( WheelZoomTool ) source = ColumnDataSource ( data = bokeh_df ) bokeh_points = Scatter ( x = f \" { x_label } _log10\" , y = f \" { y_label } _log10\" , fill_color = cmap , line_color = cmap , marker = \"circle\" , size = 5 ) bokeh_g1 = fig . add_glyph ( source_or_glyph = source , glyph = bokeh_points ) hover = HoverTool ( renderers = [ bokeh_g1 ], tooltips = [ ( \"source\" , \"@name\" ), ( \" \\u03B7 \" , f \"@ { x_label } \" ), ( \"V\" , f \"@ { y_label } \" ), ( \"id\" , \"@id\" ) ], mode = 'mouse' ) fig . add_tools ( hover ) color_bar = ColorBar ( color_mapper = cmap [ 'transform' ], title = cb_title ) fig . add_layout ( color_bar , 'below' ) # axis histograms # filter out any forced-phot points for these x_hist = figure ( plot_width = PLOT_WIDTH , plot_height = 100 , x_range = fig . x_range , y_axis_type = None , x_axis_type = \"linear\" , x_axis_location = \"above\" , sizing_mode = \"stretch_width\" , title = \"VAST eta-V {} \" . format ( title ), tools = \"\" , output_backend = \"webgl\" , ) x_hist_data , x_hist_edges = np . histogram ( df [ f \" { x_label } _log10\" ], density = True , bins = 50 , ) x_hist . quad ( top = x_hist_data , bottom = 0 , left = x_hist_edges [: - 1 ], right = x_hist_edges [ 1 :], ) x_hist . line ( eta_x , eta_y , color = \"black\" ) x_hist_sigma_span = Span ( location = eta_cutoff_log10 , dimension = \"height\" , line_color = \"black\" , line_dash = \"dashed\" , ) x_hist . add_layout ( x_hist_sigma_span ) fig . add_layout ( x_hist_sigma_span ) y_hist = figure ( plot_height = PLOT_HEIGHT , plot_width = 100 , y_range = fig . y_range , x_axis_type = None , y_axis_type = \"linear\" , y_axis_location = \"right\" , sizing_mode = \"stretch_height\" , tools = \"\" , output_backend = \"webgl\" , ) y_hist_data , y_hist_edges = np . histogram ( ( df [ f \" { y_label } _log10\" ]), density = True , bins = 50 , ) y_hist . quad ( right = y_hist_data , left = 0 , top = y_hist_edges [: - 1 ], bottom = y_hist_edges [ 1 :], ) y_hist . line ( v_y , v_x , color = \"black\" ) y_hist_sigma_span = Span ( location = v_cutoff_log10 , dimension = \"width\" , line_color = \"black\" , line_dash = \"dashed\" , ) y_hist . add_layout ( y_hist_sigma_span ) fig . add_layout ( y_hist_sigma_span ) variable_region = BoxAnnotation ( left = eta_cutoff_log10 , bottom = v_cutoff_log10 , fill_color = \"orange\" , fill_alpha = 0.3 , level = \"underlay\" , ) fig . add_layout ( variable_region ) eta_slider = Slider ( start = 0 , end = 10 , step = 0.1 , value = eta_sigma , title = \" \\u03B7 sigma value\" , sizing_mode = 'stretch_width' ) v_slider = Slider ( start = 0 , end = 10 , step = 0.1 , value = v_sigma , title = \"V sigma value\" , sizing_mode = 'stretch_width' ) labels = [ 'Peak' , 'Integrated' ] active = 0 if use_peak_flux else 1 flux_choice_radio = RadioButtonGroup ( labels = labels , active = active , sizing_mode = 'stretch_width' ) button = Button ( label = \"Apply\" , button_type = \"primary\" , sizing_mode = 'stretch_width' ) button . js_on_click ( CustomJS ( args = dict ( eta_slider = eta_slider , v_slider = v_slider , button = button , flux_choice_radio = flux_choice_radio ), code = \"\"\" button.label = \"Loading...\" var e = eta_slider.value; var v = v_slider.value; const peak = [\"peak\", \"int\"]; var fluxType = peak[flux_choice_radio.active]; getEtaVPlot(e, v, fluxType); \"\"\" ) ) grid = gridplot ( [ [ x_hist , Spacer ( width = 100 , height = 100 )], [ fig , y_hist ], ] ) plot_column = column ( grid , flux_choice_radio , eta_slider , v_slider , button , sizing_mode = 'stretch_width' ) plot_column . css_classes . append ( \"mx-auto\" ) source = ColumnDataSource ( data = bokeh_df ) callback = CustomJS ( args = dict ( source = source , flux_choice_radio = flux_choice_radio ), code = \"\"\" const d1 = source.data; const i = cb_data.source.selected.indices[0]; const id = d1['id'][i]; const peak = [\"peak\", \"int\"]; var fluxType = peak[flux_choice_radio.active]; $(document).ready(function () { update_card(id); getLightcurvePlot(id, fluxType); }); \"\"\" ) tap = TapTool ( callback = callback , renderers = [ bokeh_g1 ]) fig . tools . append ( tap ) plot_row = row ( plot_column , sizing_mode = \"stretch_width\" ) plot_row . css_classes . append ( \"mx-auto\" ) return plot_row plot_lightcurve ( source , vs_abs_min = 4.3 , m_abs_min = 0.26 , use_peak_flux = True ) \u00b6 Create the lightcurve and 2-epoch metric graph for a source with Bokeh. Parameters: Name Type Description Default source Source Source object. required vs_abs_min float pairs of Measurement objects with an absolute vs metric greater than vs_abs_min and m metric greater than m_abs_min will be connected in the metric graph. Defaults to 4.3. 4.3 m_abs_min float See vs_abs_min . Defaults to 0.26. 0.26 use_peak_flux bool If True, use peak fluxes, otherwise use integrated fluxes. Defaults to True. True Returns: Type Description Row Row: Bokeh Row layout object containing the lightcurve and graph plots. Source code in vast_pipeline/plots.py def plot_lightcurve ( source : Source , vs_abs_min : float = 4.3 , m_abs_min : float = 0.26 , use_peak_flux : bool = True , ) -> Row : \"\"\"Create the lightcurve and 2-epoch metric graph for a source with Bokeh. Args: source (Source): Source object. vs_abs_min (float, optional): pairs of Measurement objects with an absolute vs metric greater than `vs_abs_min` and m metric greater than `m_abs_min` will be connected in the metric graph. Defaults to 4.3. m_abs_min (float, optional): See `vs_abs_min`. Defaults to 0.26. use_peak_flux (bool, optional): If True, use peak fluxes, otherwise use integrated fluxes. Defaults to True. Returns: Row: Bokeh Row layout object containing the lightcurve and graph plots. \"\"\" PLOT_WIDTH = 800 PLOT_HEIGHT = 300 flux_column = \"flux_peak\" if use_peak_flux else \"flux_int\" metric_suffix = \"peak\" if use_peak_flux else \"int\" measurements_qs = ( Measurement . objects . filter ( source__id = source . id ) . annotate ( taustart_ts = F ( \"image__datetime\" ), flux = F ( flux_column ), flux_err_lower = F ( flux_column ) - F ( f \" { flux_column } _err\" ), flux_err_upper = F ( flux_column ) + F ( f \" { flux_column } _err\" ), ) . values ( \"id\" , \"pk\" , \"taustart_ts\" , \"flux\" , \"flux_err_upper\" , \"flux_err_lower\" , \"forced\" , \"name\" ) . order_by ( \"taustart_ts\" ) ) # lightcurve required cols: taustart_ts, flux, flux_err_upper, flux_err_lower, forced lightcurve = pd . DataFrame ( measurements_qs ) # remap method values to labels to make a better legend lightcurve [ \"method\" ] = lightcurve . forced . map ( { True : \"Forced\" , False : \"Selavy\" } ) lightcurve [ 'cutout' ] = lightcurve [ 'id' ] . apply ( lambda x : f '/cutout/ { x } /normal/?img_type=png' ) lc_source = ColumnDataSource ( lightcurve ) method_mapper = factor_cmap ( \"method\" , palette = \"Colorblind3\" , factors = [ \"Selavy\" , \"Forced\" ] ) min_y = min ( 0 , lightcurve . flux_err_lower . min ()) max_y = lightcurve . flux_err_upper . max () y_padding = ( max_y - min_y ) * 0.1 fig_lc = figure ( plot_width = PLOT_WIDTH , plot_height = PLOT_HEIGHT , sizing_mode = \"stretch_width\" , x_axis_type = \"datetime\" , x_range = DataRange1d ( default_span = timedelta ( days = 1 )), y_range = DataRange1d ( start = min_y , end = max_y + y_padding ), ) # line source must be a COPY of the data for the scatter source for the hover and # selection to work properly, using the same ColumnDataSource will break it fig_lc . line ( \"taustart_ts\" , \"flux\" , source = lightcurve ) lc_scatter = fig_lc . scatter ( \"taustart_ts\" , \"flux\" , marker = \"circle\" , size = 6 , color = method_mapper , nonselection_color = method_mapper , selection_color = \"red\" , nonselection_alpha = 1.0 , hover_color = \"red\" , alpha = 1.0 , source = lc_source , legend_group = \"method\" , ) fig_lc . add_layout ( Whisker ( base = \"taustart_ts\" , upper = \"flux_err_upper\" , lower = \"flux_err_lower\" , source = lc_source , ) ) fig_lc . xaxis . axis_label = \"Datetime\" fig_lc . xaxis [ 0 ] . formatter = DatetimeTickFormatter ( days = \" %F \" , hours = '%H:%M' ) fig_lc . yaxis . axis_label = ( \"Peak flux (mJy/beam)\" if use_peak_flux else \"Integrated flux (mJy)\" ) # determine legend location: either bottom_left or top_left legend_location = ( \"top_left\" if lightcurve . sort_values ( \"taustart_ts\" ) . iloc [ 0 ] . flux < ( max_y - min_y ) / 2 else \"bottom_left\" ) fig_lc . legend . location = legend_location # TODO add vs and m metrics to graph edges # create plot fig_graph = figure ( plot_width = PLOT_HEIGHT , plot_height = PLOT_HEIGHT , x_range = Range1d ( - 1.1 , 1.1 ), y_range = Range1d ( - 1.1 , 1.1 ), x_axis_type = None , y_axis_type = None , sizing_mode = \"fixed\" , ) hover_tool_lc_callback = None measurement_pairs = source . get_measurement_pairs () if len ( measurement_pairs ) > 0 : candidate_measurement_pairs_df = pd . DataFrame ( measurement_pairs ) . query ( f \"m_ { metric_suffix } .abs() >= { m_abs_min } and vs_ { metric_suffix } .abs() >= { vs_abs_min } \" ) . reset_index () g = nx . Graph () for _row in candidate_measurement_pairs_df . itertuples ( index = False ): g . add_edge ( _row . measurement_a_id , _row . measurement_b_id ) node_layout = nx . circular_layout ( g , scale = 1 , center = ( 0 , 0 )) # add node positions to dataframe for suffix in [ \"a\" , \"b\" ]: pos_df = pd . DataFrame ( candidate_measurement_pairs_df [ f \"measurement_ { suffix } _id\" ] . map ( node_layout ) . to_list (), columns = [ f \"measurement_ { suffix } _x\" , f \"measurement_ { suffix } _y\" ], ) candidate_measurement_pairs_df = candidate_measurement_pairs_df . join ( pos_df ) candidate_measurement_pairs_df [ \"measurement_x\" ] = list ( zip ( candidate_measurement_pairs_df . measurement_a_x . values , candidate_measurement_pairs_df . measurement_b_x . values , ) ) candidate_measurement_pairs_df [ \"measurement_y\" ] = list ( zip ( candidate_measurement_pairs_df . measurement_a_y . values , candidate_measurement_pairs_df . measurement_b_y . values , ) ) node_positions_df = pd . DataFrame . from_dict ( node_layout , orient = \"index\" , columns = [ \"x\" , \"y\" ] ) node_positions_df [ \"lc_index\" ] = node_positions_df . index . map ( { v : k for k , v in lightcurve . id . to_dict () . items ()} ) . values node_source = ColumnDataSource ( node_positions_df ) edge_source = ColumnDataSource ( candidate_measurement_pairs_df ) # add edges to plot edge_renderer = fig_graph . multi_line ( \"measurement_x\" , \"measurement_y\" , line_width = 5 , hover_color = \"red\" , source = edge_source , name = \"edges\" , ) # add nodes to plot node_renderer = fig_graph . circle ( \"x\" , \"y\" , size = 20 , hover_color = \"red\" , selection_color = \"red\" , nonselection_alpha = 1.0 , source = node_source , name = \"nodes\" , ) # create hover tool for node edges edge_callback_code = \"\"\" // get edge index let indices_a = cb_data.index.indices.map(i => edge_data.data.measurement_a_id[i]); let indices_b = cb_data.index.indices.map(i => edge_data.data.measurement_b_id[i]); let indices = indices_a.concat(indices_b); let lightcurve_indices = indices.map(i => lightcurve_data.data.id.indexOf(i)); lightcurve_data.selected.indices = lightcurve_indices; \"\"\" hover_tool_edges = HoverTool ( tooltips = [ ( f \"Vs { metric_suffix } \" , f \"@vs_ { metric_suffix } \" ), ( f \"m { metric_suffix } \" , f \"@m_ { metric_suffix } \" ), ], renderers = [ edge_renderer ], callback = CustomJS ( args = { \"lightcurve_data\" : lc_scatter . data_source , \"edge_data\" : edge_renderer . data_source , }, code = edge_callback_code , ), ) fig_graph . add_tools ( hover_tool_edges ) # create labels for nodes graph_source = ColumnDataSource ( node_positions_df ) labels = LabelSet ( x = \"x\" , y = \"y\" , text = \"lc_index\" , source = graph_source , text_align = \"center\" , text_baseline = \"middle\" , text_font_size = \"1em\" , text_color = \"white\" , ) fig_graph . renderers . append ( labels ) # prepare a JS callback for the lightcurve hover tool to mark the associated nodes hover_tool_lc_callback = CustomJS ( args = { \"node_data\" : node_renderer . data_source , \"lightcurve_data\" : lc_scatter . data_source , }, code = \"\"\" let ids = cb_data.index.indices.map(i => lightcurve_data.data.id[i]); let node_indices = ids.map(i => node_data.data.index.indexOf(i)); node_data.selected.indices = node_indices; \"\"\" , ) # create hover tool for lightcurve hover_tool_lc = HoverTool ( # tooltips=[ # (\"Index\", \"@index\"), # (\"Date\", \"@taustart_ts{%F}\"), # (f\"Flux {metric_suffix}\", \"@flux mJy\"), # ('Cutout', \"@cutout\") # ], tooltips = \"\"\" <div style=\"width:200;\"> <div> <img src=@cutout height=\"100\" alt=@cutout width=\"100\" style=\"float: left; margin: 0px 15px 15px 0px;\" border=\"2\" ></img> </div> <div> <div style=\"font-size: 12px; font-weight: bold;\">Date: </div> <div style=\"font-size: 12px; color: #966;\">@taustart_ts{ %F }</div> </div> <div> <div style=\"font-size: 12px; font-weight: bold;\">Flux:</div> <div style=\"font-size: 12px; color: #966;\">@flux mJy</div> </div> <div> <div style=\"font-size: 12px; font-weight: bold;\">Index:</div> <div style=\"font-size: 12px; color: #966;\">@index</div> </div> </div> \"\"\" , formatters = { \"@taustart_ts\" : \"datetime\" , }, mode = \"vline\" , callback = hover_tool_lc_callback , ) fig_lc . add_tools ( hover_tool_lc ) plot_row = row ( fig_lc , fig_graph , sizing_mode = \"stretch_width\" ) plot_row . css_classes . append ( \"mx-auto\" ) return plot_row","title":"plots.py"},{"location":"reference/plots/#vast_pipeline.plots.fit_eta_v","text":"Fits the eta and v distributions with Gaussians. Used from within the 'run_eta_v_analysis' method. Parameters: Name Type Description Default df DataFrame DataFrame containing the sources from the pipeline run. A pandas.core.frame.DataFrame instance. required use_peak_flux bool Use peak fluxes for the analysis instead of integrated fluxes, defaults to 'False'. False Tuple containing the eta_fit_mean, eta_fit_sigma, v_fit_mean and the v_fit_sigma. Source code in vast_pipeline/plots.py def fit_eta_v ( df : pd . DataFrame , use_peak_flux : bool = False ) -> Tuple [ float , float , float , float ]: \"\"\" Fits the eta and v distributions with Gaussians. Used from within the 'run_eta_v_analysis' method. Args: df: DataFrame containing the sources from the pipeline run. A `pandas.core.frame.DataFrame` instance. use_peak_flux: Use peak fluxes for the analysis instead of integrated fluxes, defaults to 'False'. Returns: Tuple containing the eta_fit_mean, eta_fit_sigma, v_fit_mean and the v_fit_sigma. \"\"\" if use_peak_flux : eta_label = 'eta_peak' v_label = 'v_peak' else : eta_label = 'eta_int' v_label = 'v_int' eta_log = np . log10 ( df [ eta_label ]) v_log = np . log10 ( df [ v_label ]) eta_log_clipped = sigma_clip ( eta_log , masked = False , stdfunc = mad_std , sigma = 3 ) v_log_clipped = sigma_clip ( v_log , masked = False , stdfunc = mad_std , sigma = 3 ) eta_fit_mean , eta_fit_sigma = norm . fit ( eta_log_clipped ) v_fit_mean , v_fit_sigma = norm . fit ( v_log_clipped ) return ( eta_fit_mean , eta_fit_sigma , v_fit_mean , v_fit_sigma )","title":"fit_eta_v()"},{"location":"reference/plots/#vast_pipeline.plots.plot_eta_v_bokeh","text":"Adapted from code written by Andrew O'Brien. Produces the eta, V candidates plot (see Rowlinson et al., 2018, https://ui.adsabs.harvard.edu/abs/2019A%26C....27..111R/abstract). Returns a bokeh version. Parameters: Name Type Description Default source Source The source model object containing the result of the query. required eta_sigma float The log10 eta_cutoff from the analysis. required v_sigma float The log10 v_cutoff from the analysis. required use_peak_flux bool Use peak fluxes for the analysis instead of integrated fluxes, defaults to 'True'. True Returns: Type Description <function gridplot at 0x7f87a2fbcf70> Bokeh grid object containing figure. Source code in vast_pipeline/plots.py def plot_eta_v_bokeh ( source : Source , eta_sigma : float , v_sigma : float , use_peak_flux : bool = True ) -> gridplot : \"\"\" Adapted from code written by Andrew O'Brien. Produces the eta, V candidates plot (see Rowlinson et al., 2018, https://ui.adsabs.harvard.edu/abs/2019A%26C....27..111R/abstract). Returns a bokeh version. Args: source: The source model object containing the result of the query. eta_sigma: The log10 eta_cutoff from the analysis. v_sigma: The log10 v_cutoff from the analysis. use_peak_flux: Use peak fluxes for the analysis instead of integrated fluxes, defaults to 'True'. Returns: Bokeh grid object containing figure. \"\"\" df = pd . DataFrame ( source . values ( \"id\" , \"name\" , \"eta_peak\" , \"eta_int\" , \"v_peak\" , \"v_int\" , \"n_meas_sel\" )) ( eta_fit_mean , eta_fit_sigma , v_fit_mean , v_fit_sigma ) = fit_eta_v ( df , use_peak_flux = use_peak_flux ) eta_cutoff_log10 = eta_fit_mean + eta_sigma * eta_fit_sigma v_cutoff_log10 = v_fit_mean + v_sigma * v_fit_sigma eta_cutoff = 10 ** eta_cutoff_log10 v_cutoff = 10 ** v_cutoff_log10 # generate fitted curve data for plotting eta_x = np . linspace ( norm . ppf ( 0.001 , loc = eta_fit_mean , scale = eta_fit_sigma ), norm . ppf ( 0.999 , loc = eta_fit_mean , scale = eta_fit_sigma ), ) eta_y = norm . pdf ( eta_x , loc = eta_fit_mean , scale = eta_fit_sigma ) v_x = np . linspace ( norm . ppf ( 0.001 , loc = v_fit_mean , scale = v_fit_sigma ), norm . ppf ( 0.999 , loc = v_fit_mean , scale = v_fit_sigma ), ) v_y = norm . pdf ( v_x , loc = v_fit_mean , scale = v_fit_sigma ) if use_peak_flux : x_label = 'eta_peak' y_label = 'v_peak' title = 'Peak Flux' else : x_label = 'eta_int' y_label = 'v_int' title = \"Int. Flux\" # PLOTTING NOTE! # Datashader does not play nice with setting the axis to log-log, in fact # it just doesn't work as of writing. # See https://github.com/holoviz/holoviews/issues/2195. # So this is why the actual log10 values are plotted instead on a linear # axis. This could be revisited if the probelm with datashader and # holoviews is resolved. for i in [ x_label , y_label ]: df [ f \" { i } _log10\" ] = np . log10 ( df [ i ]) PLOT_WIDTH = 700 PLOT_HEIGHT = PLOT_WIDTH x_axis_label = \"log \\u03B7 \" y_axis_label = \"log V\" cmap = linear_cmap ( \"n_meas_sel\" , cc . kb , df [ \"n_meas_sel\" ] . min (), df [ \"n_meas_sel\" ] . max (), ) cb_title = \"Number of Selavy Measurements\" if df . shape [ 0 ] > settings . ETA_V_DATASHADER_THRESHOLD : hv . extension ( 'bokeh' ) # create dfs for bokeh and datashader mask = (( df [ x_label ] >= eta_cutoff ) & ( df [ y_label ] >= v_cutoff )) bokeh_df = df . loc [ mask ] ds_df = df . loc [ ~ mask ] # create datashader version first points = spread ( datashade ( hv . Points ( ds_df [[ f \" { x_label } _log10\" , f \" { y_label } _log10\" ]]), cmap = \"Blues\" ), px = 1 , shape = 'square' ) . opts ( height = PLOT_HEIGHT , width = PLOT_WIDTH ) fig = hv . render ( points ) fig . xaxis . axis_label = x_axis_label fig . yaxis . axis_label = y_axis_label fig . aspect_scale = 1 fig . sizing_mode = 'stretch_width' fig . output_backend = \"webgl\" # update the y axis default range if bokeh_df . shape [ 0 ] > 0 : fig . y_range . end = bokeh_df [ f ' { y_label } _log10' ] . max () + 0.2 cb_title += \" (interactive points only)\" else : bokeh_df = df fig = figure ( output_backend = \"webgl\" , plot_width = PLOT_WIDTH , plot_height = PLOT_HEIGHT , aspect_scale = 1 , x_axis_label = x_axis_label , y_axis_label = y_axis_label , sizing_mode = \"stretch_width\" , ) # activate scroll wheel zoom by default fig . toolbar . active_scroll = fig . select_one ( WheelZoomTool ) source = ColumnDataSource ( data = bokeh_df ) bokeh_points = Scatter ( x = f \" { x_label } _log10\" , y = f \" { y_label } _log10\" , fill_color = cmap , line_color = cmap , marker = \"circle\" , size = 5 ) bokeh_g1 = fig . add_glyph ( source_or_glyph = source , glyph = bokeh_points ) hover = HoverTool ( renderers = [ bokeh_g1 ], tooltips = [ ( \"source\" , \"@name\" ), ( \" \\u03B7 \" , f \"@ { x_label } \" ), ( \"V\" , f \"@ { y_label } \" ), ( \"id\" , \"@id\" ) ], mode = 'mouse' ) fig . add_tools ( hover ) color_bar = ColorBar ( color_mapper = cmap [ 'transform' ], title = cb_title ) fig . add_layout ( color_bar , 'below' ) # axis histograms # filter out any forced-phot points for these x_hist = figure ( plot_width = PLOT_WIDTH , plot_height = 100 , x_range = fig . x_range , y_axis_type = None , x_axis_type = \"linear\" , x_axis_location = \"above\" , sizing_mode = \"stretch_width\" , title = \"VAST eta-V {} \" . format ( title ), tools = \"\" , output_backend = \"webgl\" , ) x_hist_data , x_hist_edges = np . histogram ( df [ f \" { x_label } _log10\" ], density = True , bins = 50 , ) x_hist . quad ( top = x_hist_data , bottom = 0 , left = x_hist_edges [: - 1 ], right = x_hist_edges [ 1 :], ) x_hist . line ( eta_x , eta_y , color = \"black\" ) x_hist_sigma_span = Span ( location = eta_cutoff_log10 , dimension = \"height\" , line_color = \"black\" , line_dash = \"dashed\" , ) x_hist . add_layout ( x_hist_sigma_span ) fig . add_layout ( x_hist_sigma_span ) y_hist = figure ( plot_height = PLOT_HEIGHT , plot_width = 100 , y_range = fig . y_range , x_axis_type = None , y_axis_type = \"linear\" , y_axis_location = \"right\" , sizing_mode = \"stretch_height\" , tools = \"\" , output_backend = \"webgl\" , ) y_hist_data , y_hist_edges = np . histogram ( ( df [ f \" { y_label } _log10\" ]), density = True , bins = 50 , ) y_hist . quad ( right = y_hist_data , left = 0 , top = y_hist_edges [: - 1 ], bottom = y_hist_edges [ 1 :], ) y_hist . line ( v_y , v_x , color = \"black\" ) y_hist_sigma_span = Span ( location = v_cutoff_log10 , dimension = \"width\" , line_color = \"black\" , line_dash = \"dashed\" , ) y_hist . add_layout ( y_hist_sigma_span ) fig . add_layout ( y_hist_sigma_span ) variable_region = BoxAnnotation ( left = eta_cutoff_log10 , bottom = v_cutoff_log10 , fill_color = \"orange\" , fill_alpha = 0.3 , level = \"underlay\" , ) fig . add_layout ( variable_region ) eta_slider = Slider ( start = 0 , end = 10 , step = 0.1 , value = eta_sigma , title = \" \\u03B7 sigma value\" , sizing_mode = 'stretch_width' ) v_slider = Slider ( start = 0 , end = 10 , step = 0.1 , value = v_sigma , title = \"V sigma value\" , sizing_mode = 'stretch_width' ) labels = [ 'Peak' , 'Integrated' ] active = 0 if use_peak_flux else 1 flux_choice_radio = RadioButtonGroup ( labels = labels , active = active , sizing_mode = 'stretch_width' ) button = Button ( label = \"Apply\" , button_type = \"primary\" , sizing_mode = 'stretch_width' ) button . js_on_click ( CustomJS ( args = dict ( eta_slider = eta_slider , v_slider = v_slider , button = button , flux_choice_radio = flux_choice_radio ), code = \"\"\" button.label = \"Loading...\" var e = eta_slider.value; var v = v_slider.value; const peak = [\"peak\", \"int\"]; var fluxType = peak[flux_choice_radio.active]; getEtaVPlot(e, v, fluxType); \"\"\" ) ) grid = gridplot ( [ [ x_hist , Spacer ( width = 100 , height = 100 )], [ fig , y_hist ], ] ) plot_column = column ( grid , flux_choice_radio , eta_slider , v_slider , button , sizing_mode = 'stretch_width' ) plot_column . css_classes . append ( \"mx-auto\" ) source = ColumnDataSource ( data = bokeh_df ) callback = CustomJS ( args = dict ( source = source , flux_choice_radio = flux_choice_radio ), code = \"\"\" const d1 = source.data; const i = cb_data.source.selected.indices[0]; const id = d1['id'][i]; const peak = [\"peak\", \"int\"]; var fluxType = peak[flux_choice_radio.active]; $(document).ready(function () { update_card(id); getLightcurvePlot(id, fluxType); }); \"\"\" ) tap = TapTool ( callback = callback , renderers = [ bokeh_g1 ]) fig . tools . append ( tap ) plot_row = row ( plot_column , sizing_mode = \"stretch_width\" ) plot_row . css_classes . append ( \"mx-auto\" ) return plot_row","title":"plot_eta_v_bokeh()"},{"location":"reference/plots/#vast_pipeline.plots.plot_lightcurve","text":"Create the lightcurve and 2-epoch metric graph for a source with Bokeh. Parameters: Name Type Description Default source Source Source object. required vs_abs_min float pairs of Measurement objects with an absolute vs metric greater than vs_abs_min and m metric greater than m_abs_min will be connected in the metric graph. Defaults to 4.3. 4.3 m_abs_min float See vs_abs_min . Defaults to 0.26. 0.26 use_peak_flux bool If True, use peak fluxes, otherwise use integrated fluxes. Defaults to True. True Returns: Type Description Row Row: Bokeh Row layout object containing the lightcurve and graph plots. Source code in vast_pipeline/plots.py def plot_lightcurve ( source : Source , vs_abs_min : float = 4.3 , m_abs_min : float = 0.26 , use_peak_flux : bool = True , ) -> Row : \"\"\"Create the lightcurve and 2-epoch metric graph for a source with Bokeh. Args: source (Source): Source object. vs_abs_min (float, optional): pairs of Measurement objects with an absolute vs metric greater than `vs_abs_min` and m metric greater than `m_abs_min` will be connected in the metric graph. Defaults to 4.3. m_abs_min (float, optional): See `vs_abs_min`. Defaults to 0.26. use_peak_flux (bool, optional): If True, use peak fluxes, otherwise use integrated fluxes. Defaults to True. Returns: Row: Bokeh Row layout object containing the lightcurve and graph plots. \"\"\" PLOT_WIDTH = 800 PLOT_HEIGHT = 300 flux_column = \"flux_peak\" if use_peak_flux else \"flux_int\" metric_suffix = \"peak\" if use_peak_flux else \"int\" measurements_qs = ( Measurement . objects . filter ( source__id = source . id ) . annotate ( taustart_ts = F ( \"image__datetime\" ), flux = F ( flux_column ), flux_err_lower = F ( flux_column ) - F ( f \" { flux_column } _err\" ), flux_err_upper = F ( flux_column ) + F ( f \" { flux_column } _err\" ), ) . values ( \"id\" , \"pk\" , \"taustart_ts\" , \"flux\" , \"flux_err_upper\" , \"flux_err_lower\" , \"forced\" , \"name\" ) . order_by ( \"taustart_ts\" ) ) # lightcurve required cols: taustart_ts, flux, flux_err_upper, flux_err_lower, forced lightcurve = pd . DataFrame ( measurements_qs ) # remap method values to labels to make a better legend lightcurve [ \"method\" ] = lightcurve . forced . map ( { True : \"Forced\" , False : \"Selavy\" } ) lightcurve [ 'cutout' ] = lightcurve [ 'id' ] . apply ( lambda x : f '/cutout/ { x } /normal/?img_type=png' ) lc_source = ColumnDataSource ( lightcurve ) method_mapper = factor_cmap ( \"method\" , palette = \"Colorblind3\" , factors = [ \"Selavy\" , \"Forced\" ] ) min_y = min ( 0 , lightcurve . flux_err_lower . min ()) max_y = lightcurve . flux_err_upper . max () y_padding = ( max_y - min_y ) * 0.1 fig_lc = figure ( plot_width = PLOT_WIDTH , plot_height = PLOT_HEIGHT , sizing_mode = \"stretch_width\" , x_axis_type = \"datetime\" , x_range = DataRange1d ( default_span = timedelta ( days = 1 )), y_range = DataRange1d ( start = min_y , end = max_y + y_padding ), ) # line source must be a COPY of the data for the scatter source for the hover and # selection to work properly, using the same ColumnDataSource will break it fig_lc . line ( \"taustart_ts\" , \"flux\" , source = lightcurve ) lc_scatter = fig_lc . scatter ( \"taustart_ts\" , \"flux\" , marker = \"circle\" , size = 6 , color = method_mapper , nonselection_color = method_mapper , selection_color = \"red\" , nonselection_alpha = 1.0 , hover_color = \"red\" , alpha = 1.0 , source = lc_source , legend_group = \"method\" , ) fig_lc . add_layout ( Whisker ( base = \"taustart_ts\" , upper = \"flux_err_upper\" , lower = \"flux_err_lower\" , source = lc_source , ) ) fig_lc . xaxis . axis_label = \"Datetime\" fig_lc . xaxis [ 0 ] . formatter = DatetimeTickFormatter ( days = \" %F \" , hours = '%H:%M' ) fig_lc . yaxis . axis_label = ( \"Peak flux (mJy/beam)\" if use_peak_flux else \"Integrated flux (mJy)\" ) # determine legend location: either bottom_left or top_left legend_location = ( \"top_left\" if lightcurve . sort_values ( \"taustart_ts\" ) . iloc [ 0 ] . flux < ( max_y - min_y ) / 2 else \"bottom_left\" ) fig_lc . legend . location = legend_location # TODO add vs and m metrics to graph edges # create plot fig_graph = figure ( plot_width = PLOT_HEIGHT , plot_height = PLOT_HEIGHT , x_range = Range1d ( - 1.1 , 1.1 ), y_range = Range1d ( - 1.1 , 1.1 ), x_axis_type = None , y_axis_type = None , sizing_mode = \"fixed\" , ) hover_tool_lc_callback = None measurement_pairs = source . get_measurement_pairs () if len ( measurement_pairs ) > 0 : candidate_measurement_pairs_df = pd . DataFrame ( measurement_pairs ) . query ( f \"m_ { metric_suffix } .abs() >= { m_abs_min } and vs_ { metric_suffix } .abs() >= { vs_abs_min } \" ) . reset_index () g = nx . Graph () for _row in candidate_measurement_pairs_df . itertuples ( index = False ): g . add_edge ( _row . measurement_a_id , _row . measurement_b_id ) node_layout = nx . circular_layout ( g , scale = 1 , center = ( 0 , 0 )) # add node positions to dataframe for suffix in [ \"a\" , \"b\" ]: pos_df = pd . DataFrame ( candidate_measurement_pairs_df [ f \"measurement_ { suffix } _id\" ] . map ( node_layout ) . to_list (), columns = [ f \"measurement_ { suffix } _x\" , f \"measurement_ { suffix } _y\" ], ) candidate_measurement_pairs_df = candidate_measurement_pairs_df . join ( pos_df ) candidate_measurement_pairs_df [ \"measurement_x\" ] = list ( zip ( candidate_measurement_pairs_df . measurement_a_x . values , candidate_measurement_pairs_df . measurement_b_x . values , ) ) candidate_measurement_pairs_df [ \"measurement_y\" ] = list ( zip ( candidate_measurement_pairs_df . measurement_a_y . values , candidate_measurement_pairs_df . measurement_b_y . values , ) ) node_positions_df = pd . DataFrame . from_dict ( node_layout , orient = \"index\" , columns = [ \"x\" , \"y\" ] ) node_positions_df [ \"lc_index\" ] = node_positions_df . index . map ( { v : k for k , v in lightcurve . id . to_dict () . items ()} ) . values node_source = ColumnDataSource ( node_positions_df ) edge_source = ColumnDataSource ( candidate_measurement_pairs_df ) # add edges to plot edge_renderer = fig_graph . multi_line ( \"measurement_x\" , \"measurement_y\" , line_width = 5 , hover_color = \"red\" , source = edge_source , name = \"edges\" , ) # add nodes to plot node_renderer = fig_graph . circle ( \"x\" , \"y\" , size = 20 , hover_color = \"red\" , selection_color = \"red\" , nonselection_alpha = 1.0 , source = node_source , name = \"nodes\" , ) # create hover tool for node edges edge_callback_code = \"\"\" // get edge index let indices_a = cb_data.index.indices.map(i => edge_data.data.measurement_a_id[i]); let indices_b = cb_data.index.indices.map(i => edge_data.data.measurement_b_id[i]); let indices = indices_a.concat(indices_b); let lightcurve_indices = indices.map(i => lightcurve_data.data.id.indexOf(i)); lightcurve_data.selected.indices = lightcurve_indices; \"\"\" hover_tool_edges = HoverTool ( tooltips = [ ( f \"Vs { metric_suffix } \" , f \"@vs_ { metric_suffix } \" ), ( f \"m { metric_suffix } \" , f \"@m_ { metric_suffix } \" ), ], renderers = [ edge_renderer ], callback = CustomJS ( args = { \"lightcurve_data\" : lc_scatter . data_source , \"edge_data\" : edge_renderer . data_source , }, code = edge_callback_code , ), ) fig_graph . add_tools ( hover_tool_edges ) # create labels for nodes graph_source = ColumnDataSource ( node_positions_df ) labels = LabelSet ( x = \"x\" , y = \"y\" , text = \"lc_index\" , source = graph_source , text_align = \"center\" , text_baseline = \"middle\" , text_font_size = \"1em\" , text_color = \"white\" , ) fig_graph . renderers . append ( labels ) # prepare a JS callback for the lightcurve hover tool to mark the associated nodes hover_tool_lc_callback = CustomJS ( args = { \"node_data\" : node_renderer . data_source , \"lightcurve_data\" : lc_scatter . data_source , }, code = \"\"\" let ids = cb_data.index.indices.map(i => lightcurve_data.data.id[i]); let node_indices = ids.map(i => node_data.data.index.indexOf(i)); node_data.selected.indices = node_indices; \"\"\" , ) # create hover tool for lightcurve hover_tool_lc = HoverTool ( # tooltips=[ # (\"Index\", \"@index\"), # (\"Date\", \"@taustart_ts{%F}\"), # (f\"Flux {metric_suffix}\", \"@flux mJy\"), # ('Cutout', \"@cutout\") # ], tooltips = \"\"\" <div style=\"width:200;\"> <div> <img src=@cutout height=\"100\" alt=@cutout width=\"100\" style=\"float: left; margin: 0px 15px 15px 0px;\" border=\"2\" ></img> </div> <div> <div style=\"font-size: 12px; font-weight: bold;\">Date: </div> <div style=\"font-size: 12px; color: #966;\">@taustart_ts{ %F }</div> </div> <div> <div style=\"font-size: 12px; font-weight: bold;\">Flux:</div> <div style=\"font-size: 12px; color: #966;\">@flux mJy</div> </div> <div> <div style=\"font-size: 12px; font-weight: bold;\">Index:</div> <div style=\"font-size: 12px; color: #966;\">@index</div> </div> </div> \"\"\" , formatters = { \"@taustart_ts\" : \"datetime\" , }, mode = \"vline\" , callback = hover_tool_lc_callback , ) fig_lc . add_tools ( hover_tool_lc ) plot_row = row ( fig_lc , fig_graph , sizing_mode = \"stretch_width\" ) plot_row . css_classes . append ( \"mx-auto\" ) return plot_row","title":"plot_lightcurve()"},{"location":"reference/signals/","text":"Functions that are executed upon receiving an application signal. delete_orphans_for_run ( sender , instance , using , ** kwargs ) \u00b6 Delete any Image and SkyRegion objects that would be orphaned by deleting the given Run. Expects to recieve the arguments sent by the pre_delete signal. See https://docs.djangoproject.com/en/3.1/ref/signals/#pre-delete . Parameters: Name Type Description Default sender Type[vast_pipeline.models.Run] Model class that sent the signal. required instance Run Model instance to be deleted. required using str Database alias. required Source code in vast_pipeline/signals.py @receiver ( pre_delete , sender = Run , dispatch_uid = \"delete_orphans_for_run\" ) def delete_orphans_for_run ( sender : Type [ Run ], instance : Run , using : str , ** kwargs ) -> None : \"\"\"Delete any Image and SkyRegion objects that would be orphaned by deleting the given Run. Expects to recieve the arguments sent by the pre_delete signal. See <https://docs.djangoproject.com/en/3.1/ref/signals/#pre-delete>. Args: sender: Model class that sent the signal. instance: Model instance to be deleted. using: Database alias. \"\"\" image_orphans = ( Image . objects . annotate ( num_runs = Count ( \"run\" )) . filter ( run = instance , num_runs = 1 ) ) n_obj_deleted , deleted_dict = image_orphans . delete () logger . info ( \"Deleted %d objects: %s \" , n_obj_deleted , \", \" . join ([ f \" { model } : { n } \" for model , n in deleted_dict . items ()]), ) skyreg_orphans = ( SkyRegion . objects . annotate ( num_runs = Count ( \"run\" )) . filter ( run = instance , num_runs = 1 ) ) n_obj_deleted , deleted_dict = skyreg_orphans . delete () logger . info ( \"Deleted %d objects: %s \" , n_obj_deleted , \", \" . join ([ f \" { model } : { n } \" for model , n in deleted_dict . items ()]), )","title":"signals.py"},{"location":"reference/signals/#vast_pipeline.signals.delete_orphans_for_run","text":"Delete any Image and SkyRegion objects that would be orphaned by deleting the given Run. Expects to recieve the arguments sent by the pre_delete signal. See https://docs.djangoproject.com/en/3.1/ref/signals/#pre-delete . Parameters: Name Type Description Default sender Type[vast_pipeline.models.Run] Model class that sent the signal. required instance Run Model instance to be deleted. required using str Database alias. required Source code in vast_pipeline/signals.py @receiver ( pre_delete , sender = Run , dispatch_uid = \"delete_orphans_for_run\" ) def delete_orphans_for_run ( sender : Type [ Run ], instance : Run , using : str , ** kwargs ) -> None : \"\"\"Delete any Image and SkyRegion objects that would be orphaned by deleting the given Run. Expects to recieve the arguments sent by the pre_delete signal. See <https://docs.djangoproject.com/en/3.1/ref/signals/#pre-delete>. Args: sender: Model class that sent the signal. instance: Model instance to be deleted. using: Database alias. \"\"\" image_orphans = ( Image . objects . annotate ( num_runs = Count ( \"run\" )) . filter ( run = instance , num_runs = 1 ) ) n_obj_deleted , deleted_dict = image_orphans . delete () logger . info ( \"Deleted %d objects: %s \" , n_obj_deleted , \", \" . join ([ f \" { model } : { n } \" for model , n in deleted_dict . items ()]), ) skyreg_orphans = ( SkyRegion . objects . annotate ( num_runs = Count ( \"run\" )) . filter ( run = instance , num_runs = 1 ) ) n_obj_deleted , deleted_dict = skyreg_orphans . delete () logger . info ( \"Deleted %d objects: %s \" , n_obj_deleted , \", \" . join ([ f \" { model } : { n } \" for model , n in deleted_dict . items ()]), )","title":"delete_orphans_for_run()"},{"location":"reference/urls/","text":"This module contains the urls used by the Django web server.","title":"urls.py"},{"location":"reference/image/main/","text":"This module contains the relevant classes for the image ingestion. FitsImage \u00b6 FitsImage class to model FITS files Attributes: Name Type Description beam_bmaj float Major axis size of the restoring beam (degrees). beam_bmin float Minor axis size of the restoring beam (degrees). beam_bpa float Position angle of the restoring beam (degrees). datetime pd.Timestamp Date of the observation. duration float Duration of the observation in seconds. Is set to 0 if duration is not in header. fov_bmaj float Estimate of the field of view in the north-south direction (degrees). fov_bmin float Estimate of the field of view in the east-west direction (degrees). ra float Right ascension coordinate of the image centre (degrees). dec float Declination coordinate of the image centre (degrees). polarisation str The polarisation of the image. __init__ ( self , path , hdu_index = 0 ) special \u00b6 Initialise a FitsImage object. Parameters: Name Type Description Default path str The system path of the FITS image. required hdu_index int The index to use on the hdu to fetch the FITS header. 0 Returns: Type Description None None. Source code in vast_pipeline/image/main.py def __init__ ( self , path : str , hdu_index : int = 0 ) -> None : \"\"\" Initialise a FitsImage object. Args: path: The system path of the FITS image. hdu_index: The index to use on the hdu to fetch the FITS header. Returns: None. \"\"\" # inherit from parent super () . __init__ ( path ) # set other attributes header = self . __get_header ( hdu_index ) # set the rest of the attributes self . __set_img_attr_for_telescope ( header ) # get the frequency self . __get_frequency ( header ) Image \u00b6 Generic abstract class for an image. Attributes: Name Type Description name str The image name taken from the file name. path str The system path to the image. __init__ ( self , path ) special \u00b6 Initiliase an image object. Parameters: Name Type Description Default path str The system path to the FITS image. The name of the image is taken from the filename in the given path. required Returns: Type Description None None. Source code in vast_pipeline/image/main.py def __init__ ( self , path : str ) -> None : \"\"\" Initiliase an image object. Args: path: The system path to the FITS image. The name of the image is taken from the filename in the given path. Returns: None. \"\"\" self . name = os . path . basename ( path ) self . path = path __repr__ ( self ) special \u00b6 Defines the printable representation. Returns: Type Description str Printable representation which is the pipeline run name. Source code in vast_pipeline/image/main.py def __repr__ ( self ) -> str : \"\"\" Defines the printable representation. Returns: Printable representation which is the pipeline run name. \"\"\" return self . name SelavyImage \u00b6 Fits images that have a selavy catalogue. Attributes: Name Type Description selavy_path str The system path to the Selavy file. noise_path str The system path to the noise image associated with the image. background_path str The system path to the background image associated with the image. config Dict The image configuration settings. __init__ ( self , path , paths , config , hdu_index = 0 ) special \u00b6 Initialise the SelavyImage. Parameters: Name Type Description Default path str The system path to the FITS image. required paths Dict[str, Dict[str, str]] Dictionary containing the system paths to the associated image products and selavy catalogue. The keys are 'selavy', 'noise', 'background'. required config Dict Configuration settings for the image ingestion. required hdu_index int The index number to use to access the header from the hdu object. 0 Returns: Type Description None None. Source code in vast_pipeline/image/main.py def __init__ ( self , path : str , paths : Dict [ str , Dict [ str , str ]], config : Dict , hdu_index : int = 0 , ) -> None : \"\"\" Initialise the SelavyImage. Args: path: The system path to the FITS image. paths: Dictionary containing the system paths to the associated image products and selavy catalogue. The keys are 'selavy', 'noise', 'background'. config: Configuration settings for the image ingestion. hdu_index: The index number to use to access the header from the hdu object. Returns: None. \"\"\" # inherit from parent self . selavy_path = paths [ 'selavy' ][ path ] self . noise_path = paths [ 'noise' ] . get ( path , '' ) self . background_path = paths [ 'background' ] . get ( path , '' ) self . config : Dict = config super () . __init__ ( path , hdu_index ) read_selavy ( self , dj_image ) \u00b6 Read the sources from the selavy catalogue, select wanted columns and remap them to correct names, followed by filtering and Condon error calculations. Parameters: Name Type Description Default dj_image Image The image model object. required Returns: Type Description DataFrame Dataframe containing the cleaned and processed Selavy components. Source code in vast_pipeline/image/main.py def read_selavy ( self , dj_image : models . Image ) -> pd . DataFrame : \"\"\" Read the sources from the selavy catalogue, select wanted columns and remap them to correct names, followed by filtering and Condon error calculations. Args: dj_image: The image model object. Returns: Dataframe containing the cleaned and processed Selavy components. \"\"\" # TODO: improve with loading only the cols we need and set datatype if self . selavy_path . endswith ( \".xml\" ) or self . selavy_path . endswith ( \".vot\" ): df = Table . read ( self . selavy_path , format = \"votable\" , use_names_over_ids = True ) . to_pandas () elif self . selavy_path . endswith ( \".csv\" ): # CSVs from CASDA have all lowercase column names df = pd . read_csv ( self . selavy_path ) . rename ( columns = { \"spectral_index_from_tt\" : \"spectral_index_from_TT\" } ) else : df = pd . read_fwf ( self . selavy_path , skiprows = [ 1 ]) # drop first line with unit of measure, select only wanted # columns and rename them df = df . loc [:, tr_selavy . keys ()] . rename ( columns = { x : tr_selavy [ x ][ \"name\" ] for x in tr_selavy } ) # fix dtype of columns for ky in tr_selavy : key = tr_selavy [ ky ] if df [ key [ 'name' ]] . dtype != key [ 'dtype' ]: df [ key [ 'name' ]] = df [ key [ 'name' ]] . astype ( key [ 'dtype' ]) # do checks and fill in missing field for uploading sources # in DB (see fields in models.py -> Source model) if df [ 'component_id' ] . duplicated () . any (): raise Exception ( 'Found duplicated names in sources' ) # drop unrealistic sources cols_to_check = [ 'bmaj' , 'bmin' , 'flux_peak' , 'flux_int' , ] bad_sources = df [( df [ cols_to_check ] == 0 ) . any ( axis = 1 )] if bad_sources . shape [ 0 ] > 0 : logger . debug ( \"Dropping %i bad sources.\" , bad_sources . shape [ 0 ]) df = df . drop ( bad_sources . index ) # dropping tiny sources nr_sources_old = df . shape [ 0 ] df = df . loc [ ( df [ 'bmaj' ] > dj_image . beam_bmaj * 500 ) & ( df [ 'bmin' ] > dj_image . beam_bmin * 500 ) ] if df . shape [ 0 ] != nr_sources_old : logger . info ( 'Dropped %i tiny sources.' , nr_sources_old - df . shape [ 0 ] ) # add fields from image and fix name column df [ 'image_id' ] = dj_image . id df [ 'time' ] = dj_image . datetime # append img prefix to source name img_prefix = dj_image . name . split ( '.i.' , 1 )[ - 1 ] . split ( '.' , 1 )[ 0 ] + '_' df [ 'name' ] = img_prefix + df [ 'component_id' ] # # fix error fluxes for col in [ 'flux_int_err' , 'flux_peak_err' ]: sel = df [ col ] < settings . FLUX_DEFAULT_MIN_ERROR if sel . any (): df . loc [ sel , col ] = settings . FLUX_DEFAULT_MIN_ERROR # # fix error ra dec for col in [ 'ra_err' , 'dec_err' ]: sel = df [ col ] < settings . POS_DEFAULT_MIN_ERROR if sel . any (): df . loc [ sel , col ] = settings . POS_DEFAULT_MIN_ERROR df [ col ] = df [ col ] / 3600. # replace 0 local_rms values using user config value df . loc [ df [ 'local_rms' ] == 0. , 'local_rms' ] = self . config [ \"selavy_local_rms_fill_value\" ] df [ 'snr' ] = df [ 'flux_peak' ] . values / df [ 'local_rms' ] . values df [ 'compactness' ] = df [ 'flux_int' ] . values / df [ 'flux_peak' ] . values if self . config [ \"condon_errors\" ]: logger . debug ( \"Calculating Condon '97 errors...\" ) theta_B = dj_image . beam_bmaj theta_b = dj_image . beam_bmin df [[ 'flux_peak_err' , 'flux_int_err' , 'err_bmaj' , 'err_bmin' , 'err_pa' , 'ra_err' , 'dec_err' , ]] = df [[ 'flux_peak' , 'flux_int' , 'bmaj' , 'bmin' , 'pa' , 'snr' , 'local_rms' , ]] . apply ( calc_condon_flux_errors , args = ( theta_B , theta_b ), axis = 1 , result_type = 'expand' ) logger . debug ( \"Condon errors done.\" ) logger . debug ( \"Calculating positional errors...\" ) # TODO: avoid extra column given that it is a single value df [ 'ew_sys_err' ] = self . config [ \"ra_uncertainty\" ] / 3600. df [ 'ns_sys_err' ] = self . config [ \"dec_uncertainty\" ] / 3600. df [ 'error_radius' ] = calc_error_radius ( df [ 'ra' ] . values , df [ 'ra_err' ] . values , df [ 'dec' ] . values , df [ 'dec_err' ] . values , ) df [ 'uncertainty_ew' ] = np . hypot ( df [ 'ew_sys_err' ] . values , df [ 'error_radius' ] . values ) df [ 'uncertainty_ns' ] = np . hypot ( df [ 'ns_sys_err' ] . values , df [ 'error_radius' ] . values ) # weight calculations to use later df [ 'weight_ew' ] = 1. / df [ 'uncertainty_ew' ] . values ** 2 df [ 'weight_ns' ] = 1. / df [ 'uncertainty_ns' ] . values ** 2 logger . debug ( 'Positional errors done.' ) # Initialise the forced column as False df [ 'forced' ] = False # Calculate island flux fractions island_flux_totals = ( df [[ 'island_id' , 'flux_int' , 'flux_peak' ]] . groupby ( 'island_id' ) . agg ( 'sum' ) ) df [ 'flux_int_isl_ratio' ] = ( df [ 'flux_int' ] . values / island_flux_totals . loc [ df [ 'island_id' ]][ 'flux_int' ] . values ) df [ 'flux_peak_isl_ratio' ] = ( df [ 'flux_peak' ] . values / island_flux_totals . loc [ df [ 'island_id' ]][ 'flux_peak' ] . values ) return df","title":"main.py"},{"location":"reference/image/main/#vast_pipeline.image.main.FitsImage","text":"FitsImage class to model FITS files Attributes: Name Type Description beam_bmaj float Major axis size of the restoring beam (degrees). beam_bmin float Minor axis size of the restoring beam (degrees). beam_bpa float Position angle of the restoring beam (degrees). datetime pd.Timestamp Date of the observation. duration float Duration of the observation in seconds. Is set to 0 if duration is not in header. fov_bmaj float Estimate of the field of view in the north-south direction (degrees). fov_bmin float Estimate of the field of view in the east-west direction (degrees). ra float Right ascension coordinate of the image centre (degrees). dec float Declination coordinate of the image centre (degrees). polarisation str The polarisation of the image.","title":"FitsImage"},{"location":"reference/image/main/#vast_pipeline.image.main.FitsImage.__init__","text":"Initialise a FitsImage object. Parameters: Name Type Description Default path str The system path of the FITS image. required hdu_index int The index to use on the hdu to fetch the FITS header. 0 Returns: Type Description None None. Source code in vast_pipeline/image/main.py def __init__ ( self , path : str , hdu_index : int = 0 ) -> None : \"\"\" Initialise a FitsImage object. Args: path: The system path of the FITS image. hdu_index: The index to use on the hdu to fetch the FITS header. Returns: None. \"\"\" # inherit from parent super () . __init__ ( path ) # set other attributes header = self . __get_header ( hdu_index ) # set the rest of the attributes self . __set_img_attr_for_telescope ( header ) # get the frequency self . __get_frequency ( header )","title":"__init__()"},{"location":"reference/image/main/#vast_pipeline.image.main.Image","text":"Generic abstract class for an image. Attributes: Name Type Description name str The image name taken from the file name. path str The system path to the image.","title":"Image"},{"location":"reference/image/main/#vast_pipeline.image.main.Image.__init__","text":"Initiliase an image object. Parameters: Name Type Description Default path str The system path to the FITS image. The name of the image is taken from the filename in the given path. required Returns: Type Description None None. Source code in vast_pipeline/image/main.py def __init__ ( self , path : str ) -> None : \"\"\" Initiliase an image object. Args: path: The system path to the FITS image. The name of the image is taken from the filename in the given path. Returns: None. \"\"\" self . name = os . path . basename ( path ) self . path = path","title":"__init__()"},{"location":"reference/image/main/#vast_pipeline.image.main.Image.__repr__","text":"Defines the printable representation. Returns: Type Description str Printable representation which is the pipeline run name. Source code in vast_pipeline/image/main.py def __repr__ ( self ) -> str : \"\"\" Defines the printable representation. Returns: Printable representation which is the pipeline run name. \"\"\" return self . name","title":"__repr__()"},{"location":"reference/image/main/#vast_pipeline.image.main.SelavyImage","text":"Fits images that have a selavy catalogue. Attributes: Name Type Description selavy_path str The system path to the Selavy file. noise_path str The system path to the noise image associated with the image. background_path str The system path to the background image associated with the image. config Dict The image configuration settings.","title":"SelavyImage"},{"location":"reference/image/main/#vast_pipeline.image.main.SelavyImage.__init__","text":"Initialise the SelavyImage. Parameters: Name Type Description Default path str The system path to the FITS image. required paths Dict[str, Dict[str, str]] Dictionary containing the system paths to the associated image products and selavy catalogue. The keys are 'selavy', 'noise', 'background'. required config Dict Configuration settings for the image ingestion. required hdu_index int The index number to use to access the header from the hdu object. 0 Returns: Type Description None None. Source code in vast_pipeline/image/main.py def __init__ ( self , path : str , paths : Dict [ str , Dict [ str , str ]], config : Dict , hdu_index : int = 0 , ) -> None : \"\"\" Initialise the SelavyImage. Args: path: The system path to the FITS image. paths: Dictionary containing the system paths to the associated image products and selavy catalogue. The keys are 'selavy', 'noise', 'background'. config: Configuration settings for the image ingestion. hdu_index: The index number to use to access the header from the hdu object. Returns: None. \"\"\" # inherit from parent self . selavy_path = paths [ 'selavy' ][ path ] self . noise_path = paths [ 'noise' ] . get ( path , '' ) self . background_path = paths [ 'background' ] . get ( path , '' ) self . config : Dict = config super () . __init__ ( path , hdu_index )","title":"__init__()"},{"location":"reference/image/main/#vast_pipeline.image.main.SelavyImage.read_selavy","text":"Read the sources from the selavy catalogue, select wanted columns and remap them to correct names, followed by filtering and Condon error calculations. Parameters: Name Type Description Default dj_image Image The image model object. required Returns: Type Description DataFrame Dataframe containing the cleaned and processed Selavy components. Source code in vast_pipeline/image/main.py def read_selavy ( self , dj_image : models . Image ) -> pd . DataFrame : \"\"\" Read the sources from the selavy catalogue, select wanted columns and remap them to correct names, followed by filtering and Condon error calculations. Args: dj_image: The image model object. Returns: Dataframe containing the cleaned and processed Selavy components. \"\"\" # TODO: improve with loading only the cols we need and set datatype if self . selavy_path . endswith ( \".xml\" ) or self . selavy_path . endswith ( \".vot\" ): df = Table . read ( self . selavy_path , format = \"votable\" , use_names_over_ids = True ) . to_pandas () elif self . selavy_path . endswith ( \".csv\" ): # CSVs from CASDA have all lowercase column names df = pd . read_csv ( self . selavy_path ) . rename ( columns = { \"spectral_index_from_tt\" : \"spectral_index_from_TT\" } ) else : df = pd . read_fwf ( self . selavy_path , skiprows = [ 1 ]) # drop first line with unit of measure, select only wanted # columns and rename them df = df . loc [:, tr_selavy . keys ()] . rename ( columns = { x : tr_selavy [ x ][ \"name\" ] for x in tr_selavy } ) # fix dtype of columns for ky in tr_selavy : key = tr_selavy [ ky ] if df [ key [ 'name' ]] . dtype != key [ 'dtype' ]: df [ key [ 'name' ]] = df [ key [ 'name' ]] . astype ( key [ 'dtype' ]) # do checks and fill in missing field for uploading sources # in DB (see fields in models.py -> Source model) if df [ 'component_id' ] . duplicated () . any (): raise Exception ( 'Found duplicated names in sources' ) # drop unrealistic sources cols_to_check = [ 'bmaj' , 'bmin' , 'flux_peak' , 'flux_int' , ] bad_sources = df [( df [ cols_to_check ] == 0 ) . any ( axis = 1 )] if bad_sources . shape [ 0 ] > 0 : logger . debug ( \"Dropping %i bad sources.\" , bad_sources . shape [ 0 ]) df = df . drop ( bad_sources . index ) # dropping tiny sources nr_sources_old = df . shape [ 0 ] df = df . loc [ ( df [ 'bmaj' ] > dj_image . beam_bmaj * 500 ) & ( df [ 'bmin' ] > dj_image . beam_bmin * 500 ) ] if df . shape [ 0 ] != nr_sources_old : logger . info ( 'Dropped %i tiny sources.' , nr_sources_old - df . shape [ 0 ] ) # add fields from image and fix name column df [ 'image_id' ] = dj_image . id df [ 'time' ] = dj_image . datetime # append img prefix to source name img_prefix = dj_image . name . split ( '.i.' , 1 )[ - 1 ] . split ( '.' , 1 )[ 0 ] + '_' df [ 'name' ] = img_prefix + df [ 'component_id' ] # # fix error fluxes for col in [ 'flux_int_err' , 'flux_peak_err' ]: sel = df [ col ] < settings . FLUX_DEFAULT_MIN_ERROR if sel . any (): df . loc [ sel , col ] = settings . FLUX_DEFAULT_MIN_ERROR # # fix error ra dec for col in [ 'ra_err' , 'dec_err' ]: sel = df [ col ] < settings . POS_DEFAULT_MIN_ERROR if sel . any (): df . loc [ sel , col ] = settings . POS_DEFAULT_MIN_ERROR df [ col ] = df [ col ] / 3600. # replace 0 local_rms values using user config value df . loc [ df [ 'local_rms' ] == 0. , 'local_rms' ] = self . config [ \"selavy_local_rms_fill_value\" ] df [ 'snr' ] = df [ 'flux_peak' ] . values / df [ 'local_rms' ] . values df [ 'compactness' ] = df [ 'flux_int' ] . values / df [ 'flux_peak' ] . values if self . config [ \"condon_errors\" ]: logger . debug ( \"Calculating Condon '97 errors...\" ) theta_B = dj_image . beam_bmaj theta_b = dj_image . beam_bmin df [[ 'flux_peak_err' , 'flux_int_err' , 'err_bmaj' , 'err_bmin' , 'err_pa' , 'ra_err' , 'dec_err' , ]] = df [[ 'flux_peak' , 'flux_int' , 'bmaj' , 'bmin' , 'pa' , 'snr' , 'local_rms' , ]] . apply ( calc_condon_flux_errors , args = ( theta_B , theta_b ), axis = 1 , result_type = 'expand' ) logger . debug ( \"Condon errors done.\" ) logger . debug ( \"Calculating positional errors...\" ) # TODO: avoid extra column given that it is a single value df [ 'ew_sys_err' ] = self . config [ \"ra_uncertainty\" ] / 3600. df [ 'ns_sys_err' ] = self . config [ \"dec_uncertainty\" ] / 3600. df [ 'error_radius' ] = calc_error_radius ( df [ 'ra' ] . values , df [ 'ra_err' ] . values , df [ 'dec' ] . values , df [ 'dec_err' ] . values , ) df [ 'uncertainty_ew' ] = np . hypot ( df [ 'ew_sys_err' ] . values , df [ 'error_radius' ] . values ) df [ 'uncertainty_ns' ] = np . hypot ( df [ 'ns_sys_err' ] . values , df [ 'error_radius' ] . values ) # weight calculations to use later df [ 'weight_ew' ] = 1. / df [ 'uncertainty_ew' ] . values ** 2 df [ 'weight_ns' ] = 1. / df [ 'uncertainty_ns' ] . values ** 2 logger . debug ( 'Positional errors done.' ) # Initialise the forced column as False df [ 'forced' ] = False # Calculate island flux fractions island_flux_totals = ( df [[ 'island_id' , 'flux_int' , 'flux_peak' ]] . groupby ( 'island_id' ) . agg ( 'sum' ) ) df [ 'flux_int_isl_ratio' ] = ( df [ 'flux_int' ] . values / island_flux_totals . loc [ df [ 'island_id' ]][ 'flux_int' ] . values ) df [ 'flux_peak_isl_ratio' ] = ( df [ 'flux_peak' ] . values / island_flux_totals . loc [ df [ 'island_id' ]][ 'flux_peak' ] . values ) return df","title":"read_selavy()"},{"location":"reference/image/utils/","text":"This module contains utility functions used by the image ingestion section of the pipeline. calc_condon_flux_errors ( row , theta_B , theta_b , alpha_maj1 = 2.5 , alpha_min1 = 0.5 , alpha_maj2 = 0.5 , alpha_min2 = 2.5 , alpha_maj3 = 1.5 , alpha_min3 = 1.5 , clean_bias = 0.0 , clean_bias_error = 0.0 , frac_flux_cal_error = 0.0 ) \u00b6 The following code for this function taken from the TraP with a few modifications. Returns the errors on parameters from Gaussian fits according to the Condon (PASP 109, 166 (1997)) formulae. These formulae are not perfect, but we'll use them for the time being. (See Refregier and Brown (astro-ph/9803279v1) for a more rigorous approach.) It also returns the corrected peak. The peak can be corrected for the overestimate due to the local noise gradient, but this is currently not used in the function. Parameters: Name Type Description Default row Series The row containing the componenet information from the Selavy component catalogue. required theta_B float The major axis size of the restoring beam of the image (degrees). required theta_b float The minor axis size of the restoring beam of the image (degrees). required alpha_maj1 float The alpha_M exponent value for x_0. 2.5 alpha_min1 float The alpha_m exponent value for x_0. 0.5 alpha_maj2 float The alpha_M exponent value for y_0. 0.5 alpha_min2 float The alpha_m exponent value for y_0. 2.5 alpha_maj3 float The alpha_M exponent value for the amplitude error. 1.5 alpha_min3 float The alpha_m exponent value for the amplitude error. 1.5 clean_bias float Clean bias value used in the peak flux correction (not currently used). 0.0 clean_bias_error float The error of the clean bias value used in the peak flux correction (not currently used). 0.0 frac_flux_cal_error float Flux calibration error value. (Unsure of exact meaning, refer to TraP). 0.0 Returns: Type Description Tuple[float, float, float, float, float, float, float] Tuple containing the following calculated values: peak flux error, integrated flux error, major axis error, minor axis error, position angle error, right ascension error and the declination error. Source code in vast_pipeline/image/utils.py def calc_condon_flux_errors ( row : pd . Series , theta_B : float , theta_b : float , alpha_maj1 : float = 2.5 , alpha_min1 : float = 0.5 , alpha_maj2 : float = 0.5 , alpha_min2 : float = 2.5 , alpha_maj3 : float = 1.5 , alpha_min3 : float = 1.5 , clean_bias : float = 0.0 , clean_bias_error : float = 0.0 , frac_flux_cal_error : float = 0.0 , ) -> Tuple [ float , float , float , float , float , float , float ]: \"\"\" The following code for this function taken from the TraP with a few modifications. Returns the errors on parameters from Gaussian fits according to the Condon (PASP 109, 166 (1997)) formulae. These formulae are not perfect, but we'll use them for the time being. (See Refregier and Brown (astro-ph/9803279v1) for a more rigorous approach.) It also returns the corrected peak. The peak can be corrected for the overestimate due to the local noise gradient, but this is currently not used in the function. Args: row (pd.Series): The row containing the componenet information from the Selavy component catalogue. theta_B (float): The major axis size of the restoring beam of the image (degrees). theta_b (float): The minor axis size of the restoring beam of the image (degrees). alpha_maj1 (float): The alpha_M exponent value for x_0. alpha_min1 (float): The alpha_m exponent value for x_0. alpha_maj2 (float): The alpha_M exponent value for y_0. alpha_min2 (float): The alpha_m exponent value for y_0. alpha_maj3 (float): The alpha_M exponent value for the amplitude error. alpha_min3 (float): The alpha_m exponent value for the amplitude error. clean_bias (float): Clean bias value used in the peak flux correction (not currently used). clean_bias_error (float): The error of the clean bias value used in the peak flux correction (not currently used). frac_flux_cal_error (float): Flux calibration error value. (Unsure of exact meaning, refer to TraP). Returns: Tuple containing the following calculated values: peak flux error, integrated flux error, major axis error, minor axis error, position angle error, right ascension error and the declination error. \"\"\" major = row . bmaj / 3600. # degrees minor = row . bmin / 3600. # degrees theta = np . deg2rad ( row . pa ) flux_peak = row [ 'flux_peak' ] flux_int = row [ 'flux_int' ] snr = row [ 'snr' ] noise = row [ 'local_rms' ] variables = [ theta_B , theta_b , major , minor , flux_peak , flux_int , snr , noise ] # return 0 if the source is unrealistic. Should be rare # given that these sources are also filtered out before hand. if 0.0 in variables : logger . debug ( variables ) return 0. , 0. , 0. , 0. , 0. , 0. , 0. try : rho_sq1 = (( major * minor / ( 4. * theta_B * theta_b )) * ( 1. + ( theta_B / major ) ** 2 ) ** alpha_maj1 * ( 1. + ( theta_b / minor ) ** 2 ) ** alpha_min1 * snr ** 2 ) rho_sq2 = (( major * minor / ( 4. * theta_B * theta_b )) * ( 1. + ( theta_B / major ) ** 2 ) ** alpha_maj2 * ( 1. + ( theta_b / minor ) ** 2 ) ** alpha_min2 * snr ** 2 ) rho_sq3 = (( major * minor / ( 4. * theta_B * theta_b )) * ( 1. + ( theta_B / major ) ** 2 ) ** alpha_maj3 * ( 1. + ( theta_b / minor ) ** 2 ) ** alpha_min3 * snr ** 2 ) rho1 = np . sqrt ( rho_sq1 ) rho2 = np . sqrt ( rho_sq2 ) rho3 = np . sqrt ( rho_sq3 ) # here we change the TraP code slightly and base it # purely on Condon 97 and not the NVSS paper. denom1 = np . sqrt ( 4. * np . log ( 2. )) * rho1 denom2 = np . sqrt ( 4. * np . log ( 2. )) * rho2 # these are the 'xo' and 'y0' errors from Condon error_par_major = major / denom1 error_par_minor = minor / denom2 # ra and dec errors errorra = np . sqrt (( error_par_major * np . sin ( theta )) ** 2 + ( error_par_minor * np . cos ( theta )) ** 2 ) errordec = np . sqrt (( error_par_major * np . cos ( theta )) ** 2 + ( error_par_minor * np . sin ( theta )) ** 2 ) errormajor = np . sqrt ( 2 ) * major / rho1 errorminor = np . sqrt ( 2 ) * minor / rho2 if major > minor : errortheta = 2.0 * ( major * minor / ( major ** 2 - minor ** 2 )) / rho2 else : errortheta = np . pi if errortheta > np . pi : errortheta = np . pi # correction to flux peak not currently used # but might be in the future. # Do not remove! # flux_peak += -noise**2 / flux_peak + clean_bias errorpeaksq = (( frac_flux_cal_error * flux_peak ) ** 2 + clean_bias_error ** 2 + 2. * flux_peak ** 2 / rho_sq3 ) errorpeak = np . sqrt ( errorpeaksq ) help1 = ( errormajor / major ) ** 2 help2 = ( errorminor / minor ) ** 2 help3 = theta_B * theta_b / ( major * minor ) errorflux = np . abs ( flux_int ) * np . sqrt ( errorpeaksq / flux_peak ** 2 + help3 * ( help1 + help2 )) # need to return flux_peak if used. return errorpeak , errorflux , errormajor , errorminor , errortheta , errorra , errordec except Exception as e : logger . debug ( \"Error in the calculation of Condon errors for a source\" , exc_info = True ) return 0. , 0. , 0. , 0. , 0. , 0. , 0. calc_error_radius ( ra , ra_err , dec , dec_err ) \u00b6 Using the fitted errors from selavy, this function estimates the largest on sky angular size of the uncertainty. The four different combinations of the errors are analysed and the maximum is returned. Logic is taken from the TraP, where this is also used. Function has been vectorised for pandas. All inputs are in degrees. Parameters: Name Type Description Default ra float The right ascension of the coordinate (degrees). required ra_err float The error associated with the ra value (degrees). required dec float The declination of the coordinate (degrees). required dec_err float The error associated with the declination value (degrees). required Returns: Type Description float The calculated error radius (degrees). Source code in vast_pipeline/image/utils.py def calc_error_radius ( ra , ra_err , dec , dec_err ) -> float : \"\"\" Using the fitted errors from selavy, this function estimates the largest on sky angular size of the uncertainty. The four different combinations of the errors are analysed and the maximum is returned. Logic is taken from the TraP, where this is also used. Function has been vectorised for pandas. All inputs are in degrees. Args: ra (float): The right ascension of the coordinate (degrees). ra_err (float): The error associated with the ra value (degrees). dec (float): The declination of the coordinate (degrees). dec_err (float): The error associated with the declination value (degrees). Returns: The calculated error radius (degrees). \"\"\" ra_1 = np . deg2rad ( ra ) dec_1 = np . deg2rad ( dec ) ra_offsets = [ ( ra + ra_err ), ( ra + ra_err ), ( ra - ra_err ), ( ra - ra_err ) ] dec_offsets = [ ( dec + dec_err ), ( dec - dec_err ), ( dec + dec_err ), ( dec - dec_err ) ] seps = [ np . rad2deg ( on_sky_sep ( ra_1 , np . deg2rad ( i ), dec_1 , np . deg2rad ( j ) )) for i , j in zip ( ra_offsets , dec_offsets ) ] seps = np . column_stack ( seps ) return np . amax ( seps , 1 ) on_sky_sep ( ra_1 , ra_2 , dec_1 , dec_2 ) \u00b6 Simple on sky distance between two RA and Dec coordinates. Needed for fast calculation on dataframes as astropy is slow. All units are radians. Parameters: Name Type Description Default ra_1 float The right ascension of coodinate 1 (radians). required ra_2 float The right ascension of coodinate 2 (radians). required dec_1 float The declination of coodinate 1 (radians). required dec_2 float The declination of coodinate 2 (radians). required Returns: Type Description float The on-sky separation distance between the two coodinates (radians). Source code in vast_pipeline/image/utils.py def on_sky_sep ( ra_1 , ra_2 , dec_1 , dec_2 ) -> float : \"\"\" Simple on sky distance between two RA and Dec coordinates. Needed for fast calculation on dataframes as astropy is slow. All units are radians. Args: ra_1 (float): The right ascension of coodinate 1 (radians). ra_2 (float): The right ascension of coodinate 2 (radians). dec_1 (float): The declination of coodinate 1 (radians). dec_2 (float): The declination of coodinate 2 (radians). Returns: The on-sky separation distance between the two coodinates (radians). \"\"\" separation = ( np . sin ( dec_1 ) * np . sin ( dec_2 ) + np . cos ( dec_1 ) * np . cos ( dec_2 ) * np . cos ( ra_1 - ra_2 ) ) # fix errors on separation values over 1 separation [ separation > 1. ] = 1. return np . arccos ( separation )","title":"utils.py"},{"location":"reference/image/utils/#vast_pipeline.image.utils.calc_condon_flux_errors","text":"The following code for this function taken from the TraP with a few modifications. Returns the errors on parameters from Gaussian fits according to the Condon (PASP 109, 166 (1997)) formulae. These formulae are not perfect, but we'll use them for the time being. (See Refregier and Brown (astro-ph/9803279v1) for a more rigorous approach.) It also returns the corrected peak. The peak can be corrected for the overestimate due to the local noise gradient, but this is currently not used in the function. Parameters: Name Type Description Default row Series The row containing the componenet information from the Selavy component catalogue. required theta_B float The major axis size of the restoring beam of the image (degrees). required theta_b float The minor axis size of the restoring beam of the image (degrees). required alpha_maj1 float The alpha_M exponent value for x_0. 2.5 alpha_min1 float The alpha_m exponent value for x_0. 0.5 alpha_maj2 float The alpha_M exponent value for y_0. 0.5 alpha_min2 float The alpha_m exponent value for y_0. 2.5 alpha_maj3 float The alpha_M exponent value for the amplitude error. 1.5 alpha_min3 float The alpha_m exponent value for the amplitude error. 1.5 clean_bias float Clean bias value used in the peak flux correction (not currently used). 0.0 clean_bias_error float The error of the clean bias value used in the peak flux correction (not currently used). 0.0 frac_flux_cal_error float Flux calibration error value. (Unsure of exact meaning, refer to TraP). 0.0 Returns: Type Description Tuple[float, float, float, float, float, float, float] Tuple containing the following calculated values: peak flux error, integrated flux error, major axis error, minor axis error, position angle error, right ascension error and the declination error. Source code in vast_pipeline/image/utils.py def calc_condon_flux_errors ( row : pd . Series , theta_B : float , theta_b : float , alpha_maj1 : float = 2.5 , alpha_min1 : float = 0.5 , alpha_maj2 : float = 0.5 , alpha_min2 : float = 2.5 , alpha_maj3 : float = 1.5 , alpha_min3 : float = 1.5 , clean_bias : float = 0.0 , clean_bias_error : float = 0.0 , frac_flux_cal_error : float = 0.0 , ) -> Tuple [ float , float , float , float , float , float , float ]: \"\"\" The following code for this function taken from the TraP with a few modifications. Returns the errors on parameters from Gaussian fits according to the Condon (PASP 109, 166 (1997)) formulae. These formulae are not perfect, but we'll use them for the time being. (See Refregier and Brown (astro-ph/9803279v1) for a more rigorous approach.) It also returns the corrected peak. The peak can be corrected for the overestimate due to the local noise gradient, but this is currently not used in the function. Args: row (pd.Series): The row containing the componenet information from the Selavy component catalogue. theta_B (float): The major axis size of the restoring beam of the image (degrees). theta_b (float): The minor axis size of the restoring beam of the image (degrees). alpha_maj1 (float): The alpha_M exponent value for x_0. alpha_min1 (float): The alpha_m exponent value for x_0. alpha_maj2 (float): The alpha_M exponent value for y_0. alpha_min2 (float): The alpha_m exponent value for y_0. alpha_maj3 (float): The alpha_M exponent value for the amplitude error. alpha_min3 (float): The alpha_m exponent value for the amplitude error. clean_bias (float): Clean bias value used in the peak flux correction (not currently used). clean_bias_error (float): The error of the clean bias value used in the peak flux correction (not currently used). frac_flux_cal_error (float): Flux calibration error value. (Unsure of exact meaning, refer to TraP). Returns: Tuple containing the following calculated values: peak flux error, integrated flux error, major axis error, minor axis error, position angle error, right ascension error and the declination error. \"\"\" major = row . bmaj / 3600. # degrees minor = row . bmin / 3600. # degrees theta = np . deg2rad ( row . pa ) flux_peak = row [ 'flux_peak' ] flux_int = row [ 'flux_int' ] snr = row [ 'snr' ] noise = row [ 'local_rms' ] variables = [ theta_B , theta_b , major , minor , flux_peak , flux_int , snr , noise ] # return 0 if the source is unrealistic. Should be rare # given that these sources are also filtered out before hand. if 0.0 in variables : logger . debug ( variables ) return 0. , 0. , 0. , 0. , 0. , 0. , 0. try : rho_sq1 = (( major * minor / ( 4. * theta_B * theta_b )) * ( 1. + ( theta_B / major ) ** 2 ) ** alpha_maj1 * ( 1. + ( theta_b / minor ) ** 2 ) ** alpha_min1 * snr ** 2 ) rho_sq2 = (( major * minor / ( 4. * theta_B * theta_b )) * ( 1. + ( theta_B / major ) ** 2 ) ** alpha_maj2 * ( 1. + ( theta_b / minor ) ** 2 ) ** alpha_min2 * snr ** 2 ) rho_sq3 = (( major * minor / ( 4. * theta_B * theta_b )) * ( 1. + ( theta_B / major ) ** 2 ) ** alpha_maj3 * ( 1. + ( theta_b / minor ) ** 2 ) ** alpha_min3 * snr ** 2 ) rho1 = np . sqrt ( rho_sq1 ) rho2 = np . sqrt ( rho_sq2 ) rho3 = np . sqrt ( rho_sq3 ) # here we change the TraP code slightly and base it # purely on Condon 97 and not the NVSS paper. denom1 = np . sqrt ( 4. * np . log ( 2. )) * rho1 denom2 = np . sqrt ( 4. * np . log ( 2. )) * rho2 # these are the 'xo' and 'y0' errors from Condon error_par_major = major / denom1 error_par_minor = minor / denom2 # ra and dec errors errorra = np . sqrt (( error_par_major * np . sin ( theta )) ** 2 + ( error_par_minor * np . cos ( theta )) ** 2 ) errordec = np . sqrt (( error_par_major * np . cos ( theta )) ** 2 + ( error_par_minor * np . sin ( theta )) ** 2 ) errormajor = np . sqrt ( 2 ) * major / rho1 errorminor = np . sqrt ( 2 ) * minor / rho2 if major > minor : errortheta = 2.0 * ( major * minor / ( major ** 2 - minor ** 2 )) / rho2 else : errortheta = np . pi if errortheta > np . pi : errortheta = np . pi # correction to flux peak not currently used # but might be in the future. # Do not remove! # flux_peak += -noise**2 / flux_peak + clean_bias errorpeaksq = (( frac_flux_cal_error * flux_peak ) ** 2 + clean_bias_error ** 2 + 2. * flux_peak ** 2 / rho_sq3 ) errorpeak = np . sqrt ( errorpeaksq ) help1 = ( errormajor / major ) ** 2 help2 = ( errorminor / minor ) ** 2 help3 = theta_B * theta_b / ( major * minor ) errorflux = np . abs ( flux_int ) * np . sqrt ( errorpeaksq / flux_peak ** 2 + help3 * ( help1 + help2 )) # need to return flux_peak if used. return errorpeak , errorflux , errormajor , errorminor , errortheta , errorra , errordec except Exception as e : logger . debug ( \"Error in the calculation of Condon errors for a source\" , exc_info = True ) return 0. , 0. , 0. , 0. , 0. , 0. , 0.","title":"calc_condon_flux_errors()"},{"location":"reference/image/utils/#vast_pipeline.image.utils.calc_error_radius","text":"Using the fitted errors from selavy, this function estimates the largest on sky angular size of the uncertainty. The four different combinations of the errors are analysed and the maximum is returned. Logic is taken from the TraP, where this is also used. Function has been vectorised for pandas. All inputs are in degrees. Parameters: Name Type Description Default ra float The right ascension of the coordinate (degrees). required ra_err float The error associated with the ra value (degrees). required dec float The declination of the coordinate (degrees). required dec_err float The error associated with the declination value (degrees). required Returns: Type Description float The calculated error radius (degrees). Source code in vast_pipeline/image/utils.py def calc_error_radius ( ra , ra_err , dec , dec_err ) -> float : \"\"\" Using the fitted errors from selavy, this function estimates the largest on sky angular size of the uncertainty. The four different combinations of the errors are analysed and the maximum is returned. Logic is taken from the TraP, where this is also used. Function has been vectorised for pandas. All inputs are in degrees. Args: ra (float): The right ascension of the coordinate (degrees). ra_err (float): The error associated with the ra value (degrees). dec (float): The declination of the coordinate (degrees). dec_err (float): The error associated with the declination value (degrees). Returns: The calculated error radius (degrees). \"\"\" ra_1 = np . deg2rad ( ra ) dec_1 = np . deg2rad ( dec ) ra_offsets = [ ( ra + ra_err ), ( ra + ra_err ), ( ra - ra_err ), ( ra - ra_err ) ] dec_offsets = [ ( dec + dec_err ), ( dec - dec_err ), ( dec + dec_err ), ( dec - dec_err ) ] seps = [ np . rad2deg ( on_sky_sep ( ra_1 , np . deg2rad ( i ), dec_1 , np . deg2rad ( j ) )) for i , j in zip ( ra_offsets , dec_offsets ) ] seps = np . column_stack ( seps ) return np . amax ( seps , 1 )","title":"calc_error_radius()"},{"location":"reference/image/utils/#vast_pipeline.image.utils.on_sky_sep","text":"Simple on sky distance between two RA and Dec coordinates. Needed for fast calculation on dataframes as astropy is slow. All units are radians. Parameters: Name Type Description Default ra_1 float The right ascension of coodinate 1 (radians). required ra_2 float The right ascension of coodinate 2 (radians). required dec_1 float The declination of coodinate 1 (radians). required dec_2 float The declination of coodinate 2 (radians). required Returns: Type Description float The on-sky separation distance between the two coodinates (radians). Source code in vast_pipeline/image/utils.py def on_sky_sep ( ra_1 , ra_2 , dec_1 , dec_2 ) -> float : \"\"\" Simple on sky distance between two RA and Dec coordinates. Needed for fast calculation on dataframes as astropy is slow. All units are radians. Args: ra_1 (float): The right ascension of coodinate 1 (radians). ra_2 (float): The right ascension of coodinate 2 (radians). dec_1 (float): The declination of coodinate 1 (radians). dec_2 (float): The declination of coodinate 2 (radians). Returns: The on-sky separation distance between the two coodinates (radians). \"\"\" separation = ( np . sin ( dec_1 ) * np . sin ( dec_2 ) + np . cos ( dec_1 ) * np . cos ( dec_2 ) * np . cos ( ra_1 - ra_2 ) ) # fix errors on separation values over 1 separation [ separation > 1. ] = 1. return np . arccos ( separation )","title":"on_sky_sep()"},{"location":"reference/management/helpers/","text":"Helper functions for the commands. get_p_run_name ( name , return_folder = False ) \u00b6 Determines the name of the pipeline run. Can also return the output folder if selected. Parameters: Name Type Description Default name str The user entered name of the pipeline run. required return_folder bool When True the pipeline directory is also returned. False Returns: Type Description Tuple[str, str] The name of the pipeline run. If return_folder is True then both the name and directory are returned. Source code in vast_pipeline/management/helpers.py def get_p_run_name ( name : str , return_folder : bool = False ) -> Tuple [ str , str ]: \"\"\" Determines the name of the pipeline run. Can also return the output folder if selected. Args: name: The user entered name of the pipeline run. return_folder: When `True` the pipeline directory is also returned. Returns: The name of the pipeline run. If return_folder is `True` then both the name and directory are returned. \"\"\" if '/' in name : folder = os . path . realpath ( name ) run_name = os . path . basename ( folder ) return ( run_name , folder ) if return_folder else run_name folder = os . path . join ( os . path . realpath ( sett . PIPELINE_WORKING_DIR ), name ) return ( name , folder ) if return_folder else name","title":"helpers.py"},{"location":"reference/management/helpers/#vast_pipeline.management.helpers.get_p_run_name","text":"Determines the name of the pipeline run. Can also return the output folder if selected. Parameters: Name Type Description Default name str The user entered name of the pipeline run. required return_folder bool When True the pipeline directory is also returned. False Returns: Type Description Tuple[str, str] The name of the pipeline run. If return_folder is True then both the name and directory are returned. Source code in vast_pipeline/management/helpers.py def get_p_run_name ( name : str , return_folder : bool = False ) -> Tuple [ str , str ]: \"\"\" Determines the name of the pipeline run. Can also return the output folder if selected. Args: name: The user entered name of the pipeline run. return_folder: When `True` the pipeline directory is also returned. Returns: The name of the pipeline run. If return_folder is `True` then both the name and directory are returned. \"\"\" if '/' in name : folder = os . path . realpath ( name ) run_name = os . path . basename ( folder ) return ( run_name , folder ) if return_folder else run_name folder = os . path . join ( os . path . realpath ( sett . PIPELINE_WORKING_DIR ), name ) return ( name , folder ) if return_folder else name","title":"get_p_run_name()"},{"location":"reference/management/commands/clearpiperun/","text":"This module defines the command for clearing a run from the database. Command \u00b6 This script is used to clean the data for pipeline run(s). Use --help for usage. add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/clearpiperun.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments (required) parser . add_argument ( 'piperuns' , nargs = '+' , type = str , default = None , help = ( 'Name or path of pipeline run(s) to delete. Pass \"clearall\" to' ' delete all the runs.' ) ) # keyword arguments (optional) parser . add_argument ( '--keep-parquet' , required = False , default = False , action = 'store_true' , help = ( 'Flag to keep the pipeline run(s) parquet files. ' 'Will also apply to arrow files if present.' ) ) parser . add_argument ( '--remove-all' , required = False , default = False , action = 'store_true' , help = 'Flag to remove all the content of the pipeline run(s) folder.' ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/clearpiperun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True if options [ 'keep_parquet' ] and options [ 'remove_all' ]: raise CommandError ( '\"--keep-parquets\" flag is incompatible with \"--remove-all\" flag' ) piperuns = options [ 'piperuns' ] flag_all_runs = True if 'clearall' in piperuns else False if flag_all_runs : logger . info ( 'clearing all pipeline run in the database' ) piperuns = list ( Run . objects . values_list ( 'name' , flat = True )) for piperun in piperuns : p_run_name = get_p_run_name ( piperun ) try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) logger . info ( \"Deleting pipeline ' %s ' from database\" , p_run_name ) with transaction . atomic (): p_run . status = 'DEL' p_run . save () p_run . delete () # remove forced measurements in db if presents forced_parquets = remove_forced_meas ( p_run . path ) # Delete parquet or folder eventually if not options [ 'keep_parquet' ] and not options [ 'remove_all' ]: logger . info ( 'Deleting pipeline \" %s \" parquets' , p_run_name ) parquets = ( glob ( os . path . join ( p_run . path , '*.parquet' )) + glob ( os . path . join ( p_run . path , '*.arrow' )) ) for parquet in parquets : try : os . remove ( parquet ) except OSError as e : self . stdout . write ( self . style . WARNING ( f 'Parquet file \" { os . path . basename ( parquet ) } \" not existent' )) pass if options [ 'remove_all' ]: logger . info ( 'Deleting pipeline folder' ) try : shutil . rmtree ( p_run . path ) except Exception as e : self . stdout . write ( self . style . WARNING ( f 'Issues in removing run folder: { e } ' )) pass","title":"clearpiperun.py"},{"location":"reference/management/commands/clearpiperun/#vast_pipeline.management.commands.clearpiperun.Command","text":"This script is used to clean the data for pipeline run(s). Use --help for usage.","title":"Command"},{"location":"reference/management/commands/clearpiperun/#vast_pipeline.management.commands.clearpiperun.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/clearpiperun.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments (required) parser . add_argument ( 'piperuns' , nargs = '+' , type = str , default = None , help = ( 'Name or path of pipeline run(s) to delete. Pass \"clearall\" to' ' delete all the runs.' ) ) # keyword arguments (optional) parser . add_argument ( '--keep-parquet' , required = False , default = False , action = 'store_true' , help = ( 'Flag to keep the pipeline run(s) parquet files. ' 'Will also apply to arrow files if present.' ) ) parser . add_argument ( '--remove-all' , required = False , default = False , action = 'store_true' , help = 'Flag to remove all the content of the pipeline run(s) folder.' )","title":"add_arguments()"},{"location":"reference/management/commands/clearpiperun/#vast_pipeline.management.commands.clearpiperun.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/clearpiperun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True if options [ 'keep_parquet' ] and options [ 'remove_all' ]: raise CommandError ( '\"--keep-parquets\" flag is incompatible with \"--remove-all\" flag' ) piperuns = options [ 'piperuns' ] flag_all_runs = True if 'clearall' in piperuns else False if flag_all_runs : logger . info ( 'clearing all pipeline run in the database' ) piperuns = list ( Run . objects . values_list ( 'name' , flat = True )) for piperun in piperuns : p_run_name = get_p_run_name ( piperun ) try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) logger . info ( \"Deleting pipeline ' %s ' from database\" , p_run_name ) with transaction . atomic (): p_run . status = 'DEL' p_run . save () p_run . delete () # remove forced measurements in db if presents forced_parquets = remove_forced_meas ( p_run . path ) # Delete parquet or folder eventually if not options [ 'keep_parquet' ] and not options [ 'remove_all' ]: logger . info ( 'Deleting pipeline \" %s \" parquets' , p_run_name ) parquets = ( glob ( os . path . join ( p_run . path , '*.parquet' )) + glob ( os . path . join ( p_run . path , '*.arrow' )) ) for parquet in parquets : try : os . remove ( parquet ) except OSError as e : self . stdout . write ( self . style . WARNING ( f 'Parquet file \" { os . path . basename ( parquet ) } \" not existent' )) pass if options [ 'remove_all' ]: logger . info ( 'Deleting pipeline folder' ) try : shutil . rmtree ( p_run . path ) except Exception as e : self . stdout . write ( self . style . WARNING ( f 'Issues in removing run folder: { e } ' )) pass","title":"handle()"},{"location":"reference/management/commands/createmeasarrow/","text":"This module defines the command for creating an arrow output file for a previously completed pipeline run. Command \u00b6 This command creates measurements and measurement_pairs arrow files for a completed pipeline run. add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/createmeasarrow.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments parser . add_argument ( 'piperun' , type = str , help = 'Path or name of the pipeline run.' ) parser . add_argument ( '--overwrite' , action = 'store_true' , required = False , default = False , help = \"Overwrite previous 'measurements.arrow' file.\" , ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/createmeasarrow.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" piperun = options [ 'piperun' ] p_run_name , run_folder = get_p_run_name ( piperun , return_folder = True ) # configure logging root_logger = logging . getLogger ( '' ) f_handler = logging . FileHandler ( os . path . join ( run_folder , timeStamped ( 'gen_arrow_log.txt' )), mode = 'w' ) f_handler . setFormatter ( root_logger . handlers [ 0 ] . formatter ) root_logger . addHandler ( f_handler ) if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) if p_run . status != 'END' : raise CommandError ( f 'Pipeline run { p_run_name } has not completed.' ) measurements_arrow = os . path . join ( run_folder , 'measurements.arrow' ) measurement_pairs_arrow = os . path . join ( run_folder , 'measurement_pairs.arrow' ) if os . path . isfile ( measurements_arrow ): if options [ 'overwrite' ]: logger . info ( \"Removing previous 'measurements.arrow' file.\" ) os . remove ( measurements_arrow ) else : logger . error ( f 'Measurements arrow file already exists for { p_run_name } ' ' and `--overwrite` has not been selected.' ) raise CommandError ( f 'Measurements arrow file already exists for { p_run_name } ' ' and `--overwrite` has not been selected.' ) if os . path . isfile ( measurement_pairs_arrow ): if options [ 'overwrite' ]: logger . info ( \"Removing previous 'measurement_pairs.arrow' file.\" ) os . remove ( measurement_pairs_arrow ) else : logger . error ( 'Measurement pairs arrow file already exists for' f ' { p_run_name } and `--overwrite` has not been selected.' ) raise CommandError ( 'Measurement pairs arrow file already exists for' f ' { p_run_name } and `--overwrite` has not been selected.' ) logger . info ( \"Creating measurements arrow file for ' %s '.\" , p_run_name ) create_measurements_arrow_file ( p_run ) logger . info ( \"Creating measurement pairs arrow file for ' %s '.\" , p_run_name ) create_measurement_pairs_arrow_file ( p_run ) logger . info ( \"Arrow files created successfully for ' %s '!\" , p_run_name )","title":"createmeasarrow.py"},{"location":"reference/management/commands/createmeasarrow/#vast_pipeline.management.commands.createmeasarrow.Command","text":"This command creates measurements and measurement_pairs arrow files for a completed pipeline run.","title":"Command"},{"location":"reference/management/commands/createmeasarrow/#vast_pipeline.management.commands.createmeasarrow.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/createmeasarrow.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments parser . add_argument ( 'piperun' , type = str , help = 'Path or name of the pipeline run.' ) parser . add_argument ( '--overwrite' , action = 'store_true' , required = False , default = False , help = \"Overwrite previous 'measurements.arrow' file.\" , )","title":"add_arguments()"},{"location":"reference/management/commands/createmeasarrow/#vast_pipeline.management.commands.createmeasarrow.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/createmeasarrow.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" piperun = options [ 'piperun' ] p_run_name , run_folder = get_p_run_name ( piperun , return_folder = True ) # configure logging root_logger = logging . getLogger ( '' ) f_handler = logging . FileHandler ( os . path . join ( run_folder , timeStamped ( 'gen_arrow_log.txt' )), mode = 'w' ) f_handler . setFormatter ( root_logger . handlers [ 0 ] . formatter ) root_logger . addHandler ( f_handler ) if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) if p_run . status != 'END' : raise CommandError ( f 'Pipeline run { p_run_name } has not completed.' ) measurements_arrow = os . path . join ( run_folder , 'measurements.arrow' ) measurement_pairs_arrow = os . path . join ( run_folder , 'measurement_pairs.arrow' ) if os . path . isfile ( measurements_arrow ): if options [ 'overwrite' ]: logger . info ( \"Removing previous 'measurements.arrow' file.\" ) os . remove ( measurements_arrow ) else : logger . error ( f 'Measurements arrow file already exists for { p_run_name } ' ' and `--overwrite` has not been selected.' ) raise CommandError ( f 'Measurements arrow file already exists for { p_run_name } ' ' and `--overwrite` has not been selected.' ) if os . path . isfile ( measurement_pairs_arrow ): if options [ 'overwrite' ]: logger . info ( \"Removing previous 'measurement_pairs.arrow' file.\" ) os . remove ( measurement_pairs_arrow ) else : logger . error ( 'Measurement pairs arrow file already exists for' f ' { p_run_name } and `--overwrite` has not been selected.' ) raise CommandError ( 'Measurement pairs arrow file already exists for' f ' { p_run_name } and `--overwrite` has not been selected.' ) logger . info ( \"Creating measurements arrow file for ' %s '.\" , p_run_name ) create_measurements_arrow_file ( p_run ) logger . info ( \"Creating measurement pairs arrow file for ' %s '.\" , p_run_name ) create_measurement_pairs_arrow_file ( p_run ) logger . info ( \"Arrow files created successfully for ' %s '!\" , p_run_name )","title":"handle()"},{"location":"reference/management/commands/debugrun/","text":"This module defines the command for debugging a pipeline run, which prints out statistics and logging. Command \u00b6 This script is used to debug data on specific pipeline run(s) or all. Use --help for usage. add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/debugrun.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments (required) parser . add_argument ( 'piperuns' , nargs = '+' , type = str , help = ( 'Name or path of pipeline run(s) to debug.Pass \"all\" to' ' print summary data of all the runs.' ) ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/debugrun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" piperuns = options [ 'piperuns' ] flag_all_runs = True if 'all' in piperuns else False if flag_all_runs : piperuns = list ( Run . objects . values_list ( 'name' , flat = True )) print ( ' ' . join ( 40 * [ '*' ])) for piperun in piperuns : p_run_name = get_p_run_name ( piperun ) try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) print ( f 'Printing summary data of pipeline run \" { p_run . name } \"' ) images = list ( p_run . image_set . values_list ( 'name' , flat = True )) print ( f 'Nr of images: { len ( images ) } ' , ) print ( 'Nr of measurements:' , Measurement . objects . filter ( image__name__in = images ) . count () ) print ( 'Nr of forced measurements:' , ( Measurement . objects . filter ( image__name__in = images , forced = True ) . count () ) ) sources = ( Source . objects . filter ( run__name = p_run . name ) . values_list ( 'id' , flat = True ) ) print ( 'Nr of sources:' , len ( sources )) print ( 'Nr of association:' , Association . objects . filter ( source_id__in = sources ) . count () ) print ( ' ' . join ( 40 * [ '*' ]))","title":"debugrun.py"},{"location":"reference/management/commands/debugrun/#vast_pipeline.management.commands.debugrun.Command","text":"This script is used to debug data on specific pipeline run(s) or all. Use --help for usage.","title":"Command"},{"location":"reference/management/commands/debugrun/#vast_pipeline.management.commands.debugrun.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/debugrun.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments (required) parser . add_argument ( 'piperuns' , nargs = '+' , type = str , help = ( 'Name or path of pipeline run(s) to debug.Pass \"all\" to' ' print summary data of all the runs.' ) )","title":"add_arguments()"},{"location":"reference/management/commands/debugrun/#vast_pipeline.management.commands.debugrun.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/debugrun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" piperuns = options [ 'piperuns' ] flag_all_runs = True if 'all' in piperuns else False if flag_all_runs : piperuns = list ( Run . objects . values_list ( 'name' , flat = True )) print ( ' ' . join ( 40 * [ '*' ])) for piperun in piperuns : p_run_name = get_p_run_name ( piperun ) try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) print ( f 'Printing summary data of pipeline run \" { p_run . name } \"' ) images = list ( p_run . image_set . values_list ( 'name' , flat = True )) print ( f 'Nr of images: { len ( images ) } ' , ) print ( 'Nr of measurements:' , Measurement . objects . filter ( image__name__in = images ) . count () ) print ( 'Nr of forced measurements:' , ( Measurement . objects . filter ( image__name__in = images , forced = True ) . count () ) ) sources = ( Source . objects . filter ( run__name = p_run . name ) . values_list ( 'id' , flat = True ) ) print ( 'Nr of sources:' , len ( sources )) print ( 'Nr of association:' , Association . objects . filter ( source_id__in = sources ) . count () ) print ( ' ' . join ( 40 * [ '*' ]))","title":"handle()"},{"location":"reference/management/commands/ingestimages/","text":"Command \u00b6 This script runs the first part of the pipeline only. It ingests a set of images into the database along with their measurements. add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/ingestimages.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" parser . add_argument ( 'image_ingest_config' , nargs = 1 , type = str , help = ( 'Image ingestion configuration filename/path.' ) ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/ingestimages.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True # Create image ingestion configuration object from input file image_config = ImageIngestConfig . from_file ( options [ 'image_ingest_config' ][ 0 ], validate = False ) # Validate the config try : image_config . validate () except PipelineConfigError as e : raise CommandError ( e ) # Create a dummy Pipeline instance using the given image ingestion configuration options d = _DummyPipeline ( image_config ) # Read, measure and upload the images listed in the image ingestion config make_upload_images ( d . img_paths , image_config . image_opts ())","title":"ingestimages.py"},{"location":"reference/management/commands/ingestimages/#vast_pipeline.management.commands.ingestimages.Command","text":"This script runs the first part of the pipeline only. It ingests a set of images into the database along with their measurements.","title":"Command"},{"location":"reference/management/commands/ingestimages/#vast_pipeline.management.commands.ingestimages.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/ingestimages.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" parser . add_argument ( 'image_ingest_config' , nargs = 1 , type = str , help = ( 'Image ingestion configuration filename/path.' ) )","title":"add_arguments()"},{"location":"reference/management/commands/ingestimages/#vast_pipeline.management.commands.ingestimages.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/ingestimages.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True # Create image ingestion configuration object from input file image_config = ImageIngestConfig . from_file ( options [ 'image_ingest_config' ][ 0 ], validate = False ) # Validate the config try : image_config . validate () except PipelineConfigError as e : raise CommandError ( e ) # Create a dummy Pipeline instance using the given image ingestion configuration options d = _DummyPipeline ( image_config ) # Read, measure and upload the images listed in the image ingestion config make_upload_images ( d . img_paths , image_config . image_opts ())","title":"handle()"},{"location":"reference/management/commands/initingest/","text":"Command \u00b6 This script creates a template image configuration file for use with image ingestion add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/initingest.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" parser . add_argument ( 'config_file_name' , nargs = 1 , type = str , help = ( 'Filename to write template ingest configuration to.' ) ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/initingest.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True template_str = make_config_template ( ImageIngestConfig . TEMPLATE_PATH , ** settings . PIPE_RUN_CONFIG_DEFAULTS ) fname = options [ 'config_file_name' ][ 0 ] # Enforce .yml extension if not fname . endswith ( '.yaml' ) and not fname . endswith ( '.yml' ): fname = fname + '.yml' print ( \"Writing template to: \" , fname ) with open ( fname , 'w' ) as f : f . write ( template_str + \" \\n \" )","title":"initingest.py"},{"location":"reference/management/commands/initingest/#vast_pipeline.management.commands.initingest.Command","text":"This script creates a template image configuration file for use with image ingestion","title":"Command"},{"location":"reference/management/commands/initingest/#vast_pipeline.management.commands.initingest.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/initingest.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" parser . add_argument ( 'config_file_name' , nargs = 1 , type = str , help = ( 'Filename to write template ingest configuration to.' ) )","title":"add_arguments()"},{"location":"reference/management/commands/initingest/#vast_pipeline.management.commands.initingest.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/initingest.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True template_str = make_config_template ( ImageIngestConfig . TEMPLATE_PATH , ** settings . PIPE_RUN_CONFIG_DEFAULTS ) fname = options [ 'config_file_name' ][ 0 ] # Enforce .yml extension if not fname . endswith ( '.yaml' ) and not fname . endswith ( '.yml' ): fname = fname + '.yml' print ( \"Writing template to: \" , fname ) with open ( fname , 'w' ) as f : f . write ( template_str + \" \\n \" )","title":"handle()"},{"location":"reference/management/commands/initpiperun/","text":"Initialises a pipeline run and creates the relevant directories. Usage: ./manage.py initpiperun pipeline_run_name Command \u00b6 This script initialise the Pipeline Run folder and related config for the pipeline. add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/initpiperun.py def add_arguments ( self , parser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments parser . add_argument ( 'runname' , type = str , help = 'Name of the pipeline run.' ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/initpiperun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True try : _ = initialise_run ( options [ 'runname' ]) except Exception as e : raise CommandError ( e ) logger . info (( 'pipeline run initialisation successful! Please modify the ' '\"config.yaml\"' )) initialise_run ( run_name , run_description = None , user = None , config = None ) \u00b6 Initialise a pipeline run. Parameters: Name Type Description Default run_name str A unique name for the run. required run_description Optional[str] Description for the run, only used if initialised with the web UI. Defaults to None. None user Optional[django.contrib.auth.models.User] User that created the run, only used if initialised with the web UI. Defaults to None. None config Optional[Dict[str, Any]] Dictionary of configuration values to pass to the run config template, only used if initialised with the web UI. Defaults to None. None Exceptions: Type Description PipelineInitError run_name was not unique. PipelineInitError A directory named run_name already exists. Returns: Type Description Run Run: The initialised pipeline Run Django model object. Source code in vast_pipeline/management/commands/initpiperun.py def initialise_run ( run_name : str , run_description : Optional [ str ] = None , user : Optional [ User ] = None , config : Optional [ Dict [ str , Any ]] = None , ) -> Run : \"\"\"Initialise a pipeline run. Args: run_name (str): A unique name for the run. run_description (Optional[str], optional): Description for the run, only used if initialised with the web UI. Defaults to None. user (Optional[User], optional): User that created the run, only used if initialised with the web UI. Defaults to None. config (Optional[Dict[str, Any]], optional): Dictionary of configuration values to pass to the run config template, only used if initialised with the web UI. Defaults to None. Raises: PipelineInitError: `run_name` was not unique. PipelineInitError: A directory named `run_name` already exists. Returns: Run: The initialised pipeline Run Django model object. \"\"\" # check for duplicated run name p_run = Run . objects . filter ( name__exact = run_name ) if p_run : msg = 'Pipeline run name already used. Change name' raise PipelineInitError ( msg ) # create the pipeline run folder run_path = os . path . join ( sett . PIPELINE_WORKING_DIR , run_name ) if os . path . exists ( run_path ): msg = 'pipeline run path already present!' raise PipelineInitError ( msg ) else : logger . info ( 'creating pipeline run folder' ) os . mkdir ( run_path ) # copy default config into the pipeline run folder logger . info ( 'copying default config in pipeline run folder' ) template_kwargs = config if config else sett . PIPE_RUN_CONFIG_DEFAULTS template_str = make_config_template ( PipelineConfig . TEMPLATE_PATH , run_path = run_path , ** template_kwargs ) with open ( os . path . join ( run_path , 'config.yaml' ), 'w' ) as fp : fp . write ( template_str ) # create entry in db p_run , _ = get_create_p_run ( run_name , run_path , run_description , user ) return p_run","title":"initpiperun.py"},{"location":"reference/management/commands/initpiperun/#vast_pipeline.management.commands.initpiperun.Command","text":"This script initialise the Pipeline Run folder and related config for the pipeline.","title":"Command"},{"location":"reference/management/commands/initpiperun/#vast_pipeline.management.commands.initpiperun.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/initpiperun.py def add_arguments ( self , parser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments parser . add_argument ( 'runname' , type = str , help = 'Name of the pipeline run.' )","title":"add_arguments()"},{"location":"reference/management/commands/initpiperun/#vast_pipeline.management.commands.initpiperun.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/initpiperun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" # configure logging if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger = logging . getLogger ( '' ) root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True try : _ = initialise_run ( options [ 'runname' ]) except Exception as e : raise CommandError ( e ) logger . info (( 'pipeline run initialisation successful! Please modify the ' '\"config.yaml\"' ))","title":"handle()"},{"location":"reference/management/commands/initpiperun/#vast_pipeline.management.commands.initpiperun.initialise_run","text":"Initialise a pipeline run. Parameters: Name Type Description Default run_name str A unique name for the run. required run_description Optional[str] Description for the run, only used if initialised with the web UI. Defaults to None. None user Optional[django.contrib.auth.models.User] User that created the run, only used if initialised with the web UI. Defaults to None. None config Optional[Dict[str, Any]] Dictionary of configuration values to pass to the run config template, only used if initialised with the web UI. Defaults to None. None Exceptions: Type Description PipelineInitError run_name was not unique. PipelineInitError A directory named run_name already exists. Returns: Type Description Run Run: The initialised pipeline Run Django model object. Source code in vast_pipeline/management/commands/initpiperun.py def initialise_run ( run_name : str , run_description : Optional [ str ] = None , user : Optional [ User ] = None , config : Optional [ Dict [ str , Any ]] = None , ) -> Run : \"\"\"Initialise a pipeline run. Args: run_name (str): A unique name for the run. run_description (Optional[str], optional): Description for the run, only used if initialised with the web UI. Defaults to None. user (Optional[User], optional): User that created the run, only used if initialised with the web UI. Defaults to None. config (Optional[Dict[str, Any]], optional): Dictionary of configuration values to pass to the run config template, only used if initialised with the web UI. Defaults to None. Raises: PipelineInitError: `run_name` was not unique. PipelineInitError: A directory named `run_name` already exists. Returns: Run: The initialised pipeline Run Django model object. \"\"\" # check for duplicated run name p_run = Run . objects . filter ( name__exact = run_name ) if p_run : msg = 'Pipeline run name already used. Change name' raise PipelineInitError ( msg ) # create the pipeline run folder run_path = os . path . join ( sett . PIPELINE_WORKING_DIR , run_name ) if os . path . exists ( run_path ): msg = 'pipeline run path already present!' raise PipelineInitError ( msg ) else : logger . info ( 'creating pipeline run folder' ) os . mkdir ( run_path ) # copy default config into the pipeline run folder logger . info ( 'copying default config in pipeline run folder' ) template_kwargs = config if config else sett . PIPE_RUN_CONFIG_DEFAULTS template_str = make_config_template ( PipelineConfig . TEMPLATE_PATH , run_path = run_path , ** template_kwargs ) with open ( os . path . join ( run_path , 'config.yaml' ), 'w' ) as fp : fp . write ( template_str ) # create entry in db p_run , _ = get_create_p_run ( run_name , run_path , run_description , user ) return p_run","title":"initialise_run()"},{"location":"reference/management/commands/restorepiperun/","text":"Command \u00b6 This command is used to restore a pipeline run to the previous verion after add mode has been used. Use --help for usage. add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/restorepiperun.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments (required) parser . add_argument ( 'piperun' , type = str , default = None , help = 'Name or path of pipeline run(s) to restore.' ) # keyword arguments (optional) parser . add_argument ( '--no-confirm' , required = False , default = False , action = 'store_true' , help = ( 'Flag to skip the confirmation stage and proceed to restore' ' the pipeline run.' ) ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/restorepiperun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" piperun = options [ 'piperun' ] p_run_name , run_folder = get_p_run_name ( piperun , return_folder = True ) # configure logging root_logger = logging . getLogger ( '' ) f_handler = logging . FileHandler ( os . path . join ( run_folder , timeStamped ( 'restore_log.txt' )), mode = 'w' ) f_handler . setFormatter ( root_logger . handlers [ 0 ] . formatter ) root_logger . addHandler ( f_handler ) if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) if p_run . status not in [ 'END' , 'ERR' ]: raise CommandError ( f \"Run { p_run_name } does not have an 'END' or 'ERR' status.\" \" Unable to run restore.\" ) path = p_run . path pipeline = Pipeline ( name = p_run_name , config_path = os . path . join ( path , 'config.yaml' ) ) try : # update pipeline run status to restoring prev_status = p_run . status pipeline . set_status ( p_run , 'RES' ) prev_config_file = os . path . join ( p_run . path , 'config.yaml.bak' ) if os . path . isfile ( prev_config_file ): shutil . copy ( prev_config_file , prev_config_file . replace ( '.yaml.bak' , '.bak.yaml' ) ) prev_config_file = prev_config_file . replace ( '.yaml.bak' , '.bak.yaml' ) prev_config = PipelineConfig . from_file ( prev_config_file ) os . remove ( prev_config_file ) else : raise CommandError ( 'Previous config file does not exist.' ' Cannot restore pipeline run.' ) bak_files = {} for i in [ 'associations' , 'bands' , 'images' , 'measurement_pairs' , 'relations' , 'skyregions' , 'sources' , 'config' ]: if i == 'config' : f_name = os . path . join ( p_run . path , f ' { i } .yaml.bak' ) else : f_name = os . path . join ( p_run . path , f ' { i } .parquet.bak' ) if os . path . isfile ( f_name ): bak_files [ i ] = f_name else : raise CommandError ( f 'File { f_name } does not exist.' ' Cannot restore pipeline run.' ) logger_msg = \"Will restore the run to the following config: \\n \" logger . info ( logger_msg + prev_config . _yaml . as_yaml ()) user_continue = True if options [ 'no_confirm' ] else yesno ( \"Would you like to restore the run\" ) if user_continue : restore_pipe ( p_run , bak_files , prev_config ) pipeline . set_status ( p_run , 'END' ) logger . info ( 'Restore complete.' ) else : pipeline . set_status ( p_run , prev_status ) logger . info ( 'No actions performed.' ) except Exception as e : logger . error ( 'Restoring failed!' ) logger . error ( e ) pipeline . set_status ( p_run , prev_status ) restore_pipe ( p_run , bak_files , prev_config ) \u00b6 Restores the pipeline to the backup files version. Parameters: Name Type Description Default p_run Run The run model object. required bak_files Dict[str, str] Dictionary containing the paths to the .bak files. required prev_config PipelineConfig Back up run configuration. required Returns: Type Description None None Source code in vast_pipeline/management/commands/restorepiperun.py def restore_pipe ( p_run : Run , bak_files : Dict [ str , str ], prev_config : PipelineConfig ) -> None : \"\"\" Restores the pipeline to the backup files version. Args: p_run (Run): The run model object. bak_files (Dict[str, str]): Dictionary containing the paths to the .bak files. prev_config (PipelineConfig): Back up run configuration. Returns: None \"\"\" # check images match img_f_list = prev_config [ \"inputs\" ][ \"image\" ] if isinstance ( img_f_list , dict ): img_f_list = [ item for sublist in img_f_list . values () for item in sublist ] img_f_list = [ os . path . basename ( i ) for i in img_f_list ] prev_images = pd . read_parquet ( bak_files [ 'images' ], columns = [ 'id' , 'name' , 'measurements_path' ] ) if sorted ( prev_images [ 'name' ] . tolist ()) != sorted ( img_f_list ): raise CommandError ( 'Images in previous config file does not' ' match those found in the previous images.parquet.bak.' ' Cannot restore pipeline run.' ) # check forced measurements monitor = prev_config [ \"source_monitoring\" ][ \"monitor\" ] if monitor : forced_parquets = glob ( os . path . join ( p_run . path , 'forced_*.parquet.bak' )) if not forced_parquets : raise CommandError ( 'source_monitoring.monitor is \\' True \\' in the previous configuration but' ' no .bak forced parquet files have been found.' ' Cannot restore pipeline run.' ) else : # load old associations bak_meas_id = pd . read_parquet ( bak_files [ 'associations' ], columns = [ 'meas_id' ] )[ 'meas_id' ] . unique () # load backup forced measurements forced_meas = pd . concat ( [ pd . read_parquet ( i , columns = [ 'id' ]) for i in forced_parquets ] ) # load image meas meas = pd . concat ( [ pd . read_parquet ( i , columns = [ 'id' ] ) for i in prev_images [ 'measurements_path' ]] ) # Get forced ids from the associations forced_meas_id = bak_meas_id [ np . isin ( bak_meas_id , meas [ 'id' ] . to_numpy (), invert = True ) ] if not np . array_equal ( np . sort ( forced_meas_id ), np . sort ( forced_meas [ 'id' ] . to_numpy ()) ): raise CommandError ( 'The forced measurements .bak files do not match the' ' previous run.' ' Cannot restore pipeline run.' ) logger . info ( \"Restoring ' %s ' from backup parquet files.\" , p_run . name ) # Delete any new sources bak_sources = pd . read_parquet ( bak_files [ 'sources' ]) sources_to_delete = ( Source . objects . filter ( run = p_run ) . exclude ( id__in = bak_sources . index . to_numpy ()) ) if sources_to_delete . exists (): with transaction . atomic (): n_del , detail_del = sources_to_delete . delete () logger . info ( ( 'Deleting new sources and associated objects to restore run' ' Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) # Delete newly created relations of sources that still exist after deleting # the new sources bak_relations = pd . read_parquet ( bak_files [ 'relations' ]) db_relations = pd . DataFrame ( list ( RelatedSource . objects . filter ( from_source_id__run = p_run ) . values ()) ) diff = pd . merge ( db_relations , bak_relations , on = [ 'from_source_id' , 'to_source_id' ], how = 'left' , indicator = 'exist' ) relations_to_drop = diff [ diff [ 'exist' ] == 'left_only' ][ 'id' ] . to_numpy () relations_to_drop = RelatedSource . objects . filter ( id__in = relations_to_drop ) with transaction . atomic (): n_del , detail_del = relations_to_drop . delete () logger . info ( ( 'Deleting left over relations after dropping new sources' ' Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) if monitor : current_forced_parquets = glob ( os . path . join ( p_run . path , 'forced_*.parquet' )) current_forced_meas = pd . concat ( [ pd . read_parquet ( i , columns = [ 'id' ] ) for i in current_forced_parquets ] ) ids_to_delete = current_forced_meas . loc [ ~ current_forced_meas [ 'id' ] . isin ( forced_meas [ 'id' ] . to_numpy ()), 'id' ] meas_to_delete = Measurement . objects . filter ( id__in = ids_to_delete ) del ids_to_delete if meas_to_delete . exists (): with transaction . atomic (): n_del , detail_del = meas_to_delete . delete () logger . info ( ( 'Deleting forced measurement and associated' ' objects to restore run. Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) # restore source metrics logger . info ( f 'Restoring metrics for { bak_sources . shape [ 0 ] } sources.' ) bak_sources = update_sources ( bak_sources ) # remove images from run images_to_remove = ( Image . objects . filter ( run = p_run ) . exclude ( id__in = prev_images [ 'id' ] . to_numpy ()) ) logger . info ( f 'Removing { len ( images_to_remove ) } images from the run.' ) if images_to_remove . exists (): with transaction . atomic (): p_run . image_set . remove ( * images_to_remove ) # load old associations to remove all new assoc bak_assoc = pd . read_parquet ( bak_files [ 'associations' ], columns = [ 'source_id' , 'meas_id' ] ) # get unique source and meas id values in the previous run bak_source_ids = bak_assoc [ 'source_id' ] . unique () bak_meas_ids = bak_assoc [ 'meas_id' ] . unique () # create query to only obtain associations that are not part of the # previous run association_criteria_1 = Q ( source_id__in = bak_source_ids ) association_criteria_2 = ~ Q ( meas_id__in = bak_meas_ids ) associations_to_delete = Association . objects . filter ( association_criteria_1 and association_criteria_2 ) if associations_to_delete . exists (): with transaction . atomic (): n_del , detail_del = associations_to_delete . delete () logger . info ( ( 'Deleting associations to restore run.' ' Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) logger . info ( 'Restoring run metrics.' ) p_run . n_images = prev_images . shape [ 0 ] p_run . n_sources = bak_sources . shape [ 0 ] if monitor : p_run . n_selavy_measurements = meas . shape [ 0 ] p_run . n_forced_measurements = forced_meas . shape [ 0 ] else : p_run . n_selavy_measurements = bak_meas_ids . shape [ 0 ] with transaction . atomic (): p_run . save () # switch files and delete backups logger . info ( 'Restoring parquet files and removing .bak files.' ) for i in bak_files : bak_file = bak_files [ i ] if i == 'config' : actual_file = bak_file . replace ( '.yaml.bak' , '_prev.yaml' ) else : actual_file = bak_file . replace ( '.bak' , '' ) shutil . copy ( bak_file , actual_file ) os . remove ( bak_file ) if monitor : for i in current_forced_parquets : os . remove ( i ) for i in forced_parquets : new_file = i . replace ( '.bak' , '' ) shutil . copy ( i , new_file ) os . remove ( i ) yesno ( question ) \u00b6 Simple Yes/No Function. Parameters: Name Type Description Default question str The question to show to the user for a y/n response. required Returns: Type Description bool True if user enters 'y', False if 'n'. Source code in vast_pipeline/management/commands/restorepiperun.py def yesno ( question : str ) -> bool : \"\"\" Simple Yes/No Function. Args: question (str): The question to show to the user for a y/n response. Returns: True if user enters 'y', False if 'n'. \"\"\" prompt = f ' { question } ? (y/n): ' ans = input ( prompt ) . strip () . lower () if ans not in [ 'y' , 'n' ]: print ( f ' { ans } is invalid, please try again...' ) return yesno ( question ) if ans == 'y' : return True return False","title":"restorepiperun.py"},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.Command","text":"This command is used to restore a pipeline run to the previous verion after add mode has been used. Use --help for usage.","title":"Command"},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/restorepiperun.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments (required) parser . add_argument ( 'piperun' , type = str , default = None , help = 'Name or path of pipeline run(s) to restore.' ) # keyword arguments (optional) parser . add_argument ( '--no-confirm' , required = False , default = False , action = 'store_true' , help = ( 'Flag to skip the confirmation stage and proceed to restore' ' the pipeline run.' ) )","title":"add_arguments()"},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/restorepiperun.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" piperun = options [ 'piperun' ] p_run_name , run_folder = get_p_run_name ( piperun , return_folder = True ) # configure logging root_logger = logging . getLogger ( '' ) f_handler = logging . FileHandler ( os . path . join ( run_folder , timeStamped ( 'restore_log.txt' )), mode = 'w' ) f_handler . setFormatter ( root_logger . handlers [ 0 ] . formatter ) root_logger . addHandler ( f_handler ) if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True try : p_run = Run . objects . get ( name = p_run_name ) except Run . DoesNotExist : raise CommandError ( f 'Pipeline run { p_run_name } does not exist' ) if p_run . status not in [ 'END' , 'ERR' ]: raise CommandError ( f \"Run { p_run_name } does not have an 'END' or 'ERR' status.\" \" Unable to run restore.\" ) path = p_run . path pipeline = Pipeline ( name = p_run_name , config_path = os . path . join ( path , 'config.yaml' ) ) try : # update pipeline run status to restoring prev_status = p_run . status pipeline . set_status ( p_run , 'RES' ) prev_config_file = os . path . join ( p_run . path , 'config.yaml.bak' ) if os . path . isfile ( prev_config_file ): shutil . copy ( prev_config_file , prev_config_file . replace ( '.yaml.bak' , '.bak.yaml' ) ) prev_config_file = prev_config_file . replace ( '.yaml.bak' , '.bak.yaml' ) prev_config = PipelineConfig . from_file ( prev_config_file ) os . remove ( prev_config_file ) else : raise CommandError ( 'Previous config file does not exist.' ' Cannot restore pipeline run.' ) bak_files = {} for i in [ 'associations' , 'bands' , 'images' , 'measurement_pairs' , 'relations' , 'skyregions' , 'sources' , 'config' ]: if i == 'config' : f_name = os . path . join ( p_run . path , f ' { i } .yaml.bak' ) else : f_name = os . path . join ( p_run . path , f ' { i } .parquet.bak' ) if os . path . isfile ( f_name ): bak_files [ i ] = f_name else : raise CommandError ( f 'File { f_name } does not exist.' ' Cannot restore pipeline run.' ) logger_msg = \"Will restore the run to the following config: \\n \" logger . info ( logger_msg + prev_config . _yaml . as_yaml ()) user_continue = True if options [ 'no_confirm' ] else yesno ( \"Would you like to restore the run\" ) if user_continue : restore_pipe ( p_run , bak_files , prev_config ) pipeline . set_status ( p_run , 'END' ) logger . info ( 'Restore complete.' ) else : pipeline . set_status ( p_run , prev_status ) logger . info ( 'No actions performed.' ) except Exception as e : logger . error ( 'Restoring failed!' ) logger . error ( e ) pipeline . set_status ( p_run , prev_status )","title":"handle()"},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.restore_pipe","text":"Restores the pipeline to the backup files version. Parameters: Name Type Description Default p_run Run The run model object. required bak_files Dict[str, str] Dictionary containing the paths to the .bak files. required prev_config PipelineConfig Back up run configuration. required Returns: Type Description None None Source code in vast_pipeline/management/commands/restorepiperun.py def restore_pipe ( p_run : Run , bak_files : Dict [ str , str ], prev_config : PipelineConfig ) -> None : \"\"\" Restores the pipeline to the backup files version. Args: p_run (Run): The run model object. bak_files (Dict[str, str]): Dictionary containing the paths to the .bak files. prev_config (PipelineConfig): Back up run configuration. Returns: None \"\"\" # check images match img_f_list = prev_config [ \"inputs\" ][ \"image\" ] if isinstance ( img_f_list , dict ): img_f_list = [ item for sublist in img_f_list . values () for item in sublist ] img_f_list = [ os . path . basename ( i ) for i in img_f_list ] prev_images = pd . read_parquet ( bak_files [ 'images' ], columns = [ 'id' , 'name' , 'measurements_path' ] ) if sorted ( prev_images [ 'name' ] . tolist ()) != sorted ( img_f_list ): raise CommandError ( 'Images in previous config file does not' ' match those found in the previous images.parquet.bak.' ' Cannot restore pipeline run.' ) # check forced measurements monitor = prev_config [ \"source_monitoring\" ][ \"monitor\" ] if monitor : forced_parquets = glob ( os . path . join ( p_run . path , 'forced_*.parquet.bak' )) if not forced_parquets : raise CommandError ( 'source_monitoring.monitor is \\' True \\' in the previous configuration but' ' no .bak forced parquet files have been found.' ' Cannot restore pipeline run.' ) else : # load old associations bak_meas_id = pd . read_parquet ( bak_files [ 'associations' ], columns = [ 'meas_id' ] )[ 'meas_id' ] . unique () # load backup forced measurements forced_meas = pd . concat ( [ pd . read_parquet ( i , columns = [ 'id' ]) for i in forced_parquets ] ) # load image meas meas = pd . concat ( [ pd . read_parquet ( i , columns = [ 'id' ] ) for i in prev_images [ 'measurements_path' ]] ) # Get forced ids from the associations forced_meas_id = bak_meas_id [ np . isin ( bak_meas_id , meas [ 'id' ] . to_numpy (), invert = True ) ] if not np . array_equal ( np . sort ( forced_meas_id ), np . sort ( forced_meas [ 'id' ] . to_numpy ()) ): raise CommandError ( 'The forced measurements .bak files do not match the' ' previous run.' ' Cannot restore pipeline run.' ) logger . info ( \"Restoring ' %s ' from backup parquet files.\" , p_run . name ) # Delete any new sources bak_sources = pd . read_parquet ( bak_files [ 'sources' ]) sources_to_delete = ( Source . objects . filter ( run = p_run ) . exclude ( id__in = bak_sources . index . to_numpy ()) ) if sources_to_delete . exists (): with transaction . atomic (): n_del , detail_del = sources_to_delete . delete () logger . info ( ( 'Deleting new sources and associated objects to restore run' ' Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) # Delete newly created relations of sources that still exist after deleting # the new sources bak_relations = pd . read_parquet ( bak_files [ 'relations' ]) db_relations = pd . DataFrame ( list ( RelatedSource . objects . filter ( from_source_id__run = p_run ) . values ()) ) diff = pd . merge ( db_relations , bak_relations , on = [ 'from_source_id' , 'to_source_id' ], how = 'left' , indicator = 'exist' ) relations_to_drop = diff [ diff [ 'exist' ] == 'left_only' ][ 'id' ] . to_numpy () relations_to_drop = RelatedSource . objects . filter ( id__in = relations_to_drop ) with transaction . atomic (): n_del , detail_del = relations_to_drop . delete () logger . info ( ( 'Deleting left over relations after dropping new sources' ' Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) if monitor : current_forced_parquets = glob ( os . path . join ( p_run . path , 'forced_*.parquet' )) current_forced_meas = pd . concat ( [ pd . read_parquet ( i , columns = [ 'id' ] ) for i in current_forced_parquets ] ) ids_to_delete = current_forced_meas . loc [ ~ current_forced_meas [ 'id' ] . isin ( forced_meas [ 'id' ] . to_numpy ()), 'id' ] meas_to_delete = Measurement . objects . filter ( id__in = ids_to_delete ) del ids_to_delete if meas_to_delete . exists (): with transaction . atomic (): n_del , detail_del = meas_to_delete . delete () logger . info ( ( 'Deleting forced measurement and associated' ' objects to restore run. Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) # restore source metrics logger . info ( f 'Restoring metrics for { bak_sources . shape [ 0 ] } sources.' ) bak_sources = update_sources ( bak_sources ) # remove images from run images_to_remove = ( Image . objects . filter ( run = p_run ) . exclude ( id__in = prev_images [ 'id' ] . to_numpy ()) ) logger . info ( f 'Removing { len ( images_to_remove ) } images from the run.' ) if images_to_remove . exists (): with transaction . atomic (): p_run . image_set . remove ( * images_to_remove ) # load old associations to remove all new assoc bak_assoc = pd . read_parquet ( bak_files [ 'associations' ], columns = [ 'source_id' , 'meas_id' ] ) # get unique source and meas id values in the previous run bak_source_ids = bak_assoc [ 'source_id' ] . unique () bak_meas_ids = bak_assoc [ 'meas_id' ] . unique () # create query to only obtain associations that are not part of the # previous run association_criteria_1 = Q ( source_id__in = bak_source_ids ) association_criteria_2 = ~ Q ( meas_id__in = bak_meas_ids ) associations_to_delete = Association . objects . filter ( association_criteria_1 and association_criteria_2 ) if associations_to_delete . exists (): with transaction . atomic (): n_del , detail_del = associations_to_delete . delete () logger . info ( ( 'Deleting associations to restore run.' ' Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) logger . info ( 'Restoring run metrics.' ) p_run . n_images = prev_images . shape [ 0 ] p_run . n_sources = bak_sources . shape [ 0 ] if monitor : p_run . n_selavy_measurements = meas . shape [ 0 ] p_run . n_forced_measurements = forced_meas . shape [ 0 ] else : p_run . n_selavy_measurements = bak_meas_ids . shape [ 0 ] with transaction . atomic (): p_run . save () # switch files and delete backups logger . info ( 'Restoring parquet files and removing .bak files.' ) for i in bak_files : bak_file = bak_files [ i ] if i == 'config' : actual_file = bak_file . replace ( '.yaml.bak' , '_prev.yaml' ) else : actual_file = bak_file . replace ( '.bak' , '' ) shutil . copy ( bak_file , actual_file ) os . remove ( bak_file ) if monitor : for i in current_forced_parquets : os . remove ( i ) for i in forced_parquets : new_file = i . replace ( '.bak' , '' ) shutil . copy ( i , new_file ) os . remove ( i )","title":"restore_pipe()"},{"location":"reference/management/commands/restorepiperun/#vast_pipeline.management.commands.restorepiperun.yesno","text":"Simple Yes/No Function. Parameters: Name Type Description Default question str The question to show to the user for a y/n response. required Returns: Type Description bool True if user enters 'y', False if 'n'. Source code in vast_pipeline/management/commands/restorepiperun.py def yesno ( question : str ) -> bool : \"\"\" Simple Yes/No Function. Args: question (str): The question to show to the user for a y/n response. Returns: True if user enters 'y', False if 'n'. \"\"\" prompt = f ' { question } ? (y/n): ' ans = input ( prompt ) . strip () . lower () if ans not in [ 'y' , 'n' ]: print ( f ' { ans } is invalid, please try again...' ) return yesno ( question ) if ans == 'y' : return True return False","title":"yesno()"},{"location":"reference/management/commands/runpipeline/","text":"The main command to launch the processing of a pipeline run. Usage: ./manage.py runpipeline pipeline_run_name Command \u00b6 This script is used to process images with the ASKAP transient pipeline. Use --help for usage, and refer README. add_arguments ( self , parser ) \u00b6 Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/runpipeline.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments parser . add_argument ( 'piperun' , type = str , help = 'Path or name of the pipeline run.' ) parser . add_argument ( '--full-rerun' , required = False , default = False , action = 'store_true' , help = ( 'Flag to signify that a full re-run is requested.' ' Old data is completely removed and replaced.' ) ) handle ( self , * args , ** options ) \u00b6 Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/runpipeline.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" p_run_name , run_folder = get_p_run_name ( options [ 'piperun' ], return_folder = True ) # configure logging root_logger = logging . getLogger ( '' ) f_handler = logging . FileHandler ( os . path . join ( run_folder , timeStamped ( 'log.txt' )), mode = 'w' ) f_handler . setFormatter ( root_logger . handlers [ 0 ] . formatter ) root_logger . addHandler ( f_handler ) if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True # p_run_name = p_run_path # remove ending / if present if p_run_name [ - 1 ] == '/' : p_run_name = p_run_name [: - 1 ] # grab only the name from the path p_run_name = p_run_name . split ( os . path . sep )[ - 1 ] debug_flag = True if options [ 'verbosity' ] > 1 else False _ = run_pipe ( p_run_name , path_name = run_folder , debug = debug_flag , full_rerun = options [ \"full_rerun\" ], ) self . stdout . write ( self . style . SUCCESS ( 'Finished' )) run_pipe ( name , path_name = None , run_dj_obj = None , cli = True , debug = False , user = None , full_rerun = False , prev_ui_status = 'END' ) \u00b6 Main function to run the pipeline. Parameters: Name Type Description Default name str The name of the pipeline run (p_run.name). required path_name Optional[str] The path of the directory of the pipeline run (p_run.path), defaults to None. None run_dj_obj Optional[vast_pipeline.models.Run] The Run object of the pipeline run, defaults to None. None cli bool Flag to signify whether the pipeline run has been run via the UI (False), or the command line (True). Defaults to True. True debug bool Flag to signify whether to enable debug verbosity to the logging output. Defaults to False. False user Optional[django.contrib.auth.models.User] The User of the request if made through the UI. Defaults to None. None full_rerun bool If the run already exists, a complete rerun will be performed which will remove and replace all the previous results. False prev_ui_status str The previous status through the UI. Defaults to 'END'. 'END' Returns: Type Description bool Boolean equal to True on a successful completion, or in cases of failures a CommandError is returned. Source code in vast_pipeline/management/commands/runpipeline.py def run_pipe ( name : str , path_name : Optional [ str ] = None , run_dj_obj : Optional [ Run ] = None , cli : bool = True , debug : bool = False , user : Optional [ User ] = None , full_rerun : bool = False , prev_ui_status : str = 'END' ) -> bool : ''' Main function to run the pipeline. Args: name: The name of the pipeline run (p_run.name). path_name: The path of the directory of the pipeline run (p_run.path), defaults to None. run_dj_obj: The Run object of the pipeline run, defaults to None. cli: Flag to signify whether the pipeline run has been run via the UI (False), or the command line (True). Defaults to True. debug: Flag to signify whether to enable debug verbosity to the logging output. Defaults to False. user: The User of the request if made through the UI. Defaults to None. full_rerun: If the run already exists, a complete rerun will be performed which will remove and replace all the previous results. prev_ui_status: The previous status through the UI. Defaults to 'END'. Returns: Boolean equal to `True` on a successful completion, or in cases of failures a CommandError is returned. ''' path = run_dj_obj . path if run_dj_obj else path_name # set up logging for running pipeline from UI if not cli : # set up the logger for the UI job root_logger = logging . getLogger ( '' ) if debug : root_logger . setLevel ( logging . DEBUG ) f_handler = logging . FileHandler ( os . path . join ( path , timeStamped ( 'log.txt' )), mode = 'w' ) f_handler . setFormatter ( root_logger . handlers [ 0 ] . formatter ) root_logger . addHandler ( f_handler ) pipeline = Pipeline ( name = run_dj_obj . name if run_dj_obj else name , config_path = os . path . join ( path , 'config.yaml' ), validate_config = False , # delay validation ) # Create the pipeline run in DB p_run , flag_exist = get_create_p_run ( pipeline . name , pipeline . config [ \"run\" ][ \"path\" ], ) # backup the last successful outputs. # if the run is being run again and the last status is END then the # user is highly likely to be attempting to add images. Making the backups # now from a guaranteed successful run is safer in case of problems # with the config file below that causes an error. if flag_exist : if cli and p_run . status == 'END' : backup_parquets ( p_run . path ) elif not cli and prev_ui_status == 'END' : backup_parquets ( p_run . path ) # validate run configuration try : pipeline . config . validate ( user = user ) except PipelineConfigError as e : if debug : traceback . print_exc () logger . exception ( 'Config error: \\n %s ' , e ) msg = f 'Config error: \\n { e } ' # If the run is already created (e.g. through UI) then set status to # error pipeline . set_status ( p_run , 'ERR' ) raise CommandError ( msg ) if cli else PipelineConfigError ( msg ) # clean up pipeline images and forced measurements for re-runs # Scenarios: # A. Complete Re-run: If the job is marked as successful then backup # old parquets and proceed to remove parquets along with forced # extractions from the database. # B. Additional Run on successful run: Backup parquets, remove current # parquets and proceed. # C. Additional Run on errored run: Do not backup parquets, just delete # current. # D. Running on initialised run that errored and is still the init run. # Flag on the pipeline object on whether the addition mode is on or off. pipeline . add_mode = False pipeline . previous_parquets = {} prev_config_exists = False try : if not flag_exist : # check for and remove any present .parquet (and .arrow) files parquets = ( glob . glob ( os . path . join ( p_run . path , \"*.parquet\" )) # TODO Remove arrow when arrow files are no longer needed. + glob . glob ( os . path . join ( p_run . path , \"*.arrow\" )) + glob . glob ( os . path . join ( p_run . path , \"*.bak\" )) ) for parquet in parquets : os . remove ( parquet ) # copy across config file at the start logger . debug ( \"Copying temp config file.\" ) create_temp_config_file ( p_run . path ) else : # Check if the status is already running or queued. Exit if this is # the case. if p_run . status in [ 'RUN' , 'RES' ]: logger . error ( \"The pipeline run requested to process already has a \" \"running or restoring status! Performing no actions. \" \"Exiting.\" ) return True # copy across config file at the start logger . debug ( \"Copying temp config file.\" ) create_temp_config_file ( p_run . path ) # Check if there is a previous run config and back up if so if os . path . isfile ( os . path . join ( p_run . path , 'config_prev.yaml' ) ): prev_config_exists = True shutil . copy ( os . path . join ( p_run . path , 'config_prev.yaml' ), os . path . join ( p_run . path , 'config.yaml.bak' ) ) logger . debug ( f 'config_prev.yaml exists: { prev_config_exists } ' ) # Check for an error status and whether any previous config file # exists - if it doesn't exist it means the run has failed during # the first run. In this case we want to clear anything that has # gone on before so to do that `complete-rerun` mode is activated. if not prev_config_exists : if cli and p_run . status == \"ERR\" : full_rerun = True elif not cli and prev_ui_status == \"ERR\" : full_rerun = True logger . debug ( f 'Full re-run: { full_rerun } ' ) # Check if the run has only been initialised, if so we don't want # to do any previous run checks or cleaning. if p_run . status == 'INI' : initial_run = True # check if coming from UI elif cli is False and prev_ui_status == 'INI' : initial_run = True else : initial_run = False if initial_run is False : parquets = ( glob . glob ( os . path . join ( p_run . path , \"*.parquet\" )) # TODO Remove arrow when arrow files are no longer needed. + glob . glob ( os . path . join ( p_run . path , \"*.arrow\" )) ) if full_rerun : logger . info ( 'Cleaning up pipeline run before re-process data' ) p_run . image_set . clear () logger . info ( 'Cleaning up forced measurements before re-process data' ) remove_forced_meas ( p_run . path ) for parquet in parquets : os . remove ( parquet ) # remove bak files bak_files = glob . glob ( os . path . join ( p_run . path , \"*.bak\" )) if bak_files : for bf in bak_files : os . remove ( bf ) # remove previous config if it exists if prev_config_exists : os . remove ( os . path . join ( p_run . path , 'config_prev.yaml' )) # reset epoch_based flag with transaction . atomic (): p_run . epoch_based = False p_run . save () else : # Before parquets are started to be copied and backed up, a # check is run to see if anything has actually changed in # the config config_diff = pipeline . config . check_prev_config_diff () if config_diff : logger . info ( \"The config file has either not changed since the\" \" previous run or other settings have changed such\" \" that a new or complete re-run should be performed\" \" instead. Performing no actions. Exiting.\" ) os . remove ( os . path . join ( p_run . path , 'config_temp.yaml' )) pipeline . set_status ( p_run , 'END' ) return True if pipeline . config . epoch_based != p_run . epoch_based : logger . info ( \"The 'epoch based' setting has changed since the\" \" previous run. A complete re-run is required if\" \" changing to epoch based mode or vice versa.\" ) os . remove ( os . path . join ( p_run . path , 'config_temp.yaml' )) pipeline . set_status ( p_run , 'END' ) return True pipeline . add_mode = True for i in [ 'images' , 'associations' , 'sources' , 'relations' , 'measurement_pairs' ]: pipeline . previous_parquets [ i ] = os . path . join ( p_run . path , f ' { i } .parquet.bak' ) except Exception as e : logger . error ( 'Unexpected error occurred in pre-run steps!' ) pipeline . set_status ( p_run , 'ERR' ) logger . exception ( 'Processing error: \\n %s ' , e ) raise CommandError ( f 'Processing error: \\n { e } ' ) if pipeline . config [ \"run\" ][ \"suppress_astropy_warnings\" ]: warnings . simplefilter ( \"ignore\" , category = AstropyWarning ) logger . info ( \"VAST Pipeline version: %s \" , pipeline_version ) logger . info ( \"Source finder: %s \" , pipeline . config [ \"measurements\" ][ \"source_finder\" ] ) logger . info ( \"Using pipeline run ' %s '\" , pipeline . name ) logger . info ( \"Source monitoring: %s \" , pipeline . config [ \"source_monitoring\" ][ \"monitor\" ] ) # log the list of input data files for posterity input_image_list = [ image for image_list in pipeline . config [ \"inputs\" ][ \"image\" ] . values () for image in image_list ] input_selavy_list = [ selavy for selavy_list in pipeline . config [ \"inputs\" ][ \"selavy\" ] . values () for selavy in selavy_list ] input_noise_list = [ noise for noise_list in pipeline . config [ \"inputs\" ][ \"noise\" ] . values () for noise in noise_list ] if \"background\" in pipeline . config [ \"inputs\" ] . keys (): input_background_list = [ background for background_list in pipeline . config [ \"inputs\" ][ \"background\" ] . values () for background in background_list ] else : input_background_list = [ \"N/A\" , ] * len ( input_image_list ) for image , selavy , noise , background in zip ( input_image_list , input_selavy_list , input_noise_list , input_background_list ): logger . info ( \"Matched inputs - image: %s , selavy: %s , noise: %s , background: %s \" , image , selavy , noise , background , ) stopwatch = StopWatch () # run the pipeline operations try : # check if max runs number is reached pipeline . check_current_runs () # run the pipeline pipeline . set_status ( p_run , 'RUN' ) pipeline . process_pipeline ( p_run ) # Create arrow file after success if selected. if pipeline . config [ \"measurements\" ][ \"write_arrow_files\" ]: create_measurements_arrow_file ( p_run ) create_measurement_pairs_arrow_file ( p_run ) except Exception as e : # set the pipeline status as error pipeline . set_status ( p_run , 'ERR' ) logger . exception ( 'Processing error: \\n %s ' , e ) raise CommandError ( f 'Processing error: \\n { e } ' ) # copy across config file now that it is successful logger . debug ( \"Copying and cleaning temp config file.\" ) shutil . copyfile ( os . path . join ( p_run . path , 'config_temp.yaml' ), os . path . join ( p_run . path , 'config_prev.yaml' )) os . remove ( os . path . join ( p_run . path , 'config_temp.yaml' )) # set the pipeline status as completed pipeline . set_status ( p_run , 'END' ) logger . info ( 'Total pipeline processing time %.2f sec' , stopwatch . reset () ) return True","title":"runpipeline.py"},{"location":"reference/management/commands/runpipeline/#vast_pipeline.management.commands.runpipeline.Command","text":"This script is used to process images with the ASKAP transient pipeline. Use --help for usage, and refer README.","title":"Command"},{"location":"reference/management/commands/runpipeline/#vast_pipeline.management.commands.runpipeline.Command.add_arguments","text":"Enables arguments for the command. Parameters: Name Type Description Default parser ArgumentParser The parser object of the command. required Returns: Type Description None None Source code in vast_pipeline/management/commands/runpipeline.py def add_arguments ( self , parser : ArgumentParser ) -> None : \"\"\" Enables arguments for the command. Args: parser (ArgumentParser): The parser object of the command. Returns: None \"\"\" # positional arguments parser . add_argument ( 'piperun' , type = str , help = 'Path or name of the pipeline run.' ) parser . add_argument ( '--full-rerun' , required = False , default = False , action = 'store_true' , help = ( 'Flag to signify that a full re-run is requested.' ' Old data is completely removed and replaced.' ) )","title":"add_arguments()"},{"location":"reference/management/commands/runpipeline/#vast_pipeline.management.commands.runpipeline.Command.handle","text":"Handle function of the command. Parameters: Name Type Description Default *args Variable length argument list. () **options Variable length options. {} Returns: Type Description None None Source code in vast_pipeline/management/commands/runpipeline.py def handle ( self , * args , ** options ) -> None : \"\"\" Handle function of the command. Args: *args: Variable length argument list. **options: Variable length options. Returns: None \"\"\" p_run_name , run_folder = get_p_run_name ( options [ 'piperun' ], return_folder = True ) # configure logging root_logger = logging . getLogger ( '' ) f_handler = logging . FileHandler ( os . path . join ( run_folder , timeStamped ( 'log.txt' )), mode = 'w' ) f_handler . setFormatter ( root_logger . handlers [ 0 ] . formatter ) root_logger . addHandler ( f_handler ) if options [ 'verbosity' ] > 1 : # set root logger to use the DEBUG level root_logger . setLevel ( logging . DEBUG ) # set the traceback on options [ 'traceback' ] = True # p_run_name = p_run_path # remove ending / if present if p_run_name [ - 1 ] == '/' : p_run_name = p_run_name [: - 1 ] # grab only the name from the path p_run_name = p_run_name . split ( os . path . sep )[ - 1 ] debug_flag = True if options [ 'verbosity' ] > 1 else False _ = run_pipe ( p_run_name , path_name = run_folder , debug = debug_flag , full_rerun = options [ \"full_rerun\" ], ) self . stdout . write ( self . style . SUCCESS ( 'Finished' ))","title":"handle()"},{"location":"reference/management/commands/runpipeline/#vast_pipeline.management.commands.runpipeline.run_pipe","text":"Main function to run the pipeline. Parameters: Name Type Description Default name str The name of the pipeline run (p_run.name). required path_name Optional[str] The path of the directory of the pipeline run (p_run.path), defaults to None. None run_dj_obj Optional[vast_pipeline.models.Run] The Run object of the pipeline run, defaults to None. None cli bool Flag to signify whether the pipeline run has been run via the UI (False), or the command line (True). Defaults to True. True debug bool Flag to signify whether to enable debug verbosity to the logging output. Defaults to False. False user Optional[django.contrib.auth.models.User] The User of the request if made through the UI. Defaults to None. None full_rerun bool If the run already exists, a complete rerun will be performed which will remove and replace all the previous results. False prev_ui_status str The previous status through the UI. Defaults to 'END'. 'END' Returns: Type Description bool Boolean equal to True on a successful completion, or in cases of failures a CommandError is returned. Source code in vast_pipeline/management/commands/runpipeline.py def run_pipe ( name : str , path_name : Optional [ str ] = None , run_dj_obj : Optional [ Run ] = None , cli : bool = True , debug : bool = False , user : Optional [ User ] = None , full_rerun : bool = False , prev_ui_status : str = 'END' ) -> bool : ''' Main function to run the pipeline. Args: name: The name of the pipeline run (p_run.name). path_name: The path of the directory of the pipeline run (p_run.path), defaults to None. run_dj_obj: The Run object of the pipeline run, defaults to None. cli: Flag to signify whether the pipeline run has been run via the UI (False), or the command line (True). Defaults to True. debug: Flag to signify whether to enable debug verbosity to the logging output. Defaults to False. user: The User of the request if made through the UI. Defaults to None. full_rerun: If the run already exists, a complete rerun will be performed which will remove and replace all the previous results. prev_ui_status: The previous status through the UI. Defaults to 'END'. Returns: Boolean equal to `True` on a successful completion, or in cases of failures a CommandError is returned. ''' path = run_dj_obj . path if run_dj_obj else path_name # set up logging for running pipeline from UI if not cli : # set up the logger for the UI job root_logger = logging . getLogger ( '' ) if debug : root_logger . setLevel ( logging . DEBUG ) f_handler = logging . FileHandler ( os . path . join ( path , timeStamped ( 'log.txt' )), mode = 'w' ) f_handler . setFormatter ( root_logger . handlers [ 0 ] . formatter ) root_logger . addHandler ( f_handler ) pipeline = Pipeline ( name = run_dj_obj . name if run_dj_obj else name , config_path = os . path . join ( path , 'config.yaml' ), validate_config = False , # delay validation ) # Create the pipeline run in DB p_run , flag_exist = get_create_p_run ( pipeline . name , pipeline . config [ \"run\" ][ \"path\" ], ) # backup the last successful outputs. # if the run is being run again and the last status is END then the # user is highly likely to be attempting to add images. Making the backups # now from a guaranteed successful run is safer in case of problems # with the config file below that causes an error. if flag_exist : if cli and p_run . status == 'END' : backup_parquets ( p_run . path ) elif not cli and prev_ui_status == 'END' : backup_parquets ( p_run . path ) # validate run configuration try : pipeline . config . validate ( user = user ) except PipelineConfigError as e : if debug : traceback . print_exc () logger . exception ( 'Config error: \\n %s ' , e ) msg = f 'Config error: \\n { e } ' # If the run is already created (e.g. through UI) then set status to # error pipeline . set_status ( p_run , 'ERR' ) raise CommandError ( msg ) if cli else PipelineConfigError ( msg ) # clean up pipeline images and forced measurements for re-runs # Scenarios: # A. Complete Re-run: If the job is marked as successful then backup # old parquets and proceed to remove parquets along with forced # extractions from the database. # B. Additional Run on successful run: Backup parquets, remove current # parquets and proceed. # C. Additional Run on errored run: Do not backup parquets, just delete # current. # D. Running on initialised run that errored and is still the init run. # Flag on the pipeline object on whether the addition mode is on or off. pipeline . add_mode = False pipeline . previous_parquets = {} prev_config_exists = False try : if not flag_exist : # check for and remove any present .parquet (and .arrow) files parquets = ( glob . glob ( os . path . join ( p_run . path , \"*.parquet\" )) # TODO Remove arrow when arrow files are no longer needed. + glob . glob ( os . path . join ( p_run . path , \"*.arrow\" )) + glob . glob ( os . path . join ( p_run . path , \"*.bak\" )) ) for parquet in parquets : os . remove ( parquet ) # copy across config file at the start logger . debug ( \"Copying temp config file.\" ) create_temp_config_file ( p_run . path ) else : # Check if the status is already running or queued. Exit if this is # the case. if p_run . status in [ 'RUN' , 'RES' ]: logger . error ( \"The pipeline run requested to process already has a \" \"running or restoring status! Performing no actions. \" \"Exiting.\" ) return True # copy across config file at the start logger . debug ( \"Copying temp config file.\" ) create_temp_config_file ( p_run . path ) # Check if there is a previous run config and back up if so if os . path . isfile ( os . path . join ( p_run . path , 'config_prev.yaml' ) ): prev_config_exists = True shutil . copy ( os . path . join ( p_run . path , 'config_prev.yaml' ), os . path . join ( p_run . path , 'config.yaml.bak' ) ) logger . debug ( f 'config_prev.yaml exists: { prev_config_exists } ' ) # Check for an error status and whether any previous config file # exists - if it doesn't exist it means the run has failed during # the first run. In this case we want to clear anything that has # gone on before so to do that `complete-rerun` mode is activated. if not prev_config_exists : if cli and p_run . status == \"ERR\" : full_rerun = True elif not cli and prev_ui_status == \"ERR\" : full_rerun = True logger . debug ( f 'Full re-run: { full_rerun } ' ) # Check if the run has only been initialised, if so we don't want # to do any previous run checks or cleaning. if p_run . status == 'INI' : initial_run = True # check if coming from UI elif cli is False and prev_ui_status == 'INI' : initial_run = True else : initial_run = False if initial_run is False : parquets = ( glob . glob ( os . path . join ( p_run . path , \"*.parquet\" )) # TODO Remove arrow when arrow files are no longer needed. + glob . glob ( os . path . join ( p_run . path , \"*.arrow\" )) ) if full_rerun : logger . info ( 'Cleaning up pipeline run before re-process data' ) p_run . image_set . clear () logger . info ( 'Cleaning up forced measurements before re-process data' ) remove_forced_meas ( p_run . path ) for parquet in parquets : os . remove ( parquet ) # remove bak files bak_files = glob . glob ( os . path . join ( p_run . path , \"*.bak\" )) if bak_files : for bf in bak_files : os . remove ( bf ) # remove previous config if it exists if prev_config_exists : os . remove ( os . path . join ( p_run . path , 'config_prev.yaml' )) # reset epoch_based flag with transaction . atomic (): p_run . epoch_based = False p_run . save () else : # Before parquets are started to be copied and backed up, a # check is run to see if anything has actually changed in # the config config_diff = pipeline . config . check_prev_config_diff () if config_diff : logger . info ( \"The config file has either not changed since the\" \" previous run or other settings have changed such\" \" that a new or complete re-run should be performed\" \" instead. Performing no actions. Exiting.\" ) os . remove ( os . path . join ( p_run . path , 'config_temp.yaml' )) pipeline . set_status ( p_run , 'END' ) return True if pipeline . config . epoch_based != p_run . epoch_based : logger . info ( \"The 'epoch based' setting has changed since the\" \" previous run. A complete re-run is required if\" \" changing to epoch based mode or vice versa.\" ) os . remove ( os . path . join ( p_run . path , 'config_temp.yaml' )) pipeline . set_status ( p_run , 'END' ) return True pipeline . add_mode = True for i in [ 'images' , 'associations' , 'sources' , 'relations' , 'measurement_pairs' ]: pipeline . previous_parquets [ i ] = os . path . join ( p_run . path , f ' { i } .parquet.bak' ) except Exception as e : logger . error ( 'Unexpected error occurred in pre-run steps!' ) pipeline . set_status ( p_run , 'ERR' ) logger . exception ( 'Processing error: \\n %s ' , e ) raise CommandError ( f 'Processing error: \\n { e } ' ) if pipeline . config [ \"run\" ][ \"suppress_astropy_warnings\" ]: warnings . simplefilter ( \"ignore\" , category = AstropyWarning ) logger . info ( \"VAST Pipeline version: %s \" , pipeline_version ) logger . info ( \"Source finder: %s \" , pipeline . config [ \"measurements\" ][ \"source_finder\" ] ) logger . info ( \"Using pipeline run ' %s '\" , pipeline . name ) logger . info ( \"Source monitoring: %s \" , pipeline . config [ \"source_monitoring\" ][ \"monitor\" ] ) # log the list of input data files for posterity input_image_list = [ image for image_list in pipeline . config [ \"inputs\" ][ \"image\" ] . values () for image in image_list ] input_selavy_list = [ selavy for selavy_list in pipeline . config [ \"inputs\" ][ \"selavy\" ] . values () for selavy in selavy_list ] input_noise_list = [ noise for noise_list in pipeline . config [ \"inputs\" ][ \"noise\" ] . values () for noise in noise_list ] if \"background\" in pipeline . config [ \"inputs\" ] . keys (): input_background_list = [ background for background_list in pipeline . config [ \"inputs\" ][ \"background\" ] . values () for background in background_list ] else : input_background_list = [ \"N/A\" , ] * len ( input_image_list ) for image , selavy , noise , background in zip ( input_image_list , input_selavy_list , input_noise_list , input_background_list ): logger . info ( \"Matched inputs - image: %s , selavy: %s , noise: %s , background: %s \" , image , selavy , noise , background , ) stopwatch = StopWatch () # run the pipeline operations try : # check if max runs number is reached pipeline . check_current_runs () # run the pipeline pipeline . set_status ( p_run , 'RUN' ) pipeline . process_pipeline ( p_run ) # Create arrow file after success if selected. if pipeline . config [ \"measurements\" ][ \"write_arrow_files\" ]: create_measurements_arrow_file ( p_run ) create_measurement_pairs_arrow_file ( p_run ) except Exception as e : # set the pipeline status as error pipeline . set_status ( p_run , 'ERR' ) logger . exception ( 'Processing error: \\n %s ' , e ) raise CommandError ( f 'Processing error: \\n { e } ' ) # copy across config file now that it is successful logger . debug ( \"Copying and cleaning temp config file.\" ) shutil . copyfile ( os . path . join ( p_run . path , 'config_temp.yaml' ), os . path . join ( p_run . path , 'config_prev.yaml' )) os . remove ( os . path . join ( p_run . path , 'config_temp.yaml' )) # set the pipeline status as completed pipeline . set_status ( p_run , 'END' ) logger . info ( 'Total pipeline processing time %.2f sec' , stopwatch . reset () ) return True","title":"run_pipe()"},{"location":"reference/pipeline/association/","text":"This module contains all the functions required to perform source association. advanced_association ( method , sources_df , skyc1_srcs , skyc1 , skyc2_srcs , skyc2 , dr_limit , bw_max , id_incr_par_assoc = 0 ) \u00b6 The loop for advanced source association that uses the astropy 'search_around_sky' function (i.e. all matching sources are found). The BMAJ of the image * the user supplied beamwidth limit is the base distance for association. This is followed by calculating the 'de Ruiter' radius. Parameters: Name Type Description Default method str The advanced association method 'advanced' or 'deruiter'. required sources_df DataFrame The dataframe containing all current measurements along with their association source and relations. required skyc1_srcs DataFrame The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. required skyc1 SkyCoord A SkyCoord object with the weighted average sky positions from skyc1_srcs. required skyc2_srcs DataFrame The same structure as sources_df containing the measurements to be associated. required skyc2 SkyCoord A SkyCoord object with the sky positions from skyc2_srcs. required dr_limit float The de Ruiter radius limit to use (applies to de ruiter only). required bw_max float The beamwidth limit to use (applies to de ruiter only). required id_incr_par_assoc int An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. Source code in vast_pipeline/pipeline/association.py def advanced_association ( method : str , sources_df : pd . DataFrame , skyc1_srcs : pd . DataFrame , skyc1 : SkyCoord , skyc2_srcs : pd . DataFrame , skyc2 : SkyCoord , dr_limit : float , bw_max : float , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: ''' The loop for advanced source association that uses the astropy 'search_around_sky' function (i.e. all matching sources are found). The BMAJ of the image * the user supplied beamwidth limit is the base distance for association. This is followed by calculating the 'de Ruiter' radius. Args: method: The advanced association method 'advanced' or 'deruiter'. sources_df: The dataframe containing all current measurements along with their association source and relations. skyc1_srcs: The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. skyc1: A SkyCoord object with the weighted average sky positions from skyc1_srcs. skyc2_srcs: The same structure as sources_df containing the measurements to be associated. skyc2: A SkyCoord object with the sky positions from skyc2_srcs. dr_limit: The de Ruiter radius limit to use (applies to de ruiter only). bw_max: The beamwidth limit to use (applies to de ruiter only). id_incr_par_assoc: An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. Returns: The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. ''' # read the needed sources fields # Step 1: get matches within semimajor axis of image. idx_skyc1 , idx_skyc2 , d2d , d3d = skyc2 . search_around_sky ( skyc1 , bw_max ) # Step 2: merge the candidates so the de ruiter can be calculated temp_skyc1_srcs = ( skyc1_srcs . loc [ idx_skyc1 ] . reset_index () . rename ( columns = { 'index' : 'index_old' }) ) temp_skyc2_srcs = ( skyc2_srcs . loc [ idx_skyc2 ] . reset_index () . rename ( columns = { 'index' : 'index_old' }) ) temp_skyc2_srcs [ 'd2d' ] = d2d . arcsec temp_srcs = temp_skyc1_srcs . merge ( temp_skyc2_srcs , left_index = True , right_index = True , suffixes = ( '_skyc1' , '_skyc2' ) ) # drop the double d2d column and keep the d2d_skyc2 as assigned above temp_srcs = ( temp_srcs . drop ([ 'd2d_skyc1' , 'dr_skyc1' , 'dr_skyc2' ], axis = 1 ) . rename ( columns = { 'd2d_skyc2' : 'd2d' }) ) del temp_skyc1_srcs , temp_skyc2_srcs # Step 3: Apply the beamwidth limit temp_srcs = temp_srcs [ d2d <= bw_max ] . copy () # Step 4: Calculate and perform De Ruiter radius cut if method == 'deruiter' : temp_srcs [ 'dr' ] = calc_de_ruiter ( temp_srcs ) temp_srcs = temp_srcs [ temp_srcs [ 'dr' ] <= dr_limit ] else : temp_srcs [ 'dr' ] = 0. # Now have the 'good' matches # Step 5: Check for one-to-many, many-to-one and many-to-many # associations. First the many-to-many temp_srcs = many_to_many_advanced ( temp_srcs , method ) # Next one-to-many # Get the sources which are doubled temp_srcs , sources_df = one_to_many_advanced ( temp_srcs , sources_df , method , id_incr_par_assoc ) # Finally many-to-one associations, the opposite of above but we # don't have to create new ids for these so it's much simpler in fact # we don't need to do anything but lets get the number for debugging. temp_srcs = many_to_one_advanced ( temp_srcs ) # Now everything in place to append # First the skyc2 sources with a match. # This is created from the temp_srcs df. # This will take care of the extra skyc2 sources needed. skyc2_srcs_toappend = skyc2_srcs . loc [ temp_srcs [ 'index_old_skyc2' ] . values ] . reset_index ( drop = True ) skyc2_srcs_toappend [ 'source' ] = temp_srcs [ 'source_skyc1' ] . values skyc2_srcs_toappend [ 'related' ] = temp_srcs [ 'related_skyc1' ] . values skyc2_srcs_toappend [ 'd2d' ] = temp_srcs [ 'd2d' ] . values skyc2_srcs_toappend [ 'dr' ] = temp_srcs [ 'dr' ] . values # and get the skyc2 sources with no match logger . info ( 'Updating sources catalogue with new sources...' ) new_sources = skyc2_srcs . loc [ skyc2_srcs . index . difference ( temp_srcs [ 'index_old_skyc2' ] . values ) ] . reset_index ( drop = True ) # update the src numbers for those sources in skyc2 with no match # using the max current src as the start and incrementing by one start_elem = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc new_sources [ 'source' ] = np . arange ( start_elem , start_elem + new_sources . shape [ 0 ], dtype = int ) skyc2_srcs_toappend = skyc2_srcs_toappend . append ( new_sources , ignore_index = True ) # and skyc2 is now ready to be appended to source_df sources_df = sources_df . append ( skyc2_srcs_toappend , ignore_index = True ) . reset_index ( drop = True ) # update skyc1 and df for next association iteration # calculate average angles for skyc1 skyc1_srcs = ( skyc1_srcs . append ( new_sources , ignore_index = True ) . reset_index ( drop = True ) ) # also need to append any related sources that created a new # source, we can use the skyc2_srcs_toappend to get these skyc1_srcs = skyc1_srcs . append ( skyc2_srcs_toappend . loc [ ~ skyc2_srcs_toappend . source . isin ( skyc1_srcs . source ) ] ) return sources_df , skyc1_srcs association ( images_df , limit , dr_limit , bw_limit , duplicate_limit , config , add_mode , previous_parquets , done_images_df , id_incr_par_assoc = 0 , parallel = False ) \u00b6 The main association function that does the common tasks between basic and advanced modes. Parameters: Name Type Description Default images_df DataFrame The input images to be associated. required limit Angle The association limit to use (applies to basic and advanced only). required dr_limit float The de Ruiter radius limit to use (applies to de ruiter only). required bw_limit float The beamwidth limit to use (applies to de ruiter only). required duplicate_limit Angle The limit of separation for which a measurement is considered to be a duplicate (epoch based association). required config PipelineConfig The pipeline configuration object. required add_mode bool Whether the pipeline is currently being run in add image mode. required previous_parquets Dict[str, str] Dictionary containing the paths of the previous successful run parquet files (used in add image mode). required done_images_df DataFrame Datafraame containing the images of the previous successful run (used in add image mode). required id_incr_par_assoc int An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. 0 parallel bool Whether parallel association is being used. False Returns: Type Description DataFrame The output sources_df containing all input measurements along with the association and relation information. Exceptions: Type Description Exception Raised if association method is not valid. Source code in vast_pipeline/pipeline/association.py def association ( images_df : pd . DataFrame , limit : Angle , dr_limit : float , bw_limit : float , duplicate_limit : Angle , config : PipelineConfig , add_mode : bool , previous_parquets : Dict [ str , str ], done_images_df : pd . DataFrame , id_incr_par_assoc : int = 0 , parallel : bool = False ) -> pd . DataFrame : ''' The main association function that does the common tasks between basic and advanced modes. Args: images_df: The input images to be associated. limit: The association limit to use (applies to basic and advanced only). dr_limit: The de Ruiter radius limit to use (applies to de ruiter only). bw_limit: The beamwidth limit to use (applies to de ruiter only). duplicate_limit: The limit of separation for which a measurement is considered to be a duplicate (epoch based association). config: The pipeline configuration object. add_mode: Whether the pipeline is currently being run in add image mode. previous_parquets: Dictionary containing the paths of the previous successful run parquet files (used in add image mode). done_images_df: Datafraame containing the images of the previous successful run (used in add image mode). id_incr_par_assoc: An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. parallel: Whether parallel association is being used. Returns: The output sources_df containing all input measurements along with the association and relation information. Raises: Exception: Raised if association method is not valid. ''' timer = StopWatch () if parallel : images_df = ( images_df . sort_values ( by = 'image_datetime' ) . drop ( 'image_datetime' , axis = 1 ) ) if 'skyreg_group' in images_df . columns : skyreg_group = images_df [ 'skyreg_group' ] . iloc [ 0 ] skyreg_tag = \" (sky region group %s )\" % skyreg_group else : skyreg_group = - 1 skyreg_tag = \"\" method = config [ \"source_association\" ][ \"method\" ] logger . info ( 'Starting association %s .' , skyreg_tag ) logger . info ( 'Association mode selected: %s .' , method ) unique_epochs = np . sort ( images_df [ 'epoch' ] . unique ()) if add_mode : # Here the skyc1_srcs and sources_df are recreated and the done images # are filtered out. image_mask = images_df [ 'image_name' ] . isin ( done_images_df [ 'name' ]) images_df_done = images_df [ image_mask ] . copy () sources_df , skyc1_srcs = reconstruct_associtaion_dfs ( images_df_done , previous_parquets , ) images_df = images_df . loc [ ~ image_mask ] if images_df . empty : logger . info ( 'No new images found, stopping association %s .' , skyreg_tag ) sources_df [ 'interim_ew' ] = ( sources_df [ 'ra_source' ] . values * sources_df [ 'weight_ew' ] . values ) sources_df [ 'interim_ns' ] = ( sources_df [ 'dec_source' ] . values * sources_df [ 'weight_ns' ] . values ) return ( sources_df . drop ([ 'ra' , 'dec' ], axis = 1 ) . rename ( columns = { 'ra_source' : 'ra' , 'dec_source' : 'dec' }) ) logger . info ( f 'Found { images_df . shape [ 0 ] } images to add to the run { skyreg_tag } .' ) # re-get the unique epochs unique_epochs = np . sort ( images_df [ 'epoch' ] . unique ()) start_epoch = 0 else : # Do full set up for a new run. first_images = ( images_df . loc [ images_df [ 'epoch' ] == unique_epochs [ 0 ], 'image_dj' ] . to_list () ) # initialise sky source dataframe skyc1_srcs = prep_skysrc_df ( first_images , config [ \"measurements\" ][ \"flux_fractional_error\" ], duplicate_limit , ini_df = True ) skyc1_srcs [ 'epoch' ] = unique_epochs [ 0 ] # create base catalogue # initialise the sources dataframe using first image as base sources_df = skyc1_srcs . copy () start_epoch = 1 if unique_epochs . shape [ 0 ] == 1 and not add_mode : # This means only one image is present - or one group of images (epoch # mode) - so the same approach as above in add mode where there are no # images to be added, the interim needs to be calculated and skyc1_srcs # can just be returned as sources_df. ra_source and dec_source can just # be dropped as the ra and dec are already the average values. logger . warning ( 'No images to associate with! %s .' , skyreg_tag ) logger . info ( 'Returning base sources only %s .' , skyreg_tag ) # reorder the columns to match Dask expectations (parallel) skyc1_srcs = skyc1_srcs [[ 'id' , 'uncertainty_ew' , 'weight_ew' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image' , 'datetime' , 'source' , 'ra' , 'dec' , 'ra_source' , 'dec_source' , 'd2d' , 'dr' , 'related' , 'epoch' , ]] skyc1_srcs [ 'interim_ew' ] = ( skyc1_srcs [ 'ra' ] . values * skyc1_srcs [ 'weight_ew' ] . values ) skyc1_srcs [ 'interim_ns' ] = ( skyc1_srcs [ 'dec' ] . values * skyc1_srcs [ 'weight_ns' ] . values ) return skyc1_srcs . drop ([ 'ra_source' , 'dec_source' ], axis = 1 ) skyc1 = SkyCoord ( skyc1_srcs [ 'ra' ] . values , skyc1_srcs [ 'dec' ] . values , unit = ( u . deg , u . deg ) ) for it , epoch in enumerate ( unique_epochs [ start_epoch :]): logger . info ( 'Association iteration: # %i%s ' , it + 1 , skyreg_tag ) # load skyc2 source measurements and create SkyCoord images = ( images_df . loc [ images_df [ 'epoch' ] == epoch , 'image_dj' ] . to_list () ) max_beam_maj = ( images_df . loc [ images_df [ 'epoch' ] == epoch , 'image_dj' ] . apply ( lambda x : x . beam_bmaj ) . max () ) skyc2_srcs = prep_skysrc_df ( images , config [ \"measurements\" ][ \"flux_fractional_error\" ], duplicate_limit ) skyc2_srcs [ 'epoch' ] = epoch skyc2 = SkyCoord ( skyc2_srcs [ 'ra' ] . values , skyc2_srcs [ 'dec' ] . values , unit = ( u . deg , u . deg ) ) if method == 'basic' : sources_df , skyc1_srcs = basic_association ( sources_df , skyc1_srcs , skyc1 , skyc2_srcs , skyc2 , limit , id_incr_par_assoc ) elif method in [ 'advanced' , 'deruiter' ]: if method == 'deruiter' : bw_max = Angle ( bw_limit * ( max_beam_maj * 3600. / 2. ) * u . arcsec ) else : bw_max = limit sources_df , skyc1_srcs = advanced_association ( method , sources_df , skyc1_srcs , skyc1 , skyc2_srcs , skyc2 , dr_limit , bw_max , id_incr_par_assoc ) else : raise Exception ( 'association method not implemented!' ) logger . info ( 'Calculating weighted average RA and Dec for sources %s ...' , skyreg_tag ) # account for RA wrapping ra_wrap_mask = sources_df . ra <= 0.1 sources_df [ 'ra_wrap' ] = sources_df . ra . values sources_df . loc [ ra_wrap_mask , 'ra_wrap' ] = sources_df [ ra_wrap_mask ] . ra . values + 360. sources_df [ 'interim_ew' ] = ( sources_df [ 'ra_wrap' ] . values * sources_df [ 'weight_ew' ] . values ) sources_df [ 'interim_ns' ] = ( sources_df [ 'dec' ] . values * sources_df [ 'weight_ns' ] . values ) sources_df = sources_df . drop ([ 'ra_wrap' ], axis = 1 ) tmp_srcs_df = ( sources_df . loc [ ( sources_df [ 'source' ] != - 1 ) & ( sources_df [ 'forced' ] == False ), [ 'ra' , 'dec' , 'uncertainty_ew' , 'uncertainty_ns' , 'source' , 'interim_ew' , 'interim_ns' , 'weight_ew' , 'weight_ns' ] ] . groupby ( 'source' ) ) stats = StopWatch () wm_ra = tmp_srcs_df [ 'interim_ew' ] . sum () / tmp_srcs_df [ 'weight_ew' ] . sum () wm_uncertainty_ew = 1. / np . sqrt ( tmp_srcs_df [ 'weight_ew' ] . sum ()) wm_dec = tmp_srcs_df [ 'interim_ns' ] . sum () / tmp_srcs_df [ 'weight_ns' ] . sum () wm_uncertainty_ns = 1. / np . sqrt ( tmp_srcs_df [ 'weight_ns' ] . sum ()) weighted_df = ( pd . concat ( [ wm_ra , wm_uncertainty_ew , wm_dec , wm_uncertainty_ns ], axis = 1 , sort = False ) . reset_index () . rename ( columns = { 0 : 'ra' , 'weight_ew' : 'uncertainty_ew' , 1 : 'dec' , 'weight_ns' : 'uncertainty_ns' }) ) # correct the RA wrapping ra_wrap_mask = weighted_df . ra >= 360. weighted_df . loc [ ra_wrap_mask , 'ra' ] = weighted_df [ ra_wrap_mask ] . ra . values - 360. logger . debug ( 'Groupby concat time %f ' , stats . reset ()) logger . info ( 'Finalising base sources catalogue ready for next iteration %s ...' , skyreg_tag ) # merge the weighted ra and dec and replace the values skyc1_srcs = skyc1_srcs . merge ( weighted_df , on = 'source' , how = 'left' , suffixes = ( '' , '_skyc2' ) ) del tmp_srcs_df , weighted_df skyc1_srcs [ 'ra' ] = skyc1_srcs [ 'ra_skyc2' ] skyc1_srcs [ 'dec' ] = skyc1_srcs [ 'dec_skyc2' ] skyc1_srcs [ 'uncertainty_ew' ] = skyc1_srcs [ 'uncertainty_ew_skyc2' ] skyc1_srcs [ 'uncertainty_ns' ] = skyc1_srcs [ 'uncertainty_ns_skyc2' ] skyc1_srcs = skyc1_srcs . drop ( [ 'ra_skyc2' , 'dec_skyc2' , 'uncertainty_ew_skyc2' , 'uncertainty_ns_skyc2' ], axis = 1 ) # generate new sky coord ready for next iteration skyc1 = SkyCoord ( skyc1_srcs [ 'ra' ] . values , skyc1_srcs [ 'dec' ] . values , unit = ( u . deg , u . deg ) ) # and update relations in skyc1 skyc1_srcs = skyc1_srcs . drop ( 'related' , axis = 1 ) relations_unique = pd . DataFrame ( sources_df [ sources_df [ 'related' ] . notna ()] . explode ( 'related' ) . groupby ( 'source' )[ 'related' ] . apply ( lambda x : x . unique () . tolist ()) ) skyc1_srcs = skyc1_srcs . merge ( relations_unique , how = 'left' , left_on = 'source' , right_index = True ) logger . info ( 'Association iteration # %i complete %s .' , it + 1 , skyreg_tag ) # End of iteration over images, ra and dec columns are actually the # average over each iteration so remove ave ra and ave dec used for # calculation and use ra_source and dec_source columns sources_df = ( sources_df . drop ([ 'ra' , 'dec' ], axis = 1 ) . rename ( columns = { 'ra_source' : 'ra' , 'dec_source' : 'dec' }) ) del skyc1_srcs , skyc2_srcs logger . info ( 'Total association time: %.2f seconds %s .' , timer . reset_init (), skyreg_tag ) return sources_df basic_association ( sources_df , skyc1_srcs , skyc1 , skyc2_srcs , skyc2 , limit , id_incr_par_assoc = 0 ) \u00b6 The loop for basic source association that uses the astropy 'match_to_catalog_sky' function (i.e. only the nearest match between the catalogs). A direct on sky separation is used to define the association. Parameters: Name Type Description Default sources_df DataFrame The dataframe containing all current measurements along with their association source and relations. required skyc1_srcs DataFrame The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. required skyc1 SkyCoord A SkyCoord object with the weighted average sky positions from skyc1_srcs. required skyc2_srcs DataFrame The same structure as sources_df containing the measurements to be associated. required skyc2 SkyCoord A SkyCoord object with the sky positions from skyc2_srcs. required limit Angle The association limit to use (applies to basic and advanced only). required id_incr_par_assoc int An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. Source code in vast_pipeline/pipeline/association.py def basic_association ( sources_df : pd . DataFrame , skyc1_srcs : pd . DataFrame , skyc1 : SkyCoord , skyc2_srcs : pd . DataFrame , skyc2 : SkyCoord , limit : Angle , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: ''' The loop for basic source association that uses the astropy 'match_to_catalog_sky' function (i.e. only the nearest match between the catalogs). A direct on sky separation is used to define the association. Args: sources_df: The dataframe containing all current measurements along with their association source and relations. skyc1_srcs: The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. skyc1: A SkyCoord object with the weighted average sky positions from skyc1_srcs. skyc2_srcs: The same structure as sources_df containing the measurements to be associated. skyc2: A SkyCoord object with the sky positions from skyc2_srcs. limit: The association limit to use (applies to basic and advanced only). id_incr_par_assoc: An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. Returns: The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. ''' # match the new sources to the base # idx gives the index of the closest match in the base for skyc2 idx , d2d , d3d = skyc2 . match_to_catalog_sky ( skyc1 ) # acceptable selection sel = d2d <= limit # The good matches can be assinged the src id from base skyc2_srcs . loc [ sel , 'source' ] = skyc1_srcs . loc [ idx [ sel ], 'source' ] . values # Need the d2d to make analysing doubles easier. skyc2_srcs . loc [ sel , 'd2d' ] = d2d [ sel ] . arcsec # must check for double matches in the acceptable matches just made # this would mean that multiple sources in skyc2 have been matched # to the same base source we want to keep closest match and move # the other match(es) back to having a -1 src id skyc2_srcs , sources_df = one_to_many_basic ( skyc2_srcs , sources_df , id_incr_par_assoc ) logger . info ( 'Updating sources catalogue with new sources...' ) # update the src numbers for those sources in skyc2 with no match # using the max current src as the start and incrementing by one start_elem = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc nan_sel = ( skyc2_srcs [ 'source' ] == - 1 ) . values skyc2_srcs . loc [ nan_sel , 'source' ] = ( np . arange ( start_elem , start_elem + skyc2_srcs . loc [ nan_sel ] . shape [ 0 ], dtype = int ) ) # and skyc2 is now ready to be appended to new sources sources_df = sources_df . append ( skyc2_srcs , ignore_index = True ) . reset_index ( drop = True ) # and update skyc1 with the sources that were created from the one # to many relations and any new sources. skyc1_srcs = skyc1_srcs . append ( skyc2_srcs [ ~ skyc2_srcs [ 'source' ] . isin ( skyc1_srcs [ 'source' ]) ], ignore_index = True ) . reset_index ( drop = True ) return sources_df , skyc1_srcs calc_de_ruiter ( df ) \u00b6 Calculates the unitless 'de Ruiter' radius of the association. Works on the 'temp_df' dataframe of the advanced association, where the two sources associated with each other have been merged into one row. Parameters: Name Type Description Default df DataFrame The 'temp_df' from advanced association. It must contain the columns ra_skyc1 , 'ra_skyc2', 'uncertainty_ew_skyc1', 'uncertainty_ew_skyc2', 'dec_skyc1', 'dec_skyc2', 'uncertainty_ns_skyc1' and 'uncertainty_ns_skyc2'. required Returns: Type Description ndarray Array containing the de Ruiter radius for all rows in the df. Source code in vast_pipeline/pipeline/association.py def calc_de_ruiter ( df : pd . DataFrame ) -> np . ndarray : \"\"\" Calculates the unitless 'de Ruiter' radius of the association. Works on the 'temp_df' dataframe of the advanced association, where the two sources associated with each other have been merged into one row. Args: df: The 'temp_df' from advanced association. It must contain the columns `ra_skyc1`, 'ra_skyc2', 'uncertainty_ew_skyc1', 'uncertainty_ew_skyc2', 'dec_skyc1', 'dec_skyc2', 'uncertainty_ns_skyc1' and 'uncertainty_ns_skyc2'. Returns: Array containing the de Ruiter radius for all rows in the df. \"\"\" ra_1 = df [ 'ra_skyc1' ] . values ra_2 = df [ 'ra_skyc2' ] . values # avoid wrapping issues ra_1 [ ra_1 > 270. ] -= 180. ra_2 [ ra_2 > 270. ] -= 180. ra_1 [ ra_1 < 90. ] += 180. ra_2 [ ra_2 < 90. ] += 180. ra_1 = np . deg2rad ( ra_1 ) ra_2 = np . deg2rad ( ra_2 ) ra_1_err = np . deg2rad ( df [ 'uncertainty_ew_skyc1' ] . values ) ra_2_err = np . deg2rad ( df [ 'uncertainty_ew_skyc2' ] . values ) dec_1 = np . deg2rad ( df [ 'dec_skyc1' ] . values ) dec_2 = np . deg2rad ( df [ 'dec_skyc2' ] . values ) dec_1_err = np . deg2rad ( df [ 'uncertainty_ns_skyc1' ] . values ) dec_2_err = np . deg2rad ( df [ 'uncertainty_ns_skyc2' ] . values ) dr1 = ( ra_1 - ra_2 ) * ( ra_1 - ra_2 ) dr1_1 = np . cos (( dec_1 + dec_2 ) / 2. ) dr1 *= dr1_1 * dr1_1 dr1 /= ra_1_err * ra_1_err + ra_2_err * ra_2_err dr2 = ( dec_1 - dec_2 ) * ( dec_1 - dec_2 ) dr2 /= dec_1_err * dec_1_err + dec_2_err * dec_2_err dr = np . sqrt ( dr1 + dr2 ) return dr many_to_many_advanced ( temp_srcs , method ) \u00b6 Finds and processes the many-to-many associations in the advanced association. We do not want to build many-to-many associations as this will make the database get very large (see TraP documentation). The skyc2 sources which are listed more than once are found, and of these, those which have a skyc1 source association which is also listed twice in the associations are selected. The closest (by limit or de Ruiter radius, depending on the method) is kept where as the other associations are dropped. This follows the same logic used by the TraP (see TraP documentation). Parameters: Name Type Description Default temp_srcs DataFrame The temporary associtation dataframe used through the advanced association process. required method str Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. required Returns: Type Description DataFrame Updated temp_srcs with the many_to_many relations dropped. Source code in vast_pipeline/pipeline/association.py def many_to_many_advanced ( temp_srcs : pd . DataFrame , method : str ) -> pd . DataFrame : ''' Finds and processes the many-to-many associations in the advanced association. We do not want to build many-to-many associations as this will make the database get very large (see TraP documentation). The skyc2 sources which are listed more than once are found, and of these, those which have a skyc1 source association which is also listed twice in the associations are selected. The closest (by limit or de Ruiter radius, depending on the method) is kept where as the other associations are dropped. This follows the same logic used by the TraP (see TraP documentation). Args: temp_srcs: The temporary associtation dataframe used through the advanced association process. method: Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. Returns: Updated temp_srcs with the many_to_many relations dropped. ''' # Select those where the extracted source is listed more than once # (e.g. index_old_skyc2 duplicated values) and of these get those that # have a source id that is listed more than once (e.g. source_skyc1 # duplicated values) in the temps_srcs df m_to_m = temp_srcs [( temp_srcs [ 'index_old_skyc2' ] . duplicated ( keep = False ) & temp_srcs [ 'source_skyc1' ] . duplicated ( keep = False ) )] . copy () if m_to_m . empty : logger . debug ( 'No many-to-many assocations.' ) return temp_srcs logger . debug ( 'Detected # %i many-to-many assocations, cleaning...' , m_to_m . shape [ 0 ] ) dist_col = 'd2d' if method == 'advanced' else 'dr' min_col = 'min_' + dist_col # get the minimum de ruiter value for each extracted source m_to_m [ min_col ] = ( m_to_m . groupby ( 'index_old_skyc2' )[ dist_col ] . transform ( 'min' ) ) # get the ids of those crossmatches that are larger than the minimum m_to_m_to_drop = m_to_m [ m_to_m [ dist_col ] != m_to_m [ min_col ]] . index . values # and drop these from the temp_srcs temp_srcs = temp_srcs . drop ( m_to_m_to_drop ) return temp_srcs many_to_one_advanced ( temp_srcs ) \u00b6 Finds and processes the many-to-one associations in the advanced association. In this case in the related column of the 'many' sources we need to append the ids of all the other 'many' (expect for itself). Parameters: Name Type Description Default temp_srcs DataFrame The temporary associtation dataframe used through the advanced association process. required Returns: Type Description DataFrame Updated temp_srcs with all many_to_one relation information added. Source code in vast_pipeline/pipeline/association.py def many_to_one_advanced ( temp_srcs : pd . DataFrame ) -> pd . DataFrame : ''' Finds and processes the many-to-one associations in the advanced association. In this case in the related column of the 'many' sources we need to append the ids of all the other 'many' (expect for itself). Args: temp_srcs: The temporary associtation dataframe used through the advanced association process. Returns: Updated temp_srcs with all many_to_one relation information added. ''' # use only these columns for easy debugging of the dataframe cols = [ 'index_old_skyc1' , 'id_skyc1' , 'source_skyc1' , 'related_skyc1' , 'index_old_skyc2' , 'id_skyc2' , 'source_skyc2' , 'd2d' , 'dr' ] # select those sources which have been matched to the same measurement # in the sky catalogue 2. duplicated_skyc2 = temp_srcs . loc [ temp_srcs [ 'index_old_skyc2' ] . duplicated ( keep = False ), cols ] # duplicated_skyc2 # +-----+-------------------+------------+----------------+ # | | index_old_skyc1 | id_skyc1 | source_skyc1 | # |-----+-------------------+------------+----------------+ # | 447 | 477 | 478 | 478 | # | 448 | 478 | 479 | 479 | # | 477 | 507 | 508 | 508 | # | 478 | 508 | 509 | 509 | # | 695 | 738 | 739 | 739 | # +-----+-------------------+------------+----------------+ # +-----------------+-------------------+------------+----------------+ # | related_skyc1 | index_old_skyc2 | id_skyc2 | source_skyc2 | # +-----------------+-------------------+------------+----------------+ # | | 305 | 5847 | -1 | # | | 305 | 5847 | -1 | # | | 648 | 6190 | -1 | # | | 648 | 6190 | -1 | # | | 561 | 6103 | -1 | # +-----------------+-------------------+------------+----------------+ # -------------+------+ # d2d | dr | # -------------+------| # 8.63598 | 0 | # 8.63598 | 0 | # 6.5777 | 0 | # 6.5777 | 0 | # 7.76527 | 0 | # -------------+------+ # if there are none no action is required. if duplicated_skyc2 . empty : logger . debug ( 'No many-to-one associations.' ) return temp_srcs logger . debug ( 'Detected # %i many-to-one associations' , duplicated_skyc2 . shape [ 0 ] ) # The new relations become that for each 'many' source we need to append # the ids of the other 'many' sources that have been associationed with the # 'one'. Below for each 'one' group we gather all the ids of the many # sources. new_relations = pd . DataFrame ( duplicated_skyc2 . groupby ( 'index_old_skyc2' ) . apply ( lambda grp : grp [ 'source_skyc1' ] . tolist ()) ) . rename ( columns = { 0 : 'new_relations' }) # new_relations # +-------------------+-----------------+ # | index_old_skyc2 | new_relations | # |-------------------+-----------------| # | 305 | [478, 479] | # | 561 | [739, 740] | # | 648 | [508, 509] | # | 764 | [841, 842] | # | 816 | [1213, 1215] | # +-------------------+-----------------+ # these new relations are then added to the duplciated dataframe so # they can easily be used by the next function. duplicated_skyc2 = duplicated_skyc2 . merge ( new_relations , left_on = 'index_old_skyc2' , right_index = True , how = 'left' ) # Remove the 'self' relations. The 'x['source_skyc1']' is an integer so it # is placed within a list notation, [], to be able to be easily subtracted # from the new_relations. duplicated_skyc2 [ 'new_relations' ] = ( duplicated_skyc2 . apply ( lambda x : list ( set ( x [ 'new_relations' ]) - set ([ x [ 'source_skyc1' ]])), axis = 1 ) ) # Use the 'add_new_many_to_one_relations' method to add tthe new relatitons # to the actual `related_skyc1' column. duplicated_skyc2 [ 'related_skyc1' ] = ( duplicated_skyc2 . apply ( add_new_many_to_one_relations , axis = 1 ) ) # Transfer the new relations from the duplicated df to the temp_srcs. The # index is explicitly declared to avoid any mixups. temp_srcs . loc [ duplicated_skyc2 . index . values , 'related_skyc1' ] = duplicated_skyc2 . loc [ duplicated_skyc2 . index . values , 'related_skyc1' ] . values return temp_srcs one_to_many_advanced ( temp_srcs , sources_df , method , id_incr_par_assoc = 0 ) \u00b6 Finds and processes the one-to-many associations in the advanced association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the basic version as the data products between the two are different. Parameters: Name Type Description Default temp_srcs DataFrame The temporary associtation dataframe used through the advanced association process. required sources_df DataFrame The sources_df produced by each step of association holding the current 'sources'. required method str Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. required id_incr_par_assoc int An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] Updated temp_srcs and sources_df with all one_to_many relation information added. Source code in vast_pipeline/pipeline/association.py def one_to_many_advanced ( temp_srcs : pd . DataFrame , sources_df : pd . DataFrame , method : str , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: ''' Finds and processes the one-to-many associations in the advanced association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the basic version as the data products between the two are different. Args: temp_srcs: The temporary associtation dataframe used through the advanced association process. sources_df: The sources_df produced by each step of association holding the current 'sources'. method: Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. id_incr_par_assoc: An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association Returns: Updated temp_srcs and sources_df with all one_to_many relation information added. ''' # use only these columns for easy debugging of the dataframe cols = [ 'index_old_skyc1' , 'id_skyc1' , 'source_skyc1' , 'related_skyc1' , 'index_old_skyc2' , 'id_skyc2' , 'source_skyc2' , 'd2d' , 'dr' ] duplicated_skyc1 = temp_srcs . loc [ temp_srcs [ 'source_skyc1' ] . duplicated ( keep = False ), cols ] . copy () # duplicated_skyc1 # +-----+-------------------+------------+----------------+ # | | index_old_skyc1 | id_skyc1 | source_skyc1 | # |-----+-------------------+------------+----------------+ # | 117 | 121 | 122 | 122 | # | 118 | 121 | 122 | 122 | # | 238 | 253 | 254 | 254 | # | 239 | 253 | 254 | 254 | # | 246 | 261 | 262 | 262 | # +-----+-------------------+------------+----------------+ # -----------------+-------------------+------------+----------------+ # related_skyc1 | index_old_skyc2 | id_skyc2 | source_skyc2 | # -----------------+-------------------+------------+----------------+ # | 526 | 6068 | -1 | # | 528 | 6070 | -1 | # | 264 | 5806 | -1 | # | 265 | 5807 | -1 | # | 327 | 5869 | -1 | # -----------------+-------------------+------------+----------------+ # -------------+------+ # d2d | dr | # -------------+------| # 3.07478 | 0 | # 6.41973 | 0 | # 2.04422 | 0 | # 6.16881 | 0 | # 3.20439 | 0 | # -------------+------+ # If no relations then no action is required if duplicated_skyc1 . empty : logger . debug ( 'No one-to-many associations.' ) return temp_srcs , sources_df logger . debug ( 'Detected # %i one-to-many assocations, cleaning...' , duplicated_skyc1 . shape [ 0 ] ) # Get the column to check for the minimum depending on the method # set the column names needed for filtering the 'to-many' # associations depending on the method (advanced or deruiter) dist_col = 'd2d' if method == 'advanced' else 'dr' # go through the doubles and # 1. Keep the closest d2d or de ruiter as the primary id # 2. Increment a new source id for others # 3. Add a copy of the previously matched # source into sources. # multi_srcs = duplicated_skyc1['source_skyc1'].unique() # Get the duplicated, sort by the distance column duplicated_skyc1 = duplicated_skyc1 . sort_values ( by = [ 'source_skyc1' , dist_col ] ) # Get those that need to be given a new ID number (i.e. not the min dist_col) idx_to_change = duplicated_skyc1 . index . values [ duplicated_skyc1 . duplicated ( 'source_skyc1' ) ] # Create a new `new_source_id` column to store the 'correct' IDs duplicated_skyc1 [ 'new_source_id' ] = duplicated_skyc1 [ 'source_skyc1' ] # +-----------------+ # | new_source_id | # +-----------------| # | 122 | # | 122 | # | 254 | # | 254 | # | 262 | # +-----------------+ # Define the range of new source ids start_new_src_id = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc # Create an arange to use to change the ones that need to be changed. new_source_ids = np . arange ( start_new_src_id , start_new_src_id + idx_to_change . shape [ 0 ], dtype = int ) # Assign the new IDs to those that need to be changed. duplicated_skyc1 . loc [ idx_to_change , 'new_source_id' ] = new_source_ids # We also need to clear the relations for these 'new' sources # otherwise it will inherit rogue relations from the original relation duplicated_skyc1 . loc [ idx_to_change , 'related_skyc1' ] = None # Now we need to sort out the related, essentially here the 'original' # and 'non original' need to be treated differently. # The original source need all the assoicated new ids appended to the # related column. # The not_original ones need just the original ID appended. not_original = duplicated_skyc1 . loc [ idx_to_change ] . copy () original = duplicated_skyc1 . drop_duplicates ( 'source_skyc1' ) . copy () # This gathers all the new ids that need to be appended # to the original related column. new_original_related = pd . DataFrame ( not_original [ [ 'source_skyc1' , 'new_source_id' ] ] . groupby ( 'source_skyc1' ) . apply ( lambda grp : grp [ 'new_source_id' ] . tolist () ) ) #new_original_related # +----------------+--------+ # | source_skyc1 | 0 | # |----------------+--------| # | 122 | [5542] | # | 254 | [5543] | # | 262 | [5544] | # | 405 | [5545] | # | 656 | [5546] | # +----------------+--------+ # Append the relations in each case, using the above 'new_original_related' # for the original ones. # The not original only require the appending of the original index. original [ 'related_skyc1' ] = ( original [[ 'related_skyc1' , 'source_skyc1' ]] . apply ( add_new_one_to_many_relations , args = ( True , new_original_related ), axis = 1 ) ) # what the column looks like after the above # +-----------------+ # | related_skyc1 | # +-----------------+ # | [5542] | # | [5543] | # | [5544] | # | [5545] | # | [5546] | # +-----------------+ not_original . loc [:, 'related_skyc1' ] = not_original . apply ( add_new_one_to_many_relations , args = ( True ,), axis = 1 ) # Merge them back together duplicated_skyc1 = original . append ( not_original ) del original , not_original # Apply the updates to the actual temp_srcs. temp_srcs . loc [ idx_to_change , 'source_skyc1' ] = new_source_ids temp_srcs . loc [ duplicated_skyc1 . index . values , 'related_skyc1' ] = duplicated_skyc1 . loc [ duplicated_skyc1 . index . values , 'related_skyc1' ] . values # Finally we need to create copies of the previous sources in the # sources_df to complete the new sources. # To do this we get only the non-original sources duplicated_skyc1 = duplicated_skyc1 . loc [ duplicated_skyc1 . duplicated ( 'source_skyc1' ) ] # Get all the indexes required for each original # `source_skyc1` value source_df_index_to_copy = pd . DataFrame ( duplicated_skyc1 . groupby ( 'source_skyc1' ) . apply ( lambda grp : sources_df [ sources_df [ 'source' ] == grp . name ] . index . values . tolist () ) ) # source_df_index_to_copy # +----------------+-------+ # | source_skyc1 | 0 | # |----------------+-------| # | 122 | [121] | # | 254 | [253] | # | 262 | [261] | # | 405 | [404] | # | 656 | [655] | # +----------------+-------+ # merge these so it's easy to explode and copy the index values. duplicated_skyc1 = ( duplicated_skyc1 . loc [:,[ 'source_skyc1' , 'new_source_id' ]] . merge ( source_df_index_to_copy , left_on = 'source_skyc1' , right_index = True , how = 'left' ) . rename ( columns = { 0 : 'source_index' }) . explode ( 'source_index' ) ) # duplicated_skyc1 # +-----+----------------+-----------------+----------------+ # | | source_skyc1 | new_source_id | source_index | # |-----+----------------+-----------------+----------------| # | 118 | 122 | 5542 | 121 | # | 239 | 254 | 5543 | 253 | # | 247 | 262 | 5544 | 261 | # | 380 | 405 | 5545 | 404 | # | 615 | 656 | 5546 | 655 | # +-----+----------------+-----------------+----------------+ # Get the sources sources_to_copy = sources_df . loc [ duplicated_skyc1 [ 'source_index' ] . values ] # Apply the new_source_id sources_to_copy [ 'source' ] = duplicated_skyc1 [ 'new_source_id' ] . values # Reset the related column to avoid rogue relations sources_to_copy [ 'related' ] = None # and finally append. sources_df = sources_df . append ( sources_to_copy , ignore_index = True ) return temp_srcs , sources_df one_to_many_basic ( skyc2_srcs , sources_df , id_incr_par_assoc = 0 ) \u00b6 Finds and processes the one-to-many associations in the basic association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the advanced version as the data products between the two are different. Parameters: Name Type Description Default skyc2_srcs DataFrame The sky catalogue 2 sources (i.e. the sources being associated to the base) used during basic association. required sources_df DataFrame The sources_df produced by each step of association holding the current 'sources'. required id_incr_par_assoc int An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] Tuple containging the updated 'skyc2_srcs' and 'sources_df' with all one_to_many relation information added. Source code in vast_pipeline/pipeline/association.py def one_to_many_basic ( skyc2_srcs : pd . DataFrame , sources_df : pd . DataFrame , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Finds and processes the one-to-many associations in the basic association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the advanced version as the data products between the two are different. Args: skyc2_srcs: The sky catalogue 2 sources (i.e. the sources being associated to the base) used during basic association. sources_df: The sources_df produced by each step of association holding the current 'sources'. id_incr_par_assoc: An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association Returns: Tuple containging the updated 'skyc2_srcs' and 'sources_df' with all one_to_many relation information added. \"\"\" # select duplicated in 'source' field in skyc2_srcs, excluding -1 duplicated_skyc2 = skyc2_srcs . loc [ ( skyc2_srcs [ 'source' ] != - 1 ) & skyc2_srcs [ 'source' ] . duplicated ( keep = False ), [ 'source' , 'related' , 'd2d' ] ] # duplicated_skyc2 # +-----+----------+-----------+---------+ # | | source | related | d2d | # |-----+----------+-----------+---------| # | 264 | 254 | | 2.04422 | # | 265 | 254 | | 6.16881 | # | 327 | 262 | | 3.20439 | # | 328 | 262 | | 3.84425 | # | 526 | 122 | | 3.07478 | # +-----+----------+-----------+---------+ if duplicated_skyc2 . empty : logger . debug ( 'No one-to-many associations.' ) return skyc2_srcs , sources_df logger . info ( 'Detected # %i double matches, cleaning...' , duplicated_skyc2 . shape [ 0 ] ) # now we have the src values which are doubled. # make the nearest match have the \"original\" src id # give the other matched source a new src id # and make sure to copy the other previously # matched sources. # Get the duplicated, sort by the distance column duplicated_skyc2 = duplicated_skyc2 . sort_values ( by = [ 'source' , 'd2d' ]) # Get those that need to be given a new ID number (i.e. not the min dist_col) idx_to_change = duplicated_skyc2 . index . values [ duplicated_skyc2 . duplicated ( 'source' ) ] # Create a new `new_source_id` column to store the 'correct' IDs duplicated_skyc2 [ 'new_source_id' ] = duplicated_skyc2 [ 'source' ] # Define the range of new source ids start_new_src_id = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc new_source_ids = np . arange ( start_new_src_id , start_new_src_id + idx_to_change . shape [ 0 ], dtype = int ) # Assign the new IDs duplicated_skyc2 . loc [ idx_to_change , 'new_source_id' ] = new_source_ids # duplicated_skyc2 # +-----+----------+-----------+---------+-----------------+ # | | source | related | d2d | new_source_id | # |-----+----------+-----------+---------+-----------------| # | 526 | 122 | | 3.07478 | 122 | # | 528 | 122 | | 6.41973 | 5542 | # | 264 | 254 | | 2.04422 | 254 | # | 265 | 254 | | 6.16881 | 5543 | # | 327 | 262 | | 3.20439 | 262 | # +-----+----------+-----------+---------+-----------------+ # Now we need to sort out the related, essentially here the 'original' # and 'non original' need to be treated differently. # The original source need all the assoicated new ids appended to the # related column. # The not_original ones need just the original ID appended. # copy() is used here to avoid chained indexing (set with copy warnings) not_original = duplicated_skyc2 . loc [ idx_to_change ] . copy () original = duplicated_skyc2 . drop_duplicates ( 'source' ) . copy () new_original_related = pd . DataFrame ( not_original [ [ 'source' , 'new_source_id' ] ] . groupby ( 'source' ) . apply ( lambda grp : grp [ 'new_source_id' ] . tolist () ) ) # new_original_related # +----------+--------+ # | source | 0 | # |----------+--------| # | 122 | [5542] | # | 254 | [5543] | # | 262 | [5544] | # | 405 | [5545] | # | 656 | [5546] | # +----------+--------+ # Append the relations in each case, using the above 'new_original_related' # for the original ones. # The not original only require the appending of the original index. original [ 'related' ] = ( original [[ 'related' , 'source' ]] . apply ( add_new_one_to_many_relations , args = ( False , new_original_related ), axis = 1 ) ) not_original [ 'related' ] = not_original . apply ( add_new_one_to_many_relations , args = ( False ,), axis = 1 ) duplicated_skyc2 = original . append ( not_original ) # duplicated_skyc2 # +-----+----------+-----------+---------+-----------------+ # | | source | related | d2d | new_source_id | # |-----+----------+-----------+---------+-----------------| # | 526 | 122 | [5542] | 3.07478 | 122 | # | 264 | 254 | [5543] | 2.04422 | 254 | # | 327 | 262 | [5544] | 3.20439 | 262 | # | 848 | 405 | [5545] | 5.52865 | 405 | # | 695 | 656 | [5546] | 4.69094 | 656 | # +-----+----------+-----------+---------+-----------------+ del original , not_original # Apply the updates to the actual temp_srcs. skyc2_srcs . loc [ idx_to_change , 'source' ] = new_source_ids skyc2_srcs . loc [ duplicated_skyc2 . index . values , 'related' ] = duplicated_skyc2 . loc [ duplicated_skyc2 . index . values , 'related' ] . values # Finally we need to copy copies of the previous sources in the # sources_df to complete the new sources. # To do this we get only the non-original sources duplicated_skyc2 = duplicated_skyc2 . loc [ duplicated_skyc2 . duplicated ( 'source' ) ] # Get all the indexes required for each original # `source_skyc1` value source_df_index_to_copy = pd . DataFrame ( duplicated_skyc2 . groupby ( 'source' ) . apply ( lambda grp : sources_df [ sources_df [ 'source' ] == grp . name ] . index . values . tolist () ) ) # source_df_index_to_copy # +----------+-------+ # | source | 0 | # |----------+-------| # | 122 | [121] | # | 254 | [253] | # | 262 | [261] | # | 405 | [404] | # | 656 | [655] | # +----------+-------+ # merge these so it's easy to explode and copy the index values. duplicated_skyc2 = ( duplicated_skyc2 [[ 'source' , 'new_source_id' ]] . merge ( source_df_index_to_copy , left_on = 'source' , right_index = True , how = 'left' ) . rename ( columns = { 0 : 'source_index' }) . explode ( 'source_index' ) ) # Get the sources - all columns from the sources_df table sources_to_copy = sources_df . loc [ duplicated_skyc2 [ 'source_index' ] . values ] # Apply the new_source_id sources_to_copy [ 'source' ] = duplicated_skyc2 [ 'new_source_id' ] . values # Reset the related column to avoid rogue relations sources_to_copy [ 'related' ] = None # and finally append. sources_df = sources_df . append ( sources_to_copy , ignore_index = True ) return skyc2_srcs , sources_df parallel_association ( images_df , limit , dr_limit , bw_limit , duplicate_limit , config , n_skyregion_groups , add_mode , previous_parquets , done_images_df , done_source_ids ) \u00b6 Launches association on different sky region groups in parallel using Dask. Parameters: Name Type Description Default images_df DataFrame Holds the images that are being processed. Also contains what sky region group the image belongs to. required limit Angle The association radius limit. required dr_limit float The de Ruiter radius limit. required bw_limit float The beamwidth limit. required duplicate_limit Angle The duplicate radius detection limit. required config PipelineConfig The pipeline config settings. required n_skyregion_groups int The number of sky region groups. required Returns: Type Description DataFrame pd.DataFrame: The combined association results of the parallel association with corrected source ids. Source code in vast_pipeline/pipeline/association.py def parallel_association ( images_df : pd . DataFrame , limit : Angle , dr_limit : float , bw_limit : float , duplicate_limit : Angle , config : PipelineConfig , n_skyregion_groups : int , add_mode : bool , previous_parquets : Dict [ str , str ], done_images_df : pd . DataFrame , done_source_ids : List [ int ] ) -> pd . DataFrame : \"\"\" Launches association on different sky region groups in parallel using Dask. Args: images_df: Holds the images that are being processed. Also contains what sky region group the image belongs to. limit: The association radius limit. dr_limit: The de Ruiter radius limit. bw_limit: The beamwidth limit. duplicate_limit: The duplicate radius detection limit. config (module): The pipeline config settings. n_skyregion_groups: The number of sky region groups. Returns: pd.DataFrame: The combined association results of the parallel association with corrected source ids. \"\"\" logger . info ( \"Running parallel association for %i sky region groups.\" , n_skyregion_groups ) timer = StopWatch () meta = { 'id' : 'i' , 'uncertainty_ew' : 'f' , 'weight_ew' : 'f' , 'uncertainty_ns' : 'f' , 'weight_ns' : 'f' , 'flux_int' : 'f' , 'flux_int_err' : 'f' , 'flux_int_isl_ratio' : 'f' , 'flux_peak' : 'f' , 'flux_peak_err' : 'f' , 'flux_peak_isl_ratio' : 'f' , 'forced' : '?' , 'compactness' : 'f' , 'has_siblings' : '?' , 'snr' : 'f' , 'image' : 'U' , 'datetime' : 'datetime64[ns]' , 'source' : 'i' , 'ra' : 'f' , 'dec' : 'f' , 'd2d' : 'f' , 'dr' : 'f' , 'related' : 'O' , 'epoch' : 'i' , 'interim_ew' : 'f' , 'interim_ns' : 'f' , } # Add an increment to any new source values when using add_mode to avoid # getting duplicates in the result laater id_incr_par_assoc = max ( done_source_ids ) if add_mode else 0 n_cpu = cpu_count () - 1 # pass each skyreg_group through the normal association process. results = ( dd . from_pandas ( images_df , n_cpu ) . groupby ( 'skyreg_group' ) . apply ( association , limit = limit , dr_limit = dr_limit , bw_limit = bw_limit , duplicate_limit = duplicate_limit , config = config , add_mode = add_mode , previous_parquets = previous_parquets , done_images_df = done_images_df , id_incr_par_assoc = id_incr_par_assoc , parallel = True , meta = meta ) . compute ( n_workers = n_cpu , scheduler = 'processes' ) ) # results are the normal dataframe of results with the columns: # 'id', 'uncertainty_ew', 'weight_ew', 'uncertainty_ns', 'weight_ns', # 'flux_int', 'flux_int_err', 'flux_peak', 'flux_peak_err', 'forced', # 'compactness', 'has_siblings', 'snr', 'image', 'datetime', 'source', # 'ra', 'dec', 'd2d', 'dr', 'related', 'epoch', 'interim_ew' and # 'interim_ns'. # The index however is now a multi index with the skyregion group and # a general result index. Hence the general result index is repeated for # each skyreg_group along with the source_ids. This needs to be collapsed # and the source id's corrected. # Index example: # id # skyreg_group # -------------------------- # 2 0 15640 # 1 15641 # 2 15642 # 3 15643 # 4 15644 # ... ... # 1 46975 53992 # 46976 54062 # 46977 54150 # 46978 54161 # 46979 54164 # Get the indexes (skyreg_groups) to loop over for source id correction indexes = results . index . levels [ 0 ] . values if add_mode : # Need to correct all skyreg_groups. # First get the starting id for new sources. new_id = max ( done_source_ids ) + 1 for i in indexes : corr_df , new_id = _correct_parallel_source_ids_add_mode ( results . loc [ i , [ 'source' , 'related' ]], done_source_ids , new_id ) results . loc [ ( i , slice ( None )), [ 'source' , 'related' ] ] = corr_df . values else : # The first index acts as the base, so the others are looped over and # corrected. for i , val in enumerate ( indexes ): # skip first one, makes the enumerate easier to deal with if i == 0 : continue # Get the maximum source ID from the previous group. max_id = results . loc [ indexes [ i - 1 ]] . source . max () # Run through the correction function, only the 'source' and # 'related' # columns are passed and returned (corrected). corr_df = _correct_parallel_source_ids ( results . loc [ val , [ 'source' , 'related' ]], max_id ) # replace the values in the results with the corrected source and # related values results . loc [ ( val , slice ( None )), [ 'source' , 'related' ] ] = corr_df . values del corr_df # reset the indeex of the final corrected and collapsed result results = results . reset_index ( drop = True ) logger . info ( 'Total parallel association time: %.2f seconds' , timer . reset_init () ) return results","title":"association.py"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.advanced_association","text":"The loop for advanced source association that uses the astropy 'search_around_sky' function (i.e. all matching sources are found). The BMAJ of the image * the user supplied beamwidth limit is the base distance for association. This is followed by calculating the 'de Ruiter' radius. Parameters: Name Type Description Default method str The advanced association method 'advanced' or 'deruiter'. required sources_df DataFrame The dataframe containing all current measurements along with their association source and relations. required skyc1_srcs DataFrame The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. required skyc1 SkyCoord A SkyCoord object with the weighted average sky positions from skyc1_srcs. required skyc2_srcs DataFrame The same structure as sources_df containing the measurements to be associated. required skyc2 SkyCoord A SkyCoord object with the sky positions from skyc2_srcs. required dr_limit float The de Ruiter radius limit to use (applies to de ruiter only). required bw_max float The beamwidth limit to use (applies to de ruiter only). required id_incr_par_assoc int An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. Source code in vast_pipeline/pipeline/association.py def advanced_association ( method : str , sources_df : pd . DataFrame , skyc1_srcs : pd . DataFrame , skyc1 : SkyCoord , skyc2_srcs : pd . DataFrame , skyc2 : SkyCoord , dr_limit : float , bw_max : float , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: ''' The loop for advanced source association that uses the astropy 'search_around_sky' function (i.e. all matching sources are found). The BMAJ of the image * the user supplied beamwidth limit is the base distance for association. This is followed by calculating the 'de Ruiter' radius. Args: method: The advanced association method 'advanced' or 'deruiter'. sources_df: The dataframe containing all current measurements along with their association source and relations. skyc1_srcs: The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. skyc1: A SkyCoord object with the weighted average sky positions from skyc1_srcs. skyc2_srcs: The same structure as sources_df containing the measurements to be associated. skyc2: A SkyCoord object with the sky positions from skyc2_srcs. dr_limit: The de Ruiter radius limit to use (applies to de ruiter only). bw_max: The beamwidth limit to use (applies to de ruiter only). id_incr_par_assoc: An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. Returns: The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. ''' # read the needed sources fields # Step 1: get matches within semimajor axis of image. idx_skyc1 , idx_skyc2 , d2d , d3d = skyc2 . search_around_sky ( skyc1 , bw_max ) # Step 2: merge the candidates so the de ruiter can be calculated temp_skyc1_srcs = ( skyc1_srcs . loc [ idx_skyc1 ] . reset_index () . rename ( columns = { 'index' : 'index_old' }) ) temp_skyc2_srcs = ( skyc2_srcs . loc [ idx_skyc2 ] . reset_index () . rename ( columns = { 'index' : 'index_old' }) ) temp_skyc2_srcs [ 'd2d' ] = d2d . arcsec temp_srcs = temp_skyc1_srcs . merge ( temp_skyc2_srcs , left_index = True , right_index = True , suffixes = ( '_skyc1' , '_skyc2' ) ) # drop the double d2d column and keep the d2d_skyc2 as assigned above temp_srcs = ( temp_srcs . drop ([ 'd2d_skyc1' , 'dr_skyc1' , 'dr_skyc2' ], axis = 1 ) . rename ( columns = { 'd2d_skyc2' : 'd2d' }) ) del temp_skyc1_srcs , temp_skyc2_srcs # Step 3: Apply the beamwidth limit temp_srcs = temp_srcs [ d2d <= bw_max ] . copy () # Step 4: Calculate and perform De Ruiter radius cut if method == 'deruiter' : temp_srcs [ 'dr' ] = calc_de_ruiter ( temp_srcs ) temp_srcs = temp_srcs [ temp_srcs [ 'dr' ] <= dr_limit ] else : temp_srcs [ 'dr' ] = 0. # Now have the 'good' matches # Step 5: Check for one-to-many, many-to-one and many-to-many # associations. First the many-to-many temp_srcs = many_to_many_advanced ( temp_srcs , method ) # Next one-to-many # Get the sources which are doubled temp_srcs , sources_df = one_to_many_advanced ( temp_srcs , sources_df , method , id_incr_par_assoc ) # Finally many-to-one associations, the opposite of above but we # don't have to create new ids for these so it's much simpler in fact # we don't need to do anything but lets get the number for debugging. temp_srcs = many_to_one_advanced ( temp_srcs ) # Now everything in place to append # First the skyc2 sources with a match. # This is created from the temp_srcs df. # This will take care of the extra skyc2 sources needed. skyc2_srcs_toappend = skyc2_srcs . loc [ temp_srcs [ 'index_old_skyc2' ] . values ] . reset_index ( drop = True ) skyc2_srcs_toappend [ 'source' ] = temp_srcs [ 'source_skyc1' ] . values skyc2_srcs_toappend [ 'related' ] = temp_srcs [ 'related_skyc1' ] . values skyc2_srcs_toappend [ 'd2d' ] = temp_srcs [ 'd2d' ] . values skyc2_srcs_toappend [ 'dr' ] = temp_srcs [ 'dr' ] . values # and get the skyc2 sources with no match logger . info ( 'Updating sources catalogue with new sources...' ) new_sources = skyc2_srcs . loc [ skyc2_srcs . index . difference ( temp_srcs [ 'index_old_skyc2' ] . values ) ] . reset_index ( drop = True ) # update the src numbers for those sources in skyc2 with no match # using the max current src as the start and incrementing by one start_elem = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc new_sources [ 'source' ] = np . arange ( start_elem , start_elem + new_sources . shape [ 0 ], dtype = int ) skyc2_srcs_toappend = skyc2_srcs_toappend . append ( new_sources , ignore_index = True ) # and skyc2 is now ready to be appended to source_df sources_df = sources_df . append ( skyc2_srcs_toappend , ignore_index = True ) . reset_index ( drop = True ) # update skyc1 and df for next association iteration # calculate average angles for skyc1 skyc1_srcs = ( skyc1_srcs . append ( new_sources , ignore_index = True ) . reset_index ( drop = True ) ) # also need to append any related sources that created a new # source, we can use the skyc2_srcs_toappend to get these skyc1_srcs = skyc1_srcs . append ( skyc2_srcs_toappend . loc [ ~ skyc2_srcs_toappend . source . isin ( skyc1_srcs . source ) ] ) return sources_df , skyc1_srcs","title":"advanced_association()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.association","text":"The main association function that does the common tasks between basic and advanced modes. Parameters: Name Type Description Default images_df DataFrame The input images to be associated. required limit Angle The association limit to use (applies to basic and advanced only). required dr_limit float The de Ruiter radius limit to use (applies to de ruiter only). required bw_limit float The beamwidth limit to use (applies to de ruiter only). required duplicate_limit Angle The limit of separation for which a measurement is considered to be a duplicate (epoch based association). required config PipelineConfig The pipeline configuration object. required add_mode bool Whether the pipeline is currently being run in add image mode. required previous_parquets Dict[str, str] Dictionary containing the paths of the previous successful run parquet files (used in add image mode). required done_images_df DataFrame Datafraame containing the images of the previous successful run (used in add image mode). required id_incr_par_assoc int An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. 0 parallel bool Whether parallel association is being used. False Returns: Type Description DataFrame The output sources_df containing all input measurements along with the association and relation information. Exceptions: Type Description Exception Raised if association method is not valid. Source code in vast_pipeline/pipeline/association.py def association ( images_df : pd . DataFrame , limit : Angle , dr_limit : float , bw_limit : float , duplicate_limit : Angle , config : PipelineConfig , add_mode : bool , previous_parquets : Dict [ str , str ], done_images_df : pd . DataFrame , id_incr_par_assoc : int = 0 , parallel : bool = False ) -> pd . DataFrame : ''' The main association function that does the common tasks between basic and advanced modes. Args: images_df: The input images to be associated. limit: The association limit to use (applies to basic and advanced only). dr_limit: The de Ruiter radius limit to use (applies to de ruiter only). bw_limit: The beamwidth limit to use (applies to de ruiter only). duplicate_limit: The limit of separation for which a measurement is considered to be a duplicate (epoch based association). config: The pipeline configuration object. add_mode: Whether the pipeline is currently being run in add image mode. previous_parquets: Dictionary containing the paths of the previous successful run parquet files (used in add image mode). done_images_df: Datafraame containing the images of the previous successful run (used in add image mode). id_incr_par_assoc: An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. parallel: Whether parallel association is being used. Returns: The output sources_df containing all input measurements along with the association and relation information. Raises: Exception: Raised if association method is not valid. ''' timer = StopWatch () if parallel : images_df = ( images_df . sort_values ( by = 'image_datetime' ) . drop ( 'image_datetime' , axis = 1 ) ) if 'skyreg_group' in images_df . columns : skyreg_group = images_df [ 'skyreg_group' ] . iloc [ 0 ] skyreg_tag = \" (sky region group %s )\" % skyreg_group else : skyreg_group = - 1 skyreg_tag = \"\" method = config [ \"source_association\" ][ \"method\" ] logger . info ( 'Starting association %s .' , skyreg_tag ) logger . info ( 'Association mode selected: %s .' , method ) unique_epochs = np . sort ( images_df [ 'epoch' ] . unique ()) if add_mode : # Here the skyc1_srcs and sources_df are recreated and the done images # are filtered out. image_mask = images_df [ 'image_name' ] . isin ( done_images_df [ 'name' ]) images_df_done = images_df [ image_mask ] . copy () sources_df , skyc1_srcs = reconstruct_associtaion_dfs ( images_df_done , previous_parquets , ) images_df = images_df . loc [ ~ image_mask ] if images_df . empty : logger . info ( 'No new images found, stopping association %s .' , skyreg_tag ) sources_df [ 'interim_ew' ] = ( sources_df [ 'ra_source' ] . values * sources_df [ 'weight_ew' ] . values ) sources_df [ 'interim_ns' ] = ( sources_df [ 'dec_source' ] . values * sources_df [ 'weight_ns' ] . values ) return ( sources_df . drop ([ 'ra' , 'dec' ], axis = 1 ) . rename ( columns = { 'ra_source' : 'ra' , 'dec_source' : 'dec' }) ) logger . info ( f 'Found { images_df . shape [ 0 ] } images to add to the run { skyreg_tag } .' ) # re-get the unique epochs unique_epochs = np . sort ( images_df [ 'epoch' ] . unique ()) start_epoch = 0 else : # Do full set up for a new run. first_images = ( images_df . loc [ images_df [ 'epoch' ] == unique_epochs [ 0 ], 'image_dj' ] . to_list () ) # initialise sky source dataframe skyc1_srcs = prep_skysrc_df ( first_images , config [ \"measurements\" ][ \"flux_fractional_error\" ], duplicate_limit , ini_df = True ) skyc1_srcs [ 'epoch' ] = unique_epochs [ 0 ] # create base catalogue # initialise the sources dataframe using first image as base sources_df = skyc1_srcs . copy () start_epoch = 1 if unique_epochs . shape [ 0 ] == 1 and not add_mode : # This means only one image is present - or one group of images (epoch # mode) - so the same approach as above in add mode where there are no # images to be added, the interim needs to be calculated and skyc1_srcs # can just be returned as sources_df. ra_source and dec_source can just # be dropped as the ra and dec are already the average values. logger . warning ( 'No images to associate with! %s .' , skyreg_tag ) logger . info ( 'Returning base sources only %s .' , skyreg_tag ) # reorder the columns to match Dask expectations (parallel) skyc1_srcs = skyc1_srcs [[ 'id' , 'uncertainty_ew' , 'weight_ew' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image' , 'datetime' , 'source' , 'ra' , 'dec' , 'ra_source' , 'dec_source' , 'd2d' , 'dr' , 'related' , 'epoch' , ]] skyc1_srcs [ 'interim_ew' ] = ( skyc1_srcs [ 'ra' ] . values * skyc1_srcs [ 'weight_ew' ] . values ) skyc1_srcs [ 'interim_ns' ] = ( skyc1_srcs [ 'dec' ] . values * skyc1_srcs [ 'weight_ns' ] . values ) return skyc1_srcs . drop ([ 'ra_source' , 'dec_source' ], axis = 1 ) skyc1 = SkyCoord ( skyc1_srcs [ 'ra' ] . values , skyc1_srcs [ 'dec' ] . values , unit = ( u . deg , u . deg ) ) for it , epoch in enumerate ( unique_epochs [ start_epoch :]): logger . info ( 'Association iteration: # %i%s ' , it + 1 , skyreg_tag ) # load skyc2 source measurements and create SkyCoord images = ( images_df . loc [ images_df [ 'epoch' ] == epoch , 'image_dj' ] . to_list () ) max_beam_maj = ( images_df . loc [ images_df [ 'epoch' ] == epoch , 'image_dj' ] . apply ( lambda x : x . beam_bmaj ) . max () ) skyc2_srcs = prep_skysrc_df ( images , config [ \"measurements\" ][ \"flux_fractional_error\" ], duplicate_limit ) skyc2_srcs [ 'epoch' ] = epoch skyc2 = SkyCoord ( skyc2_srcs [ 'ra' ] . values , skyc2_srcs [ 'dec' ] . values , unit = ( u . deg , u . deg ) ) if method == 'basic' : sources_df , skyc1_srcs = basic_association ( sources_df , skyc1_srcs , skyc1 , skyc2_srcs , skyc2 , limit , id_incr_par_assoc ) elif method in [ 'advanced' , 'deruiter' ]: if method == 'deruiter' : bw_max = Angle ( bw_limit * ( max_beam_maj * 3600. / 2. ) * u . arcsec ) else : bw_max = limit sources_df , skyc1_srcs = advanced_association ( method , sources_df , skyc1_srcs , skyc1 , skyc2_srcs , skyc2 , dr_limit , bw_max , id_incr_par_assoc ) else : raise Exception ( 'association method not implemented!' ) logger . info ( 'Calculating weighted average RA and Dec for sources %s ...' , skyreg_tag ) # account for RA wrapping ra_wrap_mask = sources_df . ra <= 0.1 sources_df [ 'ra_wrap' ] = sources_df . ra . values sources_df . loc [ ra_wrap_mask , 'ra_wrap' ] = sources_df [ ra_wrap_mask ] . ra . values + 360. sources_df [ 'interim_ew' ] = ( sources_df [ 'ra_wrap' ] . values * sources_df [ 'weight_ew' ] . values ) sources_df [ 'interim_ns' ] = ( sources_df [ 'dec' ] . values * sources_df [ 'weight_ns' ] . values ) sources_df = sources_df . drop ([ 'ra_wrap' ], axis = 1 ) tmp_srcs_df = ( sources_df . loc [ ( sources_df [ 'source' ] != - 1 ) & ( sources_df [ 'forced' ] == False ), [ 'ra' , 'dec' , 'uncertainty_ew' , 'uncertainty_ns' , 'source' , 'interim_ew' , 'interim_ns' , 'weight_ew' , 'weight_ns' ] ] . groupby ( 'source' ) ) stats = StopWatch () wm_ra = tmp_srcs_df [ 'interim_ew' ] . sum () / tmp_srcs_df [ 'weight_ew' ] . sum () wm_uncertainty_ew = 1. / np . sqrt ( tmp_srcs_df [ 'weight_ew' ] . sum ()) wm_dec = tmp_srcs_df [ 'interim_ns' ] . sum () / tmp_srcs_df [ 'weight_ns' ] . sum () wm_uncertainty_ns = 1. / np . sqrt ( tmp_srcs_df [ 'weight_ns' ] . sum ()) weighted_df = ( pd . concat ( [ wm_ra , wm_uncertainty_ew , wm_dec , wm_uncertainty_ns ], axis = 1 , sort = False ) . reset_index () . rename ( columns = { 0 : 'ra' , 'weight_ew' : 'uncertainty_ew' , 1 : 'dec' , 'weight_ns' : 'uncertainty_ns' }) ) # correct the RA wrapping ra_wrap_mask = weighted_df . ra >= 360. weighted_df . loc [ ra_wrap_mask , 'ra' ] = weighted_df [ ra_wrap_mask ] . ra . values - 360. logger . debug ( 'Groupby concat time %f ' , stats . reset ()) logger . info ( 'Finalising base sources catalogue ready for next iteration %s ...' , skyreg_tag ) # merge the weighted ra and dec and replace the values skyc1_srcs = skyc1_srcs . merge ( weighted_df , on = 'source' , how = 'left' , suffixes = ( '' , '_skyc2' ) ) del tmp_srcs_df , weighted_df skyc1_srcs [ 'ra' ] = skyc1_srcs [ 'ra_skyc2' ] skyc1_srcs [ 'dec' ] = skyc1_srcs [ 'dec_skyc2' ] skyc1_srcs [ 'uncertainty_ew' ] = skyc1_srcs [ 'uncertainty_ew_skyc2' ] skyc1_srcs [ 'uncertainty_ns' ] = skyc1_srcs [ 'uncertainty_ns_skyc2' ] skyc1_srcs = skyc1_srcs . drop ( [ 'ra_skyc2' , 'dec_skyc2' , 'uncertainty_ew_skyc2' , 'uncertainty_ns_skyc2' ], axis = 1 ) # generate new sky coord ready for next iteration skyc1 = SkyCoord ( skyc1_srcs [ 'ra' ] . values , skyc1_srcs [ 'dec' ] . values , unit = ( u . deg , u . deg ) ) # and update relations in skyc1 skyc1_srcs = skyc1_srcs . drop ( 'related' , axis = 1 ) relations_unique = pd . DataFrame ( sources_df [ sources_df [ 'related' ] . notna ()] . explode ( 'related' ) . groupby ( 'source' )[ 'related' ] . apply ( lambda x : x . unique () . tolist ()) ) skyc1_srcs = skyc1_srcs . merge ( relations_unique , how = 'left' , left_on = 'source' , right_index = True ) logger . info ( 'Association iteration # %i complete %s .' , it + 1 , skyreg_tag ) # End of iteration over images, ra and dec columns are actually the # average over each iteration so remove ave ra and ave dec used for # calculation and use ra_source and dec_source columns sources_df = ( sources_df . drop ([ 'ra' , 'dec' ], axis = 1 ) . rename ( columns = { 'ra_source' : 'ra' , 'dec_source' : 'dec' }) ) del skyc1_srcs , skyc2_srcs logger . info ( 'Total association time: %.2f seconds %s .' , timer . reset_init (), skyreg_tag ) return sources_df","title":"association()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.basic_association","text":"The loop for basic source association that uses the astropy 'match_to_catalog_sky' function (i.e. only the nearest match between the catalogs). A direct on sky separation is used to define the association. Parameters: Name Type Description Default sources_df DataFrame The dataframe containing all current measurements along with their association source and relations. required skyc1_srcs DataFrame The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. required skyc1 SkyCoord A SkyCoord object with the weighted average sky positions from skyc1_srcs. required skyc2_srcs DataFrame The same structure as sources_df containing the measurements to be associated. required skyc2 SkyCoord A SkyCoord object with the sky positions from skyc2_srcs. required limit Angle The association limit to use (applies to basic and advanced only). required id_incr_par_assoc int An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. Source code in vast_pipeline/pipeline/association.py def basic_association ( sources_df : pd . DataFrame , skyc1_srcs : pd . DataFrame , skyc1 : SkyCoord , skyc2_srcs : pd . DataFrame , skyc2 : SkyCoord , limit : Angle , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: ''' The loop for basic source association that uses the astropy 'match_to_catalog_sky' function (i.e. only the nearest match between the catalogs). A direct on sky separation is used to define the association. Args: sources_df: The dataframe containing all current measurements along with their association source and relations. skyc1_srcs: The same structure as sources_df but only has one entry per 'source' along with a weighted average sky position of the current assoociated sources. skyc1: A SkyCoord object with the weighted average sky positions from skyc1_srcs. skyc2_srcs: The same structure as sources_df containing the measurements to be associated. skyc2: A SkyCoord object with the sky positions from skyc2_srcs. limit: The association limit to use (applies to basic and advanced only). id_incr_par_assoc: An increment value to be applied to source numbering when adding new sources to the associations (applies when parallel and add image are being used). Defaults to 0. Returns: The output sources_df containing all input measurements along with the association and relation information. The output skyc1_srcs with updated with new sources from the association. ''' # match the new sources to the base # idx gives the index of the closest match in the base for skyc2 idx , d2d , d3d = skyc2 . match_to_catalog_sky ( skyc1 ) # acceptable selection sel = d2d <= limit # The good matches can be assinged the src id from base skyc2_srcs . loc [ sel , 'source' ] = skyc1_srcs . loc [ idx [ sel ], 'source' ] . values # Need the d2d to make analysing doubles easier. skyc2_srcs . loc [ sel , 'd2d' ] = d2d [ sel ] . arcsec # must check for double matches in the acceptable matches just made # this would mean that multiple sources in skyc2 have been matched # to the same base source we want to keep closest match and move # the other match(es) back to having a -1 src id skyc2_srcs , sources_df = one_to_many_basic ( skyc2_srcs , sources_df , id_incr_par_assoc ) logger . info ( 'Updating sources catalogue with new sources...' ) # update the src numbers for those sources in skyc2 with no match # using the max current src as the start and incrementing by one start_elem = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc nan_sel = ( skyc2_srcs [ 'source' ] == - 1 ) . values skyc2_srcs . loc [ nan_sel , 'source' ] = ( np . arange ( start_elem , start_elem + skyc2_srcs . loc [ nan_sel ] . shape [ 0 ], dtype = int ) ) # and skyc2 is now ready to be appended to new sources sources_df = sources_df . append ( skyc2_srcs , ignore_index = True ) . reset_index ( drop = True ) # and update skyc1 with the sources that were created from the one # to many relations and any new sources. skyc1_srcs = skyc1_srcs . append ( skyc2_srcs [ ~ skyc2_srcs [ 'source' ] . isin ( skyc1_srcs [ 'source' ]) ], ignore_index = True ) . reset_index ( drop = True ) return sources_df , skyc1_srcs","title":"basic_association()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.calc_de_ruiter","text":"Calculates the unitless 'de Ruiter' radius of the association. Works on the 'temp_df' dataframe of the advanced association, where the two sources associated with each other have been merged into one row. Parameters: Name Type Description Default df DataFrame The 'temp_df' from advanced association. It must contain the columns ra_skyc1 , 'ra_skyc2', 'uncertainty_ew_skyc1', 'uncertainty_ew_skyc2', 'dec_skyc1', 'dec_skyc2', 'uncertainty_ns_skyc1' and 'uncertainty_ns_skyc2'. required Returns: Type Description ndarray Array containing the de Ruiter radius for all rows in the df. Source code in vast_pipeline/pipeline/association.py def calc_de_ruiter ( df : pd . DataFrame ) -> np . ndarray : \"\"\" Calculates the unitless 'de Ruiter' radius of the association. Works on the 'temp_df' dataframe of the advanced association, where the two sources associated with each other have been merged into one row. Args: df: The 'temp_df' from advanced association. It must contain the columns `ra_skyc1`, 'ra_skyc2', 'uncertainty_ew_skyc1', 'uncertainty_ew_skyc2', 'dec_skyc1', 'dec_skyc2', 'uncertainty_ns_skyc1' and 'uncertainty_ns_skyc2'. Returns: Array containing the de Ruiter radius for all rows in the df. \"\"\" ra_1 = df [ 'ra_skyc1' ] . values ra_2 = df [ 'ra_skyc2' ] . values # avoid wrapping issues ra_1 [ ra_1 > 270. ] -= 180. ra_2 [ ra_2 > 270. ] -= 180. ra_1 [ ra_1 < 90. ] += 180. ra_2 [ ra_2 < 90. ] += 180. ra_1 = np . deg2rad ( ra_1 ) ra_2 = np . deg2rad ( ra_2 ) ra_1_err = np . deg2rad ( df [ 'uncertainty_ew_skyc1' ] . values ) ra_2_err = np . deg2rad ( df [ 'uncertainty_ew_skyc2' ] . values ) dec_1 = np . deg2rad ( df [ 'dec_skyc1' ] . values ) dec_2 = np . deg2rad ( df [ 'dec_skyc2' ] . values ) dec_1_err = np . deg2rad ( df [ 'uncertainty_ns_skyc1' ] . values ) dec_2_err = np . deg2rad ( df [ 'uncertainty_ns_skyc2' ] . values ) dr1 = ( ra_1 - ra_2 ) * ( ra_1 - ra_2 ) dr1_1 = np . cos (( dec_1 + dec_2 ) / 2. ) dr1 *= dr1_1 * dr1_1 dr1 /= ra_1_err * ra_1_err + ra_2_err * ra_2_err dr2 = ( dec_1 - dec_2 ) * ( dec_1 - dec_2 ) dr2 /= dec_1_err * dec_1_err + dec_2_err * dec_2_err dr = np . sqrt ( dr1 + dr2 ) return dr","title":"calc_de_ruiter()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.many_to_many_advanced","text":"Finds and processes the many-to-many associations in the advanced association. We do not want to build many-to-many associations as this will make the database get very large (see TraP documentation). The skyc2 sources which are listed more than once are found, and of these, those which have a skyc1 source association which is also listed twice in the associations are selected. The closest (by limit or de Ruiter radius, depending on the method) is kept where as the other associations are dropped. This follows the same logic used by the TraP (see TraP documentation). Parameters: Name Type Description Default temp_srcs DataFrame The temporary associtation dataframe used through the advanced association process. required method str Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. required Returns: Type Description DataFrame Updated temp_srcs with the many_to_many relations dropped. Source code in vast_pipeline/pipeline/association.py def many_to_many_advanced ( temp_srcs : pd . DataFrame , method : str ) -> pd . DataFrame : ''' Finds and processes the many-to-many associations in the advanced association. We do not want to build many-to-many associations as this will make the database get very large (see TraP documentation). The skyc2 sources which are listed more than once are found, and of these, those which have a skyc1 source association which is also listed twice in the associations are selected. The closest (by limit or de Ruiter radius, depending on the method) is kept where as the other associations are dropped. This follows the same logic used by the TraP (see TraP documentation). Args: temp_srcs: The temporary associtation dataframe used through the advanced association process. method: Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. Returns: Updated temp_srcs with the many_to_many relations dropped. ''' # Select those where the extracted source is listed more than once # (e.g. index_old_skyc2 duplicated values) and of these get those that # have a source id that is listed more than once (e.g. source_skyc1 # duplicated values) in the temps_srcs df m_to_m = temp_srcs [( temp_srcs [ 'index_old_skyc2' ] . duplicated ( keep = False ) & temp_srcs [ 'source_skyc1' ] . duplicated ( keep = False ) )] . copy () if m_to_m . empty : logger . debug ( 'No many-to-many assocations.' ) return temp_srcs logger . debug ( 'Detected # %i many-to-many assocations, cleaning...' , m_to_m . shape [ 0 ] ) dist_col = 'd2d' if method == 'advanced' else 'dr' min_col = 'min_' + dist_col # get the minimum de ruiter value for each extracted source m_to_m [ min_col ] = ( m_to_m . groupby ( 'index_old_skyc2' )[ dist_col ] . transform ( 'min' ) ) # get the ids of those crossmatches that are larger than the minimum m_to_m_to_drop = m_to_m [ m_to_m [ dist_col ] != m_to_m [ min_col ]] . index . values # and drop these from the temp_srcs temp_srcs = temp_srcs . drop ( m_to_m_to_drop ) return temp_srcs","title":"many_to_many_advanced()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.many_to_one_advanced","text":"Finds and processes the many-to-one associations in the advanced association. In this case in the related column of the 'many' sources we need to append the ids of all the other 'many' (expect for itself). Parameters: Name Type Description Default temp_srcs DataFrame The temporary associtation dataframe used through the advanced association process. required Returns: Type Description DataFrame Updated temp_srcs with all many_to_one relation information added. Source code in vast_pipeline/pipeline/association.py def many_to_one_advanced ( temp_srcs : pd . DataFrame ) -> pd . DataFrame : ''' Finds and processes the many-to-one associations in the advanced association. In this case in the related column of the 'many' sources we need to append the ids of all the other 'many' (expect for itself). Args: temp_srcs: The temporary associtation dataframe used through the advanced association process. Returns: Updated temp_srcs with all many_to_one relation information added. ''' # use only these columns for easy debugging of the dataframe cols = [ 'index_old_skyc1' , 'id_skyc1' , 'source_skyc1' , 'related_skyc1' , 'index_old_skyc2' , 'id_skyc2' , 'source_skyc2' , 'd2d' , 'dr' ] # select those sources which have been matched to the same measurement # in the sky catalogue 2. duplicated_skyc2 = temp_srcs . loc [ temp_srcs [ 'index_old_skyc2' ] . duplicated ( keep = False ), cols ] # duplicated_skyc2 # +-----+-------------------+------------+----------------+ # | | index_old_skyc1 | id_skyc1 | source_skyc1 | # |-----+-------------------+------------+----------------+ # | 447 | 477 | 478 | 478 | # | 448 | 478 | 479 | 479 | # | 477 | 507 | 508 | 508 | # | 478 | 508 | 509 | 509 | # | 695 | 738 | 739 | 739 | # +-----+-------------------+------------+----------------+ # +-----------------+-------------------+------------+----------------+ # | related_skyc1 | index_old_skyc2 | id_skyc2 | source_skyc2 | # +-----------------+-------------------+------------+----------------+ # | | 305 | 5847 | -1 | # | | 305 | 5847 | -1 | # | | 648 | 6190 | -1 | # | | 648 | 6190 | -1 | # | | 561 | 6103 | -1 | # +-----------------+-------------------+------------+----------------+ # -------------+------+ # d2d | dr | # -------------+------| # 8.63598 | 0 | # 8.63598 | 0 | # 6.5777 | 0 | # 6.5777 | 0 | # 7.76527 | 0 | # -------------+------+ # if there are none no action is required. if duplicated_skyc2 . empty : logger . debug ( 'No many-to-one associations.' ) return temp_srcs logger . debug ( 'Detected # %i many-to-one associations' , duplicated_skyc2 . shape [ 0 ] ) # The new relations become that for each 'many' source we need to append # the ids of the other 'many' sources that have been associationed with the # 'one'. Below for each 'one' group we gather all the ids of the many # sources. new_relations = pd . DataFrame ( duplicated_skyc2 . groupby ( 'index_old_skyc2' ) . apply ( lambda grp : grp [ 'source_skyc1' ] . tolist ()) ) . rename ( columns = { 0 : 'new_relations' }) # new_relations # +-------------------+-----------------+ # | index_old_skyc2 | new_relations | # |-------------------+-----------------| # | 305 | [478, 479] | # | 561 | [739, 740] | # | 648 | [508, 509] | # | 764 | [841, 842] | # | 816 | [1213, 1215] | # +-------------------+-----------------+ # these new relations are then added to the duplciated dataframe so # they can easily be used by the next function. duplicated_skyc2 = duplicated_skyc2 . merge ( new_relations , left_on = 'index_old_skyc2' , right_index = True , how = 'left' ) # Remove the 'self' relations. The 'x['source_skyc1']' is an integer so it # is placed within a list notation, [], to be able to be easily subtracted # from the new_relations. duplicated_skyc2 [ 'new_relations' ] = ( duplicated_skyc2 . apply ( lambda x : list ( set ( x [ 'new_relations' ]) - set ([ x [ 'source_skyc1' ]])), axis = 1 ) ) # Use the 'add_new_many_to_one_relations' method to add tthe new relatitons # to the actual `related_skyc1' column. duplicated_skyc2 [ 'related_skyc1' ] = ( duplicated_skyc2 . apply ( add_new_many_to_one_relations , axis = 1 ) ) # Transfer the new relations from the duplicated df to the temp_srcs. The # index is explicitly declared to avoid any mixups. temp_srcs . loc [ duplicated_skyc2 . index . values , 'related_skyc1' ] = duplicated_skyc2 . loc [ duplicated_skyc2 . index . values , 'related_skyc1' ] . values return temp_srcs","title":"many_to_one_advanced()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.one_to_many_advanced","text":"Finds and processes the one-to-many associations in the advanced association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the basic version as the data products between the two are different. Parameters: Name Type Description Default temp_srcs DataFrame The temporary associtation dataframe used through the advanced association process. required sources_df DataFrame The sources_df produced by each step of association holding the current 'sources'. required method str Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. required id_incr_par_assoc int An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] Updated temp_srcs and sources_df with all one_to_many relation information added. Source code in vast_pipeline/pipeline/association.py def one_to_many_advanced ( temp_srcs : pd . DataFrame , sources_df : pd . DataFrame , method : str , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: ''' Finds and processes the one-to-many associations in the advanced association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the basic version as the data products between the two are different. Args: temp_srcs: The temporary associtation dataframe used through the advanced association process. sources_df: The sources_df produced by each step of association holding the current 'sources'. method: Can be either 'advanced' or 'deruiter' to represent the advanced association method being used. id_incr_par_assoc: An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association Returns: Updated temp_srcs and sources_df with all one_to_many relation information added. ''' # use only these columns for easy debugging of the dataframe cols = [ 'index_old_skyc1' , 'id_skyc1' , 'source_skyc1' , 'related_skyc1' , 'index_old_skyc2' , 'id_skyc2' , 'source_skyc2' , 'd2d' , 'dr' ] duplicated_skyc1 = temp_srcs . loc [ temp_srcs [ 'source_skyc1' ] . duplicated ( keep = False ), cols ] . copy () # duplicated_skyc1 # +-----+-------------------+------------+----------------+ # | | index_old_skyc1 | id_skyc1 | source_skyc1 | # |-----+-------------------+------------+----------------+ # | 117 | 121 | 122 | 122 | # | 118 | 121 | 122 | 122 | # | 238 | 253 | 254 | 254 | # | 239 | 253 | 254 | 254 | # | 246 | 261 | 262 | 262 | # +-----+-------------------+------------+----------------+ # -----------------+-------------------+------------+----------------+ # related_skyc1 | index_old_skyc2 | id_skyc2 | source_skyc2 | # -----------------+-------------------+------------+----------------+ # | 526 | 6068 | -1 | # | 528 | 6070 | -1 | # | 264 | 5806 | -1 | # | 265 | 5807 | -1 | # | 327 | 5869 | -1 | # -----------------+-------------------+------------+----------------+ # -------------+------+ # d2d | dr | # -------------+------| # 3.07478 | 0 | # 6.41973 | 0 | # 2.04422 | 0 | # 6.16881 | 0 | # 3.20439 | 0 | # -------------+------+ # If no relations then no action is required if duplicated_skyc1 . empty : logger . debug ( 'No one-to-many associations.' ) return temp_srcs , sources_df logger . debug ( 'Detected # %i one-to-many assocations, cleaning...' , duplicated_skyc1 . shape [ 0 ] ) # Get the column to check for the minimum depending on the method # set the column names needed for filtering the 'to-many' # associations depending on the method (advanced or deruiter) dist_col = 'd2d' if method == 'advanced' else 'dr' # go through the doubles and # 1. Keep the closest d2d or de ruiter as the primary id # 2. Increment a new source id for others # 3. Add a copy of the previously matched # source into sources. # multi_srcs = duplicated_skyc1['source_skyc1'].unique() # Get the duplicated, sort by the distance column duplicated_skyc1 = duplicated_skyc1 . sort_values ( by = [ 'source_skyc1' , dist_col ] ) # Get those that need to be given a new ID number (i.e. not the min dist_col) idx_to_change = duplicated_skyc1 . index . values [ duplicated_skyc1 . duplicated ( 'source_skyc1' ) ] # Create a new `new_source_id` column to store the 'correct' IDs duplicated_skyc1 [ 'new_source_id' ] = duplicated_skyc1 [ 'source_skyc1' ] # +-----------------+ # | new_source_id | # +-----------------| # | 122 | # | 122 | # | 254 | # | 254 | # | 262 | # +-----------------+ # Define the range of new source ids start_new_src_id = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc # Create an arange to use to change the ones that need to be changed. new_source_ids = np . arange ( start_new_src_id , start_new_src_id + idx_to_change . shape [ 0 ], dtype = int ) # Assign the new IDs to those that need to be changed. duplicated_skyc1 . loc [ idx_to_change , 'new_source_id' ] = new_source_ids # We also need to clear the relations for these 'new' sources # otherwise it will inherit rogue relations from the original relation duplicated_skyc1 . loc [ idx_to_change , 'related_skyc1' ] = None # Now we need to sort out the related, essentially here the 'original' # and 'non original' need to be treated differently. # The original source need all the assoicated new ids appended to the # related column. # The not_original ones need just the original ID appended. not_original = duplicated_skyc1 . loc [ idx_to_change ] . copy () original = duplicated_skyc1 . drop_duplicates ( 'source_skyc1' ) . copy () # This gathers all the new ids that need to be appended # to the original related column. new_original_related = pd . DataFrame ( not_original [ [ 'source_skyc1' , 'new_source_id' ] ] . groupby ( 'source_skyc1' ) . apply ( lambda grp : grp [ 'new_source_id' ] . tolist () ) ) #new_original_related # +----------------+--------+ # | source_skyc1 | 0 | # |----------------+--------| # | 122 | [5542] | # | 254 | [5543] | # | 262 | [5544] | # | 405 | [5545] | # | 656 | [5546] | # +----------------+--------+ # Append the relations in each case, using the above 'new_original_related' # for the original ones. # The not original only require the appending of the original index. original [ 'related_skyc1' ] = ( original [[ 'related_skyc1' , 'source_skyc1' ]] . apply ( add_new_one_to_many_relations , args = ( True , new_original_related ), axis = 1 ) ) # what the column looks like after the above # +-----------------+ # | related_skyc1 | # +-----------------+ # | [5542] | # | [5543] | # | [5544] | # | [5545] | # | [5546] | # +-----------------+ not_original . loc [:, 'related_skyc1' ] = not_original . apply ( add_new_one_to_many_relations , args = ( True ,), axis = 1 ) # Merge them back together duplicated_skyc1 = original . append ( not_original ) del original , not_original # Apply the updates to the actual temp_srcs. temp_srcs . loc [ idx_to_change , 'source_skyc1' ] = new_source_ids temp_srcs . loc [ duplicated_skyc1 . index . values , 'related_skyc1' ] = duplicated_skyc1 . loc [ duplicated_skyc1 . index . values , 'related_skyc1' ] . values # Finally we need to create copies of the previous sources in the # sources_df to complete the new sources. # To do this we get only the non-original sources duplicated_skyc1 = duplicated_skyc1 . loc [ duplicated_skyc1 . duplicated ( 'source_skyc1' ) ] # Get all the indexes required for each original # `source_skyc1` value source_df_index_to_copy = pd . DataFrame ( duplicated_skyc1 . groupby ( 'source_skyc1' ) . apply ( lambda grp : sources_df [ sources_df [ 'source' ] == grp . name ] . index . values . tolist () ) ) # source_df_index_to_copy # +----------------+-------+ # | source_skyc1 | 0 | # |----------------+-------| # | 122 | [121] | # | 254 | [253] | # | 262 | [261] | # | 405 | [404] | # | 656 | [655] | # +----------------+-------+ # merge these so it's easy to explode and copy the index values. duplicated_skyc1 = ( duplicated_skyc1 . loc [:,[ 'source_skyc1' , 'new_source_id' ]] . merge ( source_df_index_to_copy , left_on = 'source_skyc1' , right_index = True , how = 'left' ) . rename ( columns = { 0 : 'source_index' }) . explode ( 'source_index' ) ) # duplicated_skyc1 # +-----+----------------+-----------------+----------------+ # | | source_skyc1 | new_source_id | source_index | # |-----+----------------+-----------------+----------------| # | 118 | 122 | 5542 | 121 | # | 239 | 254 | 5543 | 253 | # | 247 | 262 | 5544 | 261 | # | 380 | 405 | 5545 | 404 | # | 615 | 656 | 5546 | 655 | # +-----+----------------+-----------------+----------------+ # Get the sources sources_to_copy = sources_df . loc [ duplicated_skyc1 [ 'source_index' ] . values ] # Apply the new_source_id sources_to_copy [ 'source' ] = duplicated_skyc1 [ 'new_source_id' ] . values # Reset the related column to avoid rogue relations sources_to_copy [ 'related' ] = None # and finally append. sources_df = sources_df . append ( sources_to_copy , ignore_index = True ) return temp_srcs , sources_df","title":"one_to_many_advanced()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.one_to_many_basic","text":"Finds and processes the one-to-many associations in the basic association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the advanced version as the data products between the two are different. Parameters: Name Type Description Default skyc2_srcs DataFrame The sky catalogue 2 sources (i.e. the sources being associated to the base) used during basic association. required sources_df DataFrame The sources_df produced by each step of association holding the current 'sources'. required id_incr_par_assoc int An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association 0 Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] Tuple containging the updated 'skyc2_srcs' and 'sources_df' with all one_to_many relation information added. Source code in vast_pipeline/pipeline/association.py def one_to_many_basic ( skyc2_srcs : pd . DataFrame , sources_df : pd . DataFrame , id_incr_par_assoc : int = 0 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Finds and processes the one-to-many associations in the basic association. For each one-to-many association, the nearest associated source is assigned the original source id, where as the others are given new ids. The original source in skyc1 then is copied to the sources_df to provide the extra association for that source, i.e. it is forked. This is needed to be separate from the advanced version as the data products between the two are different. Args: skyc2_srcs: The sky catalogue 2 sources (i.e. the sources being associated to the base) used during basic association. sources_df: The sources_df produced by each step of association holding the current 'sources'. id_incr_par_assoc: An increment value to add to new source ids when creating them. Mainly useful for add mode with parallel association Returns: Tuple containging the updated 'skyc2_srcs' and 'sources_df' with all one_to_many relation information added. \"\"\" # select duplicated in 'source' field in skyc2_srcs, excluding -1 duplicated_skyc2 = skyc2_srcs . loc [ ( skyc2_srcs [ 'source' ] != - 1 ) & skyc2_srcs [ 'source' ] . duplicated ( keep = False ), [ 'source' , 'related' , 'd2d' ] ] # duplicated_skyc2 # +-----+----------+-----------+---------+ # | | source | related | d2d | # |-----+----------+-----------+---------| # | 264 | 254 | | 2.04422 | # | 265 | 254 | | 6.16881 | # | 327 | 262 | | 3.20439 | # | 328 | 262 | | 3.84425 | # | 526 | 122 | | 3.07478 | # +-----+----------+-----------+---------+ if duplicated_skyc2 . empty : logger . debug ( 'No one-to-many associations.' ) return skyc2_srcs , sources_df logger . info ( 'Detected # %i double matches, cleaning...' , duplicated_skyc2 . shape [ 0 ] ) # now we have the src values which are doubled. # make the nearest match have the \"original\" src id # give the other matched source a new src id # and make sure to copy the other previously # matched sources. # Get the duplicated, sort by the distance column duplicated_skyc2 = duplicated_skyc2 . sort_values ( by = [ 'source' , 'd2d' ]) # Get those that need to be given a new ID number (i.e. not the min dist_col) idx_to_change = duplicated_skyc2 . index . values [ duplicated_skyc2 . duplicated ( 'source' ) ] # Create a new `new_source_id` column to store the 'correct' IDs duplicated_skyc2 [ 'new_source_id' ] = duplicated_skyc2 [ 'source' ] # Define the range of new source ids start_new_src_id = sources_df [ 'source' ] . values . max () + 1 + id_incr_par_assoc new_source_ids = np . arange ( start_new_src_id , start_new_src_id + idx_to_change . shape [ 0 ], dtype = int ) # Assign the new IDs duplicated_skyc2 . loc [ idx_to_change , 'new_source_id' ] = new_source_ids # duplicated_skyc2 # +-----+----------+-----------+---------+-----------------+ # | | source | related | d2d | new_source_id | # |-----+----------+-----------+---------+-----------------| # | 526 | 122 | | 3.07478 | 122 | # | 528 | 122 | | 6.41973 | 5542 | # | 264 | 254 | | 2.04422 | 254 | # | 265 | 254 | | 6.16881 | 5543 | # | 327 | 262 | | 3.20439 | 262 | # +-----+----------+-----------+---------+-----------------+ # Now we need to sort out the related, essentially here the 'original' # and 'non original' need to be treated differently. # The original source need all the assoicated new ids appended to the # related column. # The not_original ones need just the original ID appended. # copy() is used here to avoid chained indexing (set with copy warnings) not_original = duplicated_skyc2 . loc [ idx_to_change ] . copy () original = duplicated_skyc2 . drop_duplicates ( 'source' ) . copy () new_original_related = pd . DataFrame ( not_original [ [ 'source' , 'new_source_id' ] ] . groupby ( 'source' ) . apply ( lambda grp : grp [ 'new_source_id' ] . tolist () ) ) # new_original_related # +----------+--------+ # | source | 0 | # |----------+--------| # | 122 | [5542] | # | 254 | [5543] | # | 262 | [5544] | # | 405 | [5545] | # | 656 | [5546] | # +----------+--------+ # Append the relations in each case, using the above 'new_original_related' # for the original ones. # The not original only require the appending of the original index. original [ 'related' ] = ( original [[ 'related' , 'source' ]] . apply ( add_new_one_to_many_relations , args = ( False , new_original_related ), axis = 1 ) ) not_original [ 'related' ] = not_original . apply ( add_new_one_to_many_relations , args = ( False ,), axis = 1 ) duplicated_skyc2 = original . append ( not_original ) # duplicated_skyc2 # +-----+----------+-----------+---------+-----------------+ # | | source | related | d2d | new_source_id | # |-----+----------+-----------+---------+-----------------| # | 526 | 122 | [5542] | 3.07478 | 122 | # | 264 | 254 | [5543] | 2.04422 | 254 | # | 327 | 262 | [5544] | 3.20439 | 262 | # | 848 | 405 | [5545] | 5.52865 | 405 | # | 695 | 656 | [5546] | 4.69094 | 656 | # +-----+----------+-----------+---------+-----------------+ del original , not_original # Apply the updates to the actual temp_srcs. skyc2_srcs . loc [ idx_to_change , 'source' ] = new_source_ids skyc2_srcs . loc [ duplicated_skyc2 . index . values , 'related' ] = duplicated_skyc2 . loc [ duplicated_skyc2 . index . values , 'related' ] . values # Finally we need to copy copies of the previous sources in the # sources_df to complete the new sources. # To do this we get only the non-original sources duplicated_skyc2 = duplicated_skyc2 . loc [ duplicated_skyc2 . duplicated ( 'source' ) ] # Get all the indexes required for each original # `source_skyc1` value source_df_index_to_copy = pd . DataFrame ( duplicated_skyc2 . groupby ( 'source' ) . apply ( lambda grp : sources_df [ sources_df [ 'source' ] == grp . name ] . index . values . tolist () ) ) # source_df_index_to_copy # +----------+-------+ # | source | 0 | # |----------+-------| # | 122 | [121] | # | 254 | [253] | # | 262 | [261] | # | 405 | [404] | # | 656 | [655] | # +----------+-------+ # merge these so it's easy to explode and copy the index values. duplicated_skyc2 = ( duplicated_skyc2 [[ 'source' , 'new_source_id' ]] . merge ( source_df_index_to_copy , left_on = 'source' , right_index = True , how = 'left' ) . rename ( columns = { 0 : 'source_index' }) . explode ( 'source_index' ) ) # Get the sources - all columns from the sources_df table sources_to_copy = sources_df . loc [ duplicated_skyc2 [ 'source_index' ] . values ] # Apply the new_source_id sources_to_copy [ 'source' ] = duplicated_skyc2 [ 'new_source_id' ] . values # Reset the related column to avoid rogue relations sources_to_copy [ 'related' ] = None # and finally append. sources_df = sources_df . append ( sources_to_copy , ignore_index = True ) return skyc2_srcs , sources_df","title":"one_to_many_basic()"},{"location":"reference/pipeline/association/#vast_pipeline.pipeline.association.parallel_association","text":"Launches association on different sky region groups in parallel using Dask. Parameters: Name Type Description Default images_df DataFrame Holds the images that are being processed. Also contains what sky region group the image belongs to. required limit Angle The association radius limit. required dr_limit float The de Ruiter radius limit. required bw_limit float The beamwidth limit. required duplicate_limit Angle The duplicate radius detection limit. required config PipelineConfig The pipeline config settings. required n_skyregion_groups int The number of sky region groups. required Returns: Type Description DataFrame pd.DataFrame: The combined association results of the parallel association with corrected source ids. Source code in vast_pipeline/pipeline/association.py def parallel_association ( images_df : pd . DataFrame , limit : Angle , dr_limit : float , bw_limit : float , duplicate_limit : Angle , config : PipelineConfig , n_skyregion_groups : int , add_mode : bool , previous_parquets : Dict [ str , str ], done_images_df : pd . DataFrame , done_source_ids : List [ int ] ) -> pd . DataFrame : \"\"\" Launches association on different sky region groups in parallel using Dask. Args: images_df: Holds the images that are being processed. Also contains what sky region group the image belongs to. limit: The association radius limit. dr_limit: The de Ruiter radius limit. bw_limit: The beamwidth limit. duplicate_limit: The duplicate radius detection limit. config (module): The pipeline config settings. n_skyregion_groups: The number of sky region groups. Returns: pd.DataFrame: The combined association results of the parallel association with corrected source ids. \"\"\" logger . info ( \"Running parallel association for %i sky region groups.\" , n_skyregion_groups ) timer = StopWatch () meta = { 'id' : 'i' , 'uncertainty_ew' : 'f' , 'weight_ew' : 'f' , 'uncertainty_ns' : 'f' , 'weight_ns' : 'f' , 'flux_int' : 'f' , 'flux_int_err' : 'f' , 'flux_int_isl_ratio' : 'f' , 'flux_peak' : 'f' , 'flux_peak_err' : 'f' , 'flux_peak_isl_ratio' : 'f' , 'forced' : '?' , 'compactness' : 'f' , 'has_siblings' : '?' , 'snr' : 'f' , 'image' : 'U' , 'datetime' : 'datetime64[ns]' , 'source' : 'i' , 'ra' : 'f' , 'dec' : 'f' , 'd2d' : 'f' , 'dr' : 'f' , 'related' : 'O' , 'epoch' : 'i' , 'interim_ew' : 'f' , 'interim_ns' : 'f' , } # Add an increment to any new source values when using add_mode to avoid # getting duplicates in the result laater id_incr_par_assoc = max ( done_source_ids ) if add_mode else 0 n_cpu = cpu_count () - 1 # pass each skyreg_group through the normal association process. results = ( dd . from_pandas ( images_df , n_cpu ) . groupby ( 'skyreg_group' ) . apply ( association , limit = limit , dr_limit = dr_limit , bw_limit = bw_limit , duplicate_limit = duplicate_limit , config = config , add_mode = add_mode , previous_parquets = previous_parquets , done_images_df = done_images_df , id_incr_par_assoc = id_incr_par_assoc , parallel = True , meta = meta ) . compute ( n_workers = n_cpu , scheduler = 'processes' ) ) # results are the normal dataframe of results with the columns: # 'id', 'uncertainty_ew', 'weight_ew', 'uncertainty_ns', 'weight_ns', # 'flux_int', 'flux_int_err', 'flux_peak', 'flux_peak_err', 'forced', # 'compactness', 'has_siblings', 'snr', 'image', 'datetime', 'source', # 'ra', 'dec', 'd2d', 'dr', 'related', 'epoch', 'interim_ew' and # 'interim_ns'. # The index however is now a multi index with the skyregion group and # a general result index. Hence the general result index is repeated for # each skyreg_group along with the source_ids. This needs to be collapsed # and the source id's corrected. # Index example: # id # skyreg_group # -------------------------- # 2 0 15640 # 1 15641 # 2 15642 # 3 15643 # 4 15644 # ... ... # 1 46975 53992 # 46976 54062 # 46977 54150 # 46978 54161 # 46979 54164 # Get the indexes (skyreg_groups) to loop over for source id correction indexes = results . index . levels [ 0 ] . values if add_mode : # Need to correct all skyreg_groups. # First get the starting id for new sources. new_id = max ( done_source_ids ) + 1 for i in indexes : corr_df , new_id = _correct_parallel_source_ids_add_mode ( results . loc [ i , [ 'source' , 'related' ]], done_source_ids , new_id ) results . loc [ ( i , slice ( None )), [ 'source' , 'related' ] ] = corr_df . values else : # The first index acts as the base, so the others are looped over and # corrected. for i , val in enumerate ( indexes ): # skip first one, makes the enumerate easier to deal with if i == 0 : continue # Get the maximum source ID from the previous group. max_id = results . loc [ indexes [ i - 1 ]] . source . max () # Run through the correction function, only the 'source' and # 'related' # columns are passed and returned (corrected). corr_df = _correct_parallel_source_ids ( results . loc [ val , [ 'source' , 'related' ]], max_id ) # replace the values in the results with the corrected source and # related values results . loc [ ( val , slice ( None )), [ 'source' , 'related' ] ] = corr_df . values del corr_df # reset the indeex of the final corrected and collapsed result results = results . reset_index ( drop = True ) logger . info ( 'Total parallel association time: %.2f seconds' , timer . reset_init () ) return results","title":"parallel_association()"},{"location":"reference/pipeline/config/","text":"ImageIngestConfig \u00b6 Image ingest configuration derived from PipelineConfig. Attributes: Name Type Description SCHEMA class attribute containing the YAML schema for the image ingest config. TEMPLATE_PATH str class attribute containing the path to the default Jinja2 image ingest config template file. Exceptions: Type Description PipelineConfigError the input YAML config violates the schema. PipelineConfig \u00b6 Pipeline run configuration. Attributes: Name Type Description SCHEMA class attribute containing the YAML schema for the run config. TEMPLATE_PATH str class attribute containing the path to the default Jinja2 run config template file. epoch_based bool boolean indicating if the original run config inputs were provided with user-defined epochs. Exceptions: Type Description PipelineConfigError the input YAML config violates the schema. __getitem__ ( self , name ) special \u00b6 Retrieves the requested YAML chunk as a native Python object. Source code in vast_pipeline/pipeline/config.py def __getitem__ ( self , name : str ): \"\"\"Retrieves the requested YAML chunk as a native Python object.\"\"\" return self . _yaml [ name ] . data __init__ ( self , config_yaml ) special \u00b6 Initialises PipelineConfig with parsed (but not necessarily validated) YAML. Parameters: Name Type Description Default config_yaml YAML Input YAML, usually the output of strictyaml.load . required Exceptions: Type Description PipelineConfigError The input YAML config violates the schema. Source code in vast_pipeline/pipeline/config.py def __init__ ( self , config_yaml : yaml . YAML ): \"\"\"Initialises PipelineConfig with parsed (but not necessarily validated) YAML. Args: config_yaml (yaml.YAML): Input YAML, usually the output of `strictyaml.load`. Raises: PipelineConfigError: The input YAML config violates the schema. \"\"\" self . _yaml : yaml . YAML = config_yaml # The epoch_based parameter below is for if the user has entered just lists we # don't have access to the dates until the Image instances are created. So we # flag this as true so that we can reorder the epochs once the date information # is available. It is also recorded in the database such that there is a record # of the fact that the run was processed in an epoch based mode. self . epoch_based : bool # Determine if epoch-based association should be used based on input files. # If inputs have been parsed to dicts, then the user has defined their own epochs. # If inputs have been parsed to lists, we must convert to dicts and auto-fill # the epochs. # ensure the inputs are valid in case .from_file(..., validate=False) was used try : self . _validate_inputs () except yaml . YAMLValidationError as e : raise PipelineConfigError ( e ) # detect simple list inputs and convert them to epoch-mode inputs for input_file_type in self . _REQUIRED_INPUT_TYPES : # skip missing optional input types, e.g. background if ( not self . _REQUIRED_INPUT_TYPES [ input_file_type ] and input_file_type not in self [ \"inputs\" ] ): continue input_files = self [ \"inputs\" ][ input_file_type ] # resolve glob expressions if present if isinstance ( input_files , dict ): # must be either a glob expression, list of glob expressions, or epoch-mode if \"glob\" in input_files : # resolve the glob expressions self . epoch_based = False file_list = self . _resolve_glob_expressions ( self . _yaml [ \"inputs\" ][ input_file_type ] ) self . _yaml [ \"inputs\" ][ input_file_type ] = self . _create_input_epochs ( file_list ) else : # epoch-mode with either a list of files or glob expressions self . epoch_based = True for epoch in input_files : if \"glob\" in input_files [ epoch ]: # resolve the glob expressions file_list = self . _resolve_glob_expressions ( self . _yaml [ \"inputs\" ][ input_file_type ][ epoch ] ) self . _yaml [ \"inputs\" ][ input_file_type ][ epoch ] = file_list else : # Epoch-based association not requested and no globs present. Replace # input lists with dicts where each input file has it's own epoch. self . epoch_based = False self . _yaml [ \"inputs\" ][ input_file_type ] = self . _create_input_epochs ( input_files ) check_prev_config_diff ( self ) \u00b6 Checks if the previous config file differs from the current config file. Used in add mode. Only returns true if the images are different and the other general settings are the same (the requirement for add mode). Otherwise False is returned. Returns: Type Description bool True if images are different but general settings are the same, otherwise False is returned. Source code in vast_pipeline/pipeline/config.py def check_prev_config_diff ( self ) -> bool : \"\"\" Checks if the previous config file differs from the current config file. Used in add mode. Only returns true if the images are different and the other general settings are the same (the requirement for add mode). Otherwise False is returned. Returns: True if images are different but general settings are the same, otherwise False is returned. \"\"\" prev_config = PipelineConfig . from_file ( os . path . join ( self [ \"run\" ][ \"path\" ], \"config_prev.yaml\" ), label = \"previous run config\" , ) if self . _yaml == prev_config . _yaml : return True # are the input image files different? images_changed = self [ \"inputs\" ][ \"image\" ] != prev_config [ \"inputs\" ][ \"image\" ] # are all the non-input file configs the same? config_dict = self . _yaml . data prev_config_dict = prev_config . _yaml . data _ = config_dict . pop ( \"inputs\" ) _ = prev_config_dict . pop ( \"inputs\" ) settings_check = config_dict == prev_config_dict if images_changed and settings_check : return False return True from_file ( yaml_path , label = 'run config' , validate = True , add_defaults = True ) classmethod \u00b6 Create a PipelineConfig object from a run configuration YAML file. Parameters: Name Type Description Default yaml_path str Path to the run config YAML file. required label str A label for the config object that will be used in error messages. Default is \"run config\". 'run config' validate bool Perform config schema validation immediately after loading the config file. If set to False, the full schema validation will not be performed until PipelineConfig.validate() is explicitly called. The inputs are always validated regardless. Defaults to True. True add_defaults bool Add missing configuration parameters using configured defaults. The defaults are read from the Django settings file. Defaults to True. True Exceptions: Type Description PipelineConfigError The run config YAML file fails schema validation. Source code in vast_pipeline/pipeline/config.py @classmethod def from_file ( cls , yaml_path : str , label : str = \"run config\" , validate : bool = True , add_defaults : bool = True , ) -> \"PipelineConfig\" : \"\"\"Create a PipelineConfig object from a run configuration YAML file. Args: yaml_path: Path to the run config YAML file. label: A label for the config object that will be used in error messages. Default is \"run config\". validate: Perform config schema validation immediately after loading the config file. If set to False, the full schema validation will not be performed until PipelineConfig.validate() is explicitly called. The inputs are always validated regardless. Defaults to True. add_defaults: Add missing configuration parameters using configured defaults. The defaults are read from the Django settings file. Defaults to True. Raises: PipelineConfigError: The run config YAML file fails schema validation. \"\"\" schema = cls . SCHEMA if validate else yaml . Any () with open ( yaml_path ) as fh : config_str = fh . read () try : config_yaml = yaml . load ( config_str , schema = schema , label = label ) except yaml . YAMLValidationError as e : raise PipelineConfigError ( e ) if add_defaults : # make a template config based on defaults config_defaults_str = make_config_template ( cls . TEMPLATE_PATH , ** settings . PIPE_RUN_CONFIG_DEFAULTS , ) config_defaults_dict : Dict [ str , Any ] = yaml . load ( config_defaults_str ) . data # merge configs config_dict = dict_merge ( config_defaults_dict , config_yaml . data ) config_yaml = yaml . as_document ( config_dict , schema = schema , label = label ) return cls ( config_yaml ) image_opts ( self ) \u00b6 Get the config options required for image ingestion only. Namely selavy_local_rms_fill_value condon_errors ra_uncertainty dec_uncertainty Returns: Type Description Dict Dict: the relevant key value pairs Source code in vast_pipeline/pipeline/config.py def image_opts ( self ) -> Dict : \"\"\" Get the config options required for image ingestion only. Namely: - selavy_local_rms_fill_value - condon_errors - ra_uncertainty - dec_uncertainty Returns: Dict: the relevant key value pairs \"\"\" keys = [ \"selavy_local_rms_fill_value\" , \"condon_errors\" , \"ra_uncertainty\" , \"dec_uncertainty\" ] return { key : self [ \"measurements\" ][ key ] for key in keys } validate ( self , user = None ) \u00b6 Perform extra validation steps not covered by the default schema validation. The following checks are performed in order. If a check fails, an exception is raised and no further checks are performed. All input files have the same number of epochs and the same number of files per epoch. The number of input files does not exceed the configured pipeline maximum. This is only enforced if a regular user (not staff/admin) created the run. There are at least two input images. Background input images are required if source monitoring is turned on. All input files exist. Parameters: Name Type Description Default user User Optional. The User of the request if made through the UI. Defaults to None. None Exceptions: Type Description PipelineConfigError a validation check failed. Source code in vast_pipeline/pipeline/config.py def validate ( self , user : User = None ): \"\"\"Perform extra validation steps not covered by the default schema validation. The following checks are performed in order. If a check fails, an exception is raised and no further checks are performed. 1. All input files have the same number of epochs and the same number of files per epoch. 2. The number of input files does not exceed the configured pipeline maximum. This is only enforced if a regular user (not staff/admin) created the run. 3. There are at least two input images. 4. Background input images are required if source monitoring is turned on. 5. All input files exist. Args: user: Optional. The User of the request if made through the UI. Defaults to None. Raises: PipelineConfigError: a validation check failed. \"\"\" # run standard base schema validation try : self . _yaml . revalidate ( self . SCHEMA ) except yaml . YAMLValidationError as e : raise PipelineConfigError ( e ) # epochs defined for images only, used as the reference list of epochs epochs_image = self [ \"inputs\" ][ \"image\" ] . keys () # map input type to a set of epochs epochs_by_input_type = { input_type : set ( self [ \"inputs\" ][ input_type ] . keys ()) for input_type in self [ \"inputs\" ] . keys () } # map input type to total number of files from all epochs n_files_by_input_type = {} for input_type , epochs_set in epochs_by_input_type . items (): n_files_by_input_type [ input_type ] = 0 for epoch in epochs_set : n_files_by_input_type [ input_type ] += len ( self [ \"inputs\" ][ input_type ][ epoch ]) n_files = 0 # total number of input files # map input type to a mapping of epoch to file count epoch_n_files : Dict [ str , Dict [ str , int ]] = {} for input_type in self [ \"inputs\" ] . keys (): epoch_n_files [ input_type ] = {} for epoch in self [ \"inputs\" ][ input_type ] . keys (): n = len ( self [ \"inputs\" ][ input_type ][ epoch ]) epoch_n_files [ input_type ][ epoch ] = n n_files += n # Note by this point the input files have been converted to a mapping regardless # of the user's input format. # Ensure all input file types have the same epochs. try : for input_type in self [ \"inputs\" ] . keys (): self . _yaml [ \"inputs\" ][ input_type ] . revalidate ( yaml . Map ({ epoch : yaml . Seq ( yaml . Str ()) for epoch in epochs_image }) ) except yaml . YAMLValidationError : # number of epochs could be different or the name of the epochs may not match # find out which by counting the number of unique epochs per input type n_epochs_per_input_type = [ len ( epochs_set ) for epochs_set in epochs_by_input_type . values () ] if len ( set ( n_epochs_per_input_type )) > 1 : if self . epoch_based : error_msg = \"The number of epochs must match for all input types. \\n \" else : error_msg = \"The number of files must match for all input types. \\n \" else : error_msg = \"The name of the epochs must match for all input types. \\n \" counts_str = \"\" if self . epoch_based : for input_type in epoch_n_files . keys (): n = len ( epoch_n_files [ input_type ]) counts_str += ( f \" { input_type } has { n } epoch { 's' if n > 1 else '' } :\" f \" { ', ' . join ( epoch_n_files [ input_type ] . keys ()) } \\n \" ) else : for input_type , n in n_files_by_input_type . items (): counts_str += f \" { input_type } has { n } file { 's' if n > 1 else '' } \\n \" counts_str = counts_str [: - 1 ] raise PipelineConfigError ( error_msg + counts_str ) # Ensure all input file type epochs have the same number of files per epoch. # This could be combined with the number of epochs validation above, but we want # to give specific feedback to the user on failure. try : for input_type in self [ \"inputs\" ] . keys (): self . _yaml [ \"inputs\" ][ input_type ] . revalidate ( yaml . Map ( { epoch : yaml . FixedSeq ( [ yaml . Str () for _ in range ( epoch_n_files [ \"image\" ][ epoch ]) ] ) for epoch in epochs_image } ) ) except yaml . YAMLValidationError : # map input type to a mapping of epoch to file count file_counts_str = \"\" for input_type in self [ \"inputs\" ] . keys (): file_counts_str += f \" { input_type } : \\n \" for epoch in sorted ( self [ \"inputs\" ][ input_type ] . keys ()): file_counts_str += ( f \" { epoch } : { len ( self [ 'inputs' ][ input_type ][ epoch ]) } \\n \" ) file_counts_str = file_counts_str [: - 1 ] raise PipelineConfigError ( \"The number of files per epoch does not match between input types. \\n \" + file_counts_str ) # ensure the number of input files is less than the user limit if user and n_files > settings . MAX_PIPERUN_IMAGES : if user . is_staff : logger . warning ( \"Maximum number of images\" f \" ( { settings . MAX_PIPERUN_IMAGES } ) rule bypassed with\" \" admin status.\" ) else : raise PipelineConfigError ( f \"The number of images entered ( { n_files } )\" \" exceeds the maximum number of images currently\" f \" allowed ( { settings . MAX_PIPERUN_IMAGES } ). Please ask\" \" an administrator for advice on processing your run.\" ) # ensure at least two inputs are provided check = [ n_files_by_input_type [ input_type ] < 2 for input_type in self [ \"inputs\" ] . keys ()] if any ( check ): raise PipelineConfigError ( \"Number of image files must to be larger than 1\" ) # ensure background files are provided if source monitoring is requested try : monitor = self [ \"source_monitoring\" ][ \"monitor\" ] except KeyError : monitor = False if monitor : inputs_schema = yaml . Map ( { k : yaml . UniqueSeq ( yaml . Str ()) | yaml . MapPattern ( yaml . Str (), yaml . UniqueSeq ( yaml . Str ())) for k in self . _REQUIRED_INPUT_TYPES } ) try : self . _yaml [ \"inputs\" ] . revalidate ( inputs_schema ) except yaml . YAMLValidationError : raise PipelineConfigError ( \"Background files must be provided if source monitoring is enabled.\" ) # ensure the input files all exist for input_type in self [ \"inputs\" ] . keys (): for epoch , file_list in self [ \"inputs\" ][ input_type ] . items (): for file in file_list : if not os . path . exists ( file ): raise PipelineConfigError ( f \" { file } does not exist.\" ) make_config_template ( template_path , ** kwargs ) \u00b6 Generate the contents of a run configuration file from on a Jinja2 template. Parameters: Name Type Description Default template_path str Path to a Jinja2 template. required **kwargs keyword arguments passed to the template renderer to fill in template variables. {} Returns: Type Description str Filled in template string. Source code in vast_pipeline/pipeline/config.py def make_config_template ( template_path : str , ** kwargs ) -> str : \"\"\"Generate the contents of a run configuration file from on a Jinja2 template. Args: template_path: Path to a Jinja2 template. **kwargs: keyword arguments passed to the template renderer to fill in template variables. Returns: Filled in template string. \"\"\" with open ( template_path , \"r\" ) as fp : template_str = fp . read () env = Environment ( trim_blocks = True , lstrip_blocks = True ) template = env . from_string ( template_str ) return template . render ( ** kwargs )","title":"config.py"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.ImageIngestConfig","text":"Image ingest configuration derived from PipelineConfig. Attributes: Name Type Description SCHEMA class attribute containing the YAML schema for the image ingest config. TEMPLATE_PATH str class attribute containing the path to the default Jinja2 image ingest config template file. Exceptions: Type Description PipelineConfigError the input YAML config violates the schema.","title":"ImageIngestConfig"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.PipelineConfig","text":"Pipeline run configuration. Attributes: Name Type Description SCHEMA class attribute containing the YAML schema for the run config. TEMPLATE_PATH str class attribute containing the path to the default Jinja2 run config template file. epoch_based bool boolean indicating if the original run config inputs were provided with user-defined epochs. Exceptions: Type Description PipelineConfigError the input YAML config violates the schema.","title":"PipelineConfig"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.PipelineConfig.__getitem__","text":"Retrieves the requested YAML chunk as a native Python object. Source code in vast_pipeline/pipeline/config.py def __getitem__ ( self , name : str ): \"\"\"Retrieves the requested YAML chunk as a native Python object.\"\"\" return self . _yaml [ name ] . data","title":"__getitem__()"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.PipelineConfig.__init__","text":"Initialises PipelineConfig with parsed (but not necessarily validated) YAML. Parameters: Name Type Description Default config_yaml YAML Input YAML, usually the output of strictyaml.load . required Exceptions: Type Description PipelineConfigError The input YAML config violates the schema. Source code in vast_pipeline/pipeline/config.py def __init__ ( self , config_yaml : yaml . YAML ): \"\"\"Initialises PipelineConfig with parsed (but not necessarily validated) YAML. Args: config_yaml (yaml.YAML): Input YAML, usually the output of `strictyaml.load`. Raises: PipelineConfigError: The input YAML config violates the schema. \"\"\" self . _yaml : yaml . YAML = config_yaml # The epoch_based parameter below is for if the user has entered just lists we # don't have access to the dates until the Image instances are created. So we # flag this as true so that we can reorder the epochs once the date information # is available. It is also recorded in the database such that there is a record # of the fact that the run was processed in an epoch based mode. self . epoch_based : bool # Determine if epoch-based association should be used based on input files. # If inputs have been parsed to dicts, then the user has defined their own epochs. # If inputs have been parsed to lists, we must convert to dicts and auto-fill # the epochs. # ensure the inputs are valid in case .from_file(..., validate=False) was used try : self . _validate_inputs () except yaml . YAMLValidationError as e : raise PipelineConfigError ( e ) # detect simple list inputs and convert them to epoch-mode inputs for input_file_type in self . _REQUIRED_INPUT_TYPES : # skip missing optional input types, e.g. background if ( not self . _REQUIRED_INPUT_TYPES [ input_file_type ] and input_file_type not in self [ \"inputs\" ] ): continue input_files = self [ \"inputs\" ][ input_file_type ] # resolve glob expressions if present if isinstance ( input_files , dict ): # must be either a glob expression, list of glob expressions, or epoch-mode if \"glob\" in input_files : # resolve the glob expressions self . epoch_based = False file_list = self . _resolve_glob_expressions ( self . _yaml [ \"inputs\" ][ input_file_type ] ) self . _yaml [ \"inputs\" ][ input_file_type ] = self . _create_input_epochs ( file_list ) else : # epoch-mode with either a list of files or glob expressions self . epoch_based = True for epoch in input_files : if \"glob\" in input_files [ epoch ]: # resolve the glob expressions file_list = self . _resolve_glob_expressions ( self . _yaml [ \"inputs\" ][ input_file_type ][ epoch ] ) self . _yaml [ \"inputs\" ][ input_file_type ][ epoch ] = file_list else : # Epoch-based association not requested and no globs present. Replace # input lists with dicts where each input file has it's own epoch. self . epoch_based = False self . _yaml [ \"inputs\" ][ input_file_type ] = self . _create_input_epochs ( input_files )","title":"__init__()"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.PipelineConfig.check_prev_config_diff","text":"Checks if the previous config file differs from the current config file. Used in add mode. Only returns true if the images are different and the other general settings are the same (the requirement for add mode). Otherwise False is returned. Returns: Type Description bool True if images are different but general settings are the same, otherwise False is returned. Source code in vast_pipeline/pipeline/config.py def check_prev_config_diff ( self ) -> bool : \"\"\" Checks if the previous config file differs from the current config file. Used in add mode. Only returns true if the images are different and the other general settings are the same (the requirement for add mode). Otherwise False is returned. Returns: True if images are different but general settings are the same, otherwise False is returned. \"\"\" prev_config = PipelineConfig . from_file ( os . path . join ( self [ \"run\" ][ \"path\" ], \"config_prev.yaml\" ), label = \"previous run config\" , ) if self . _yaml == prev_config . _yaml : return True # are the input image files different? images_changed = self [ \"inputs\" ][ \"image\" ] != prev_config [ \"inputs\" ][ \"image\" ] # are all the non-input file configs the same? config_dict = self . _yaml . data prev_config_dict = prev_config . _yaml . data _ = config_dict . pop ( \"inputs\" ) _ = prev_config_dict . pop ( \"inputs\" ) settings_check = config_dict == prev_config_dict if images_changed and settings_check : return False return True","title":"check_prev_config_diff()"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.PipelineConfig.from_file","text":"Create a PipelineConfig object from a run configuration YAML file. Parameters: Name Type Description Default yaml_path str Path to the run config YAML file. required label str A label for the config object that will be used in error messages. Default is \"run config\". 'run config' validate bool Perform config schema validation immediately after loading the config file. If set to False, the full schema validation will not be performed until PipelineConfig.validate() is explicitly called. The inputs are always validated regardless. Defaults to True. True add_defaults bool Add missing configuration parameters using configured defaults. The defaults are read from the Django settings file. Defaults to True. True Exceptions: Type Description PipelineConfigError The run config YAML file fails schema validation. Source code in vast_pipeline/pipeline/config.py @classmethod def from_file ( cls , yaml_path : str , label : str = \"run config\" , validate : bool = True , add_defaults : bool = True , ) -> \"PipelineConfig\" : \"\"\"Create a PipelineConfig object from a run configuration YAML file. Args: yaml_path: Path to the run config YAML file. label: A label for the config object that will be used in error messages. Default is \"run config\". validate: Perform config schema validation immediately after loading the config file. If set to False, the full schema validation will not be performed until PipelineConfig.validate() is explicitly called. The inputs are always validated regardless. Defaults to True. add_defaults: Add missing configuration parameters using configured defaults. The defaults are read from the Django settings file. Defaults to True. Raises: PipelineConfigError: The run config YAML file fails schema validation. \"\"\" schema = cls . SCHEMA if validate else yaml . Any () with open ( yaml_path ) as fh : config_str = fh . read () try : config_yaml = yaml . load ( config_str , schema = schema , label = label ) except yaml . YAMLValidationError as e : raise PipelineConfigError ( e ) if add_defaults : # make a template config based on defaults config_defaults_str = make_config_template ( cls . TEMPLATE_PATH , ** settings . PIPE_RUN_CONFIG_DEFAULTS , ) config_defaults_dict : Dict [ str , Any ] = yaml . load ( config_defaults_str ) . data # merge configs config_dict = dict_merge ( config_defaults_dict , config_yaml . data ) config_yaml = yaml . as_document ( config_dict , schema = schema , label = label ) return cls ( config_yaml )","title":"from_file()"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.PipelineConfig.image_opts","text":"Get the config options required for image ingestion only. Namely selavy_local_rms_fill_value condon_errors ra_uncertainty dec_uncertainty Returns: Type Description Dict Dict: the relevant key value pairs Source code in vast_pipeline/pipeline/config.py def image_opts ( self ) -> Dict : \"\"\" Get the config options required for image ingestion only. Namely: - selavy_local_rms_fill_value - condon_errors - ra_uncertainty - dec_uncertainty Returns: Dict: the relevant key value pairs \"\"\" keys = [ \"selavy_local_rms_fill_value\" , \"condon_errors\" , \"ra_uncertainty\" , \"dec_uncertainty\" ] return { key : self [ \"measurements\" ][ key ] for key in keys }","title":"image_opts()"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.PipelineConfig.validate","text":"Perform extra validation steps not covered by the default schema validation. The following checks are performed in order. If a check fails, an exception is raised and no further checks are performed. All input files have the same number of epochs and the same number of files per epoch. The number of input files does not exceed the configured pipeline maximum. This is only enforced if a regular user (not staff/admin) created the run. There are at least two input images. Background input images are required if source monitoring is turned on. All input files exist. Parameters: Name Type Description Default user User Optional. The User of the request if made through the UI. Defaults to None. None Exceptions: Type Description PipelineConfigError a validation check failed. Source code in vast_pipeline/pipeline/config.py def validate ( self , user : User = None ): \"\"\"Perform extra validation steps not covered by the default schema validation. The following checks are performed in order. If a check fails, an exception is raised and no further checks are performed. 1. All input files have the same number of epochs and the same number of files per epoch. 2. The number of input files does not exceed the configured pipeline maximum. This is only enforced if a regular user (not staff/admin) created the run. 3. There are at least two input images. 4. Background input images are required if source monitoring is turned on. 5. All input files exist. Args: user: Optional. The User of the request if made through the UI. Defaults to None. Raises: PipelineConfigError: a validation check failed. \"\"\" # run standard base schema validation try : self . _yaml . revalidate ( self . SCHEMA ) except yaml . YAMLValidationError as e : raise PipelineConfigError ( e ) # epochs defined for images only, used as the reference list of epochs epochs_image = self [ \"inputs\" ][ \"image\" ] . keys () # map input type to a set of epochs epochs_by_input_type = { input_type : set ( self [ \"inputs\" ][ input_type ] . keys ()) for input_type in self [ \"inputs\" ] . keys () } # map input type to total number of files from all epochs n_files_by_input_type = {} for input_type , epochs_set in epochs_by_input_type . items (): n_files_by_input_type [ input_type ] = 0 for epoch in epochs_set : n_files_by_input_type [ input_type ] += len ( self [ \"inputs\" ][ input_type ][ epoch ]) n_files = 0 # total number of input files # map input type to a mapping of epoch to file count epoch_n_files : Dict [ str , Dict [ str , int ]] = {} for input_type in self [ \"inputs\" ] . keys (): epoch_n_files [ input_type ] = {} for epoch in self [ \"inputs\" ][ input_type ] . keys (): n = len ( self [ \"inputs\" ][ input_type ][ epoch ]) epoch_n_files [ input_type ][ epoch ] = n n_files += n # Note by this point the input files have been converted to a mapping regardless # of the user's input format. # Ensure all input file types have the same epochs. try : for input_type in self [ \"inputs\" ] . keys (): self . _yaml [ \"inputs\" ][ input_type ] . revalidate ( yaml . Map ({ epoch : yaml . Seq ( yaml . Str ()) for epoch in epochs_image }) ) except yaml . YAMLValidationError : # number of epochs could be different or the name of the epochs may not match # find out which by counting the number of unique epochs per input type n_epochs_per_input_type = [ len ( epochs_set ) for epochs_set in epochs_by_input_type . values () ] if len ( set ( n_epochs_per_input_type )) > 1 : if self . epoch_based : error_msg = \"The number of epochs must match for all input types. \\n \" else : error_msg = \"The number of files must match for all input types. \\n \" else : error_msg = \"The name of the epochs must match for all input types. \\n \" counts_str = \"\" if self . epoch_based : for input_type in epoch_n_files . keys (): n = len ( epoch_n_files [ input_type ]) counts_str += ( f \" { input_type } has { n } epoch { 's' if n > 1 else '' } :\" f \" { ', ' . join ( epoch_n_files [ input_type ] . keys ()) } \\n \" ) else : for input_type , n in n_files_by_input_type . items (): counts_str += f \" { input_type } has { n } file { 's' if n > 1 else '' } \\n \" counts_str = counts_str [: - 1 ] raise PipelineConfigError ( error_msg + counts_str ) # Ensure all input file type epochs have the same number of files per epoch. # This could be combined with the number of epochs validation above, but we want # to give specific feedback to the user on failure. try : for input_type in self [ \"inputs\" ] . keys (): self . _yaml [ \"inputs\" ][ input_type ] . revalidate ( yaml . Map ( { epoch : yaml . FixedSeq ( [ yaml . Str () for _ in range ( epoch_n_files [ \"image\" ][ epoch ]) ] ) for epoch in epochs_image } ) ) except yaml . YAMLValidationError : # map input type to a mapping of epoch to file count file_counts_str = \"\" for input_type in self [ \"inputs\" ] . keys (): file_counts_str += f \" { input_type } : \\n \" for epoch in sorted ( self [ \"inputs\" ][ input_type ] . keys ()): file_counts_str += ( f \" { epoch } : { len ( self [ 'inputs' ][ input_type ][ epoch ]) } \\n \" ) file_counts_str = file_counts_str [: - 1 ] raise PipelineConfigError ( \"The number of files per epoch does not match between input types. \\n \" + file_counts_str ) # ensure the number of input files is less than the user limit if user and n_files > settings . MAX_PIPERUN_IMAGES : if user . is_staff : logger . warning ( \"Maximum number of images\" f \" ( { settings . MAX_PIPERUN_IMAGES } ) rule bypassed with\" \" admin status.\" ) else : raise PipelineConfigError ( f \"The number of images entered ( { n_files } )\" \" exceeds the maximum number of images currently\" f \" allowed ( { settings . MAX_PIPERUN_IMAGES } ). Please ask\" \" an administrator for advice on processing your run.\" ) # ensure at least two inputs are provided check = [ n_files_by_input_type [ input_type ] < 2 for input_type in self [ \"inputs\" ] . keys ()] if any ( check ): raise PipelineConfigError ( \"Number of image files must to be larger than 1\" ) # ensure background files are provided if source monitoring is requested try : monitor = self [ \"source_monitoring\" ][ \"monitor\" ] except KeyError : monitor = False if monitor : inputs_schema = yaml . Map ( { k : yaml . UniqueSeq ( yaml . Str ()) | yaml . MapPattern ( yaml . Str (), yaml . UniqueSeq ( yaml . Str ())) for k in self . _REQUIRED_INPUT_TYPES } ) try : self . _yaml [ \"inputs\" ] . revalidate ( inputs_schema ) except yaml . YAMLValidationError : raise PipelineConfigError ( \"Background files must be provided if source monitoring is enabled.\" ) # ensure the input files all exist for input_type in self [ \"inputs\" ] . keys (): for epoch , file_list in self [ \"inputs\" ][ input_type ] . items (): for file in file_list : if not os . path . exists ( file ): raise PipelineConfigError ( f \" { file } does not exist.\" )","title":"validate()"},{"location":"reference/pipeline/config/#vast_pipeline.pipeline.config.make_config_template","text":"Generate the contents of a run configuration file from on a Jinja2 template. Parameters: Name Type Description Default template_path str Path to a Jinja2 template. required **kwargs keyword arguments passed to the template renderer to fill in template variables. {} Returns: Type Description str Filled in template string. Source code in vast_pipeline/pipeline/config.py def make_config_template ( template_path : str , ** kwargs ) -> str : \"\"\"Generate the contents of a run configuration file from on a Jinja2 template. Args: template_path: Path to a Jinja2 template. **kwargs: keyword arguments passed to the template renderer to fill in template variables. Returns: Filled in template string. \"\"\" with open ( template_path , \"r\" ) as fp : template_str = fp . read () env = Environment ( trim_blocks = True , lstrip_blocks = True ) template = env . from_string ( template_str ) return template . render ( ** kwargs )","title":"make_config_template()"},{"location":"reference/pipeline/errors/","text":"Defines errors for the pipeline to return. MaxPipelineRunsError \u00b6 Error for reporting the number of concurrent jobs is maxed out. PipelineConfigError \u00b6 Error for issue in the pipeline configuration __init__ ( self , msg = None ) special \u00b6 Initialises the config error. Parameters: Name Type Description Default msg The error message returned by the pipeline. None Source code in vast_pipeline/pipeline/errors.py def __init__ ( self , msg = None ): \"\"\" Initialises the config error. Args: msg: The error message returned by the pipeline. \"\"\" super ( PipelineConfigError , self ) . __init__ ( msg ) PipelineError \u00b6 Generic pipeline error Attributes: Name Type Description msg str The full error string to return. __init__ ( self , msg = None ) special \u00b6 Initialises the error. Parameters: Name Type Description Default msg str The error message returned by the pipeline. None Source code in vast_pipeline/pipeline/errors.py def __init__ ( self , msg : str = None ) -> None : \"\"\" Initialises the error. Args: msg: The error message returned by the pipeline. \"\"\" self . msg = ( 'Pipeline error: {0} .' . format ( msg ) if msg else 'Undefined Pipeline error.' ) __str__ ( self ) special \u00b6 Returns the string representation. Returns: Type Description str The string representation of the error. Source code in vast_pipeline/pipeline/errors.py def __str__ ( self ) -> str : \"\"\" Returns the string representation. Returns: The string representation of the error. \"\"\" return self . msg PipelineInitError \u00b6 Error for issue in the pipeline initialisation __init__ ( self , msg = None ) special \u00b6 Initialises the init error. Parameters: Name Type Description Default msg The error message returned by the pipeline. None Source code in vast_pipeline/pipeline/errors.py def __init__ ( self , msg = None ): \"\"\" Initialises the init error. Args: msg: The error message returned by the pipeline. \"\"\" super ( PipelineInitError , self ) . __init__ ( msg )","title":"errors.py"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.MaxPipelineRunsError","text":"Error for reporting the number of concurrent jobs is maxed out.","title":"MaxPipelineRunsError"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineConfigError","text":"Error for issue in the pipeline configuration","title":"PipelineConfigError"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineConfigError.__init__","text":"Initialises the config error. Parameters: Name Type Description Default msg The error message returned by the pipeline. None Source code in vast_pipeline/pipeline/errors.py def __init__ ( self , msg = None ): \"\"\" Initialises the config error. Args: msg: The error message returned by the pipeline. \"\"\" super ( PipelineConfigError , self ) . __init__ ( msg )","title":"__init__()"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineError","text":"Generic pipeline error Attributes: Name Type Description msg str The full error string to return.","title":"PipelineError"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineError.__init__","text":"Initialises the error. Parameters: Name Type Description Default msg str The error message returned by the pipeline. None Source code in vast_pipeline/pipeline/errors.py def __init__ ( self , msg : str = None ) -> None : \"\"\" Initialises the error. Args: msg: The error message returned by the pipeline. \"\"\" self . msg = ( 'Pipeline error: {0} .' . format ( msg ) if msg else 'Undefined Pipeline error.' )","title":"__init__()"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineError.__str__","text":"Returns the string representation. Returns: Type Description str The string representation of the error. Source code in vast_pipeline/pipeline/errors.py def __str__ ( self ) -> str : \"\"\" Returns the string representation. Returns: The string representation of the error. \"\"\" return self . msg","title":"__str__()"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineInitError","text":"Error for issue in the pipeline initialisation","title":"PipelineInitError"},{"location":"reference/pipeline/errors/#vast_pipeline.pipeline.errors.PipelineInitError.__init__","text":"Initialises the init error. Parameters: Name Type Description Default msg The error message returned by the pipeline. None Source code in vast_pipeline/pipeline/errors.py def __init__ ( self , msg = None ): \"\"\" Initialises the init error. Args: msg: The error message returned by the pipeline. \"\"\" super ( PipelineInitError , self ) . __init__ ( msg )","title":"__init__()"},{"location":"reference/pipeline/finalise/","text":"calculate_measurement_pair_aggregate_metrics ( measurement_pairs_df , min_vs , flux_type = 'peak' ) \u00b6 Calculate the aggregate maximum measurement pair variability metrics to be stored in Source objects. Only measurement pairs with abs(Vs metric) >= min_vs are considered. The measurement pairs are filtered on abs(Vs metric) >= min_vs , grouped by the source ID column source , then the row index of the maximum abs(m) metric is found. The absolute Vs and m metric values from this row are returned for each source. Parameters: Name Type Description Default measurement_pairs_df DataFrame The measurement pairs and their variability metrics. Must at least contain the columns: source, vs_{flux_type}, m_{flux_type}. required min_vs float The minimum value of the Vs metric (i.e. column vs_{flux_type} ) the measurement pair must have to be included in the aggregate metric determination. required flux_type str The flux type on which to perform the aggregation, either \"peak\" or \"int\". Default is \"peak\". 'peak' Returns: Type Description DataFrame Measurement pair aggregate metrics indexed by the source ID, source . The metric columns are named: vs_abs_significant_max_{flux_type} and m_abs_significant_max_{flux_type} . Source code in vast_pipeline/pipeline/finalise.py def calculate_measurement_pair_aggregate_metrics ( measurement_pairs_df : pd . DataFrame , min_vs : float , flux_type : str = \"peak\" , ) -> pd . DataFrame : \"\"\" Calculate the aggregate maximum measurement pair variability metrics to be stored in `Source` objects. Only measurement pairs with abs(Vs metric) >= `min_vs` are considered. The measurement pairs are filtered on abs(Vs metric) >= `min_vs`, grouped by the source ID column `source`, then the row index of the maximum abs(m) metric is found. The absolute Vs and m metric values from this row are returned for each source. Args: measurement_pairs_df: The measurement pairs and their variability metrics. Must at least contain the columns: source, vs_{flux_type}, m_{flux_type}. min_vs: The minimum value of the Vs metric (i.e. column `vs_{flux_type}`) the measurement pair must have to be included in the aggregate metric determination. flux_type: The flux type on which to perform the aggregation, either \"peak\" or \"int\". Default is \"peak\". Returns: Measurement pair aggregate metrics indexed by the source ID, `source`. The metric columns are named: `vs_abs_significant_max_{flux_type}` and `m_abs_significant_max_{flux_type}`. \"\"\" check_df = measurement_pairs_df . query ( f \"abs(vs_ { flux_type } ) >= @min_vs\" ) # This check is performed due to a bug that was occuring after updating the # pandas dependancy (1.4) when performing the tests. The bug was that the # grouby and agg stage below was being performed on an empty series in the # basic association test and causing a failure. Hence this only performs # the groupby if the original query dataframe is not empty. if check_df . empty : pair_agg_metrics = pd . DataFrame ( columns = [ f \"vs_ { flux_type } \" , f \"m_ { flux_type } \" , \"source\" ] ) else : pair_agg_metrics = measurement_pairs_df . iloc [ check_df . groupby ( \"source\" ) . agg ( m_abs_max_idx = ( f \"m_ { flux_type } \" , lambda x : x . abs () . idxmax ()),) . astype ( np . int32 )[ \"m_abs_max_idx\" ] # cast row indices to int and select them . reset_index ( drop = True ) # keep only the row indices ][[ f \"vs_ { flux_type } \" , f \"m_ { flux_type } \" , \"source\" ]] pair_agg_metrics = pair_agg_metrics . abs () . rename ( columns = { f \"vs_ { flux_type } \" : f \"vs_abs_significant_max_ { flux_type } \" , f \"m_ { flux_type } \" : f \"m_abs_significant_max_ { flux_type } \" , }) . set_index ( 'source' ) return pair_agg_metrics final_operations ( sources_df , p_run , new_sources_df , source_aggregate_pair_metrics_min_abs_vs , add_mode , done_source_ids , previous_parquets ) \u00b6 Performs the final operations of the pipeline: - Calculates the statistics for the final sources. - Uploads sources and writes parquet. - Uploads related sources and writes parquet. - Uploads associations and writes parquet. Parameters: Name Type Description Default sources_df DataFrame The main sources_df dataframe produced from the pipeline. Contains all measurements and the association information. The id column is the Measurement object primary key that has already been saved to the database. required p_run Run The pipeline Run object of which the sources are associated with. required new_sources_df DataFrame The new sources dataframe, only contains the 'new_source_high_sigma' column (source_id is the index). required source_aggregate_pair_metrics_min_abs_vs float Only measurement pairs where the Vs metric exceeds this value are selected for the aggregate pair metrics that are stored in Source objects. required add_mode bool Whether the pipeline is running in add mode. required done_source_ids List[int] A list containing the source ids that have already been uploaded in the previous run in add mode. required Returns: Type Description int The number of sources contained in the pipeline (used in the next steps of main.py). Source code in vast_pipeline/pipeline/finalise.py def final_operations ( sources_df : pd . DataFrame , p_run : Run , new_sources_df : pd . DataFrame , source_aggregate_pair_metrics_min_abs_vs : float , add_mode : bool , done_source_ids : List [ int ], previous_parquets : Dict [ str , str ] ) -> int : \"\"\" Performs the final operations of the pipeline: - Calculates the statistics for the final sources. - Uploads sources and writes parquet. - Uploads related sources and writes parquet. - Uploads associations and writes parquet. Args: sources_df: The main sources_df dataframe produced from the pipeline. Contains all measurements and the association information. The `id` column is the Measurement object primary key that has already been saved to the database. p_run: The pipeline Run object of which the sources are associated with. new_sources_df: The new sources dataframe, only contains the 'new_source_high_sigma' column (source_id is the index). source_aggregate_pair_metrics_min_abs_vs: Only measurement pairs where the Vs metric exceeds this value are selected for the aggregate pair metrics that are stored in `Source` objects. add_mode: Whether the pipeline is running in add mode. done_source_ids: A list containing the source ids that have already been uploaded in the previous run in add mode. Returns: The number of sources contained in the pipeline (used in the next steps of main.py). \"\"\" timer = StopWatch () # calculate source fields logger . info ( 'Calculating statistics for %i sources...' , sources_df . source . unique () . shape [ 0 ] ) srcs_df = parallel_groupby ( sources_df ) logger . info ( 'Groupby-apply time: %.2f seconds' , timer . reset ()) # add new sources srcs_df [ \"new\" ] = srcs_df . index . isin ( new_sources_df . index ) srcs_df = pd . merge ( srcs_df , new_sources_df [ \"new_high_sigma\" ], left_on = \"source\" , right_index = True , how = \"left\" , ) srcs_df [ \"new_high_sigma\" ] = srcs_df [ \"new_high_sigma\" ] . fillna ( 0.0 ) # calculate nearest neighbour srcs_skycoord = SkyCoord ( srcs_df [ 'wavg_ra' ] . values , srcs_df [ 'wavg_dec' ] . values , unit = ( u . deg , u . deg ) ) idx , d2d , _ = srcs_skycoord . match_to_catalog_sky ( srcs_skycoord , nthneighbor = 2 ) # add the separation distance in degrees srcs_df [ 'n_neighbour_dist' ] = d2d . deg # create measurement pairs, aka 2-epoch metrics timer . reset () measurement_pairs_df = calculate_measurement_pair_metrics ( sources_df ) logger . info ( 'Measurement pair metrics time: %.2f seconds' , timer . reset ()) # calculate measurement pair metric aggregates for sources by finding the row indices # of the aggregate max of the abs(m) metric for each flux type. pair_agg_metrics = pd . merge ( calculate_measurement_pair_aggregate_metrics ( measurement_pairs_df , source_aggregate_pair_metrics_min_abs_vs , flux_type = \"peak\" , ), calculate_measurement_pair_aggregate_metrics ( measurement_pairs_df , source_aggregate_pair_metrics_min_abs_vs , flux_type = \"int\" , ), how = \"outer\" , left_index = True , right_index = True , ) # join with sources and replace agg metrics NaNs with 0 as the DataTables API JSON # serialization doesn't like them srcs_df = srcs_df . join ( pair_agg_metrics ) . fillna ( value = { \"vs_abs_significant_max_peak\" : 0.0 , \"m_abs_significant_max_peak\" : 0.0 , \"vs_abs_significant_max_int\" : 0.0 , \"m_abs_significant_max_int\" : 0.0 , }) logger . info ( \"Measurement pair aggregate metrics time: %.2f seconds\" , timer . reset ()) # upload sources to DB, column 'id' with DB id is contained in return if add_mode : # if add mode is being used some sources need to updated where as some # need to be newly uploaded. # upload new ones first (new id's are fetched) src_done_mask = srcs_df . index . isin ( done_source_ids ) srcs_df_upload = srcs_df . loc [ ~ src_done_mask ] . copy () srcs_df_upload = make_upload_sources ( srcs_df_upload , p_run , add_mode ) # And now update srcs_df_update = srcs_df . loc [ src_done_mask ] . copy () logger . info ( f \"Updating { srcs_df_update . shape [ 0 ] } sources with new metrics.\" ) srcs_df = update_sources ( srcs_df_update , batch_size = 1000 ) # Add back together if not srcs_df_upload . empty : srcs_df = srcs_df . append ( srcs_df_upload ) else : srcs_df = make_upload_sources ( srcs_df , p_run , add_mode ) # gather the related df, upload to db and save to parquet file # the df will look like # # from_source_id to_source_id # source # 714 60 14396 # 1211 94 12961 # # the index ('source') has the initial id generated by the pipeline to # identify unique sources, the 'from_source_id' column has the django # model id (in db), the 'to_source_id' has the pipeline index related_df = ( srcs_df . loc [ srcs_df [ \"related_list\" ] != - 1 , [ \"id\" , \"related_list\" ]] . explode ( \"related_list\" ) . rename ( columns = { \"id\" : \"from_source_id\" , \"related_list\" : \"to_source_id\" }) ) # for the column 'from_source_id', replace relation source ids with db id related_df [ \"to_source_id\" ] = related_df [ \"to_source_id\" ] . map ( srcs_df [ \"id\" ] . to_dict ()) # drop relationships with the same source related_df = related_df [ related_df [ \"from_source_id\" ] != related_df [ \"to_source_id\" ]] # write symmetrical relations to parquet related_df . to_parquet ( os . path . join ( p_run . path , 'relations.parquet' ), index = False ) # upload the relations to DB # check for add_mode first if add_mode : # Load old relations so the already uploaded ones can be removed old_relations = ( pd . read_parquet ( previous_parquets [ 'relations' ]) ) related_df = ( related_df . append ( old_relations , ignore_index = True ) . drop_duplicates ( keep = False ) ) logger . debug ( f 'Add mode: # { related_df . shape [ 0 ] } relations to upload.' ) make_upload_related_sources ( related_df ) del related_df # write sources to parquet file srcs_df = srcs_df . drop ([ \"related_list\" , \"img_list\" ], axis = 1 ) ( srcs_df . set_index ( 'id' ) # set the index to db ids, dropping the source idx . to_parquet ( os . path . join ( p_run . path , 'sources.parquet' )) ) # update measurments with sources to get associations sources_df = ( sources_df . drop ( 'related' , axis = 1 ) . merge ( srcs_df . rename ( columns = { 'id' : 'source_id' }), on = 'source' ) ) if add_mode : # Load old associations so the already uploaded ones can be removed old_assoications = ( pd . read_parquet ( previous_parquets [ 'associations' ]) . rename ( columns = { 'meas_id' : 'id' }) ) sources_df_upload = sources_df . append ( old_assoications , ignore_index = True ) sources_df_upload = sources_df_upload . drop_duplicates ( [ 'source_id' , 'id' , 'd2d' , 'dr' ], keep = False ) logger . debug ( f 'Add mode: # { sources_df_upload . shape [ 0 ] } associations to upload.' ) else : sources_df_upload = sources_df # upload associations into DB make_upload_associations ( sources_df_upload ) # write associations to parquet file sources_df . rename ( columns = { 'id' : 'meas_id' })[ [ 'source_id' , 'meas_id' , 'd2d' , 'dr' ] ] . to_parquet ( os . path . join ( p_run . path , 'associations.parquet' )) # get the Source object primary keys for the measurement pairs measurement_pairs_df = measurement_pairs_df . join ( srcs_df . id . rename ( \"source_id\" ), on = \"source\" ) # optimize measurement pair DataFrame and save to parquet file measurement_pairs_df = optimize_ints ( optimize_floats ( measurement_pairs_df . drop ( columns = [ \"source\" ]) . rename ( columns = { \"id_a\" : \"meas_id_a\" , \"id_b\" : \"meas_id_b\" } ) ) ) measurement_pairs_df . to_parquet ( os . path . join ( p_run . path , \"measurement_pairs.parquet\" ), index = False ) logger . info ( \"Total final operations time: %.2f seconds\" , timer . reset_init ()) # calculate and return total number of extracted sources return srcs_df [ \"id\" ] . count ()","title":"finalise.py"},{"location":"reference/pipeline/finalise/#vast_pipeline.pipeline.finalise.calculate_measurement_pair_aggregate_metrics","text":"Calculate the aggregate maximum measurement pair variability metrics to be stored in Source objects. Only measurement pairs with abs(Vs metric) >= min_vs are considered. The measurement pairs are filtered on abs(Vs metric) >= min_vs , grouped by the source ID column source , then the row index of the maximum abs(m) metric is found. The absolute Vs and m metric values from this row are returned for each source. Parameters: Name Type Description Default measurement_pairs_df DataFrame The measurement pairs and their variability metrics. Must at least contain the columns: source, vs_{flux_type}, m_{flux_type}. required min_vs float The minimum value of the Vs metric (i.e. column vs_{flux_type} ) the measurement pair must have to be included in the aggregate metric determination. required flux_type str The flux type on which to perform the aggregation, either \"peak\" or \"int\". Default is \"peak\". 'peak' Returns: Type Description DataFrame Measurement pair aggregate metrics indexed by the source ID, source . The metric columns are named: vs_abs_significant_max_{flux_type} and m_abs_significant_max_{flux_type} . Source code in vast_pipeline/pipeline/finalise.py def calculate_measurement_pair_aggregate_metrics ( measurement_pairs_df : pd . DataFrame , min_vs : float , flux_type : str = \"peak\" , ) -> pd . DataFrame : \"\"\" Calculate the aggregate maximum measurement pair variability metrics to be stored in `Source` objects. Only measurement pairs with abs(Vs metric) >= `min_vs` are considered. The measurement pairs are filtered on abs(Vs metric) >= `min_vs`, grouped by the source ID column `source`, then the row index of the maximum abs(m) metric is found. The absolute Vs and m metric values from this row are returned for each source. Args: measurement_pairs_df: The measurement pairs and their variability metrics. Must at least contain the columns: source, vs_{flux_type}, m_{flux_type}. min_vs: The minimum value of the Vs metric (i.e. column `vs_{flux_type}`) the measurement pair must have to be included in the aggregate metric determination. flux_type: The flux type on which to perform the aggregation, either \"peak\" or \"int\". Default is \"peak\". Returns: Measurement pair aggregate metrics indexed by the source ID, `source`. The metric columns are named: `vs_abs_significant_max_{flux_type}` and `m_abs_significant_max_{flux_type}`. \"\"\" check_df = measurement_pairs_df . query ( f \"abs(vs_ { flux_type } ) >= @min_vs\" ) # This check is performed due to a bug that was occuring after updating the # pandas dependancy (1.4) when performing the tests. The bug was that the # grouby and agg stage below was being performed on an empty series in the # basic association test and causing a failure. Hence this only performs # the groupby if the original query dataframe is not empty. if check_df . empty : pair_agg_metrics = pd . DataFrame ( columns = [ f \"vs_ { flux_type } \" , f \"m_ { flux_type } \" , \"source\" ] ) else : pair_agg_metrics = measurement_pairs_df . iloc [ check_df . groupby ( \"source\" ) . agg ( m_abs_max_idx = ( f \"m_ { flux_type } \" , lambda x : x . abs () . idxmax ()),) . astype ( np . int32 )[ \"m_abs_max_idx\" ] # cast row indices to int and select them . reset_index ( drop = True ) # keep only the row indices ][[ f \"vs_ { flux_type } \" , f \"m_ { flux_type } \" , \"source\" ]] pair_agg_metrics = pair_agg_metrics . abs () . rename ( columns = { f \"vs_ { flux_type } \" : f \"vs_abs_significant_max_ { flux_type } \" , f \"m_ { flux_type } \" : f \"m_abs_significant_max_ { flux_type } \" , }) . set_index ( 'source' ) return pair_agg_metrics","title":"calculate_measurement_pair_aggregate_metrics()"},{"location":"reference/pipeline/finalise/#vast_pipeline.pipeline.finalise.final_operations","text":"Performs the final operations of the pipeline: - Calculates the statistics for the final sources. - Uploads sources and writes parquet. - Uploads related sources and writes parquet. - Uploads associations and writes parquet. Parameters: Name Type Description Default sources_df DataFrame The main sources_df dataframe produced from the pipeline. Contains all measurements and the association information. The id column is the Measurement object primary key that has already been saved to the database. required p_run Run The pipeline Run object of which the sources are associated with. required new_sources_df DataFrame The new sources dataframe, only contains the 'new_source_high_sigma' column (source_id is the index). required source_aggregate_pair_metrics_min_abs_vs float Only measurement pairs where the Vs metric exceeds this value are selected for the aggregate pair metrics that are stored in Source objects. required add_mode bool Whether the pipeline is running in add mode. required done_source_ids List[int] A list containing the source ids that have already been uploaded in the previous run in add mode. required Returns: Type Description int The number of sources contained in the pipeline (used in the next steps of main.py). Source code in vast_pipeline/pipeline/finalise.py def final_operations ( sources_df : pd . DataFrame , p_run : Run , new_sources_df : pd . DataFrame , source_aggregate_pair_metrics_min_abs_vs : float , add_mode : bool , done_source_ids : List [ int ], previous_parquets : Dict [ str , str ] ) -> int : \"\"\" Performs the final operations of the pipeline: - Calculates the statistics for the final sources. - Uploads sources and writes parquet. - Uploads related sources and writes parquet. - Uploads associations and writes parquet. Args: sources_df: The main sources_df dataframe produced from the pipeline. Contains all measurements and the association information. The `id` column is the Measurement object primary key that has already been saved to the database. p_run: The pipeline Run object of which the sources are associated with. new_sources_df: The new sources dataframe, only contains the 'new_source_high_sigma' column (source_id is the index). source_aggregate_pair_metrics_min_abs_vs: Only measurement pairs where the Vs metric exceeds this value are selected for the aggregate pair metrics that are stored in `Source` objects. add_mode: Whether the pipeline is running in add mode. done_source_ids: A list containing the source ids that have already been uploaded in the previous run in add mode. Returns: The number of sources contained in the pipeline (used in the next steps of main.py). \"\"\" timer = StopWatch () # calculate source fields logger . info ( 'Calculating statistics for %i sources...' , sources_df . source . unique () . shape [ 0 ] ) srcs_df = parallel_groupby ( sources_df ) logger . info ( 'Groupby-apply time: %.2f seconds' , timer . reset ()) # add new sources srcs_df [ \"new\" ] = srcs_df . index . isin ( new_sources_df . index ) srcs_df = pd . merge ( srcs_df , new_sources_df [ \"new_high_sigma\" ], left_on = \"source\" , right_index = True , how = \"left\" , ) srcs_df [ \"new_high_sigma\" ] = srcs_df [ \"new_high_sigma\" ] . fillna ( 0.0 ) # calculate nearest neighbour srcs_skycoord = SkyCoord ( srcs_df [ 'wavg_ra' ] . values , srcs_df [ 'wavg_dec' ] . values , unit = ( u . deg , u . deg ) ) idx , d2d , _ = srcs_skycoord . match_to_catalog_sky ( srcs_skycoord , nthneighbor = 2 ) # add the separation distance in degrees srcs_df [ 'n_neighbour_dist' ] = d2d . deg # create measurement pairs, aka 2-epoch metrics timer . reset () measurement_pairs_df = calculate_measurement_pair_metrics ( sources_df ) logger . info ( 'Measurement pair metrics time: %.2f seconds' , timer . reset ()) # calculate measurement pair metric aggregates for sources by finding the row indices # of the aggregate max of the abs(m) metric for each flux type. pair_agg_metrics = pd . merge ( calculate_measurement_pair_aggregate_metrics ( measurement_pairs_df , source_aggregate_pair_metrics_min_abs_vs , flux_type = \"peak\" , ), calculate_measurement_pair_aggregate_metrics ( measurement_pairs_df , source_aggregate_pair_metrics_min_abs_vs , flux_type = \"int\" , ), how = \"outer\" , left_index = True , right_index = True , ) # join with sources and replace agg metrics NaNs with 0 as the DataTables API JSON # serialization doesn't like them srcs_df = srcs_df . join ( pair_agg_metrics ) . fillna ( value = { \"vs_abs_significant_max_peak\" : 0.0 , \"m_abs_significant_max_peak\" : 0.0 , \"vs_abs_significant_max_int\" : 0.0 , \"m_abs_significant_max_int\" : 0.0 , }) logger . info ( \"Measurement pair aggregate metrics time: %.2f seconds\" , timer . reset ()) # upload sources to DB, column 'id' with DB id is contained in return if add_mode : # if add mode is being used some sources need to updated where as some # need to be newly uploaded. # upload new ones first (new id's are fetched) src_done_mask = srcs_df . index . isin ( done_source_ids ) srcs_df_upload = srcs_df . loc [ ~ src_done_mask ] . copy () srcs_df_upload = make_upload_sources ( srcs_df_upload , p_run , add_mode ) # And now update srcs_df_update = srcs_df . loc [ src_done_mask ] . copy () logger . info ( f \"Updating { srcs_df_update . shape [ 0 ] } sources with new metrics.\" ) srcs_df = update_sources ( srcs_df_update , batch_size = 1000 ) # Add back together if not srcs_df_upload . empty : srcs_df = srcs_df . append ( srcs_df_upload ) else : srcs_df = make_upload_sources ( srcs_df , p_run , add_mode ) # gather the related df, upload to db and save to parquet file # the df will look like # # from_source_id to_source_id # source # 714 60 14396 # 1211 94 12961 # # the index ('source') has the initial id generated by the pipeline to # identify unique sources, the 'from_source_id' column has the django # model id (in db), the 'to_source_id' has the pipeline index related_df = ( srcs_df . loc [ srcs_df [ \"related_list\" ] != - 1 , [ \"id\" , \"related_list\" ]] . explode ( \"related_list\" ) . rename ( columns = { \"id\" : \"from_source_id\" , \"related_list\" : \"to_source_id\" }) ) # for the column 'from_source_id', replace relation source ids with db id related_df [ \"to_source_id\" ] = related_df [ \"to_source_id\" ] . map ( srcs_df [ \"id\" ] . to_dict ()) # drop relationships with the same source related_df = related_df [ related_df [ \"from_source_id\" ] != related_df [ \"to_source_id\" ]] # write symmetrical relations to parquet related_df . to_parquet ( os . path . join ( p_run . path , 'relations.parquet' ), index = False ) # upload the relations to DB # check for add_mode first if add_mode : # Load old relations so the already uploaded ones can be removed old_relations = ( pd . read_parquet ( previous_parquets [ 'relations' ]) ) related_df = ( related_df . append ( old_relations , ignore_index = True ) . drop_duplicates ( keep = False ) ) logger . debug ( f 'Add mode: # { related_df . shape [ 0 ] } relations to upload.' ) make_upload_related_sources ( related_df ) del related_df # write sources to parquet file srcs_df = srcs_df . drop ([ \"related_list\" , \"img_list\" ], axis = 1 ) ( srcs_df . set_index ( 'id' ) # set the index to db ids, dropping the source idx . to_parquet ( os . path . join ( p_run . path , 'sources.parquet' )) ) # update measurments with sources to get associations sources_df = ( sources_df . drop ( 'related' , axis = 1 ) . merge ( srcs_df . rename ( columns = { 'id' : 'source_id' }), on = 'source' ) ) if add_mode : # Load old associations so the already uploaded ones can be removed old_assoications = ( pd . read_parquet ( previous_parquets [ 'associations' ]) . rename ( columns = { 'meas_id' : 'id' }) ) sources_df_upload = sources_df . append ( old_assoications , ignore_index = True ) sources_df_upload = sources_df_upload . drop_duplicates ( [ 'source_id' , 'id' , 'd2d' , 'dr' ], keep = False ) logger . debug ( f 'Add mode: # { sources_df_upload . shape [ 0 ] } associations to upload.' ) else : sources_df_upload = sources_df # upload associations into DB make_upload_associations ( sources_df_upload ) # write associations to parquet file sources_df . rename ( columns = { 'id' : 'meas_id' })[ [ 'source_id' , 'meas_id' , 'd2d' , 'dr' ] ] . to_parquet ( os . path . join ( p_run . path , 'associations.parquet' )) # get the Source object primary keys for the measurement pairs measurement_pairs_df = measurement_pairs_df . join ( srcs_df . id . rename ( \"source_id\" ), on = \"source\" ) # optimize measurement pair DataFrame and save to parquet file measurement_pairs_df = optimize_ints ( optimize_floats ( measurement_pairs_df . drop ( columns = [ \"source\" ]) . rename ( columns = { \"id_a\" : \"meas_id_a\" , \"id_b\" : \"meas_id_b\" } ) ) ) measurement_pairs_df . to_parquet ( os . path . join ( p_run . path , \"measurement_pairs.parquet\" ), index = False ) logger . info ( \"Total final operations time: %.2f seconds\" , timer . reset_init ()) # calculate and return total number of extracted sources return srcs_df [ \"id\" ] . count ()","title":"final_operations()"},{"location":"reference/pipeline/forced_extraction/","text":"extract_from_image ( df , image , background , noise , edge_buffer , cluster_threshold , allow_nan ) \u00b6 Extract the flux, its erros and chi squared data from the image files (image FIT, background and noise files) and return a dictionary with the dataframe and image name Parameters: Name Type Description Default df DataFrame input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak] required image str a string with the path of the image FIT file required background str a string with the path of the image background file required noise str a string with the path of the image noise file required edge_buffer float flag to pass to ForcedPhot.measure method required cluster_threshold float flag to pass to ForcedPhot.measure method required allow_nan bool flag to pass to ForcedPhot.measure method required Returns: Type Description Dict Dictionary with input dataframe with added columns (flux_int, flux_int_err, chi_squared_fit) and image name. Source code in vast_pipeline/pipeline/forced_extraction.py def extract_from_image ( df : pd . DataFrame , image : str , background : str , noise : str , edge_buffer : float , cluster_threshold : float , allow_nan : bool ) -> Dict : \"\"\" Extract the flux, its erros and chi squared data from the image files (image FIT, background and noise files) and return a dictionary with the dataframe and image name Args: df: input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak] image: a string with the path of the image FIT file background: a string with the path of the image background file noise: a string with the path of the image noise file edge_buffer: flag to pass to ForcedPhot.measure method cluster_threshold: flag to pass to ForcedPhot.measure method allow_nan: flag to pass to ForcedPhot.measure method Returns: Dictionary with input dataframe with added columns (flux_int, flux_int_err, chi_squared_fit) and image name. \"\"\" # create the skycoord obj to pass to the forced extraction # see usage https://github.com/dlakaplan/forced_phot P_islands = SkyCoord ( df [ 'wavg_ra' ] . values , df [ 'wavg_dec' ] . values , unit = ( u . deg , u . deg ) ) FP = ForcedPhot ( image , background , noise ) flux , flux_err , chisq , DOF , cluster_id = FP . measure ( P_islands , cluster_threshold = cluster_threshold , allow_nan = allow_nan , edge_buffer = edge_buffer ) df [ 'flux_int' ] = flux * 1.e3 df [ 'flux_int_err' ] = flux_err * 1.e3 df [ 'chi_squared_fit' ] = chisq return { 'df' : df , 'image' : df [ 'image_name' ] . iloc [ 0 ]} finalise_forced_dfs ( df , prefix , max_id , beam_bmaj , beam_bmin , beam_bpa , id , datetime , image ) \u00b6 Compute populate leftover columns for the dataframe with forced photometry data given the input parameters Parameters: Name Type Description Default df DataFrame input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak, flux_int, flux_int_err, chi_squared_fit] required prefix str string to use to generate the 'island_id' column required max_id int integer to use to generate the 'island_id' column required beam_bmaj float image beam major axis required beam_bmin float image beam minor axis required beam_bpa float image beam position angle required id int image id in database required datetime datetime timestamp of the image file (from header) required image str string with the image name required Returns: Type Description DataFrame Input dataframe with added columns island_id, component_id, name, bmaj, bmin, pa, image_id, time. Source code in vast_pipeline/pipeline/forced_extraction.py def finalise_forced_dfs ( df : pd . DataFrame , prefix : str , max_id : int , beam_bmaj : float , beam_bmin : float , beam_bpa : float , id : int , datetime : datetime . datetime , image : str ) -> pd . DataFrame : \"\"\" Compute populate leftover columns for the dataframe with forced photometry data given the input parameters Args: df: input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak, flux_int, flux_int_err, chi_squared_fit] prefix: string to use to generate the 'island_id' column max_id: integer to use to generate the 'island_id' column beam_bmaj: image beam major axis beam_bmin: image beam minor axis beam_bpa: image beam position angle id: image id in database datetime: timestamp of the image file (from header) image: string with the image name Returns: Input dataframe with added columns island_id, component_id, name, bmaj, bmin, pa, image_id, time. \"\"\" # make up the measurements name from the image island_id and component_id df [ 'island_id' ] = np . char . add ( prefix , np . arange ( max_id , max_id + df . shape [ 0 ]) . astype ( str ) ) df [ 'component_id' ] = df [ 'island_id' ] . str . replace ( 'island' , 'component' ) + 'a' img_prefix = image . split ( '.' )[ 0 ] + '_' df [ 'name' ] = img_prefix + df [ 'component_id' ] # assign all the other columns # convert fluxes to mJy # store source bmaj and bmin in arcsec df [ 'bmaj' ] = beam_bmaj * 3600. df [ 'bmin' ] = beam_bmin * 3600. df [ 'pa' ] = beam_bpa # add image id and time df [ 'image_id' ] = id df [ 'time' ] = datetime return df forced_extraction ( sources_df , cfg_err_ra , cfg_err_dec , p_run , extr_df , min_sigma , edge_buffer , cluster_threshold , allow_nan , add_mode , done_images_df , done_source_ids ) \u00b6 Check and extract expected measurements, and associated them with the related source(s). Parameters: Name Type Description Default sources_df DataFrame Dataframe containing all the extracted measurements and associations (product from association step). required cfg_err_ra float The minimum RA error from the config file (in degrees). required cfg_err_dec float The minimum declination error from the config file (in degrees). required p_run Run The pipeline run object. required extr_df DataFrame The dataframe containing the information on what sources are missing from which images (output from get_src_skyregion_merged_df in main.py). required min_sigma float Minimum sigma value to drop forced extracted measurements. required edge_buffer float Flag to pass to ForcedPhot.measure method. required cluster_threshold float Flag to pass to ForcedPhot.measure method. required allow_nan bool Flag to pass to ForcedPhot.measure method. required add_mode bool True when the pipeline is running in add image mode. required done_images_df DataFrame Dataframe containing the images that thave already been processed in a previous run (used in add image mode). required done_source_ids List[int] List of the source ids that were already present in the previous run (used in add image mode). required Returns: Type Description Tuple[pandas.core.frame.DataFrame, int] The sources_df with the extracted sources added and n_forced is the total number of forced measurements present in the run. Source code in vast_pipeline/pipeline/forced_extraction.py def forced_extraction ( sources_df : pd . DataFrame , cfg_err_ra : float , cfg_err_dec : float , p_run : Run , extr_df : pd . DataFrame , min_sigma : float , edge_buffer : float , cluster_threshold : float , allow_nan : bool , add_mode : bool , done_images_df : pd . DataFrame , done_source_ids : List [ int ] ) -> Tuple [ pd . DataFrame , int ]: \"\"\" Check and extract expected measurements, and associated them with the related source(s). Args: sources_df: Dataframe containing all the extracted measurements and associations (product from association step). cfg_err_ra: The minimum RA error from the config file (in degrees). cfg_err_dec: The minimum declination error from the config file (in degrees). p_run: The pipeline run object. extr_df: The dataframe containing the information on what sources are missing from which images (output from get_src_skyregion_merged_df in main.py). min_sigma: Minimum sigma value to drop forced extracted measurements. edge_buffer: Flag to pass to ForcedPhot.measure method. cluster_threshold: Flag to pass to ForcedPhot.measure method. allow_nan: Flag to pass to ForcedPhot.measure method. add_mode: True when the pipeline is running in add image mode. done_images_df: Dataframe containing the images that thave already been processed in a previous run (used in add image mode). done_source_ids: List of the source ids that were already present in the previous run (used in add image mode). Returns: The sources_df with the extracted sources added and n_forced is the total number of forced measurements present in the run. \"\"\" logger . info ( 'Starting force extraction step.' ) timer = StopWatch () # get all the skyregions and related images cols = [ 'id' , 'name' , 'measurements_path' , 'path' , 'noise_path' , 'beam_bmaj' , 'beam_bmin' , 'beam_bpa' , 'background_path' , 'rms_min' , 'datetime' , 'skyreg__centre_ra' , 'skyreg__centre_dec' , 'skyreg__xtr_radius' ] images_df = pd . DataFrame ( list ( Image . objects . filter ( run = p_run ) . select_related ( 'skyreg' ) . order_by ( 'datetime' ) . values ( * tuple ( cols )) )) . set_index ( 'name' ) # | name | id | measurements_path | path | noise_path | # |:------------------------------|-----:|:--------------------|:-------------|:-------------| # | VAST_2118-06A.EPOCH01.I.fits | 1 | path/to/file | path/to/file | path/to/file | # | VAST_2118-06A.EPOCH03x.I.fits | 3 | path/to/file | path/to/file | path/to/file | # | VAST_2118-06A.EPOCH02.I.fits | 2 | path/to/file | path/to/file | path/to/file | # | name | beam_bmaj | beam_bmin | beam_bpa | background_path | # |:------------------------------|------------:|------------:|-----------:|:------------------| # | VAST_2118-06A.EPOCH01.I.fits | 0.00589921 | 0.00326088 | -70.4032 | path/to/file | # | VAST_2118-06A.EPOCH03x.I.fits | 0.00470991 | 0.00300502 | -83.1128 | path/to/file | # | VAST_2118-06A.EPOCH02.I.fits | 0.00351331 | 0.00308565 | 77.2395 | path/to/file | # | name | rms_min | datetime | skyreg__centre_ra | skyreg__centre_dec | skyreg__xtr_radius | # |:------------------------------|----------:|:---------------------------------|--------------------:|---------------------:|---------------------:| # | VAST_2118-06A.EPOCH01.I.fits | 0.173946 | 2019-08-27 18:12:16.700000+00:00 | 319.652 | -6.2989 | 6.7401 | # | VAST_2118-06A.EPOCH03x.I.fits | 0.165395 | 2019-10-29 10:01:20.500000+00:00 | 319.652 | -6.2989 | 6.7401 | # | VAST_2118-06A.EPOCH02.I.fits | 0.16323 | 2019-10-30 08:31:20.200000+00:00 | 319.652 | -6.2989 | 6.7401 | # Explode out the img_diff column. extr_df = extr_df . explode ( 'img_diff' ) . reset_index () total_to_extract = extr_df . shape [ 0 ] if add_mode : # If we are adding images to the run we assume that monitoring was # also performed before (enforced by the pre-run checks) so now we # only want to force extract in three situations: # 1. Any force extraction in a new image. # 2. The forced extraction is attached to a new source from the new # images. # 3. A new relation has been created and they need the forced # measuremnts filled in (actually covered by 2.) extr_df = ( extr_df [ ~ extr_df [ 'img_diff' ] . isin ( done_images_df [ 'name' ])] . append ( extr_df [ ( ~ extr_df [ 'source' ] . isin ( done_source_ids )) & ( extr_df [ 'img_diff' ] . isin ( done_images_df . name )) ]) . sort_index () ) logger . info ( f \" { extr_df . shape [ 0 ] } new measurements to force extract\" f \" (from { total_to_extract } total)\" ) timer . reset () extr_df = parallel_extraction ( extr_df , images_df , sources_df [[ 'source' , 'image' , 'flux_peak' ]], min_sigma , edge_buffer , cluster_threshold , allow_nan , add_mode , p_run . path ) logger . info ( 'Force extraction step time: %.2f seconds' , timer . reset () ) # make measurement names unique for db constraint extr_df [ 'name' ] = extr_df [ 'name' ] + f '_f_run { p_run . id : 06d } ' # select sensible flux values and set the columns with fix values values = { 'flux_int' : 0 , 'flux_int_err' : 0 } extr_df = extr_df . fillna ( value = values ) extr_df = extr_df [ ( extr_df [ 'flux_int' ] != 0 ) & ( extr_df [ 'flux_int_err' ] != 0 ) & ( extr_df [ 'chi_squared_fit' ] != np . inf ) & ( extr_df [ 'chi_squared_fit' ] != np . nan ) ] default_pos_err = settings . POS_DEFAULT_MIN_ERROR / 3600. extr_df [ 'ra_err' ] = default_pos_err extr_df [ 'dec_err' ] = default_pos_err extr_df [ 'err_bmaj' ] = 0. extr_df [ 'err_bmin' ] = 0. extr_df [ 'err_pa' ] = 0. extr_df [ 'ew_sys_err' ] = cfg_err_ra extr_df [ 'ns_sys_err' ] = cfg_err_dec extr_df [ 'error_radius' ] = 0. extr_df [ 'uncertainty_ew' ] = np . hypot ( cfg_err_ra , default_pos_err ) extr_df [ 'weight_ew' ] = 1. / extr_df [ 'uncertainty_ew' ] . values ** 2 extr_df [ 'uncertainty_ns' ] = np . hypot ( cfg_err_dec , default_pos_err ) extr_df [ 'weight_ns' ] = 1. / extr_df [ 'uncertainty_ns' ] . values ** 2 extr_df [ 'flux_peak' ] = extr_df [ 'flux_int' ] extr_df [ 'flux_peak_err' ] = extr_df [ 'flux_int_err' ] extr_df [ 'local_rms' ] = extr_df [ 'flux_int_err' ] extr_df [ 'snr' ] = ( extr_df [ 'flux_peak' ] . values / extr_df [ 'local_rms' ] . values ) extr_df [ 'spectral_index' ] = 0. extr_df [ 'dr' ] = 0. extr_df [ 'd2d' ] = 0. extr_df [ 'forced' ] = True extr_df [ 'compactness' ] = 1. extr_df [ 'psf_bmaj' ] = extr_df [ 'bmaj' ] extr_df [ 'psf_bmin' ] = extr_df [ 'bmin' ] extr_df [ 'psf_pa' ] = extr_df [ 'pa' ] extr_df [ 'flag_c4' ] = False extr_df [ 'spectral_index_from_TT' ] = False extr_df [ 'has_siblings' ] = False extr_df [ 'flux_int_isl_ratio' ] = 1.0 extr_df [ 'flux_peak_isl_ratio' ] = 1.0 col_order = read_schema ( images_df . iloc [ 0 ][ 'measurements_path' ] ) . names col_order . remove ( 'id' ) remaining = list ( set ( extr_df . columns ) - set ( col_order )) extr_df = extr_df [ col_order + remaining ] # upload the measurements, a column 'id' is returned with the DB id extr_df = make_upload_measurements ( extr_df ) extr_df = extr_df . rename ( columns = { 'source_tmp_id' : 'source' }) # write forced measurements to specific parquet logger . info ( 'Saving forced measurements to specific parquet file...' ) parallel_write_parquet ( extr_df , p_run . path , add_mode ) # Required to rename this column for the image add mode. extr_df = extr_df . rename ( columns = { 'time' : 'datetime' }) # append new meas into main df and proceed with source groupby etc sources_df = sources_df . append ( extr_df . loc [:, extr_df . columns . isin ( sources_df . columns )], ignore_index = True ) # get the number of forced extractions for the run forced_parquets = glob ( os . path . join ( p_run . path , \"forced_measurements*.parquet\" )) if forced_parquets : n_forced = ( dd . read_parquet ( forced_parquets , columns = [ 'id' ]) . count () . compute () . values [ 0 ] ) else : n_forced = 0 logger . info ( 'Total forced extraction time: %.2f seconds' , timer . reset_init () ) return sources_df , n_forced get_data_from_parquet ( file , p_run_path , add_mode = False ) \u00b6 Get the prefix, max id and image id from the measurements parquets Parameters: Name Type Description Default file str a string with the path of the measurements parquet file required p_run_path str Pipeline run path to get forced parquet in case of add mode. required add_mode bool Whether image add mode is being used where the forced parquet needs to be used instead. False Returns: Type Description Dict Dictionary with prefix string, an interger max_id and a string with the id of the image Source code in vast_pipeline/pipeline/forced_extraction.py def get_data_from_parquet ( file : str , p_run_path : str , add_mode : bool = False ,) -> Dict : ''' Get the prefix, max id and image id from the measurements parquets Args: file: a string with the path of the measurements parquet file p_run_path: Pipeline run path to get forced parquet in case of add mode. add_mode: Whether image add mode is being used where the forced parquet needs to be used instead. Returns: Dictionary with prefix string, an interger max_id and a string with the id of the image ''' if add_mode : image_name = file . split ( \"/\" )[ - 2 ] forced_parquet = os . path . join ( p_run_path , f \"forced_measurements_ { image_name } .parquet\" ) if os . path . isfile ( forced_parquet ): file = forced_parquet # get max component id from parquet file df = pd . read_parquet ( file , columns = [ 'island_id' , 'image_id' ]) prefix = df [ 'island_id' ] . iloc [ 0 ] . rsplit ( '_' , maxsplit = 1 )[ 0 ] + '_' max_id = ( df [ 'island_id' ] . str . rsplit ( '_' , n = 1 ) . str . get ( - 1 ) . astype ( int ) . values . max () + 1 ) return { 'prefix' : prefix , 'max_id' : max_id , 'id' : df [ 'image_id' ] . iloc [ 0 ]} parallel_extraction ( df , df_images , df_sources , min_sigma , edge_buffer , cluster_threshold , allow_nan , add_mode , p_run_path ) \u00b6 Parallelize forced extraction with Dask Parameters: Name Type Description Default df DataFrame dataframe with columns 'wavg_ra', 'wavg_dec', 'img_diff', 'detection' required df_images DataFrame dataframe with the images data and columns 'id', 'measurements_path', 'path', 'noise_path', 'beam_bmaj', 'beam_bmin', 'beam_bpa', 'background_path', 'rms_min', 'datetime', 'skyreg__centre_ra', 'skyreg__centre_dec', 'skyreg__xtr_radius' and 'name' as the index. required df_sources DataFrame dataframe derived from the measurement data with columns 'source', 'image', 'flux_peak'. required min_sigma float minimum sigma value to drop forced extracted measurements. required edge_buffer float flag to pass to ForcedPhot.measure method. required cluster_threshold float flag to pass to ForcedPhot.measure method. required allow_nan bool flag to pass to ForcedPhot.measure method. required add_mode bool True when the pipeline is running in add image mode. required p_run_path str The system path of the pipeline run output. required Returns: Type Description DataFrame Dataframe with forced extracted measurements data, columns are 'source_tmp_id', 'ra', 'dec', 'image', 'flux_peak', 'island_id', 'component_id', 'name', 'flux_int', 'flux_int_err' Source code in vast_pipeline/pipeline/forced_extraction.py def parallel_extraction ( df : pd . DataFrame , df_images : pd . DataFrame , df_sources : pd . DataFrame , min_sigma : float , edge_buffer : float , cluster_threshold : float , allow_nan : bool , add_mode : bool , p_run_path : str ) -> pd . DataFrame : \"\"\" Parallelize forced extraction with Dask Args: df: dataframe with columns 'wavg_ra', 'wavg_dec', 'img_diff', 'detection' df_images: dataframe with the images data and columns 'id', 'measurements_path', 'path', 'noise_path', 'beam_bmaj', 'beam_bmin', 'beam_bpa', 'background_path', 'rms_min', 'datetime', 'skyreg__centre_ra', 'skyreg__centre_dec', 'skyreg__xtr_radius' and 'name' as the index. df_sources: dataframe derived from the measurement data with columns 'source', 'image', 'flux_peak'. min_sigma: minimum sigma value to drop forced extracted measurements. edge_buffer: flag to pass to ForcedPhot.measure method. cluster_threshold: flag to pass to ForcedPhot.measure method. allow_nan: flag to pass to ForcedPhot.measure method. add_mode: True when the pipeline is running in add image mode. p_run_path: The system path of the pipeline run output. Returns: Dataframe with forced extracted measurements data, columns are 'source_tmp_id', 'ra', 'dec', 'image', 'flux_peak', 'island_id', 'component_id', 'name', 'flux_int', 'flux_int_err' \"\"\" # explode the lists in 'img_diff' column (this will make a copy of the df) out = ( df . rename ( columns = { 'img_diff' : 'image' , 'source' : 'source_tmp_id' }) # merge the rms_min column from df_images . merge ( df_images [[ 'rms_min' ]], left_on = 'image' , right_on = 'name' , how = 'left' ) . rename ( columns = { 'rms_min' : 'image_rms_min' }) # merge the measurements columns 'source', 'image', 'flux_peak' . merge ( df_sources , left_on = [ 'source_tmp_id' , 'detection' ], right_on = [ 'source' , 'image' ], how = 'left' ) . drop ( columns = [ 'image_y' , 'source' ]) . rename ( columns = { 'image_x' : 'image' }) ) # drop the source for which we would have no hope of detecting predrop_shape = out . shape [ 0 ] out [ 'max_snr' ] = out [ 'flux_peak' ] . values / out [ 'image_rms_min' ] . values out = out [ out [ 'max_snr' ] > min_sigma ] . reset_index ( drop = True ) logger . debug ( \"Min forced sigma dropped %i sources\" , predrop_shape - out . shape [ 0 ] ) # drop some columns that are no longer needed and the df should look like # out # | | source_tmp_id | wavg_ra | wavg_dec | image_name | flux_peak | # |--:|--------------:|--------:|---------:|:-----------------|----------:| # | 0 | 81 | 317.607 | -8.66952 | VAST_2118-06A... | 11.555 | # | 1 | 894 | 323.803 | -2.6899 | VAST_2118-06A... | 2.178 | # | 2 | 1076 | 316.147 | -3.11408 | VAST_2118-06A... | 6.815 | # | 3 | 1353 | 322.094 | -4.44977 | VAST_2118-06A... | 1.879 | # | 4 | 1387 | 321.734 | -6.82934 | VAST_2118-06A... | 1.61 | out = ( out . drop ([ 'max_snr' , 'image_rms_min' , 'detection' ], axis = 1 ) . rename ( columns = { 'image' : 'image_name' }) ) # get the unique images to extract from unique_images_to_extract = out [ 'image_name' ] . unique () . tolist () # create a list of dictionaries with image file paths and dataframes # with data related to each images image_data_func = lambda x : { 'image' : df_images . at [ x , 'path' ], 'background' : df_images . at [ x , 'background_path' ], 'noise' : df_images . at [ x , 'noise_path' ], 'df' : out [ out [ 'image_name' ] == x ] } list_to_map = list ( map ( image_data_func , unique_images_to_extract )) # create a list of all the measurements parquet files to extract data from, # such as prefix and max_id list_meas_parquets = list ( map ( lambda el : df_images . at [ el , 'measurements_path' ], unique_images_to_extract )) del out , unique_images_to_extract , image_data_func # get a map of the columns that have a fixed value mapping = ( db . from_sequence ( list_meas_parquets , npartitions = len ( list_meas_parquets ) ) . map ( get_data_from_parquet , p_run_path , add_mode ) . compute () ) mapping = pd . DataFrame ( mapping ) # remove not used columns from images_df and merge into mapping col_to_drop = list ( filter ( lambda x : ( 'path' in x ) or ( 'skyreg' in x ), df_images . columns . values . tolist () )) mapping = ( mapping . merge ( df_images . drop ( col_to_drop , axis = 1 ) . reset_index (), on = 'id' , how = 'left' ) . drop ( 'rms_min' , axis = 1 ) . set_index ( 'name' ) ) del col_to_drop n_cpu = cpu_count () - 1 bags = db . from_sequence ( list_to_map , npartitions = len ( list_to_map )) forced_dfs = ( bags . map ( lambda x : extract_from_image ( edge_buffer = edge_buffer , cluster_threshold = cluster_threshold , allow_nan = allow_nan , ** x )) . compute () ) del bags # create intermediates dfs combining the mapping data and the forced # extracted data from the images intermediate_df = list ( map ( lambda x : { ** ( mapping . loc [ x [ 'image' ], :] . to_dict ()), ** x }, forced_dfs )) # compute the rest of the columns intermediate_df = ( db . from_sequence ( intermediate_df ) . map ( lambda x : finalise_forced_dfs ( ** x )) . compute () ) df_out = ( pd . concat ( intermediate_df , axis = 0 , sort = False ) . rename ( columns = { 'wavg_ra' : 'ra' , 'wavg_dec' : 'dec' , 'image_name' : 'image' } ) ) return df_out parallel_write_parquet ( df , run_path , add_mode = False ) \u00b6 Parallelize writing parquet files for forced measurements. Parameters: Name Type Description Default df DataFrame Dataframe containing all the extracted measurements. required run_path str The run path of the pipeline run. required add_mode bool True when the pipeline is running in add image mode. False Returns: Type Description None None Source code in vast_pipeline/pipeline/forced_extraction.py def parallel_write_parquet ( df : pd . DataFrame , run_path : str , add_mode : bool = False ) -> None : ''' Parallelize writing parquet files for forced measurements. Args: df: Dataframe containing all the extracted measurements. run_path: The run path of the pipeline run. add_mode: True when the pipeline is running in add image mode. Returns: None ''' images = df [ 'image' ] . unique () . tolist () get_fname = lambda n : os . path . join ( run_path , 'forced_measurements_' + n . replace ( '.' , '_' ) + '.parquet' ) dfs = list ( map ( lambda x : ( df [ df [ 'image' ] == x ], get_fname ( x )), images )) n_cpu = cpu_count () - 1 # writing parquets using Dask bag bags = db . from_sequence ( dfs ) bags = bags . starmap ( lambda df , fname : write_group_to_parquet ( df , fname , add_mode )) bags . compute ( num_workers = n_cpu ) pass remove_forced_meas ( run_path ) \u00b6 Remove forced measurements from the database if forced parquet files are found. Parameters: Name Type Description Default run_path str The run path of the pipeline run. required Returns: Type Description None None Source code in vast_pipeline/pipeline/forced_extraction.py def remove_forced_meas ( run_path : str ) -> None : ''' Remove forced measurements from the database if forced parquet files are found. Args: run_path: The run path of the pipeline run. Returns: None ''' path_glob = glob ( os . path . join ( run_path , 'forced_measurements_*.parquet' ) ) if path_glob : ids = ( dd . read_parquet ( path_glob , columns = 'id' ) . values . compute () . tolist () ) obj_to_delete = Measurement . objects . filter ( id__in = ids ) del ids if obj_to_delete . exists (): with transaction . atomic (): n_del , detail_del = obj_to_delete . delete () logger . info ( ( 'Deleting all previous forced measurement and association' ' objects for this run. Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) write_group_to_parquet ( df , fname , add_mode ) \u00b6 Write a dataframe correpondent to a single group/image to a parquet file. Parameters: Name Type Description Default df DataFrame Dataframe containing all the extracted measurements. required fname str The file name of the output parquet. required add_mode bool True when the pipeline is running in add image mode. required Returns: Type Description None None Source code in vast_pipeline/pipeline/forced_extraction.py def write_group_to_parquet ( df : pd . DataFrame , fname : str , add_mode : bool ) -> None : ''' Write a dataframe correpondent to a single group/image to a parquet file. Args: df: Dataframe containing all the extracted measurements. fname: The file name of the output parquet. add_mode: True when the pipeline is running in add image mode. Returns: None ''' out_df = df . drop ([ 'd2d' , 'dr' , 'source' , 'image' ], axis = 1 ) if os . path . isfile ( fname ) and add_mode : exist_df = pd . read_parquet ( fname ) out_df = exist_df . append ( out_df ) out_df . to_parquet ( fname , index = False ) pass","title":"forced_extraction.py"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.extract_from_image","text":"Extract the flux, its erros and chi squared data from the image files (image FIT, background and noise files) and return a dictionary with the dataframe and image name Parameters: Name Type Description Default df DataFrame input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak] required image str a string with the path of the image FIT file required background str a string with the path of the image background file required noise str a string with the path of the image noise file required edge_buffer float flag to pass to ForcedPhot.measure method required cluster_threshold float flag to pass to ForcedPhot.measure method required allow_nan bool flag to pass to ForcedPhot.measure method required Returns: Type Description Dict Dictionary with input dataframe with added columns (flux_int, flux_int_err, chi_squared_fit) and image name. Source code in vast_pipeline/pipeline/forced_extraction.py def extract_from_image ( df : pd . DataFrame , image : str , background : str , noise : str , edge_buffer : float , cluster_threshold : float , allow_nan : bool ) -> Dict : \"\"\" Extract the flux, its erros and chi squared data from the image files (image FIT, background and noise files) and return a dictionary with the dataframe and image name Args: df: input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak] image: a string with the path of the image FIT file background: a string with the path of the image background file noise: a string with the path of the image noise file edge_buffer: flag to pass to ForcedPhot.measure method cluster_threshold: flag to pass to ForcedPhot.measure method allow_nan: flag to pass to ForcedPhot.measure method Returns: Dictionary with input dataframe with added columns (flux_int, flux_int_err, chi_squared_fit) and image name. \"\"\" # create the skycoord obj to pass to the forced extraction # see usage https://github.com/dlakaplan/forced_phot P_islands = SkyCoord ( df [ 'wavg_ra' ] . values , df [ 'wavg_dec' ] . values , unit = ( u . deg , u . deg ) ) FP = ForcedPhot ( image , background , noise ) flux , flux_err , chisq , DOF , cluster_id = FP . measure ( P_islands , cluster_threshold = cluster_threshold , allow_nan = allow_nan , edge_buffer = edge_buffer ) df [ 'flux_int' ] = flux * 1.e3 df [ 'flux_int_err' ] = flux_err * 1.e3 df [ 'chi_squared_fit' ] = chisq return { 'df' : df , 'image' : df [ 'image_name' ] . iloc [ 0 ]}","title":"extract_from_image()"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.finalise_forced_dfs","text":"Compute populate leftover columns for the dataframe with forced photometry data given the input parameters Parameters: Name Type Description Default df DataFrame input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak, flux_int, flux_int_err, chi_squared_fit] required prefix str string to use to generate the 'island_id' column required max_id int integer to use to generate the 'island_id' column required beam_bmaj float image beam major axis required beam_bmin float image beam minor axis required beam_bpa float image beam position angle required id int image id in database required datetime datetime timestamp of the image file (from header) required image str string with the image name required Returns: Type Description DataFrame Input dataframe with added columns island_id, component_id, name, bmaj, bmin, pa, image_id, time. Source code in vast_pipeline/pipeline/forced_extraction.py def finalise_forced_dfs ( df : pd . DataFrame , prefix : str , max_id : int , beam_bmaj : float , beam_bmin : float , beam_bpa : float , id : int , datetime : datetime . datetime , image : str ) -> pd . DataFrame : \"\"\" Compute populate leftover columns for the dataframe with forced photometry data given the input parameters Args: df: input dataframe with columns [source_tmp_id, wavg_ra, wavg_dec, image_name, flux_peak, flux_int, flux_int_err, chi_squared_fit] prefix: string to use to generate the 'island_id' column max_id: integer to use to generate the 'island_id' column beam_bmaj: image beam major axis beam_bmin: image beam minor axis beam_bpa: image beam position angle id: image id in database datetime: timestamp of the image file (from header) image: string with the image name Returns: Input dataframe with added columns island_id, component_id, name, bmaj, bmin, pa, image_id, time. \"\"\" # make up the measurements name from the image island_id and component_id df [ 'island_id' ] = np . char . add ( prefix , np . arange ( max_id , max_id + df . shape [ 0 ]) . astype ( str ) ) df [ 'component_id' ] = df [ 'island_id' ] . str . replace ( 'island' , 'component' ) + 'a' img_prefix = image . split ( '.' )[ 0 ] + '_' df [ 'name' ] = img_prefix + df [ 'component_id' ] # assign all the other columns # convert fluxes to mJy # store source bmaj and bmin in arcsec df [ 'bmaj' ] = beam_bmaj * 3600. df [ 'bmin' ] = beam_bmin * 3600. df [ 'pa' ] = beam_bpa # add image id and time df [ 'image_id' ] = id df [ 'time' ] = datetime return df","title":"finalise_forced_dfs()"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.forced_extraction","text":"Check and extract expected measurements, and associated them with the related source(s). Parameters: Name Type Description Default sources_df DataFrame Dataframe containing all the extracted measurements and associations (product from association step). required cfg_err_ra float The minimum RA error from the config file (in degrees). required cfg_err_dec float The minimum declination error from the config file (in degrees). required p_run Run The pipeline run object. required extr_df DataFrame The dataframe containing the information on what sources are missing from which images (output from get_src_skyregion_merged_df in main.py). required min_sigma float Minimum sigma value to drop forced extracted measurements. required edge_buffer float Flag to pass to ForcedPhot.measure method. required cluster_threshold float Flag to pass to ForcedPhot.measure method. required allow_nan bool Flag to pass to ForcedPhot.measure method. required add_mode bool True when the pipeline is running in add image mode. required done_images_df DataFrame Dataframe containing the images that thave already been processed in a previous run (used in add image mode). required done_source_ids List[int] List of the source ids that were already present in the previous run (used in add image mode). required Returns: Type Description Tuple[pandas.core.frame.DataFrame, int] The sources_df with the extracted sources added and n_forced is the total number of forced measurements present in the run. Source code in vast_pipeline/pipeline/forced_extraction.py def forced_extraction ( sources_df : pd . DataFrame , cfg_err_ra : float , cfg_err_dec : float , p_run : Run , extr_df : pd . DataFrame , min_sigma : float , edge_buffer : float , cluster_threshold : float , allow_nan : bool , add_mode : bool , done_images_df : pd . DataFrame , done_source_ids : List [ int ] ) -> Tuple [ pd . DataFrame , int ]: \"\"\" Check and extract expected measurements, and associated them with the related source(s). Args: sources_df: Dataframe containing all the extracted measurements and associations (product from association step). cfg_err_ra: The minimum RA error from the config file (in degrees). cfg_err_dec: The minimum declination error from the config file (in degrees). p_run: The pipeline run object. extr_df: The dataframe containing the information on what sources are missing from which images (output from get_src_skyregion_merged_df in main.py). min_sigma: Minimum sigma value to drop forced extracted measurements. edge_buffer: Flag to pass to ForcedPhot.measure method. cluster_threshold: Flag to pass to ForcedPhot.measure method. allow_nan: Flag to pass to ForcedPhot.measure method. add_mode: True when the pipeline is running in add image mode. done_images_df: Dataframe containing the images that thave already been processed in a previous run (used in add image mode). done_source_ids: List of the source ids that were already present in the previous run (used in add image mode). Returns: The sources_df with the extracted sources added and n_forced is the total number of forced measurements present in the run. \"\"\" logger . info ( 'Starting force extraction step.' ) timer = StopWatch () # get all the skyregions and related images cols = [ 'id' , 'name' , 'measurements_path' , 'path' , 'noise_path' , 'beam_bmaj' , 'beam_bmin' , 'beam_bpa' , 'background_path' , 'rms_min' , 'datetime' , 'skyreg__centre_ra' , 'skyreg__centre_dec' , 'skyreg__xtr_radius' ] images_df = pd . DataFrame ( list ( Image . objects . filter ( run = p_run ) . select_related ( 'skyreg' ) . order_by ( 'datetime' ) . values ( * tuple ( cols )) )) . set_index ( 'name' ) # | name | id | measurements_path | path | noise_path | # |:------------------------------|-----:|:--------------------|:-------------|:-------------| # | VAST_2118-06A.EPOCH01.I.fits | 1 | path/to/file | path/to/file | path/to/file | # | VAST_2118-06A.EPOCH03x.I.fits | 3 | path/to/file | path/to/file | path/to/file | # | VAST_2118-06A.EPOCH02.I.fits | 2 | path/to/file | path/to/file | path/to/file | # | name | beam_bmaj | beam_bmin | beam_bpa | background_path | # |:------------------------------|------------:|------------:|-----------:|:------------------| # | VAST_2118-06A.EPOCH01.I.fits | 0.00589921 | 0.00326088 | -70.4032 | path/to/file | # | VAST_2118-06A.EPOCH03x.I.fits | 0.00470991 | 0.00300502 | -83.1128 | path/to/file | # | VAST_2118-06A.EPOCH02.I.fits | 0.00351331 | 0.00308565 | 77.2395 | path/to/file | # | name | rms_min | datetime | skyreg__centre_ra | skyreg__centre_dec | skyreg__xtr_radius | # |:------------------------------|----------:|:---------------------------------|--------------------:|---------------------:|---------------------:| # | VAST_2118-06A.EPOCH01.I.fits | 0.173946 | 2019-08-27 18:12:16.700000+00:00 | 319.652 | -6.2989 | 6.7401 | # | VAST_2118-06A.EPOCH03x.I.fits | 0.165395 | 2019-10-29 10:01:20.500000+00:00 | 319.652 | -6.2989 | 6.7401 | # | VAST_2118-06A.EPOCH02.I.fits | 0.16323 | 2019-10-30 08:31:20.200000+00:00 | 319.652 | -6.2989 | 6.7401 | # Explode out the img_diff column. extr_df = extr_df . explode ( 'img_diff' ) . reset_index () total_to_extract = extr_df . shape [ 0 ] if add_mode : # If we are adding images to the run we assume that monitoring was # also performed before (enforced by the pre-run checks) so now we # only want to force extract in three situations: # 1. Any force extraction in a new image. # 2. The forced extraction is attached to a new source from the new # images. # 3. A new relation has been created and they need the forced # measuremnts filled in (actually covered by 2.) extr_df = ( extr_df [ ~ extr_df [ 'img_diff' ] . isin ( done_images_df [ 'name' ])] . append ( extr_df [ ( ~ extr_df [ 'source' ] . isin ( done_source_ids )) & ( extr_df [ 'img_diff' ] . isin ( done_images_df . name )) ]) . sort_index () ) logger . info ( f \" { extr_df . shape [ 0 ] } new measurements to force extract\" f \" (from { total_to_extract } total)\" ) timer . reset () extr_df = parallel_extraction ( extr_df , images_df , sources_df [[ 'source' , 'image' , 'flux_peak' ]], min_sigma , edge_buffer , cluster_threshold , allow_nan , add_mode , p_run . path ) logger . info ( 'Force extraction step time: %.2f seconds' , timer . reset () ) # make measurement names unique for db constraint extr_df [ 'name' ] = extr_df [ 'name' ] + f '_f_run { p_run . id : 06d } ' # select sensible flux values and set the columns with fix values values = { 'flux_int' : 0 , 'flux_int_err' : 0 } extr_df = extr_df . fillna ( value = values ) extr_df = extr_df [ ( extr_df [ 'flux_int' ] != 0 ) & ( extr_df [ 'flux_int_err' ] != 0 ) & ( extr_df [ 'chi_squared_fit' ] != np . inf ) & ( extr_df [ 'chi_squared_fit' ] != np . nan ) ] default_pos_err = settings . POS_DEFAULT_MIN_ERROR / 3600. extr_df [ 'ra_err' ] = default_pos_err extr_df [ 'dec_err' ] = default_pos_err extr_df [ 'err_bmaj' ] = 0. extr_df [ 'err_bmin' ] = 0. extr_df [ 'err_pa' ] = 0. extr_df [ 'ew_sys_err' ] = cfg_err_ra extr_df [ 'ns_sys_err' ] = cfg_err_dec extr_df [ 'error_radius' ] = 0. extr_df [ 'uncertainty_ew' ] = np . hypot ( cfg_err_ra , default_pos_err ) extr_df [ 'weight_ew' ] = 1. / extr_df [ 'uncertainty_ew' ] . values ** 2 extr_df [ 'uncertainty_ns' ] = np . hypot ( cfg_err_dec , default_pos_err ) extr_df [ 'weight_ns' ] = 1. / extr_df [ 'uncertainty_ns' ] . values ** 2 extr_df [ 'flux_peak' ] = extr_df [ 'flux_int' ] extr_df [ 'flux_peak_err' ] = extr_df [ 'flux_int_err' ] extr_df [ 'local_rms' ] = extr_df [ 'flux_int_err' ] extr_df [ 'snr' ] = ( extr_df [ 'flux_peak' ] . values / extr_df [ 'local_rms' ] . values ) extr_df [ 'spectral_index' ] = 0. extr_df [ 'dr' ] = 0. extr_df [ 'd2d' ] = 0. extr_df [ 'forced' ] = True extr_df [ 'compactness' ] = 1. extr_df [ 'psf_bmaj' ] = extr_df [ 'bmaj' ] extr_df [ 'psf_bmin' ] = extr_df [ 'bmin' ] extr_df [ 'psf_pa' ] = extr_df [ 'pa' ] extr_df [ 'flag_c4' ] = False extr_df [ 'spectral_index_from_TT' ] = False extr_df [ 'has_siblings' ] = False extr_df [ 'flux_int_isl_ratio' ] = 1.0 extr_df [ 'flux_peak_isl_ratio' ] = 1.0 col_order = read_schema ( images_df . iloc [ 0 ][ 'measurements_path' ] ) . names col_order . remove ( 'id' ) remaining = list ( set ( extr_df . columns ) - set ( col_order )) extr_df = extr_df [ col_order + remaining ] # upload the measurements, a column 'id' is returned with the DB id extr_df = make_upload_measurements ( extr_df ) extr_df = extr_df . rename ( columns = { 'source_tmp_id' : 'source' }) # write forced measurements to specific parquet logger . info ( 'Saving forced measurements to specific parquet file...' ) parallel_write_parquet ( extr_df , p_run . path , add_mode ) # Required to rename this column for the image add mode. extr_df = extr_df . rename ( columns = { 'time' : 'datetime' }) # append new meas into main df and proceed with source groupby etc sources_df = sources_df . append ( extr_df . loc [:, extr_df . columns . isin ( sources_df . columns )], ignore_index = True ) # get the number of forced extractions for the run forced_parquets = glob ( os . path . join ( p_run . path , \"forced_measurements*.parquet\" )) if forced_parquets : n_forced = ( dd . read_parquet ( forced_parquets , columns = [ 'id' ]) . count () . compute () . values [ 0 ] ) else : n_forced = 0 logger . info ( 'Total forced extraction time: %.2f seconds' , timer . reset_init () ) return sources_df , n_forced","title":"forced_extraction()"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.get_data_from_parquet","text":"Get the prefix, max id and image id from the measurements parquets Parameters: Name Type Description Default file str a string with the path of the measurements parquet file required p_run_path str Pipeline run path to get forced parquet in case of add mode. required add_mode bool Whether image add mode is being used where the forced parquet needs to be used instead. False Returns: Type Description Dict Dictionary with prefix string, an interger max_id and a string with the id of the image Source code in vast_pipeline/pipeline/forced_extraction.py def get_data_from_parquet ( file : str , p_run_path : str , add_mode : bool = False ,) -> Dict : ''' Get the prefix, max id and image id from the measurements parquets Args: file: a string with the path of the measurements parquet file p_run_path: Pipeline run path to get forced parquet in case of add mode. add_mode: Whether image add mode is being used where the forced parquet needs to be used instead. Returns: Dictionary with prefix string, an interger max_id and a string with the id of the image ''' if add_mode : image_name = file . split ( \"/\" )[ - 2 ] forced_parquet = os . path . join ( p_run_path , f \"forced_measurements_ { image_name } .parquet\" ) if os . path . isfile ( forced_parquet ): file = forced_parquet # get max component id from parquet file df = pd . read_parquet ( file , columns = [ 'island_id' , 'image_id' ]) prefix = df [ 'island_id' ] . iloc [ 0 ] . rsplit ( '_' , maxsplit = 1 )[ 0 ] + '_' max_id = ( df [ 'island_id' ] . str . rsplit ( '_' , n = 1 ) . str . get ( - 1 ) . astype ( int ) . values . max () + 1 ) return { 'prefix' : prefix , 'max_id' : max_id , 'id' : df [ 'image_id' ] . iloc [ 0 ]}","title":"get_data_from_parquet()"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.parallel_extraction","text":"Parallelize forced extraction with Dask Parameters: Name Type Description Default df DataFrame dataframe with columns 'wavg_ra', 'wavg_dec', 'img_diff', 'detection' required df_images DataFrame dataframe with the images data and columns 'id', 'measurements_path', 'path', 'noise_path', 'beam_bmaj', 'beam_bmin', 'beam_bpa', 'background_path', 'rms_min', 'datetime', 'skyreg__centre_ra', 'skyreg__centre_dec', 'skyreg__xtr_radius' and 'name' as the index. required df_sources DataFrame dataframe derived from the measurement data with columns 'source', 'image', 'flux_peak'. required min_sigma float minimum sigma value to drop forced extracted measurements. required edge_buffer float flag to pass to ForcedPhot.measure method. required cluster_threshold float flag to pass to ForcedPhot.measure method. required allow_nan bool flag to pass to ForcedPhot.measure method. required add_mode bool True when the pipeline is running in add image mode. required p_run_path str The system path of the pipeline run output. required Returns: Type Description DataFrame Dataframe with forced extracted measurements data, columns are 'source_tmp_id', 'ra', 'dec', 'image', 'flux_peak', 'island_id', 'component_id', 'name', 'flux_int', 'flux_int_err' Source code in vast_pipeline/pipeline/forced_extraction.py def parallel_extraction ( df : pd . DataFrame , df_images : pd . DataFrame , df_sources : pd . DataFrame , min_sigma : float , edge_buffer : float , cluster_threshold : float , allow_nan : bool , add_mode : bool , p_run_path : str ) -> pd . DataFrame : \"\"\" Parallelize forced extraction with Dask Args: df: dataframe with columns 'wavg_ra', 'wavg_dec', 'img_diff', 'detection' df_images: dataframe with the images data and columns 'id', 'measurements_path', 'path', 'noise_path', 'beam_bmaj', 'beam_bmin', 'beam_bpa', 'background_path', 'rms_min', 'datetime', 'skyreg__centre_ra', 'skyreg__centre_dec', 'skyreg__xtr_radius' and 'name' as the index. df_sources: dataframe derived from the measurement data with columns 'source', 'image', 'flux_peak'. min_sigma: minimum sigma value to drop forced extracted measurements. edge_buffer: flag to pass to ForcedPhot.measure method. cluster_threshold: flag to pass to ForcedPhot.measure method. allow_nan: flag to pass to ForcedPhot.measure method. add_mode: True when the pipeline is running in add image mode. p_run_path: The system path of the pipeline run output. Returns: Dataframe with forced extracted measurements data, columns are 'source_tmp_id', 'ra', 'dec', 'image', 'flux_peak', 'island_id', 'component_id', 'name', 'flux_int', 'flux_int_err' \"\"\" # explode the lists in 'img_diff' column (this will make a copy of the df) out = ( df . rename ( columns = { 'img_diff' : 'image' , 'source' : 'source_tmp_id' }) # merge the rms_min column from df_images . merge ( df_images [[ 'rms_min' ]], left_on = 'image' , right_on = 'name' , how = 'left' ) . rename ( columns = { 'rms_min' : 'image_rms_min' }) # merge the measurements columns 'source', 'image', 'flux_peak' . merge ( df_sources , left_on = [ 'source_tmp_id' , 'detection' ], right_on = [ 'source' , 'image' ], how = 'left' ) . drop ( columns = [ 'image_y' , 'source' ]) . rename ( columns = { 'image_x' : 'image' }) ) # drop the source for which we would have no hope of detecting predrop_shape = out . shape [ 0 ] out [ 'max_snr' ] = out [ 'flux_peak' ] . values / out [ 'image_rms_min' ] . values out = out [ out [ 'max_snr' ] > min_sigma ] . reset_index ( drop = True ) logger . debug ( \"Min forced sigma dropped %i sources\" , predrop_shape - out . shape [ 0 ] ) # drop some columns that are no longer needed and the df should look like # out # | | source_tmp_id | wavg_ra | wavg_dec | image_name | flux_peak | # |--:|--------------:|--------:|---------:|:-----------------|----------:| # | 0 | 81 | 317.607 | -8.66952 | VAST_2118-06A... | 11.555 | # | 1 | 894 | 323.803 | -2.6899 | VAST_2118-06A... | 2.178 | # | 2 | 1076 | 316.147 | -3.11408 | VAST_2118-06A... | 6.815 | # | 3 | 1353 | 322.094 | -4.44977 | VAST_2118-06A... | 1.879 | # | 4 | 1387 | 321.734 | -6.82934 | VAST_2118-06A... | 1.61 | out = ( out . drop ([ 'max_snr' , 'image_rms_min' , 'detection' ], axis = 1 ) . rename ( columns = { 'image' : 'image_name' }) ) # get the unique images to extract from unique_images_to_extract = out [ 'image_name' ] . unique () . tolist () # create a list of dictionaries with image file paths and dataframes # with data related to each images image_data_func = lambda x : { 'image' : df_images . at [ x , 'path' ], 'background' : df_images . at [ x , 'background_path' ], 'noise' : df_images . at [ x , 'noise_path' ], 'df' : out [ out [ 'image_name' ] == x ] } list_to_map = list ( map ( image_data_func , unique_images_to_extract )) # create a list of all the measurements parquet files to extract data from, # such as prefix and max_id list_meas_parquets = list ( map ( lambda el : df_images . at [ el , 'measurements_path' ], unique_images_to_extract )) del out , unique_images_to_extract , image_data_func # get a map of the columns that have a fixed value mapping = ( db . from_sequence ( list_meas_parquets , npartitions = len ( list_meas_parquets ) ) . map ( get_data_from_parquet , p_run_path , add_mode ) . compute () ) mapping = pd . DataFrame ( mapping ) # remove not used columns from images_df and merge into mapping col_to_drop = list ( filter ( lambda x : ( 'path' in x ) or ( 'skyreg' in x ), df_images . columns . values . tolist () )) mapping = ( mapping . merge ( df_images . drop ( col_to_drop , axis = 1 ) . reset_index (), on = 'id' , how = 'left' ) . drop ( 'rms_min' , axis = 1 ) . set_index ( 'name' ) ) del col_to_drop n_cpu = cpu_count () - 1 bags = db . from_sequence ( list_to_map , npartitions = len ( list_to_map )) forced_dfs = ( bags . map ( lambda x : extract_from_image ( edge_buffer = edge_buffer , cluster_threshold = cluster_threshold , allow_nan = allow_nan , ** x )) . compute () ) del bags # create intermediates dfs combining the mapping data and the forced # extracted data from the images intermediate_df = list ( map ( lambda x : { ** ( mapping . loc [ x [ 'image' ], :] . to_dict ()), ** x }, forced_dfs )) # compute the rest of the columns intermediate_df = ( db . from_sequence ( intermediate_df ) . map ( lambda x : finalise_forced_dfs ( ** x )) . compute () ) df_out = ( pd . concat ( intermediate_df , axis = 0 , sort = False ) . rename ( columns = { 'wavg_ra' : 'ra' , 'wavg_dec' : 'dec' , 'image_name' : 'image' } ) ) return df_out","title":"parallel_extraction()"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.parallel_write_parquet","text":"Parallelize writing parquet files for forced measurements. Parameters: Name Type Description Default df DataFrame Dataframe containing all the extracted measurements. required run_path str The run path of the pipeline run. required add_mode bool True when the pipeline is running in add image mode. False Returns: Type Description None None Source code in vast_pipeline/pipeline/forced_extraction.py def parallel_write_parquet ( df : pd . DataFrame , run_path : str , add_mode : bool = False ) -> None : ''' Parallelize writing parquet files for forced measurements. Args: df: Dataframe containing all the extracted measurements. run_path: The run path of the pipeline run. add_mode: True when the pipeline is running in add image mode. Returns: None ''' images = df [ 'image' ] . unique () . tolist () get_fname = lambda n : os . path . join ( run_path , 'forced_measurements_' + n . replace ( '.' , '_' ) + '.parquet' ) dfs = list ( map ( lambda x : ( df [ df [ 'image' ] == x ], get_fname ( x )), images )) n_cpu = cpu_count () - 1 # writing parquets using Dask bag bags = db . from_sequence ( dfs ) bags = bags . starmap ( lambda df , fname : write_group_to_parquet ( df , fname , add_mode )) bags . compute ( num_workers = n_cpu ) pass","title":"parallel_write_parquet()"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.remove_forced_meas","text":"Remove forced measurements from the database if forced parquet files are found. Parameters: Name Type Description Default run_path str The run path of the pipeline run. required Returns: Type Description None None Source code in vast_pipeline/pipeline/forced_extraction.py def remove_forced_meas ( run_path : str ) -> None : ''' Remove forced measurements from the database if forced parquet files are found. Args: run_path: The run path of the pipeline run. Returns: None ''' path_glob = glob ( os . path . join ( run_path , 'forced_measurements_*.parquet' ) ) if path_glob : ids = ( dd . read_parquet ( path_glob , columns = 'id' ) . values . compute () . tolist () ) obj_to_delete = Measurement . objects . filter ( id__in = ids ) del ids if obj_to_delete . exists (): with transaction . atomic (): n_del , detail_del = obj_to_delete . delete () logger . info ( ( 'Deleting all previous forced measurement and association' ' objects for this run. Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del )","title":"remove_forced_meas()"},{"location":"reference/pipeline/forced_extraction/#vast_pipeline.pipeline.forced_extraction.write_group_to_parquet","text":"Write a dataframe correpondent to a single group/image to a parquet file. Parameters: Name Type Description Default df DataFrame Dataframe containing all the extracted measurements. required fname str The file name of the output parquet. required add_mode bool True when the pipeline is running in add image mode. required Returns: Type Description None None Source code in vast_pipeline/pipeline/forced_extraction.py def write_group_to_parquet ( df : pd . DataFrame , fname : str , add_mode : bool ) -> None : ''' Write a dataframe correpondent to a single group/image to a parquet file. Args: df: Dataframe containing all the extracted measurements. fname: The file name of the output parquet. add_mode: True when the pipeline is running in add image mode. Returns: None ''' out_df = df . drop ([ 'd2d' , 'dr' , 'source' , 'image' ], axis = 1 ) if os . path . isfile ( fname ) and add_mode : exist_df = pd . read_parquet ( fname ) out_df = exist_df . append ( out_df ) out_df . to_parquet ( fname , index = False ) pass","title":"write_group_to_parquet()"},{"location":"reference/pipeline/loading/","text":"bulk_upload_model ( djmodel , generator , batch_size = 10000 , return_ids = False ) \u00b6 Bulk upload a list of generator objects of django models to db. Parameters: Name Type Description Default djmodel Model The Django pipeline model to be uploaded. required generator Iterable[Generator[django.db.models.base.Model, NoneType, NoneType]] The generator objects of the model to upload. required batch_size int How many records to upload at once. 10000 return_ids bool When set to True, the database IDs of the uploaded objects are returned. False Returns: Type Description List[int] None or a list of the database IDs of the uploaded objects. Source code in vast_pipeline/pipeline/loading.py @transaction . atomic def bulk_upload_model ( djmodel : models . Model , generator : Iterable [ Generator [ models . Model , None , None ]], batch_size : int = 10_000 , return_ids : bool = False ) -> List [ int ]: ''' Bulk upload a list of generator objects of django models to db. Args: djmodel: The Django pipeline model to be uploaded. generator: The generator objects of the model to upload. batch_size: How many records to upload at once. return_ids: When set to True, the database IDs of the uploaded objects are returned. Returns: None or a list of the database IDs of the uploaded objects. ''' bulk_ids = [] while True : items = list ( islice ( generator , batch_size )) if not items : break out_bulk = djmodel . objects . bulk_create ( items ) logger . info ( 'Bulk created # %i %s ' , len ( out_bulk ), djmodel . __name__ ) # save the DB ids to return if return_ids : bulk_ids . extend ( list ( map ( lambda i : i . id , out_bulk ))) if return_ids : return bulk_ids make_upload_associations ( associations_df ) \u00b6 Uploads the associations from the supplied associations DataFrame. Parameters: Name Type Description Default associations_df DataFrame DataFrame containing the associations information from the pipeline. required Returns: Type Description None None. Source code in vast_pipeline/pipeline/loading.py def make_upload_associations ( associations_df : pd . DataFrame ) -> None : \"\"\" Uploads the associations from the supplied associations DataFrame. Args: associations_df: DataFrame containing the associations information from the pipeline. Returns: None. \"\"\" logger . info ( 'Upload associations...' ) bulk_upload_model ( Association , association_models_generator ( associations_df ) ) make_upload_images ( paths , image_config ) \u00b6 Carry the first part of the pipeline, by uploading all the images to the image table and populated band and skyregion objects. Parameters: Name Type Description Default paths Dict[str, Dict[str, str]] Dictionary containing the image, noise and background paths of all the images in the pipeline run. The primary keys are selavy , 'noise' and 'background' with the secondary key being the image name. required image_config Dict Dictionary of configuration options for the image ingestion. required Returns: Type Description Tuple[List[vast_pipeline.models.Image], List[vast_pipeline.models.SkyRegion], List[vast_pipeline.models.Band]] A list of image, sky region and band objects that have been uploaded. Source code in vast_pipeline/pipeline/loading.py def make_upload_images ( paths : Dict [ str , Dict [ str , str ]], image_config : Dict ) -> Tuple [ List [ Image ], List [ SkyRegion ], List [ Band ]]: ''' Carry the first part of the pipeline, by uploading all the images to the image table and populated band and skyregion objects. Args: paths: Dictionary containing the image, noise and background paths of all the images in the pipeline run. The primary keys are `selavy`, 'noise' and 'background' with the secondary key being the image name. image_config: Dictionary of configuration options for the image ingestion. Returns: A list of image, sky region and band objects that have been uploaded. ''' timer = StopWatch () images = [] skyregions = [] bands = [] for path in paths [ 'selavy' ]: # STEP #1: Load image and measurements image = SelavyImage ( path , paths , image_config ) logger . info ( 'Reading image %s ...' , image . name ) # 1.1 get/create the frequency band with transaction . atomic (): band = get_create_img_band ( image ) if band not in bands : bands . append ( band ) # 1.2 create image and skyregion entry in DB with transaction . atomic (): img , exists_f = get_create_img ( band . id , image ) skyreg = img . skyreg # add image and skyregion to respective lists images . append ( img ) if skyreg not in skyregions : skyregions . append ( skyreg ) if exists_f : logger . info ( 'Image %s already processed' , img . name ) continue # 1.3 get the image measurements and save them in DB measurements = image . read_selavy ( img ) logger . info ( 'Processed measurements dataframe of shape: ( %i , %i )' , measurements . shape [ 0 ], measurements . shape [ 1 ] ) # upload measurements, a column with the db is added to the df measurements = make_upload_measurements ( measurements ) # save measurements to parquet file in pipeline run folder base_folder = os . path . dirname ( img . measurements_path ) if not os . path . exists ( base_folder ): os . makedirs ( base_folder ) measurements . to_parquet ( img . measurements_path , index = False ) del measurements , image , band , img logger . info ( 'Total images upload/loading time: %.2f seconds' , timer . reset_init () ) return images , skyregions , bands make_upload_measurements ( measurements_df ) \u00b6 Uploads the measurements from the supplied measurements DataFrame. Parameters: Name Type Description Default measurements_df DataFrame DataFrame containing the measurements information from the pipeline. required Returns: Type Description DataFrame Original DataFrame with the database ID attached to each row. Source code in vast_pipeline/pipeline/loading.py def make_upload_measurements ( measurements_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Uploads the measurements from the supplied measurements DataFrame. Args: measurements_df: DataFrame containing the measurements information from the pipeline. Returns: Original DataFrame with the database ID attached to each row. \"\"\" meas_dj_ids = bulk_upload_model ( Measurement , measurement_models_generator ( measurements_df ), return_ids = True ) measurements_df [ 'id' ] = meas_dj_ids return measurements_df make_upload_related_sources ( related_df ) \u00b6 Uploads the related sources from the supplied related sources DataFrame. Parameters: Name Type Description Default related_df DataFrame DataFrame containing the related sources information from the pipeline. required Returns: Type Description None None. Source code in vast_pipeline/pipeline/loading.py def make_upload_related_sources ( related_df : pd . DataFrame ) -> None : \"\"\" Uploads the related sources from the supplied related sources DataFrame. Args: related_df: DataFrame containing the related sources information from the pipeline. Returns: None. \"\"\" logger . info ( 'Populate \"related\" field of sources...' ) bulk_upload_model ( RelatedSource , related_models_generator ( related_df )) make_upload_sources ( sources_df , pipeline_run , add_mode = False ) \u00b6 Delete previous sources for given pipeline run and bulk upload new found sources as well as related sources. Parameters: Name Type Description Default sources_df DataFrame Holds the measurements associated into sources. The output of of thE association step. required pipeline_run Run The pipeline Run object. required add_mode bool Whether the pipeline is running in add image mode. False Returns: Type Description DataFrame The input dataframe with the 'id' column added. Source code in vast_pipeline/pipeline/loading.py def make_upload_sources ( sources_df : pd . DataFrame , pipeline_run : Run , add_mode : bool = False ) -> pd . DataFrame : ''' Delete previous sources for given pipeline run and bulk upload new found sources as well as related sources. Args: sources_df: Holds the measurements associated into sources. The output of of thE association step. pipeline_run: The pipeline Run object. add_mode: Whether the pipeline is running in add image mode. Returns: The input dataframe with the 'id' column added. ''' # create sources in DB with transaction . atomic (): if ( add_mode is False and Source . objects . filter ( run = pipeline_run ) . exists ()): logger . info ( 'Removing objects from previous pipeline run' ) n_del , detail_del = ( Source . objects . filter ( run = pipeline_run ) . delete () ) logger . info ( ( 'Deleting all sources and related objects for this run. ' 'Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) src_dj_ids = bulk_upload_model ( Source , source_models_generator ( sources_df , pipeline_run = pipeline_run ), return_ids = True ) sources_df [ 'id' ] = src_dj_ids return sources_df SQL_update ( df , model , index = None , columns = None ) \u00b6 Generate the SQL code required to update the database. Parameters: Name Type Description Default df DataFrame DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. required model Model The model that is being updated. required index Optional[str] Header of the column to join on, determines which rows in the different tables match. If None, then use the primary key column. None columns Optional[List[str]] The column headers of the columns to be updated. If None, updates all columns except the index column. None Returns: Type Description str The SQL command to update the database. Source code in vast_pipeline/pipeline/loading.py def SQL_update ( df : pd . DataFrame , model : models . Model , index : Optional [ str ] = None , columns : Optional [ List [ str ]] = None ) -> str : ''' Generate the SQL code required to update the database. Args: df: DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. model: The model that is being updated. index: Header of the column to join on, determines which rows in the different tables match. If None, then use the primary key column. columns: The column headers of the columns to be updated. If None, updates all columns except the index column. Returns: The SQL command to update the database. ''' # set index and columns if None if index is None : index = model . _meta . pk . name if columns is None : columns = df . columns . tolist () columns . remove ( index ) # get names table = model . _meta . db_table new_columns = ', ' . join ( 'new_' + c for c in columns ) set_columns = ', ' . join ( c + '=new_' + c for c in columns ) # get index values and new values column_headers = [ index ] column_headers . extend ( columns ) data_arr = df [ column_headers ] . to_numpy () values = [] for row in data_arr : val_row = '(' + ', ' . join ( f ' { val } ' for val in row ) + ')' values . append ( val_row ) values = ', ' . join ( values ) # update database SQL_comm = f \"\"\" UPDATE { table } SET { set_columns } FROM (VALUES { values } ) AS new_values (index_col, { new_columns } ) WHERE { index } =index_col; \"\"\" return SQL_comm update_sources ( sources_df , batch_size = 10000 ) \u00b6 Update database using SQL code. This function opens one connection to the database, and closes it after the update is done. Parameters: Name Type Description Default sources_df DataFrame DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. required batch_size int The df rows are broken into chunks, each chunk is executed in a separate SQL command, batch_size determines the maximum size of the chunk. 10000 Returns: Type Description DataFrame DataFrame containing the new data to be uploaded to the database. Source code in vast_pipeline/pipeline/loading.py def update_sources ( sources_df : pd . DataFrame , batch_size : int = 10_000 ) -> pd . DataFrame : ''' Update database using SQL code. This function opens one connection to the database, and closes it after the update is done. Args: sources_df: DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. batch_size: The df rows are broken into chunks, each chunk is executed in a separate SQL command, batch_size determines the maximum size of the chunk. Returns: DataFrame containing the new data to be uploaded to the database. ''' # Get all possible columns from the model all_source_table_cols = [ fld . attname for fld in Source . _meta . get_fields () if getattr ( fld , 'attname' , None ) is not None ] # Filter to those present in sources_df columns = [ col for col in all_source_table_cols if col in sources_df . columns ] sources_df [ 'id' ] = sources_df . index . values batches = np . ceil ( len ( sources_df ) / batch_size ) dfs = np . array_split ( sources_df , batches ) with connection . cursor () as cursor : for df_batch in dfs : SQL_comm = SQL_update ( df_batch , Source , index = 'id' , columns = columns ) cursor . execute ( SQL_comm ) return sources_df","title":"loading.py"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.bulk_upload_model","text":"Bulk upload a list of generator objects of django models to db. Parameters: Name Type Description Default djmodel Model The Django pipeline model to be uploaded. required generator Iterable[Generator[django.db.models.base.Model, NoneType, NoneType]] The generator objects of the model to upload. required batch_size int How many records to upload at once. 10000 return_ids bool When set to True, the database IDs of the uploaded objects are returned. False Returns: Type Description List[int] None or a list of the database IDs of the uploaded objects. Source code in vast_pipeline/pipeline/loading.py @transaction . atomic def bulk_upload_model ( djmodel : models . Model , generator : Iterable [ Generator [ models . Model , None , None ]], batch_size : int = 10_000 , return_ids : bool = False ) -> List [ int ]: ''' Bulk upload a list of generator objects of django models to db. Args: djmodel: The Django pipeline model to be uploaded. generator: The generator objects of the model to upload. batch_size: How many records to upload at once. return_ids: When set to True, the database IDs of the uploaded objects are returned. Returns: None or a list of the database IDs of the uploaded objects. ''' bulk_ids = [] while True : items = list ( islice ( generator , batch_size )) if not items : break out_bulk = djmodel . objects . bulk_create ( items ) logger . info ( 'Bulk created # %i %s ' , len ( out_bulk ), djmodel . __name__ ) # save the DB ids to return if return_ids : bulk_ids . extend ( list ( map ( lambda i : i . id , out_bulk ))) if return_ids : return bulk_ids","title":"bulk_upload_model()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_associations","text":"Uploads the associations from the supplied associations DataFrame. Parameters: Name Type Description Default associations_df DataFrame DataFrame containing the associations information from the pipeline. required Returns: Type Description None None. Source code in vast_pipeline/pipeline/loading.py def make_upload_associations ( associations_df : pd . DataFrame ) -> None : \"\"\" Uploads the associations from the supplied associations DataFrame. Args: associations_df: DataFrame containing the associations information from the pipeline. Returns: None. \"\"\" logger . info ( 'Upload associations...' ) bulk_upload_model ( Association , association_models_generator ( associations_df ) )","title":"make_upload_associations()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_images","text":"Carry the first part of the pipeline, by uploading all the images to the image table and populated band and skyregion objects. Parameters: Name Type Description Default paths Dict[str, Dict[str, str]] Dictionary containing the image, noise and background paths of all the images in the pipeline run. The primary keys are selavy , 'noise' and 'background' with the secondary key being the image name. required image_config Dict Dictionary of configuration options for the image ingestion. required Returns: Type Description Tuple[List[vast_pipeline.models.Image], List[vast_pipeline.models.SkyRegion], List[vast_pipeline.models.Band]] A list of image, sky region and band objects that have been uploaded. Source code in vast_pipeline/pipeline/loading.py def make_upload_images ( paths : Dict [ str , Dict [ str , str ]], image_config : Dict ) -> Tuple [ List [ Image ], List [ SkyRegion ], List [ Band ]]: ''' Carry the first part of the pipeline, by uploading all the images to the image table and populated band and skyregion objects. Args: paths: Dictionary containing the image, noise and background paths of all the images in the pipeline run. The primary keys are `selavy`, 'noise' and 'background' with the secondary key being the image name. image_config: Dictionary of configuration options for the image ingestion. Returns: A list of image, sky region and band objects that have been uploaded. ''' timer = StopWatch () images = [] skyregions = [] bands = [] for path in paths [ 'selavy' ]: # STEP #1: Load image and measurements image = SelavyImage ( path , paths , image_config ) logger . info ( 'Reading image %s ...' , image . name ) # 1.1 get/create the frequency band with transaction . atomic (): band = get_create_img_band ( image ) if band not in bands : bands . append ( band ) # 1.2 create image and skyregion entry in DB with transaction . atomic (): img , exists_f = get_create_img ( band . id , image ) skyreg = img . skyreg # add image and skyregion to respective lists images . append ( img ) if skyreg not in skyregions : skyregions . append ( skyreg ) if exists_f : logger . info ( 'Image %s already processed' , img . name ) continue # 1.3 get the image measurements and save them in DB measurements = image . read_selavy ( img ) logger . info ( 'Processed measurements dataframe of shape: ( %i , %i )' , measurements . shape [ 0 ], measurements . shape [ 1 ] ) # upload measurements, a column with the db is added to the df measurements = make_upload_measurements ( measurements ) # save measurements to parquet file in pipeline run folder base_folder = os . path . dirname ( img . measurements_path ) if not os . path . exists ( base_folder ): os . makedirs ( base_folder ) measurements . to_parquet ( img . measurements_path , index = False ) del measurements , image , band , img logger . info ( 'Total images upload/loading time: %.2f seconds' , timer . reset_init () ) return images , skyregions , bands","title":"make_upload_images()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_measurements","text":"Uploads the measurements from the supplied measurements DataFrame. Parameters: Name Type Description Default measurements_df DataFrame DataFrame containing the measurements information from the pipeline. required Returns: Type Description DataFrame Original DataFrame with the database ID attached to each row. Source code in vast_pipeline/pipeline/loading.py def make_upload_measurements ( measurements_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Uploads the measurements from the supplied measurements DataFrame. Args: measurements_df: DataFrame containing the measurements information from the pipeline. Returns: Original DataFrame with the database ID attached to each row. \"\"\" meas_dj_ids = bulk_upload_model ( Measurement , measurement_models_generator ( measurements_df ), return_ids = True ) measurements_df [ 'id' ] = meas_dj_ids return measurements_df","title":"make_upload_measurements()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_related_sources","text":"Uploads the related sources from the supplied related sources DataFrame. Parameters: Name Type Description Default related_df DataFrame DataFrame containing the related sources information from the pipeline. required Returns: Type Description None None. Source code in vast_pipeline/pipeline/loading.py def make_upload_related_sources ( related_df : pd . DataFrame ) -> None : \"\"\" Uploads the related sources from the supplied related sources DataFrame. Args: related_df: DataFrame containing the related sources information from the pipeline. Returns: None. \"\"\" logger . info ( 'Populate \"related\" field of sources...' ) bulk_upload_model ( RelatedSource , related_models_generator ( related_df ))","title":"make_upload_related_sources()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.make_upload_sources","text":"Delete previous sources for given pipeline run and bulk upload new found sources as well as related sources. Parameters: Name Type Description Default sources_df DataFrame Holds the measurements associated into sources. The output of of thE association step. required pipeline_run Run The pipeline Run object. required add_mode bool Whether the pipeline is running in add image mode. False Returns: Type Description DataFrame The input dataframe with the 'id' column added. Source code in vast_pipeline/pipeline/loading.py def make_upload_sources ( sources_df : pd . DataFrame , pipeline_run : Run , add_mode : bool = False ) -> pd . DataFrame : ''' Delete previous sources for given pipeline run and bulk upload new found sources as well as related sources. Args: sources_df: Holds the measurements associated into sources. The output of of thE association step. pipeline_run: The pipeline Run object. add_mode: Whether the pipeline is running in add image mode. Returns: The input dataframe with the 'id' column added. ''' # create sources in DB with transaction . atomic (): if ( add_mode is False and Source . objects . filter ( run = pipeline_run ) . exists ()): logger . info ( 'Removing objects from previous pipeline run' ) n_del , detail_del = ( Source . objects . filter ( run = pipeline_run ) . delete () ) logger . info ( ( 'Deleting all sources and related objects for this run. ' 'Total objects deleted: %i ' ), n_del , ) logger . debug ( '(type, #deleted): %s ' , detail_del ) src_dj_ids = bulk_upload_model ( Source , source_models_generator ( sources_df , pipeline_run = pipeline_run ), return_ids = True ) sources_df [ 'id' ] = src_dj_ids return sources_df","title":"make_upload_sources()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.SQL_update","text":"Generate the SQL code required to update the database. Parameters: Name Type Description Default df DataFrame DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. required model Model The model that is being updated. required index Optional[str] Header of the column to join on, determines which rows in the different tables match. If None, then use the primary key column. None columns Optional[List[str]] The column headers of the columns to be updated. If None, updates all columns except the index column. None Returns: Type Description str The SQL command to update the database. Source code in vast_pipeline/pipeline/loading.py def SQL_update ( df : pd . DataFrame , model : models . Model , index : Optional [ str ] = None , columns : Optional [ List [ str ]] = None ) -> str : ''' Generate the SQL code required to update the database. Args: df: DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. model: The model that is being updated. index: Header of the column to join on, determines which rows in the different tables match. If None, then use the primary key column. columns: The column headers of the columns to be updated. If None, updates all columns except the index column. Returns: The SQL command to update the database. ''' # set index and columns if None if index is None : index = model . _meta . pk . name if columns is None : columns = df . columns . tolist () columns . remove ( index ) # get names table = model . _meta . db_table new_columns = ', ' . join ( 'new_' + c for c in columns ) set_columns = ', ' . join ( c + '=new_' + c for c in columns ) # get index values and new values column_headers = [ index ] column_headers . extend ( columns ) data_arr = df [ column_headers ] . to_numpy () values = [] for row in data_arr : val_row = '(' + ', ' . join ( f ' { val } ' for val in row ) + ')' values . append ( val_row ) values = ', ' . join ( values ) # update database SQL_comm = f \"\"\" UPDATE { table } SET { set_columns } FROM (VALUES { values } ) AS new_values (index_col, { new_columns } ) WHERE { index } =index_col; \"\"\" return SQL_comm","title":"SQL_update()"},{"location":"reference/pipeline/loading/#vast_pipeline.pipeline.loading.update_sources","text":"Update database using SQL code. This function opens one connection to the database, and closes it after the update is done. Parameters: Name Type Description Default sources_df DataFrame DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. required batch_size int The df rows are broken into chunks, each chunk is executed in a separate SQL command, batch_size determines the maximum size of the chunk. 10000 Returns: Type Description DataFrame DataFrame containing the new data to be uploaded to the database. Source code in vast_pipeline/pipeline/loading.py def update_sources ( sources_df : pd . DataFrame , batch_size : int = 10_000 ) -> pd . DataFrame : ''' Update database using SQL code. This function opens one connection to the database, and closes it after the update is done. Args: sources_df: DataFrame containing the new data to be uploaded to the database. The columns to be updated need to have the same headers between the df and the table in the database. batch_size: The df rows are broken into chunks, each chunk is executed in a separate SQL command, batch_size determines the maximum size of the chunk. Returns: DataFrame containing the new data to be uploaded to the database. ''' # Get all possible columns from the model all_source_table_cols = [ fld . attname for fld in Source . _meta . get_fields () if getattr ( fld , 'attname' , None ) is not None ] # Filter to those present in sources_df columns = [ col for col in all_source_table_cols if col in sources_df . columns ] sources_df [ 'id' ] = sources_df . index . values batches = np . ceil ( len ( sources_df ) / batch_size ) dfs = np . array_split ( sources_df , batches ) with connection . cursor () as cursor : for df_batch in dfs : SQL_comm = SQL_update ( df_batch , Source , index = 'id' , columns = columns ) cursor . execute ( SQL_comm ) return sources_df","title":"update_sources()"},{"location":"reference/pipeline/main/","text":"This module contains the main pipeline class used for processing a pipeline run. Pipeline \u00b6 Instance of a pipeline. All the methods runs the pipeline opearations, such as association. Attributes: Name Type Description name str The name of the pipeline run. config PipelineConfig The pipeline run configuration. img_paths Dict[str, Dict[str, str]] A dict mapping input image paths to their selavy/noise/background counterpart path. e.g. img_paths[\"selavy\"][<image_path>] contains the selavy catalogue file path for <image_path> . img_epochs Dict[str, str] A dict mapping input image names to their provided epoch. add_mode bool A boolean indicating if this run is adding new images to a previously executed pipeline run. previous_parquets Dict[str, str] A dict mapping that provides the paths to parquet files for previous executions of this pipeline run. __init__ ( self , name , config_path , validate_config = True ) special \u00b6 Initialise an instance of Pipeline with a name and configuration file path. Parameters: Name Type Description Default name str The name of the pipeline run. required config_path str The path to a YAML run configuration file. required validate_config bool Validate the run configuration immediately. Defaults to True. True Source code in vast_pipeline/pipeline/main.py def __init__ ( self , name : str , config_path : str , validate_config : bool = True ): \"\"\"Initialise an instance of Pipeline with a name and configuration file path. Args: name (str): The name of the pipeline run. config_path (str): The path to a YAML run configuration file. validate_config (bool, optional): Validate the run configuration immediately. Defaults to True. \"\"\" self . name : str = name self . config : PipelineConfig = PipelineConfig . from_file ( config_path , validate = validate_config ) self . img_paths : Dict [ str , Dict [ str , str ]] = { 'selavy' : {}, 'noise' : {}, 'background' : {}, } # maps input image paths to their selavy/noise/background counterpart path self . img_epochs : Dict [ str , str ] = {} # maps image names to their provided epoch self . add_mode : bool = False self . previous_parquets : Dict [ str , str ] check_current_runs () staticmethod \u00b6 Checks the number of pipeline runs currently being processed. Returns: Type Description None None Exceptions: Type Description MaxPipelineRunsError Raised if the number of pipeline runs currently being processed is larger than the allowed maximum. Source code in vast_pipeline/pipeline/main.py @staticmethod def check_current_runs () -> None : \"\"\" Checks the number of pipeline runs currently being processed. Returns: None Raises: MaxPipelineRunsError: Raised if the number of pipeline runs currently being processed is larger than the allowed maximum. \"\"\" if Run . objects . check_max_runs ( settings . MAX_PIPELINE_RUNS ): raise MaxPipelineRunsError match_images_to_data ( self ) \u00b6 Loops through images and matches the selavy, noise and bkg images. Assumes that user has enteted images and other data in the same order. Returns: Type Description None None Source code in vast_pipeline/pipeline/main.py def match_images_to_data ( self ) -> None : \"\"\" Loops through images and matches the selavy, noise and bkg images. Assumes that user has enteted images and other data in the same order. Returns: None \"\"\" for key in sorted ( self . config [ \"inputs\" ][ \"image\" ] . keys ()): for x , y in zip ( self . config [ \"inputs\" ][ \"image\" ][ key ], self . config [ \"inputs\" ][ \"selavy\" ][ key ], ): self . img_paths [ \"selavy\" ][ x ] = y self . img_epochs [ os . path . basename ( x )] = key for x , y in zip ( self . config [ \"inputs\" ][ \"image\" ][ key ], self . config [ \"inputs\" ][ \"noise\" ][ key ] ): self . img_paths [ \"noise\" ][ x ] = y if \"background\" in self . config [ \"inputs\" ]: for x , y in zip ( self . config [ \"inputs\" ][ \"image\" ][ key ], self . config [ \"inputs\" ][ \"background\" ][ key ], ): self . img_paths [ \"background\" ][ x ] = y process_pipeline ( self , p_run ) \u00b6 The function that performs the processing operations of the pipeline run. Parameters: Name Type Description Default p_run Run The pipeline run model object. required Returns: Type Description None None Source code in vast_pipeline/pipeline/main.py def process_pipeline ( self , p_run : Run ) -> None : \"\"\" The function that performs the processing operations of the pipeline run. Args: p_run: The pipeline run model object. Returns: None \"\"\" logger . info ( f 'Epoch based association: { self . config . epoch_based } ' ) if self . add_mode : logger . info ( 'Running in image add mode.' ) # Update epoch based flag to not cause user confusion when running # the pipeline (i.e. if it was only updated at the end). It is not # updated if the pipeline is being run in add mode. if self . config . epoch_based and not self . add_mode : with transaction . atomic (): p_run . epoch_based = self . config . epoch_based p_run . save () # Match the image files to the respective selavy, noise and bkg files. # Do this after validation is successful. self . match_images_to_data () # upload/retrieve image data images , skyregions , bands = make_upload_images ( self . img_paths , self . config . image_opts () ) # associate the pipeline run with each image with transaction . atomic (): for img in images : add_run_to_img ( p_run , img ) # write parquet files and retrieve skyregions as a dataframe skyregs_df = write_parquets ( images , skyregions , bands , self . config [ \"run\" ][ \"path\" ]) # STEP #2: measurements association # order images by time images . sort ( key = operator . attrgetter ( 'datetime' )) # If the user has given lists we need to reorder the # image epochs such that they are in date order. if self . config . epoch_based is False : self . img_epochs = {} for i , img in enumerate ( images ): self . img_epochs [ img . name ] = i + 1 image_epochs = [ self . img_epochs [ img . name ] for img in images ] limit = Angle ( self . config [ \"source_association\" ][ \"radius\" ] * u . arcsec ) dr_limit = self . config [ \"source_association\" ][ \"deruiter_radius\" ] bw_limit = self . config [ \"source_association\" ][ \"deruiter_beamwidth_limit\" ] duplicate_limit = Angle ( self . config [ \"source_association\" ][ \"epoch_duplicate_radius\" ] * u . arcsec ) # 2.1 Check if sky regions to be associated can be # split into connected point groups skyregion_groups = group_skyregions ( skyregs_df [[ 'id' , 'centre_ra' , 'centre_dec' , 'xtr_radius' ]] ) n_skyregion_groups = skyregion_groups [ 'skyreg_group' ] . unique () . shape [ 0 ] # Get already done images if in add mode if self . add_mode : done_images_df = pd . read_parquet ( self . previous_parquets [ 'images' ], columns = [ 'id' , 'name' ] ) done_source_ids = pd . read_parquet ( self . previous_parquets [ 'sources' ], columns = [ 'wavg_ra' ] ) . index . tolist () else : done_images_df = None done_source_ids = None # 2.2 Associate with other measurements if self . config [ \"source_association\" ][ \"parallel\" ] and n_skyregion_groups > 1 : images_df = get_parallel_assoc_image_df ( images , skyregion_groups ) images_df [ 'epoch' ] = image_epochs sources_df = parallel_association ( images_df , limit , dr_limit , bw_limit , duplicate_limit , self . config , n_skyregion_groups , self . add_mode , self . previous_parquets , done_images_df , done_source_ids ) else : images_df = pd . DataFrame . from_dict ( { 'image_dj' : images , 'epoch' : image_epochs } ) images_df [ 'skyreg_id' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . skyreg_id ) images_df [ 'image_name' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . name ) sources_df = association ( images_df , limit , dr_limit , bw_limit , duplicate_limit , self . config , self . add_mode , self . previous_parquets , done_images_df ) # Obtain the number of selavy measurements for the run # n_selavy_measurements = sources_df. nr_selavy_measurements = sources_df [ 'id' ] . unique () . shape [ 0 ] # STEP #3: Merge sky regions and sources ready for # steps 4 and 5 below. missing_source_cols = [ 'source' , 'datetime' , 'image' , 'epoch' , 'interim_ew' , 'weight_ew' , 'interim_ns' , 'weight_ns' ] # need to make sure no forced measurments are being passed which # could happen in add mode, otherwise the wrong detection image is # assigned. missing_sources_df = get_src_skyregion_merged_df ( sources_df . loc [ sources_df [ 'forced' ] == False , missing_source_cols ], images_df , skyregs_df , ) # STEP #4 New source analysis new_sources_df = new_sources ( sources_df , missing_sources_df , self . config [ \"new_sources\" ][ \"min_sigma\" ], self . config [ \"source_monitoring\" ][ \"edge_buffer_scale\" ], p_run ) # Drop column no longer required in missing_sources_df. missing_sources_df = ( missing_sources_df . drop ([ 'in_primary' ], axis = 1 ) ) # STEP #5: Run forced extraction/photometry if asked if self . config [ \"source_monitoring\" ][ \"monitor\" ]: ( sources_df , nr_forced_measurements ) = forced_extraction ( sources_df , self . config [ \"measurements\" ][ \"ra_uncertainty\" ] / 3600. , self . config [ \"measurements\" ][ \"dec_uncertainty\" ] / 3600. , p_run , missing_sources_df , self . config [ \"source_monitoring\" ][ \"min_sigma\" ], self . config [ \"source_monitoring\" ][ \"edge_buffer_scale\" ], self . config [ \"source_monitoring\" ][ \"cluster_threshold\" ], self . config [ \"source_monitoring\" ][ \"allow_nan\" ], self . add_mode , done_images_df , done_source_ids ) del missing_sources_df # STEP #6: finalise the df getting unique sources, calculating # metrics and upload data to database nr_sources = final_operations ( sources_df , p_run , new_sources_df , self . config [ \"variability\" ][ \"source_aggregate_pair_metrics_min_abs_vs\" ], self . add_mode , done_source_ids , self . previous_parquets ) # calculate number processed images nr_img_processed = len ( images ) # update pipeline run with the nr images and sources with transaction . atomic (): p_run . n_images = nr_img_processed p_run . n_sources = nr_sources p_run . n_selavy_measurements = nr_selavy_measurements p_run . n_forced_measurements = ( nr_forced_measurements if self . config [ \"source_monitoring\" ][ \"monitor\" ] else 0 ) p_run . save () pass set_status ( pipe_run , status = None ) staticmethod \u00b6 Function to change the status of a pipeline run model object and save to the database. Parameters: Name Type Description Default pipe_run Run The pipeline run model object. required status str The status to set. None Returns: Type Description None None Source code in vast_pipeline/pipeline/main.py @staticmethod def set_status ( pipe_run : Run , status : str = None ) -> None : \"\"\" Function to change the status of a pipeline run model object and save to the database. Args: pipe_run: The pipeline run model object. status: The status to set. Returns: None \"\"\" #TODO: This function gives no feedback if the status is not accepted? choices = [ x [ 0 ] for x in Run . _meta . get_field ( 'status' ) . choices ] if status and status in choices and pipe_run . status != status : with transaction . atomic (): pipe_run . status = status pipe_run . save ()","title":"main.py"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline","text":"Instance of a pipeline. All the methods runs the pipeline opearations, such as association. Attributes: Name Type Description name str The name of the pipeline run. config PipelineConfig The pipeline run configuration. img_paths Dict[str, Dict[str, str]] A dict mapping input image paths to their selavy/noise/background counterpart path. e.g. img_paths[\"selavy\"][<image_path>] contains the selavy catalogue file path for <image_path> . img_epochs Dict[str, str] A dict mapping input image names to their provided epoch. add_mode bool A boolean indicating if this run is adding new images to a previously executed pipeline run. previous_parquets Dict[str, str] A dict mapping that provides the paths to parquet files for previous executions of this pipeline run.","title":"Pipeline"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.__init__","text":"Initialise an instance of Pipeline with a name and configuration file path. Parameters: Name Type Description Default name str The name of the pipeline run. required config_path str The path to a YAML run configuration file. required validate_config bool Validate the run configuration immediately. Defaults to True. True Source code in vast_pipeline/pipeline/main.py def __init__ ( self , name : str , config_path : str , validate_config : bool = True ): \"\"\"Initialise an instance of Pipeline with a name and configuration file path. Args: name (str): The name of the pipeline run. config_path (str): The path to a YAML run configuration file. validate_config (bool, optional): Validate the run configuration immediately. Defaults to True. \"\"\" self . name : str = name self . config : PipelineConfig = PipelineConfig . from_file ( config_path , validate = validate_config ) self . img_paths : Dict [ str , Dict [ str , str ]] = { 'selavy' : {}, 'noise' : {}, 'background' : {}, } # maps input image paths to their selavy/noise/background counterpart path self . img_epochs : Dict [ str , str ] = {} # maps image names to their provided epoch self . add_mode : bool = False self . previous_parquets : Dict [ str , str ]","title":"__init__()"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.check_current_runs","text":"Checks the number of pipeline runs currently being processed. Returns: Type Description None None Exceptions: Type Description MaxPipelineRunsError Raised if the number of pipeline runs currently being processed is larger than the allowed maximum. Source code in vast_pipeline/pipeline/main.py @staticmethod def check_current_runs () -> None : \"\"\" Checks the number of pipeline runs currently being processed. Returns: None Raises: MaxPipelineRunsError: Raised if the number of pipeline runs currently being processed is larger than the allowed maximum. \"\"\" if Run . objects . check_max_runs ( settings . MAX_PIPELINE_RUNS ): raise MaxPipelineRunsError","title":"check_current_runs()"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.match_images_to_data","text":"Loops through images and matches the selavy, noise and bkg images. Assumes that user has enteted images and other data in the same order. Returns: Type Description None None Source code in vast_pipeline/pipeline/main.py def match_images_to_data ( self ) -> None : \"\"\" Loops through images and matches the selavy, noise and bkg images. Assumes that user has enteted images and other data in the same order. Returns: None \"\"\" for key in sorted ( self . config [ \"inputs\" ][ \"image\" ] . keys ()): for x , y in zip ( self . config [ \"inputs\" ][ \"image\" ][ key ], self . config [ \"inputs\" ][ \"selavy\" ][ key ], ): self . img_paths [ \"selavy\" ][ x ] = y self . img_epochs [ os . path . basename ( x )] = key for x , y in zip ( self . config [ \"inputs\" ][ \"image\" ][ key ], self . config [ \"inputs\" ][ \"noise\" ][ key ] ): self . img_paths [ \"noise\" ][ x ] = y if \"background\" in self . config [ \"inputs\" ]: for x , y in zip ( self . config [ \"inputs\" ][ \"image\" ][ key ], self . config [ \"inputs\" ][ \"background\" ][ key ], ): self . img_paths [ \"background\" ][ x ] = y","title":"match_images_to_data()"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.process_pipeline","text":"The function that performs the processing operations of the pipeline run. Parameters: Name Type Description Default p_run Run The pipeline run model object. required Returns: Type Description None None Source code in vast_pipeline/pipeline/main.py def process_pipeline ( self , p_run : Run ) -> None : \"\"\" The function that performs the processing operations of the pipeline run. Args: p_run: The pipeline run model object. Returns: None \"\"\" logger . info ( f 'Epoch based association: { self . config . epoch_based } ' ) if self . add_mode : logger . info ( 'Running in image add mode.' ) # Update epoch based flag to not cause user confusion when running # the pipeline (i.e. if it was only updated at the end). It is not # updated if the pipeline is being run in add mode. if self . config . epoch_based and not self . add_mode : with transaction . atomic (): p_run . epoch_based = self . config . epoch_based p_run . save () # Match the image files to the respective selavy, noise and bkg files. # Do this after validation is successful. self . match_images_to_data () # upload/retrieve image data images , skyregions , bands = make_upload_images ( self . img_paths , self . config . image_opts () ) # associate the pipeline run with each image with transaction . atomic (): for img in images : add_run_to_img ( p_run , img ) # write parquet files and retrieve skyregions as a dataframe skyregs_df = write_parquets ( images , skyregions , bands , self . config [ \"run\" ][ \"path\" ]) # STEP #2: measurements association # order images by time images . sort ( key = operator . attrgetter ( 'datetime' )) # If the user has given lists we need to reorder the # image epochs such that they are in date order. if self . config . epoch_based is False : self . img_epochs = {} for i , img in enumerate ( images ): self . img_epochs [ img . name ] = i + 1 image_epochs = [ self . img_epochs [ img . name ] for img in images ] limit = Angle ( self . config [ \"source_association\" ][ \"radius\" ] * u . arcsec ) dr_limit = self . config [ \"source_association\" ][ \"deruiter_radius\" ] bw_limit = self . config [ \"source_association\" ][ \"deruiter_beamwidth_limit\" ] duplicate_limit = Angle ( self . config [ \"source_association\" ][ \"epoch_duplicate_radius\" ] * u . arcsec ) # 2.1 Check if sky regions to be associated can be # split into connected point groups skyregion_groups = group_skyregions ( skyregs_df [[ 'id' , 'centre_ra' , 'centre_dec' , 'xtr_radius' ]] ) n_skyregion_groups = skyregion_groups [ 'skyreg_group' ] . unique () . shape [ 0 ] # Get already done images if in add mode if self . add_mode : done_images_df = pd . read_parquet ( self . previous_parquets [ 'images' ], columns = [ 'id' , 'name' ] ) done_source_ids = pd . read_parquet ( self . previous_parquets [ 'sources' ], columns = [ 'wavg_ra' ] ) . index . tolist () else : done_images_df = None done_source_ids = None # 2.2 Associate with other measurements if self . config [ \"source_association\" ][ \"parallel\" ] and n_skyregion_groups > 1 : images_df = get_parallel_assoc_image_df ( images , skyregion_groups ) images_df [ 'epoch' ] = image_epochs sources_df = parallel_association ( images_df , limit , dr_limit , bw_limit , duplicate_limit , self . config , n_skyregion_groups , self . add_mode , self . previous_parquets , done_images_df , done_source_ids ) else : images_df = pd . DataFrame . from_dict ( { 'image_dj' : images , 'epoch' : image_epochs } ) images_df [ 'skyreg_id' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . skyreg_id ) images_df [ 'image_name' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . name ) sources_df = association ( images_df , limit , dr_limit , bw_limit , duplicate_limit , self . config , self . add_mode , self . previous_parquets , done_images_df ) # Obtain the number of selavy measurements for the run # n_selavy_measurements = sources_df. nr_selavy_measurements = sources_df [ 'id' ] . unique () . shape [ 0 ] # STEP #3: Merge sky regions and sources ready for # steps 4 and 5 below. missing_source_cols = [ 'source' , 'datetime' , 'image' , 'epoch' , 'interim_ew' , 'weight_ew' , 'interim_ns' , 'weight_ns' ] # need to make sure no forced measurments are being passed which # could happen in add mode, otherwise the wrong detection image is # assigned. missing_sources_df = get_src_skyregion_merged_df ( sources_df . loc [ sources_df [ 'forced' ] == False , missing_source_cols ], images_df , skyregs_df , ) # STEP #4 New source analysis new_sources_df = new_sources ( sources_df , missing_sources_df , self . config [ \"new_sources\" ][ \"min_sigma\" ], self . config [ \"source_monitoring\" ][ \"edge_buffer_scale\" ], p_run ) # Drop column no longer required in missing_sources_df. missing_sources_df = ( missing_sources_df . drop ([ 'in_primary' ], axis = 1 ) ) # STEP #5: Run forced extraction/photometry if asked if self . config [ \"source_monitoring\" ][ \"monitor\" ]: ( sources_df , nr_forced_measurements ) = forced_extraction ( sources_df , self . config [ \"measurements\" ][ \"ra_uncertainty\" ] / 3600. , self . config [ \"measurements\" ][ \"dec_uncertainty\" ] / 3600. , p_run , missing_sources_df , self . config [ \"source_monitoring\" ][ \"min_sigma\" ], self . config [ \"source_monitoring\" ][ \"edge_buffer_scale\" ], self . config [ \"source_monitoring\" ][ \"cluster_threshold\" ], self . config [ \"source_monitoring\" ][ \"allow_nan\" ], self . add_mode , done_images_df , done_source_ids ) del missing_sources_df # STEP #6: finalise the df getting unique sources, calculating # metrics and upload data to database nr_sources = final_operations ( sources_df , p_run , new_sources_df , self . config [ \"variability\" ][ \"source_aggregate_pair_metrics_min_abs_vs\" ], self . add_mode , done_source_ids , self . previous_parquets ) # calculate number processed images nr_img_processed = len ( images ) # update pipeline run with the nr images and sources with transaction . atomic (): p_run . n_images = nr_img_processed p_run . n_sources = nr_sources p_run . n_selavy_measurements = nr_selavy_measurements p_run . n_forced_measurements = ( nr_forced_measurements if self . config [ \"source_monitoring\" ][ \"monitor\" ] else 0 ) p_run . save () pass","title":"process_pipeline()"},{"location":"reference/pipeline/main/#vast_pipeline.pipeline.main.Pipeline.set_status","text":"Function to change the status of a pipeline run model object and save to the database. Parameters: Name Type Description Default pipe_run Run The pipeline run model object. required status str The status to set. None Returns: Type Description None None Source code in vast_pipeline/pipeline/main.py @staticmethod def set_status ( pipe_run : Run , status : str = None ) -> None : \"\"\" Function to change the status of a pipeline run model object and save to the database. Args: pipe_run: The pipeline run model object. status: The status to set. Returns: None \"\"\" #TODO: This function gives no feedback if the status is not accepted? choices = [ x [ 0 ] for x in Run . _meta . get_field ( 'status' ) . choices ] if status and status in choices and pipe_run . status != status : with transaction . atomic (): pipe_run . status = status pipe_run . save ()","title":"set_status()"},{"location":"reference/pipeline/model_generator/","text":"association_models_generator ( assoc_df ) \u00b6 Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Parameters: Name Type Description Default assoc_df DataFrame The dataframe from the pipeline containing the associations between measurements and sources. required Returns: Type Description Iterable[Generator[vast_pipeline.models.Association, NoneType, NoneType]] An iterable generator object containing the yielded Association objects. Source code in vast_pipeline/pipeline/model_generator.py def association_models_generator ( assoc_df : pd . DataFrame ) -> Iterable [ Generator [ Association , None , None ]]: \"\"\" Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Args: assoc_df: The dataframe from the pipeline containing the associations between measurements and sources. Returns: An iterable generator object containing the yielded Association objects. \"\"\" for i , row in assoc_df . iterrows (): yield Association ( meas_id = row [ 'id' ], source_id = row [ 'source_id' ], d2d = row [ 'd2d' ], dr = row [ 'dr' ], ) measurement_models_generator ( meas_df ) \u00b6 Creates a generator object containing yielded Measurement objects from an input pipeline measurement dataframe. Parameters: Name Type Description Default meas_df DataFrame The dataframe from the pipeline containing the measurements of an image. required Returns: Type Description Iterable[Generator[vast_pipeline.models.Measurement, NoneType, NoneType]] An iterable generator object containing the yielded Measurement objects. Source code in vast_pipeline/pipeline/model_generator.py def measurement_models_generator ( meas_df : pd . DataFrame ) -> Iterable [ Generator [ Measurement , None , None ]]: \"\"\" Creates a generator object containing yielded Measurement objects from an input pipeline measurement dataframe. Args: meas_df: The dataframe from the pipeline containing the measurements of an image. Returns: An iterable generator object containing the yielded Measurement objects. \"\"\" for i , row in meas_df . iterrows (): one_m = Measurement () for fld in one_m . _meta . get_fields (): if getattr ( fld , 'attname' , None ) and fld . attname in row . index : setattr ( one_m , fld . attname , row [ fld . attname ]) yield one_m related_models_generator ( related_df ) \u00b6 Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Parameters: Name Type Description Default related_df DataFrame The dataframe from the pipeline containing the relations between sources. required Returns: Type Description Iterable[Generator[vast_pipeline.models.RelatedSource, NoneType, NoneType]] An iterable generator object containing the yielded Association objects. Source code in vast_pipeline/pipeline/model_generator.py def related_models_generator ( related_df : pd . DataFrame ) -> Iterable [ Generator [ RelatedSource , None , None ]]: \"\"\" Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Args: related_df: The dataframe from the pipeline containing the relations between sources. Returns: An iterable generator object containing the yielded Association objects. \"\"\" for i , row in related_df . iterrows (): yield RelatedSource ( ** row . to_dict ()) source_models_generator ( src_df , pipeline_run ) \u00b6 Creates a generator object containing yielded Source objects from an input pipeline sources dataframe. Parameters: Name Type Description Default src_df DataFrame The dataframe from the pipeline containing the measurements of an image. required pipeline_run Run The pipeline Run object of which the sources are associated with. required Returns: Type Description Iterable[Generator[vast_pipeline.models.Source, NoneType, NoneType]] An iterable generator object containing the yielded Source objects. Source code in vast_pipeline/pipeline/model_generator.py def source_models_generator ( src_df : pd . DataFrame , pipeline_run : Run ) -> Iterable [ Generator [ Source , None , None ]]: \"\"\" Creates a generator object containing yielded Source objects from an input pipeline sources dataframe. Args: src_df: The dataframe from the pipeline containing the measurements of an image. pipeline_run: The pipeline Run object of which the sources are associated with. Returns: An iterable generator object containing the yielded Source objects. \"\"\" for i , row in src_df . iterrows (): # generate an IAU compliant source name, see # https://cdsweb.u-strasbg.fr/Dic/iau-spec.html name = ( f \"J { deg2hms ( row [ 'wavg_ra' ], precision = 1 , truncate = True ) } \" f \" { deg2dms ( row [ 'wavg_dec' ], precision = 0 , truncate = True ) } \" ) . replace ( \":\" , \"\" ) src = Source () src . run_id = pipeline_run . id src . name = name for fld in src . _meta . get_fields (): if getattr ( fld , 'attname' , None ) and fld . attname in row . index : setattr ( src , fld . attname , row [ fld . attname ]) yield src","title":"model_generator.py"},{"location":"reference/pipeline/model_generator/#vast_pipeline.pipeline.model_generator.association_models_generator","text":"Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Parameters: Name Type Description Default assoc_df DataFrame The dataframe from the pipeline containing the associations between measurements and sources. required Returns: Type Description Iterable[Generator[vast_pipeline.models.Association, NoneType, NoneType]] An iterable generator object containing the yielded Association objects. Source code in vast_pipeline/pipeline/model_generator.py def association_models_generator ( assoc_df : pd . DataFrame ) -> Iterable [ Generator [ Association , None , None ]]: \"\"\" Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Args: assoc_df: The dataframe from the pipeline containing the associations between measurements and sources. Returns: An iterable generator object containing the yielded Association objects. \"\"\" for i , row in assoc_df . iterrows (): yield Association ( meas_id = row [ 'id' ], source_id = row [ 'source_id' ], d2d = row [ 'd2d' ], dr = row [ 'dr' ], )","title":"association_models_generator()"},{"location":"reference/pipeline/model_generator/#vast_pipeline.pipeline.model_generator.measurement_models_generator","text":"Creates a generator object containing yielded Measurement objects from an input pipeline measurement dataframe. Parameters: Name Type Description Default meas_df DataFrame The dataframe from the pipeline containing the measurements of an image. required Returns: Type Description Iterable[Generator[vast_pipeline.models.Measurement, NoneType, NoneType]] An iterable generator object containing the yielded Measurement objects. Source code in vast_pipeline/pipeline/model_generator.py def measurement_models_generator ( meas_df : pd . DataFrame ) -> Iterable [ Generator [ Measurement , None , None ]]: \"\"\" Creates a generator object containing yielded Measurement objects from an input pipeline measurement dataframe. Args: meas_df: The dataframe from the pipeline containing the measurements of an image. Returns: An iterable generator object containing the yielded Measurement objects. \"\"\" for i , row in meas_df . iterrows (): one_m = Measurement () for fld in one_m . _meta . get_fields (): if getattr ( fld , 'attname' , None ) and fld . attname in row . index : setattr ( one_m , fld . attname , row [ fld . attname ]) yield one_m","title":"measurement_models_generator()"},{"location":"reference/pipeline/model_generator/#vast_pipeline.pipeline.model_generator.related_models_generator","text":"Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Parameters: Name Type Description Default related_df DataFrame The dataframe from the pipeline containing the relations between sources. required Returns: Type Description Iterable[Generator[vast_pipeline.models.RelatedSource, NoneType, NoneType]] An iterable generator object containing the yielded Association objects. Source code in vast_pipeline/pipeline/model_generator.py def related_models_generator ( related_df : pd . DataFrame ) -> Iterable [ Generator [ RelatedSource , None , None ]]: \"\"\" Creates a generator object containing yielded Association objects from an input pipeline association dataframe. Args: related_df: The dataframe from the pipeline containing the relations between sources. Returns: An iterable generator object containing the yielded Association objects. \"\"\" for i , row in related_df . iterrows (): yield RelatedSource ( ** row . to_dict ())","title":"related_models_generator()"},{"location":"reference/pipeline/model_generator/#vast_pipeline.pipeline.model_generator.source_models_generator","text":"Creates a generator object containing yielded Source objects from an input pipeline sources dataframe. Parameters: Name Type Description Default src_df DataFrame The dataframe from the pipeline containing the measurements of an image. required pipeline_run Run The pipeline Run object of which the sources are associated with. required Returns: Type Description Iterable[Generator[vast_pipeline.models.Source, NoneType, NoneType]] An iterable generator object containing the yielded Source objects. Source code in vast_pipeline/pipeline/model_generator.py def source_models_generator ( src_df : pd . DataFrame , pipeline_run : Run ) -> Iterable [ Generator [ Source , None , None ]]: \"\"\" Creates a generator object containing yielded Source objects from an input pipeline sources dataframe. Args: src_df: The dataframe from the pipeline containing the measurements of an image. pipeline_run: The pipeline Run object of which the sources are associated with. Returns: An iterable generator object containing the yielded Source objects. \"\"\" for i , row in src_df . iterrows (): # generate an IAU compliant source name, see # https://cdsweb.u-strasbg.fr/Dic/iau-spec.html name = ( f \"J { deg2hms ( row [ 'wavg_ra' ], precision = 1 , truncate = True ) } \" f \" { deg2dms ( row [ 'wavg_dec' ], precision = 0 , truncate = True ) } \" ) . replace ( \":\" , \"\" ) src = Source () src . run_id = pipeline_run . id src . name = name for fld in src . _meta . get_fields (): if getattr ( fld , 'attname' , None ) and fld . attname in row . index : setattr ( src , fld . attname , row [ fld . attname ]) yield src","title":"source_models_generator()"},{"location":"reference/pipeline/new_sources/","text":"check_primary_image ( row ) \u00b6 Checks if the primary image is in the image list. Parameters: Name Type Description Default row Series Row of the missing_sources_df, need the keys 'primary' and 'img_list'. required Returns: Type Description bool True if the primary image is in the image list. Source code in vast_pipeline/pipeline/new_sources.py def check_primary_image ( row : pd . Series ) -> bool : \"\"\" Checks if the primary image is in the image list. Args: row: Row of the missing_sources_df, need the keys 'primary' and 'img_list'. Returns: True if the primary image is in the image list. \"\"\" return row [ 'primary' ] in row [ 'img_list' ] gen_array_coords_from_wcs ( coords , wcs ) \u00b6 Converts SkyCoord coordinates to array coordinates given a wcs. Parameters: Name Type Description Default coords SkyCoord The coordinates to convert. required wcs WCS The WCS to use for the conversion. required Returns: Type Description ndarray Array containing the x and y array coordinates of the input sky coordinates, e.g.: np.array([[x1, x2, x3], [y1, y2, y3]]) Source code in vast_pipeline/pipeline/new_sources.py def gen_array_coords_from_wcs ( coords : SkyCoord , wcs : WCS ) -> np . ndarray : \"\"\" Converts SkyCoord coordinates to array coordinates given a wcs. Args: coords: The coordinates to convert. wcs: The WCS to use for the conversion. Returns: Array containing the x and y array coordinates of the input sky coordinates, e.g.: np.array([[x1, x2, x3], [y1, y2, y3]]) \"\"\" array_coords = wcs . world_to_array_index ( coords ) array_coords = np . array ([ np . array ( array_coords [ 0 ]), np . array ( array_coords [ 1 ]), ]) return array_coords get_image_rms_measurements ( group , nbeam = 3 , edge_buffer = 1.0 ) \u00b6 Take the coordinates provided from the group and measure the array cell value in the provided image. Parameters: Name Type Description Default group DataFrame The group of sources to measure in the image, requiring the columns: 'source', 'wavg_ra', 'wavg_dec' and 'img_diff_rms_path'. required nbeam int The number of half beamwidths (BMAJ) away from the edge of the image or a NaN value that is acceptable. 3 edge_buffer float Multiplicative factor applied to nbeam to act as a buffer. 1.0 Returns: Type Description DataFrame The group dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Source code in vast_pipeline/pipeline/new_sources.py def get_image_rms_measurements ( group : pd . DataFrame , nbeam : int = 3 , edge_buffer : float = 1.0 ) -> pd . DataFrame : \"\"\" Take the coordinates provided from the group and measure the array cell value in the provided image. Args: group: The group of sources to measure in the image, requiring the columns: 'source', 'wavg_ra', 'wavg_dec' and 'img_diff_rms_path'. nbeam: The number of half beamwidths (BMAJ) away from the edge of the image or a NaN value that is acceptable. edge_buffer: Multiplicative factor applied to nbeam to act as a buffer. Returns: The group dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. \"\"\" image = group . iloc [ 0 ][ 'img_diff_rms_path' ] with fits . open ( image ) as hdul : header = hdul [ 0 ] . header wcs = WCS ( header , naxis = 2 ) data = hdul [ 0 ] . data . squeeze () # Here we mimic the forced fits behaviour, # sources within 3 half BMAJ widths of the image # edges are ignored. The user buffer is also # applied for consistency. pixelscale = ( proj_plane_pixel_scales ( wcs )[ 1 ] * u . deg ) . to ( u . arcsec ) bmaj = header [ \"BMAJ\" ] * u . deg npix = round ( ( nbeam / 2. * bmaj . to ( 'arcsec' ) / pixelscale ) . value ) npix = int ( round ( npix * edge_buffer )) coords = SkyCoord ( group . wavg_ra , group . wavg_dec , unit = ( u . deg , u . deg ) ) array_coords = gen_array_coords_from_wcs ( coords , wcs ) # check for pixel wrapping x_valid = np . logical_or ( array_coords [ 0 ] >= ( data . shape [ 0 ] - npix ), array_coords [ 0 ] < npix ) y_valid = np . logical_or ( array_coords [ 1 ] >= ( data . shape [ 1 ] - npix ), array_coords [ 1 ] < npix ) valid = ~ np . logical_or ( x_valid , y_valid ) valid_indexes = group [ valid ] . index . values group = group . loc [ valid_indexes ] if group . empty : # early return if all sources failed range check logger . debug ( 'All sources out of range in new source rms measurement' f ' for image { image } .' ) group [ 'img_diff_true_rms' ] = np . nan return group # Now we also need to check proximity to NaN values # as forced fits may also drop these values coords = SkyCoord ( group . wavg_ra , group . wavg_dec , unit = ( u . deg , u . deg ) ) array_coords = gen_array_coords_from_wcs ( coords , wcs ) acceptable_no_nan_dist = int ( round ( bmaj . to ( 'arcsec' ) . value / 2. / pixelscale . value ) ) nan_valid = [] # Get slices of each source and check NaN is not included. for i , j in zip ( array_coords [ 0 ], array_coords [ 1 ]): sl = tuple (( slice ( i - acceptable_no_nan_dist , i + acceptable_no_nan_dist ), slice ( j - acceptable_no_nan_dist , j + acceptable_no_nan_dist ) )) if np . any ( np . isnan ( data [ sl ])): nan_valid . append ( False ) else : nan_valid . append ( True ) valid_indexes = group [ nan_valid ] . index . values if np . any ( nan_valid ): # only run if there are actual values to measure rms_values = data [ array_coords [ 0 ][ nan_valid ], array_coords [ 1 ][ nan_valid ] ] # not matched ones will be NaN. group . loc [ valid_indexes , 'img_diff_true_rms' ] = rms_values . astype ( np . float64 ) * 1.e3 else : group [ 'img_diff_true_rms' ] = np . nan return group new_sources ( sources_df , missing_sources_df , min_sigma , edge_buffer , p_run ) \u00b6 Processes the new sources detected to check that they are valid new sources. This involves checking to see that the source should be seen at all in the images where it is not detected. For valid new sources the snr value the source would have in non-detected images is also calculated. Parameters: Name Type Description Default sources_df DataFrame The sources found from the association step. required missing_sources_df DataFrame The dataframe containing the 'missing detections' for each source. +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 776 | ['VAST_0127-73A.EPOCH01.I.fits'] | 31.8223 | -70.4674 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+ img_diff | ----------------------------------| ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ----------------------------------+ required min_sigma float The minimum sigma value acceptable when compared to the minimum rms of the respective image. required edge_buffer float Multiplicative factor to be passed to the 'get_image_rms_measurements' function. required p_run Run The pipeline run. required Returns: Type Description DataFrame The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Columns: source - source id, int. img_list - list of images, List. wavg_ra - weighted average RA, float. wavg_dec - weighted average Dec, float. skyreg_img_list - list of sky regions of images in img_list, List. img_diff - The images missing from coverage, List. primary - What should be the first image, str. detection - The first detection image, str. detection_time - Datetime of detection, datetime.datetime. img_diff_time - Datetime of img_diff list, datetime.datetime. img_diff_rms_min - Minimum rms of diff images, float. img_diff_rms_median - Median rms of diff images, float. img_diff_rms_path - rms path of diff images, str. flux_peak - Flux peak of source (detection), float. diff_sigma - SNR in differnce images (compared to minimum), float. img_diff_true_rms - The true rms value from the diff images, float. new_high_sigma - peak flux / true rms value, float. Source code in vast_pipeline/pipeline/new_sources.py def new_sources ( sources_df : pd . DataFrame , missing_sources_df : pd . DataFrame , min_sigma : float , edge_buffer : float , p_run : Run ) -> pd . DataFrame : \"\"\" Processes the new sources detected to check that they are valid new sources. This involves checking to see that the source *should* be seen at all in the images where it is not detected. For valid new sources the snr value the source would have in non-detected images is also calculated. Args: sources_df: The sources found from the association step. missing_sources_df: The dataframe containing the 'missing detections' for each source. +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 776 | ['VAST_0127-73A.EPOCH01.I.fits'] | 31.8223 | -70.4674 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+ img_diff | ----------------------------------| ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ----------------------------------+ min_sigma: The minimum sigma value acceptable when compared to the minimum rms of the respective image. edge_buffer: Multiplicative factor to be passed to the 'get_image_rms_measurements' function. p_run: The pipeline run. Returns: The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Columns: source - source id, int. img_list - list of images, List. wavg_ra - weighted average RA, float. wavg_dec - weighted average Dec, float. skyreg_img_list - list of sky regions of images in img_list, List. img_diff - The images missing from coverage, List. primary - What should be the first image, str. detection - The first detection image, str. detection_time - Datetime of detection, datetime.datetime. img_diff_time - Datetime of img_diff list, datetime.datetime. img_diff_rms_min - Minimum rms of diff images, float. img_diff_rms_median - Median rms of diff images, float. img_diff_rms_path - rms path of diff images, str. flux_peak - Flux peak of source (detection), float. diff_sigma - SNR in differnce images (compared to minimum), float. img_diff_true_rms - The true rms value from the diff images, float. new_high_sigma - peak flux / true rms value, float. \"\"\" timer = StopWatch () logger . info ( \"Starting new source analysis.\" ) cols = [ 'id' , 'name' , 'noise_path' , 'datetime' , 'rms_median' , 'rms_min' , 'rms_max' , ] images_df = pd . DataFrame ( list ( Image . objects . filter ( run = p_run ) . values ( * tuple ( cols )) )) . set_index ( 'name' ) # Get rid of sources that are not 'new', i.e. sources which the # first sky region image is not in the image list new_sources_df = missing_sources_df [ missing_sources_df [ 'in_primary' ] == False ] . drop ( columns = [ 'in_primary' ] ) # Check if the previous sources would have actually been seen # i.e. are the previous images sensitive enough # save the index before exploding new_sources_df = new_sources_df . reset_index () # Explode now to avoid two loops below new_sources_df = new_sources_df . explode ( 'img_diff' ) # Merge the respective image information to the df new_sources_df = new_sources_df . merge ( images_df [[ 'datetime' ]], left_on = 'detection' , right_on = 'name' , how = 'left' ) . rename ( columns = { 'datetime' : 'detection_time' }) new_sources_df = new_sources_df . merge ( images_df [[ 'datetime' , 'rms_min' , 'rms_median' , 'noise_path' ]], left_on = 'img_diff' , right_on = 'name' , how = 'left' ) . rename ( columns = { 'datetime' : 'img_diff_time' , 'rms_min' : 'img_diff_rms_min' , 'rms_median' : 'img_diff_rms_median' , 'noise_path' : 'img_diff_rms_path' }) # Select only those images that come before the detection image # in time. new_sources_df = new_sources_df [ new_sources_df . img_diff_time < new_sources_df . detection_time ] # merge the detection fluxes in new_sources_df = pd . merge ( new_sources_df , sources_df [[ 'source' , 'image' , 'flux_peak' ]], left_on = [ 'source' , 'detection' ], right_on = [ 'source' , 'image' ], how = 'left' ) . drop ( columns = [ 'image' ]) # calculate the sigma of the source if it was placed in the # minimum rms region of the previous images new_sources_df [ 'diff_sigma' ] = ( new_sources_df [ 'flux_peak' ] . values / new_sources_df [ 'img_diff_rms_min' ] . values ) # keep those that are above the user specified threshold new_sources_df = new_sources_df . loc [ new_sources_df [ 'diff_sigma' ] >= min_sigma ] # Now have list of sources that should have been seen before given # previous images minimum rms values. # Current inaccurate sky regions may mean that the source # was in a previous 'NaN' area of the image. This needs to be # checked. Currently the check is done by filtering out of range # pixels once the values have been obtained (below). # This could be done using MOCpy however this is reasonably # fast and the io of a MOC fits may take more time. # So these sources will be flagged as new sources, but we can also # make a guess of how signficant they are. For this the next step is # to measure the true rms at the source location. # measure the actual rms in the previous images at # the source location. new_sources_df = parallel_get_rms_measurements ( new_sources_df , edge_buffer = edge_buffer ) # this removes those that are out of range new_sources_df [ 'img_diff_true_rms' ] = ( new_sources_df [ 'img_diff_true_rms' ] . fillna ( 0. ) ) new_sources_df = new_sources_df [ new_sources_df [ 'img_diff_true_rms' ] != 0 ] # calculate the true sigma new_sources_df [ 'true_sigma' ] = ( new_sources_df [ 'flux_peak' ] . values / new_sources_df [ 'img_diff_true_rms' ] . values ) # We only care about the highest true sigma new_sources_df = new_sources_df . sort_values ( by = [ 'source' , 'true_sigma' ] ) # keep only the highest for each source, rename for the daatabase new_sources_df = ( new_sources_df . drop_duplicates ( 'source' ) . set_index ( 'source' ) . rename ( columns = { 'true_sigma' : 'new_high_sigma' }) ) # moving forward only the new_high_sigma columns is needed, drop all others. new_sources_df = new_sources_df [[ 'new_high_sigma' ]] logger . info ( 'Total new source analysis time: %.2f seconds' , timer . reset_init () ) return new_sources_df parallel_get_rms_measurements ( df , edge_buffer = 1.0 ) \u00b6 Wrapper function to use 'get_image_rms_measurements' in parallel with Dask. nbeam is not an option here as that parameter is fixed in forced extraction and so is made sure to be fixed here to. This may change in the future. Parameters: Name Type Description Default df DataFrame The group of sources to measure in the images. required edge_buffer float Multiplicative factor to be passed to the 'get_image_rms_measurements' function. 1.0 Returns: Type Description DataFrame The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Source code in vast_pipeline/pipeline/new_sources.py def parallel_get_rms_measurements ( df : pd . DataFrame , edge_buffer : float = 1.0 ) -> pd . DataFrame : \"\"\" Wrapper function to use 'get_image_rms_measurements' in parallel with Dask. nbeam is not an option here as that parameter is fixed in forced extraction and so is made sure to be fixed here to. This may change in the future. Args: df: The group of sources to measure in the images. edge_buffer: Multiplicative factor to be passed to the 'get_image_rms_measurements' function. Returns: The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. \"\"\" out = df [[ 'source' , 'wavg_ra' , 'wavg_dec' , 'img_diff_rms_path' ]] col_dtype = { 'source' : 'i' , 'wavg_ra' : 'f' , 'wavg_dec' : 'f' , 'img_diff_rms_path' : 'U' , 'img_diff_true_rms' : 'f' , } n_cpu = cpu_count () - 1 out = ( dd . from_pandas ( out , n_cpu ) . groupby ( 'img_diff_rms_path' ) . apply ( get_image_rms_measurements , edge_buffer = edge_buffer , meta = col_dtype ) . compute ( num_workers = n_cpu , scheduler = 'processes' ) ) df = df . merge ( out [[ 'source' , 'img_diff_true_rms' ]], left_on = 'source' , right_on = 'source' , how = 'left' ) return df","title":"new_sources.py"},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.check_primary_image","text":"Checks if the primary image is in the image list. Parameters: Name Type Description Default row Series Row of the missing_sources_df, need the keys 'primary' and 'img_list'. required Returns: Type Description bool True if the primary image is in the image list. Source code in vast_pipeline/pipeline/new_sources.py def check_primary_image ( row : pd . Series ) -> bool : \"\"\" Checks if the primary image is in the image list. Args: row: Row of the missing_sources_df, need the keys 'primary' and 'img_list'. Returns: True if the primary image is in the image list. \"\"\" return row [ 'primary' ] in row [ 'img_list' ]","title":"check_primary_image()"},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.gen_array_coords_from_wcs","text":"Converts SkyCoord coordinates to array coordinates given a wcs. Parameters: Name Type Description Default coords SkyCoord The coordinates to convert. required wcs WCS The WCS to use for the conversion. required Returns: Type Description ndarray Array containing the x and y array coordinates of the input sky coordinates, e.g.: np.array([[x1, x2, x3], [y1, y2, y3]]) Source code in vast_pipeline/pipeline/new_sources.py def gen_array_coords_from_wcs ( coords : SkyCoord , wcs : WCS ) -> np . ndarray : \"\"\" Converts SkyCoord coordinates to array coordinates given a wcs. Args: coords: The coordinates to convert. wcs: The WCS to use for the conversion. Returns: Array containing the x and y array coordinates of the input sky coordinates, e.g.: np.array([[x1, x2, x3], [y1, y2, y3]]) \"\"\" array_coords = wcs . world_to_array_index ( coords ) array_coords = np . array ([ np . array ( array_coords [ 0 ]), np . array ( array_coords [ 1 ]), ]) return array_coords","title":"gen_array_coords_from_wcs()"},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.get_image_rms_measurements","text":"Take the coordinates provided from the group and measure the array cell value in the provided image. Parameters: Name Type Description Default group DataFrame The group of sources to measure in the image, requiring the columns: 'source', 'wavg_ra', 'wavg_dec' and 'img_diff_rms_path'. required nbeam int The number of half beamwidths (BMAJ) away from the edge of the image or a NaN value that is acceptable. 3 edge_buffer float Multiplicative factor applied to nbeam to act as a buffer. 1.0 Returns: Type Description DataFrame The group dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Source code in vast_pipeline/pipeline/new_sources.py def get_image_rms_measurements ( group : pd . DataFrame , nbeam : int = 3 , edge_buffer : float = 1.0 ) -> pd . DataFrame : \"\"\" Take the coordinates provided from the group and measure the array cell value in the provided image. Args: group: The group of sources to measure in the image, requiring the columns: 'source', 'wavg_ra', 'wavg_dec' and 'img_diff_rms_path'. nbeam: The number of half beamwidths (BMAJ) away from the edge of the image or a NaN value that is acceptable. edge_buffer: Multiplicative factor applied to nbeam to act as a buffer. Returns: The group dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. \"\"\" image = group . iloc [ 0 ][ 'img_diff_rms_path' ] with fits . open ( image ) as hdul : header = hdul [ 0 ] . header wcs = WCS ( header , naxis = 2 ) data = hdul [ 0 ] . data . squeeze () # Here we mimic the forced fits behaviour, # sources within 3 half BMAJ widths of the image # edges are ignored. The user buffer is also # applied for consistency. pixelscale = ( proj_plane_pixel_scales ( wcs )[ 1 ] * u . deg ) . to ( u . arcsec ) bmaj = header [ \"BMAJ\" ] * u . deg npix = round ( ( nbeam / 2. * bmaj . to ( 'arcsec' ) / pixelscale ) . value ) npix = int ( round ( npix * edge_buffer )) coords = SkyCoord ( group . wavg_ra , group . wavg_dec , unit = ( u . deg , u . deg ) ) array_coords = gen_array_coords_from_wcs ( coords , wcs ) # check for pixel wrapping x_valid = np . logical_or ( array_coords [ 0 ] >= ( data . shape [ 0 ] - npix ), array_coords [ 0 ] < npix ) y_valid = np . logical_or ( array_coords [ 1 ] >= ( data . shape [ 1 ] - npix ), array_coords [ 1 ] < npix ) valid = ~ np . logical_or ( x_valid , y_valid ) valid_indexes = group [ valid ] . index . values group = group . loc [ valid_indexes ] if group . empty : # early return if all sources failed range check logger . debug ( 'All sources out of range in new source rms measurement' f ' for image { image } .' ) group [ 'img_diff_true_rms' ] = np . nan return group # Now we also need to check proximity to NaN values # as forced fits may also drop these values coords = SkyCoord ( group . wavg_ra , group . wavg_dec , unit = ( u . deg , u . deg ) ) array_coords = gen_array_coords_from_wcs ( coords , wcs ) acceptable_no_nan_dist = int ( round ( bmaj . to ( 'arcsec' ) . value / 2. / pixelscale . value ) ) nan_valid = [] # Get slices of each source and check NaN is not included. for i , j in zip ( array_coords [ 0 ], array_coords [ 1 ]): sl = tuple (( slice ( i - acceptable_no_nan_dist , i + acceptable_no_nan_dist ), slice ( j - acceptable_no_nan_dist , j + acceptable_no_nan_dist ) )) if np . any ( np . isnan ( data [ sl ])): nan_valid . append ( False ) else : nan_valid . append ( True ) valid_indexes = group [ nan_valid ] . index . values if np . any ( nan_valid ): # only run if there are actual values to measure rms_values = data [ array_coords [ 0 ][ nan_valid ], array_coords [ 1 ][ nan_valid ] ] # not matched ones will be NaN. group . loc [ valid_indexes , 'img_diff_true_rms' ] = rms_values . astype ( np . float64 ) * 1.e3 else : group [ 'img_diff_true_rms' ] = np . nan return group","title":"get_image_rms_measurements()"},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.new_sources","text":"Processes the new sources detected to check that they are valid new sources. This involves checking to see that the source should be seen at all in the images where it is not detected. For valid new sources the snr value the source would have in non-detected images is also calculated. Parameters: Name Type Description Default sources_df DataFrame The sources found from the association step. required missing_sources_df DataFrame The dataframe containing the 'missing detections' for each source. +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 776 | ['VAST_0127-73A.EPOCH01.I.fits'] | 31.8223 | -70.4674 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+ img_diff | ----------------------------------| ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ----------------------------------+ required min_sigma float The minimum sigma value acceptable when compared to the minimum rms of the respective image. required edge_buffer float Multiplicative factor to be passed to the 'get_image_rms_measurements' function. required p_run Run The pipeline run. required Returns: Type Description DataFrame The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Columns: source - source id, int. img_list - list of images, List. wavg_ra - weighted average RA, float. wavg_dec - weighted average Dec, float. skyreg_img_list - list of sky regions of images in img_list, List. img_diff - The images missing from coverage, List. primary - What should be the first image, str. detection - The first detection image, str. detection_time - Datetime of detection, datetime.datetime. img_diff_time - Datetime of img_diff list, datetime.datetime. img_diff_rms_min - Minimum rms of diff images, float. img_diff_rms_median - Median rms of diff images, float. img_diff_rms_path - rms path of diff images, str. flux_peak - Flux peak of source (detection), float. diff_sigma - SNR in differnce images (compared to minimum), float. img_diff_true_rms - The true rms value from the diff images, float. new_high_sigma - peak flux / true rms value, float. Source code in vast_pipeline/pipeline/new_sources.py def new_sources ( sources_df : pd . DataFrame , missing_sources_df : pd . DataFrame , min_sigma : float , edge_buffer : float , p_run : Run ) -> pd . DataFrame : \"\"\" Processes the new sources detected to check that they are valid new sources. This involves checking to see that the source *should* be seen at all in the images where it is not detected. For valid new sources the snr value the source would have in non-detected images is also calculated. Args: sources_df: The sources found from the association step. missing_sources_df: The dataframe containing the 'missing detections' for each source. +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 776 | ['VAST_0127-73A.EPOCH01.I.fits'] | 31.8223 | -70.4674 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+ img_diff | ----------------------------------| ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH08.I.fits'] | ----------------------------------+ min_sigma: The minimum sigma value acceptable when compared to the minimum rms of the respective image. edge_buffer: Multiplicative factor to be passed to the 'get_image_rms_measurements' function. p_run: The pipeline run. Returns: The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Columns: source - source id, int. img_list - list of images, List. wavg_ra - weighted average RA, float. wavg_dec - weighted average Dec, float. skyreg_img_list - list of sky regions of images in img_list, List. img_diff - The images missing from coverage, List. primary - What should be the first image, str. detection - The first detection image, str. detection_time - Datetime of detection, datetime.datetime. img_diff_time - Datetime of img_diff list, datetime.datetime. img_diff_rms_min - Minimum rms of diff images, float. img_diff_rms_median - Median rms of diff images, float. img_diff_rms_path - rms path of diff images, str. flux_peak - Flux peak of source (detection), float. diff_sigma - SNR in differnce images (compared to minimum), float. img_diff_true_rms - The true rms value from the diff images, float. new_high_sigma - peak flux / true rms value, float. \"\"\" timer = StopWatch () logger . info ( \"Starting new source analysis.\" ) cols = [ 'id' , 'name' , 'noise_path' , 'datetime' , 'rms_median' , 'rms_min' , 'rms_max' , ] images_df = pd . DataFrame ( list ( Image . objects . filter ( run = p_run ) . values ( * tuple ( cols )) )) . set_index ( 'name' ) # Get rid of sources that are not 'new', i.e. sources which the # first sky region image is not in the image list new_sources_df = missing_sources_df [ missing_sources_df [ 'in_primary' ] == False ] . drop ( columns = [ 'in_primary' ] ) # Check if the previous sources would have actually been seen # i.e. are the previous images sensitive enough # save the index before exploding new_sources_df = new_sources_df . reset_index () # Explode now to avoid two loops below new_sources_df = new_sources_df . explode ( 'img_diff' ) # Merge the respective image information to the df new_sources_df = new_sources_df . merge ( images_df [[ 'datetime' ]], left_on = 'detection' , right_on = 'name' , how = 'left' ) . rename ( columns = { 'datetime' : 'detection_time' }) new_sources_df = new_sources_df . merge ( images_df [[ 'datetime' , 'rms_min' , 'rms_median' , 'noise_path' ]], left_on = 'img_diff' , right_on = 'name' , how = 'left' ) . rename ( columns = { 'datetime' : 'img_diff_time' , 'rms_min' : 'img_diff_rms_min' , 'rms_median' : 'img_diff_rms_median' , 'noise_path' : 'img_diff_rms_path' }) # Select only those images that come before the detection image # in time. new_sources_df = new_sources_df [ new_sources_df . img_diff_time < new_sources_df . detection_time ] # merge the detection fluxes in new_sources_df = pd . merge ( new_sources_df , sources_df [[ 'source' , 'image' , 'flux_peak' ]], left_on = [ 'source' , 'detection' ], right_on = [ 'source' , 'image' ], how = 'left' ) . drop ( columns = [ 'image' ]) # calculate the sigma of the source if it was placed in the # minimum rms region of the previous images new_sources_df [ 'diff_sigma' ] = ( new_sources_df [ 'flux_peak' ] . values / new_sources_df [ 'img_diff_rms_min' ] . values ) # keep those that are above the user specified threshold new_sources_df = new_sources_df . loc [ new_sources_df [ 'diff_sigma' ] >= min_sigma ] # Now have list of sources that should have been seen before given # previous images minimum rms values. # Current inaccurate sky regions may mean that the source # was in a previous 'NaN' area of the image. This needs to be # checked. Currently the check is done by filtering out of range # pixels once the values have been obtained (below). # This could be done using MOCpy however this is reasonably # fast and the io of a MOC fits may take more time. # So these sources will be flagged as new sources, but we can also # make a guess of how signficant they are. For this the next step is # to measure the true rms at the source location. # measure the actual rms in the previous images at # the source location. new_sources_df = parallel_get_rms_measurements ( new_sources_df , edge_buffer = edge_buffer ) # this removes those that are out of range new_sources_df [ 'img_diff_true_rms' ] = ( new_sources_df [ 'img_diff_true_rms' ] . fillna ( 0. ) ) new_sources_df = new_sources_df [ new_sources_df [ 'img_diff_true_rms' ] != 0 ] # calculate the true sigma new_sources_df [ 'true_sigma' ] = ( new_sources_df [ 'flux_peak' ] . values / new_sources_df [ 'img_diff_true_rms' ] . values ) # We only care about the highest true sigma new_sources_df = new_sources_df . sort_values ( by = [ 'source' , 'true_sigma' ] ) # keep only the highest for each source, rename for the daatabase new_sources_df = ( new_sources_df . drop_duplicates ( 'source' ) . set_index ( 'source' ) . rename ( columns = { 'true_sigma' : 'new_high_sigma' }) ) # moving forward only the new_high_sigma columns is needed, drop all others. new_sources_df = new_sources_df [[ 'new_high_sigma' ]] logger . info ( 'Total new source analysis time: %.2f seconds' , timer . reset_init () ) return new_sources_df","title":"new_sources()"},{"location":"reference/pipeline/new_sources/#vast_pipeline.pipeline.new_sources.parallel_get_rms_measurements","text":"Wrapper function to use 'get_image_rms_measurements' in parallel with Dask. nbeam is not an option here as that parameter is fixed in forced extraction and so is made sure to be fixed here to. This may change in the future. Parameters: Name Type Description Default df DataFrame The group of sources to measure in the images. required edge_buffer float Multiplicative factor to be passed to the 'get_image_rms_measurements' function. 1.0 Returns: Type Description DataFrame The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. Source code in vast_pipeline/pipeline/new_sources.py def parallel_get_rms_measurements ( df : pd . DataFrame , edge_buffer : float = 1.0 ) -> pd . DataFrame : \"\"\" Wrapper function to use 'get_image_rms_measurements' in parallel with Dask. nbeam is not an option here as that parameter is fixed in forced extraction and so is made sure to be fixed here to. This may change in the future. Args: df: The group of sources to measure in the images. edge_buffer: Multiplicative factor to be passed to the 'get_image_rms_measurements' function. Returns: The original input dataframe with the 'img_diff_true_rms' column added. The column will contain 'NaN' entires for sources that fail. \"\"\" out = df [[ 'source' , 'wavg_ra' , 'wavg_dec' , 'img_diff_rms_path' ]] col_dtype = { 'source' : 'i' , 'wavg_ra' : 'f' , 'wavg_dec' : 'f' , 'img_diff_rms_path' : 'U' , 'img_diff_true_rms' : 'f' , } n_cpu = cpu_count () - 1 out = ( dd . from_pandas ( out , n_cpu ) . groupby ( 'img_diff_rms_path' ) . apply ( get_image_rms_measurements , edge_buffer = edge_buffer , meta = col_dtype ) . compute ( num_workers = n_cpu , scheduler = 'processes' ) ) df = df . merge ( out [[ 'source' , 'img_diff_true_rms' ]], left_on = 'source' , right_on = 'source' , how = 'left' ) return df","title":"parallel_get_rms_measurements()"},{"location":"reference/pipeline/pairs/","text":"calculate_m_metric ( flux_a , flux_b ) \u00b6 Calculate the m variability metric which is the modulation index between two fluxes. This is proportional to the fractional variability. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Parameters: Name Type Description Default flux_a float flux value \"A\". required flux_b float flux value \"B\". required Returns: Type Description float float: the m metric for flux values \"A\" and \"B\". Source code in vast_pipeline/pipeline/pairs.py def calculate_m_metric ( flux_a : float , flux_b : float ) -> float : \"\"\"Calculate the m variability metric which is the modulation index between two fluxes. This is proportional to the fractional variability. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Args: flux_a (float): flux value \"A\". flux_b (float): flux value \"B\". Returns: float: the m metric for flux values \"A\" and \"B\". \"\"\" return 2 * (( flux_a - flux_b ) / ( flux_a + flux_b )) calculate_measurement_pair_metrics ( df ) \u00b6 Generate a DataFrame of measurement pairs and their 2-epoch variability metrics from a DataFrame of measurements. For more information on the variability metrics, see Section 5 of Mooley et al. (2016), DOI: 10.3847/0004-637X/818/2/105. Parameters: Name Type Description Default df DataFrame Input measurements. Must contain columns: id, source, flux_int, flux_int_err, flux_peak, flux_peak_err, has_siblings. required Returns: Type Description DataFrame pd.DataFrame: Measurement pairs and 2-epoch metrics. Will contain columns: source - the source ID id_a, id_b - the measurement IDs flux_int_a, flux_int_b - measurement integrated fluxes in mJy flux_int_err_a, flux_int_err_b - measurement integrated flux errors in mJy flux_peak_a, flux_peak_b - measurement peak fluxes in mJy/beam flux_peak_err_a, flux_peak_err_b - measurement peak flux errors in mJy/beam vs_peak, vs_int - variability t-statistic m_peak, m_int - variability modulation index Source code in vast_pipeline/pipeline/pairs.py def calculate_measurement_pair_metrics ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Generate a DataFrame of measurement pairs and their 2-epoch variability metrics from a DataFrame of measurements. For more information on the variability metrics, see Section 5 of Mooley et al. (2016), DOI: 10.3847/0004-637X/818/2/105. Args: df (pd.DataFrame): Input measurements. Must contain columns: id, source, flux_int, flux_int_err, flux_peak, flux_peak_err, has_siblings. Returns: pd.DataFrame: Measurement pairs and 2-epoch metrics. Will contain columns: source - the source ID id_a, id_b - the measurement IDs flux_int_a, flux_int_b - measurement integrated fluxes in mJy flux_int_err_a, flux_int_err_b - measurement integrated flux errors in mJy flux_peak_a, flux_peak_b - measurement peak fluxes in mJy/beam flux_peak_err_a, flux_peak_err_b - measurement peak flux errors in mJy/beam vs_peak, vs_int - variability t-statistic m_peak, m_int - variability modulation index \"\"\" n_cpu = cpu_count () - 1 \"\"\"Create a DataFrame containing all measurement ID combinations per source. Resultant DataFrame will have a MultiIndex([\"source\", RangeIndex]) where \"source\" is the source ID and RangeIndex is an unnamed temporary ID for each measurement pair, unique only together with source. DataFrame will have columns [0, 1], each containing a measurement ID. e.g. 0 1 source 1 0 1 9284 1 1 17597 2 1 26984 3 9284 17597 4 9284 26984 ... ... ... 11105 2 11845 19961 11124 0 3573 12929 1 3573 21994 2 12929 21994 11128 0 6216 23534 \"\"\" measurement_combinations = ( dd . from_pandas ( df , n_cpu ) . groupby ( \"source\" )[ \"id\" ] . apply ( lambda x : pd . DataFrame ( list ( combinations ( x , 2 ))), meta = { 0 : \"i\" , 1 : \"i\" },) . compute ( num_workers = n_cpu , scheduler = \"processes\" ) ) \"\"\"Drop the RangeIndex from the MultiIndex as it isn't required and rename the columns. Example resultant DataFrame: source id_a id_b 0 1 1 9284 1 1 1 17597 2 1 1 26984 3 1 9284 17597 4 1 9284 26984 ... ... ... ... 33640 11105 11845 19961 33641 11124 3573 12929 33642 11124 3573 21994 33643 11124 12929 21994 33644 11128 6216 23534 Where source is the source ID, id_a and id_b are measurement IDs. \"\"\" measurement_combinations = measurement_combinations . reset_index ( level = 1 , drop = True ) . rename ( columns = { 0 : \"id_a\" , 1 : \"id_b\" }) . astype ( int ) . reset_index () # Dask has a tendency to swap which order the measurement pairs are # defined in, even if the dataframe is pre-sorted. We want the pairs to be # in date order (a < b) so the code below corrects any that are not. measurement_combinations = measurement_combinations . join ( df [[ 'source' , 'id' , 'datetime' ]] . set_index ([ 'source' , 'id' ]), on = [ 'source' , 'id_a' ], ) measurement_combinations = measurement_combinations . join ( df [[ 'source' , 'id' , 'datetime' ]] . set_index ([ 'source' , 'id' ]), on = [ 'source' , 'id_b' ], lsuffix = '_a' , rsuffix = '_b' ) to_correct_mask = ( measurement_combinations [ 'datetime_a' ] > measurement_combinations [ 'datetime_b' ] ) if np . any ( to_correct_mask ): logger . debug ( 'Correcting measurement pairs order' ) ( measurement_combinations . loc [ to_correct_mask , 'id_a' ], measurement_combinations . loc [ to_correct_mask , 'id_b' ] ) = np . array ([ measurement_combinations . loc [ to_correct_mask , 'id_b' ] . values , measurement_combinations . loc [ to_correct_mask , 'id_a' ] . values ]) measurement_combinations = measurement_combinations . drop ( [ 'datetime_a' , 'datetime_b' ], axis = 1 ) # add the measurement fluxes and errors association_fluxes = df . set_index ([ \"source\" , \"id\" ])[ [ \"flux_int\" , \"flux_int_err\" , \"flux_peak\" , \"flux_peak_err\" , \"image\" ] ] . rename ( columns = { \"image\" : \"image_name\" }) measurement_combinations = measurement_combinations . join ( association_fluxes , on = [ \"source\" , \"id_a\" ], ) . join ( association_fluxes , on = [ \"source\" , \"id_b\" ], lsuffix = \"_a\" , rsuffix = \"_b\" , ) # calculate 2-epoch metrics measurement_combinations [ \"vs_peak\" ] = calculate_vs_metric ( measurement_combinations . flux_peak_a , measurement_combinations . flux_peak_b , measurement_combinations . flux_peak_err_a , measurement_combinations . flux_peak_err_b , ) measurement_combinations [ \"vs_int\" ] = calculate_vs_metric ( measurement_combinations . flux_int_a , measurement_combinations . flux_int_b , measurement_combinations . flux_int_err_a , measurement_combinations . flux_int_err_b , ) measurement_combinations [ \"m_peak\" ] = calculate_m_metric ( measurement_combinations . flux_peak_a , measurement_combinations . flux_peak_b , ) measurement_combinations [ \"m_int\" ] = calculate_m_metric ( measurement_combinations . flux_int_a , measurement_combinations . flux_int_b , ) return measurement_combinations calculate_vs_metric ( flux_a , flux_b , flux_err_a , flux_err_b ) \u00b6 Calculate the Vs variability metric which is the t-statistic that the provided fluxes are variable. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Parameters: Name Type Description Default flux_a float flux value \"A\". required flux_b float flux value \"B\". required flux_err_a float error of flux_a . required flux_err_b float error of flux_b . required Returns: Type Description float float: the Vs metric for flux values \"A\" and \"B\". Source code in vast_pipeline/pipeline/pairs.py def calculate_vs_metric ( flux_a : float , flux_b : float , flux_err_a : float , flux_err_b : float ) -> float : \"\"\"Calculate the Vs variability metric which is the t-statistic that the provided fluxes are variable. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Args: flux_a (float): flux value \"A\". flux_b (float): flux value \"B\". flux_err_a (float): error of `flux_a`. flux_err_b (float): error of `flux_b`. Returns: float: the Vs metric for flux values \"A\" and \"B\". \"\"\" return ( flux_a - flux_b ) / np . hypot ( flux_err_a , flux_err_b )","title":"pairs.py"},{"location":"reference/pipeline/pairs/#vast_pipeline.pipeline.pairs.calculate_m_metric","text":"Calculate the m variability metric which is the modulation index between two fluxes. This is proportional to the fractional variability. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Parameters: Name Type Description Default flux_a float flux value \"A\". required flux_b float flux value \"B\". required Returns: Type Description float float: the m metric for flux values \"A\" and \"B\". Source code in vast_pipeline/pipeline/pairs.py def calculate_m_metric ( flux_a : float , flux_b : float ) -> float : \"\"\"Calculate the m variability metric which is the modulation index between two fluxes. This is proportional to the fractional variability. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Args: flux_a (float): flux value \"A\". flux_b (float): flux value \"B\". Returns: float: the m metric for flux values \"A\" and \"B\". \"\"\" return 2 * (( flux_a - flux_b ) / ( flux_a + flux_b ))","title":"calculate_m_metric()"},{"location":"reference/pipeline/pairs/#vast_pipeline.pipeline.pairs.calculate_measurement_pair_metrics","text":"Generate a DataFrame of measurement pairs and their 2-epoch variability metrics from a DataFrame of measurements. For more information on the variability metrics, see Section 5 of Mooley et al. (2016), DOI: 10.3847/0004-637X/818/2/105. Parameters: Name Type Description Default df DataFrame Input measurements. Must contain columns: id, source, flux_int, flux_int_err, flux_peak, flux_peak_err, has_siblings. required Returns: Type Description DataFrame pd.DataFrame: Measurement pairs and 2-epoch metrics. Will contain columns: source - the source ID id_a, id_b - the measurement IDs flux_int_a, flux_int_b - measurement integrated fluxes in mJy flux_int_err_a, flux_int_err_b - measurement integrated flux errors in mJy flux_peak_a, flux_peak_b - measurement peak fluxes in mJy/beam flux_peak_err_a, flux_peak_err_b - measurement peak flux errors in mJy/beam vs_peak, vs_int - variability t-statistic m_peak, m_int - variability modulation index Source code in vast_pipeline/pipeline/pairs.py def calculate_measurement_pair_metrics ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Generate a DataFrame of measurement pairs and their 2-epoch variability metrics from a DataFrame of measurements. For more information on the variability metrics, see Section 5 of Mooley et al. (2016), DOI: 10.3847/0004-637X/818/2/105. Args: df (pd.DataFrame): Input measurements. Must contain columns: id, source, flux_int, flux_int_err, flux_peak, flux_peak_err, has_siblings. Returns: pd.DataFrame: Measurement pairs and 2-epoch metrics. Will contain columns: source - the source ID id_a, id_b - the measurement IDs flux_int_a, flux_int_b - measurement integrated fluxes in mJy flux_int_err_a, flux_int_err_b - measurement integrated flux errors in mJy flux_peak_a, flux_peak_b - measurement peak fluxes in mJy/beam flux_peak_err_a, flux_peak_err_b - measurement peak flux errors in mJy/beam vs_peak, vs_int - variability t-statistic m_peak, m_int - variability modulation index \"\"\" n_cpu = cpu_count () - 1 \"\"\"Create a DataFrame containing all measurement ID combinations per source. Resultant DataFrame will have a MultiIndex([\"source\", RangeIndex]) where \"source\" is the source ID and RangeIndex is an unnamed temporary ID for each measurement pair, unique only together with source. DataFrame will have columns [0, 1], each containing a measurement ID. e.g. 0 1 source 1 0 1 9284 1 1 17597 2 1 26984 3 9284 17597 4 9284 26984 ... ... ... 11105 2 11845 19961 11124 0 3573 12929 1 3573 21994 2 12929 21994 11128 0 6216 23534 \"\"\" measurement_combinations = ( dd . from_pandas ( df , n_cpu ) . groupby ( \"source\" )[ \"id\" ] . apply ( lambda x : pd . DataFrame ( list ( combinations ( x , 2 ))), meta = { 0 : \"i\" , 1 : \"i\" },) . compute ( num_workers = n_cpu , scheduler = \"processes\" ) ) \"\"\"Drop the RangeIndex from the MultiIndex as it isn't required and rename the columns. Example resultant DataFrame: source id_a id_b 0 1 1 9284 1 1 1 17597 2 1 1 26984 3 1 9284 17597 4 1 9284 26984 ... ... ... ... 33640 11105 11845 19961 33641 11124 3573 12929 33642 11124 3573 21994 33643 11124 12929 21994 33644 11128 6216 23534 Where source is the source ID, id_a and id_b are measurement IDs. \"\"\" measurement_combinations = measurement_combinations . reset_index ( level = 1 , drop = True ) . rename ( columns = { 0 : \"id_a\" , 1 : \"id_b\" }) . astype ( int ) . reset_index () # Dask has a tendency to swap which order the measurement pairs are # defined in, even if the dataframe is pre-sorted. We want the pairs to be # in date order (a < b) so the code below corrects any that are not. measurement_combinations = measurement_combinations . join ( df [[ 'source' , 'id' , 'datetime' ]] . set_index ([ 'source' , 'id' ]), on = [ 'source' , 'id_a' ], ) measurement_combinations = measurement_combinations . join ( df [[ 'source' , 'id' , 'datetime' ]] . set_index ([ 'source' , 'id' ]), on = [ 'source' , 'id_b' ], lsuffix = '_a' , rsuffix = '_b' ) to_correct_mask = ( measurement_combinations [ 'datetime_a' ] > measurement_combinations [ 'datetime_b' ] ) if np . any ( to_correct_mask ): logger . debug ( 'Correcting measurement pairs order' ) ( measurement_combinations . loc [ to_correct_mask , 'id_a' ], measurement_combinations . loc [ to_correct_mask , 'id_b' ] ) = np . array ([ measurement_combinations . loc [ to_correct_mask , 'id_b' ] . values , measurement_combinations . loc [ to_correct_mask , 'id_a' ] . values ]) measurement_combinations = measurement_combinations . drop ( [ 'datetime_a' , 'datetime_b' ], axis = 1 ) # add the measurement fluxes and errors association_fluxes = df . set_index ([ \"source\" , \"id\" ])[ [ \"flux_int\" , \"flux_int_err\" , \"flux_peak\" , \"flux_peak_err\" , \"image\" ] ] . rename ( columns = { \"image\" : \"image_name\" }) measurement_combinations = measurement_combinations . join ( association_fluxes , on = [ \"source\" , \"id_a\" ], ) . join ( association_fluxes , on = [ \"source\" , \"id_b\" ], lsuffix = \"_a\" , rsuffix = \"_b\" , ) # calculate 2-epoch metrics measurement_combinations [ \"vs_peak\" ] = calculate_vs_metric ( measurement_combinations . flux_peak_a , measurement_combinations . flux_peak_b , measurement_combinations . flux_peak_err_a , measurement_combinations . flux_peak_err_b , ) measurement_combinations [ \"vs_int\" ] = calculate_vs_metric ( measurement_combinations . flux_int_a , measurement_combinations . flux_int_b , measurement_combinations . flux_int_err_a , measurement_combinations . flux_int_err_b , ) measurement_combinations [ \"m_peak\" ] = calculate_m_metric ( measurement_combinations . flux_peak_a , measurement_combinations . flux_peak_b , ) measurement_combinations [ \"m_int\" ] = calculate_m_metric ( measurement_combinations . flux_int_a , measurement_combinations . flux_int_b , ) return measurement_combinations","title":"calculate_measurement_pair_metrics()"},{"location":"reference/pipeline/pairs/#vast_pipeline.pipeline.pairs.calculate_vs_metric","text":"Calculate the Vs variability metric which is the t-statistic that the provided fluxes are variable. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Parameters: Name Type Description Default flux_a float flux value \"A\". required flux_b float flux value \"B\". required flux_err_a float error of flux_a . required flux_err_b float error of flux_b . required Returns: Type Description float float: the Vs metric for flux values \"A\" and \"B\". Source code in vast_pipeline/pipeline/pairs.py def calculate_vs_metric ( flux_a : float , flux_b : float , flux_err_a : float , flux_err_b : float ) -> float : \"\"\"Calculate the Vs variability metric which is the t-statistic that the provided fluxes are variable. See Section 5 of Mooley et al. (2016) for details, DOI: 10.3847/0004-637X/818/2/105. Args: flux_a (float): flux value \"A\". flux_b (float): flux value \"B\". flux_err_a (float): error of `flux_a`. flux_err_b (float): error of `flux_b`. Returns: float: the Vs metric for flux values \"A\" and \"B\". \"\"\" return ( flux_a - flux_b ) / np . hypot ( flux_err_a , flux_err_b )","title":"calculate_vs_metric()"},{"location":"reference/pipeline/utils/","text":"This module contains utility functions that are used by the pipeline during the processing of a run. add_new_many_to_one_relations ( row ) \u00b6 This handles the relation information being created from the many_to_one function in advanced association. It is a lot simpler than the one_to_many case as it purely just adds the new relations to the relation column, taking into account if it is already a list of relations or not (i.e. no previous relations). Parameters: Name Type Description Default row Series The relation information Series from the association dataframe. Only the columns ['related_skyc1', 'new_relations'] are required. required Returns: Type Description List[int] The new related field for the source in question, containing the appended ids. Source code in vast_pipeline/pipeline/utils.py def add_new_many_to_one_relations ( row : pd . Series ) -> List [ int ]: \"\"\" This handles the relation information being created from the many_to_one function in advanced association. It is a lot simpler than the one_to_many case as it purely just adds the new relations to the relation column, taking into account if it is already a list of relations or not (i.e. no previous relations). Args: row: The relation information Series from the association dataframe. Only the columns ['related_skyc1', 'new_relations'] are required. Returns: The new related field for the source in question, containing the appended ids. \"\"\" out = row [ 'new_relations' ] . copy () if isinstance ( row [ 'related_skyc1' ], list ): out += row [ 'related_skyc1' ] . copy () return out add_new_one_to_many_relations ( row , advanced = False , source_ids = None ) \u00b6 This handles the relation information being created from the one_to_many functions in association. Parameters: Name Type Description Default row Series The relation information Series from the association dataframe. Only the columns ['related_skyc1', 'source_skyc1'] are required for advanced, these are instead called ['related', 'source'] for basic. required advanced bool Whether advanced association is being used which changes the names of the columns involved. False source_ids Optional[pandas.core.frame.DataFrame] A dataframe that contains the other ids to append to related for each original source. +----------------+--------+ | source_skyc1 | 0 | |----------------+--------| | 122 | [5542] | | 254 | [5543] | | 262 | [5544] | | 405 | [5545] | | 656 | [5546] | +----------------+--------+ None Returns: Type Description List[int] The new related field for the source in question, containing the appended ids. Source code in vast_pipeline/pipeline/utils.py def add_new_one_to_many_relations ( row : pd . Series , advanced : bool = False , source_ids : Optional [ pd . DataFrame ] = None ) -> List [ int ]: \"\"\" This handles the relation information being created from the one_to_many functions in association. Args: row: The relation information Series from the association dataframe. Only the columns ['related_skyc1', 'source_skyc1'] are required for advanced, these are instead called ['related', 'source'] for basic. advanced: Whether advanced association is being used which changes the names of the columns involved. source_ids: A dataframe that contains the other ids to append to related for each original source. +----------------+--------+ | source_skyc1 | 0 | |----------------+--------| | 122 | [5542] | | 254 | [5543] | | 262 | [5544] | | 405 | [5545] | | 656 | [5546] | +----------------+--------+ Returns: The new related field for the source in question, containing the appended ids. \"\"\" if source_ids is None : source_ids = pd . DataFrame () related_col = 'related_skyc1' if advanced else 'related' source_col = 'source_skyc1' if advanced else 'source' # this is the not_original case where the original source id is appended. if source_ids . empty : if isinstance ( row [ related_col ], list ): out = row [ related_col ] out . append ( row [ source_col ]) else : out = [ row [ source_col ], ] else : # the original case to append all the new ids. source_ids = source_ids . loc [ row [ source_col ]] . iloc [ 0 ] if isinstance ( row [ related_col ], list ): out = row [ related_col ] + source_ids else : out = source_ids return out add_run_to_img ( pipeline_run , img ) \u00b6 Add a pipeline run to an Image (and corresponding SkyRegion) in the db Parameters: Name Type Description Default pipeline_run Run Pipeline run object you want to add. required img Image Image object you want to add to. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def add_run_to_img ( pipeline_run : Run , img : Image ) -> None : \"\"\" Add a pipeline run to an Image (and corresponding SkyRegion) in the db Args: pipeline_run: Pipeline run object you want to add. img: Image object you want to add to. Returns: None \"\"\" skyreg = img . skyreg # check and add the many to many if not existent if not Image . objects . filter ( id = img . id , run__id = pipeline_run . id ) . exists (): logger . info ( 'Adding %s to image %s ' , pipeline_run , img . name ) img . run . add ( pipeline_run ) if pipeline_run not in skyreg . run . all (): logger . info ( 'Adding %s to sky region %s ' , pipeline_run , skyreg ) skyreg . run . add ( pipeline_run ) backup_parquets ( p_run_path ) \u00b6 Backups up all the existing parquet files in a pipeline run directory. Backups are named with a '.bak' suffix in the pipeline run directory. Parameters: Name Type Description Default p_run_path str The path of the pipeline run where the parquets are stored. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def backup_parquets ( p_run_path : str ) -> None : \"\"\" Backups up all the existing parquet files in a pipeline run directory. Backups are named with a '.bak' suffix in the pipeline run directory. Args: p_run_path: The path of the pipeline run where the parquets are stored. Returns: None \"\"\" parquets = ( glob . glob ( os . path . join ( p_run_path , \"*.parquet\" )) # TODO Remove arrow when arrow files are no longer required. + glob . glob ( os . path . join ( p_run_path , \"*.arrow\" ))) for i in parquets : backup_name = i + '.bak' if os . path . isfile ( backup_name ): logger . debug ( f 'Removing old backup file: { backup_name } .' ) os . remove ( backup_name ) shutil . copyfile ( i , backup_name ) calc_ave_coord ( grp ) \u00b6 Calculates the average coordinate of the grouped by sources dataframe for each unique group, along with defining the image and epoch list for each unique source (group). Parameters: Name Type Description Default grp DataFrame The current group dataframe (unique source) of the grouped by dataframe being acted upon. required Returns: Type Description Series A pandas series containing the average coordinate along with the image and epoch lists. Source code in vast_pipeline/pipeline/utils.py def calc_ave_coord ( grp : pd . DataFrame ) -> pd . Series : \"\"\" Calculates the average coordinate of the grouped by sources dataframe for each unique group, along with defining the image and epoch list for each unique source (group). Args: grp: The current group dataframe (unique source) of the grouped by dataframe being acted upon. Returns: A pandas series containing the average coordinate along with the image and epoch lists. \"\"\" d = {} grp = grp . sort_values ( by = 'datetime' ) d [ 'img_list' ] = grp [ 'image' ] . values . tolist () d [ 'epoch_list' ] = grp [ 'epoch' ] . values . tolist () d [ 'wavg_ra' ] = grp [ 'interim_ew' ] . sum () / grp [ 'weight_ew' ] . sum () d [ 'wavg_dec' ] = grp [ 'interim_ns' ] . sum () / grp [ 'weight_ns' ] . sum () return pd . Series ( d ) check_primary_image ( row ) \u00b6 Checks whether the primary image of the ideal source dataframe is in the image list for the source. Parameters: Name Type Description Default row Series Input dataframe row, with columns ['primary'] and ['img_list']. required Returns: Type Description bool True if primary in image list else False. Source code in vast_pipeline/pipeline/utils.py def check_primary_image ( row : pd . Series ) -> bool : \"\"\" Checks whether the primary image of the ideal source dataframe is in the image list for the source. Args: row: Input dataframe row, with columns ['primary'] and ['img_list']. Returns: True if primary in image list else False. \"\"\" return row [ 'primary' ] in row [ 'img_list' ] create_measurement_pairs_arrow_file ( p_run ) \u00b6 Creates a measurement_pairs.arrow file using the parquet outputs of a pipeline run. Parameters: Name Type Description Default p_run Run Pipeline model instance. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def create_measurement_pairs_arrow_file ( p_run : Run ) -> None : \"\"\" Creates a measurement_pairs.arrow file using the parquet outputs of a pipeline run. Args: p_run: Pipeline model instance. Returns: None \"\"\" logger . info ( 'Creating measurement_pairs.arrow for run %s .' , p_run . name ) measurement_pairs_df = pd . read_parquet ( os . path . join ( p_run . path , 'measurement_pairs.parquet' ) ) logger . debug ( 'Optimising dataframe.' ) measurement_pairs_df = optimize_ints ( optimize_floats ( measurement_pairs_df )) logger . debug ( \"Loading to pyarrow table.\" ) measurement_pairs_df = pa . Table . from_pandas ( measurement_pairs_df ) logger . debug ( \"Exporting to arrow file.\" ) outname = os . path . join ( p_run . path , 'measurement_pairs.arrow' ) local = pa . fs . LocalFileSystem () with local . open_output_stream ( outname ) as file : with pa . RecordBatchFileWriter ( file , measurement_pairs_df . schema ) as writer : writer . write_table ( measurement_pairs_df ) create_measurements_arrow_file ( p_run ) \u00b6 Creates a measurements.arrow file using the parquet outputs of a pipeline run. Parameters: Name Type Description Default p_run Run Pipeline model instance. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def create_measurements_arrow_file ( p_run : Run ) -> None : \"\"\" Creates a measurements.arrow file using the parquet outputs of a pipeline run. Args: p_run: Pipeline model instance. Returns: None \"\"\" logger . info ( 'Creating measurements.arrow for run %s .' , p_run . name ) associations = pd . read_parquet ( os . path . join ( p_run . path , 'associations.parquet' ) ) images = pd . read_parquet ( os . path . join ( p_run . path , 'images.parquet' ) ) m_files = images [ 'measurements_path' ] . tolist () m_files += glob . glob ( os . path . join ( p_run . path , 'forced*.parquet' )) logger . debug ( 'Loading %i files...' , len ( m_files )) measurements = dd . read_parquet ( m_files , engine = 'pyarrow' ) . compute () measurements = measurements . loc [ measurements [ 'id' ] . isin ( associations [ 'meas_id' ] . values ) ] measurements = ( associations . loc [:, [ 'meas_id' , 'source_id' ]] . set_index ( 'meas_id' ) . merge ( measurements , left_index = True , right_on = 'id' ) . rename ( columns = { 'source_id' : 'source' }) ) # drop timezone from datetime for vaex compatibility # TODO: Look to keep the timezone if/when vaex is compatible. measurements [ 'time' ] = measurements [ 'time' ] . dt . tz_localize ( None ) logger . debug ( 'Optimising dataframes.' ) measurements = optimize_ints ( optimize_floats ( measurements )) logger . debug ( \"Loading to pyarrow table.\" ) measurements = pa . Table . from_pandas ( measurements ) logger . debug ( \"Exporting to arrow file.\" ) outname = os . path . join ( p_run . path , 'measurements.arrow' ) local = pa . fs . LocalFileSystem () with local . open_output_stream ( outname ) as file : with pa . RecordBatchFileWriter ( file , measurements . schema ) as writer : writer . write_table ( measurements ) create_temp_config_file ( p_run_path ) \u00b6 Creates the temp config file which is saved at the beginning of each run. It is to avoid issues created by users changing the config while the run is running. Parameters: Name Type Description Default p_run_path str The path of the pipeline run of the config to be copied. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def create_temp_config_file ( p_run_path : str ) -> None : \"\"\" Creates the temp config file which is saved at the beginning of each run. It is to avoid issues created by users changing the config while the run is running. Args: p_run_path: The path of the pipeline run of the config to be copied. Returns: None \"\"\" config_name = 'config.yaml' temp_config_name = 'config_temp.yaml' shutil . copyfile ( os . path . join ( p_run_path , config_name ), os . path . join ( p_run_path , temp_config_name ) ) cross_join ( left , right ) \u00b6 A convenience function to merge two dataframes. Parameters: Name Type Description Default left DataFrame The base pandas DataFrame to merge. required right DataFrame The pandas DataFrame to merge to the left. required Returns: Type Description DataFrame The resultant merged DataFrame. Source code in vast_pipeline/pipeline/utils.py def cross_join ( left : pd . DataFrame , right : pd . DataFrame ) -> pd . DataFrame : \"\"\" A convenience function to merge two dataframes. Args: left: The base pandas DataFrame to merge. right: The pandas DataFrame to merge to the left. Returns: The resultant merged DataFrame. \"\"\" return ( left . assign ( key = 1 ) . merge ( right . assign ( key = 1 ), on = 'key' ) . drop ( 'key' , axis = 1 ) ) get_create_img ( band_id , image ) \u00b6 Function to fetch or create the Image and Sky Region objects for an image. Parameters: Name Type Description Default band_id int The integer database id value of the frequency band of the image. required image SelavyImage The image object. required Returns: Type Description Tuple[vast_pipeline.models.Image, bool] The resulting image django ORM object, and a bool value denoting if the image already existed in the database. Source code in vast_pipeline/pipeline/utils.py def get_create_img ( band_id : int , image : SelavyImage ) -> Tuple [ Image , bool ]: \"\"\" Function to fetch or create the Image and Sky Region objects for an image. Args: band_id: The integer database id value of the frequency band of the image. image: The image object. Returns: The resulting image django ORM object, and a bool value denoting if the image already existed in the database. \"\"\" images = Image . objects . filter ( name__exact = image . name ) exists = images . exists () if exists : img : Image = images . get () # Add background path if not originally provided if image . background_path and not img . background_path : img . background_path = image . background_path img . save () else : # at this stage, measurement parquet file is not created but # assume location img_folder_name = image . name . replace ( '.' , '_' ) measurements_path = os . path . join ( settings . PIPELINE_WORKING_DIR , 'images' , img_folder_name , 'measurements.parquet' ) img = Image ( band_id = band_id , measurements_path = measurements_path ) # set the attributes and save the image, # by selecting only valid (not hidden) attributes # FYI attributs and/or method starting with _ are hidden # and with __ can't be modified/called for fld in img . _meta . get_fields (): if getattr ( fld , 'attname' , None ) and ( getattr ( image , fld . attname , None ) is not None ): setattr ( img , fld . attname , getattr ( image , fld . attname )) img . rms_median , img . rms_min , img . rms_max = get_rms_noise_image_values ( img . noise_path ) # get create the sky region and associate with image img . skyreg = get_create_skyreg ( img ) img . save () return ( img , exists ) get_create_img_band ( image ) \u00b6 Return the existing Band row for the given FitsImage. An image is considered to belong to a band if its frequency is within some tolerance of the band's frequency. Returns a Band row or None if no matching band. Parameters: Name Type Description Default image FitsImage The image Django ORM object. required Returns: Type Description Band The band Django ORM object. Source code in vast_pipeline/pipeline/utils.py def get_create_img_band ( image : FitsImage ) -> Band : ''' Return the existing Band row for the given FitsImage. An image is considered to belong to a band if its frequency is within some tolerance of the band's frequency. Returns a Band row or None if no matching band. Args: image: The image Django ORM object. Returns: The band Django ORM object. ''' # For now we match bands using the central frequency. # This assumes that every band has a unique frequency, # which is true for the data we've used so far. freq = int ( image . freq_eff * 1.e-6 ) freq_band = int ( image . freq_bw * 1.e-6 ) # TODO: refine the band query for band in Band . objects . all (): diff = abs ( freq - band . frequency ) / float ( band . frequency ) if diff < 0.02 : return band # no band has been found so create it band = Band ( name = str ( freq ), frequency = freq , bandwidth = freq_band ) logger . info ( 'Adding new frequency band: %s ' , band ) band . save () return band get_create_p_run ( name , path , description = None , user = None ) \u00b6 Get or create a pipeline run in db, return the run django object and a flag True/False if has been created or already exists. Parameters: Name Type Description Default name str The name of the pipeline run. required path str The system path to the pipeline run folder which contains the configuration file and where outputs will be saved. required description str An optional description of the pipeline run. None user User The Django user that launched the pipeline run. None Returns: Type Description Tuple[vast_pipeline.models.Run, bool] The pipeline run object and a boolean object representing whether the pipeline run already existed ('True') or not ('False'). Source code in vast_pipeline/pipeline/utils.py def get_create_p_run ( name : str , path : str , description : str = None , user : User = None ) -> Tuple [ Run , bool ]: ''' Get or create a pipeline run in db, return the run django object and a flag True/False if has been created or already exists. Args: name: The name of the pipeline run. path: The system path to the pipeline run folder which contains the configuration file and where outputs will be saved. description: An optional description of the pipeline run. user: The Django user that launched the pipeline run. Returns: The pipeline run object and a boolean object representing whether the pipeline run already existed ('True') or not ('False'). ''' p_run = Run . objects . filter ( name__exact = name ) if p_run : return p_run . get (), True description = \"\" if description is None else description p_run = Run ( name = name , description = description , path = path ) if user : p_run . user = user p_run . save () return p_run , False get_create_skyreg ( image ) \u00b6 This creates a Sky Region object in Django ORM given the related image object. Parameters: Name Type Description Default image Image The image Django ORM object. required Returns: Type Description SkyRegion The sky region Django ORM object. Source code in vast_pipeline/pipeline/utils.py def get_create_skyreg ( image : Image ) -> SkyRegion : ''' This creates a Sky Region object in Django ORM given the related image object. Args: image: The image Django ORM object. Returns: The sky region Django ORM object. ''' # In the calculations below, it is assumed the image has square # pixels (this pipeline has been designed for ASKAP images, so it # should always be square). It will likely give wrong results if not skyregions = SkyRegion . objects . filter ( centre_ra = image . ra , centre_dec = image . dec , xtr_radius = image . fov_bmin ) if skyregions : skyr = skyregions . get () logger . info ( 'Found sky region %s ' , skyr ) else : x , y , z = eq_to_cart ( image . ra , image . dec ) skyr = SkyRegion ( centre_ra = image . ra , centre_dec = image . dec , width_ra = image . physical_bmin , width_dec = image . physical_bmaj , xtr_radius = image . fov_bmin , x = x , y = y , z = z , ) skyr . save () logger . info ( 'Created sky region %s ' , skyr ) return skyr get_eta_metric ( row , df , peak = False ) \u00b6 Calculates the eta variability metric of a source. Works on the grouped by dataframe using the fluxes of the associated measurements. Parameters: Name Type Description Default row Dict[str, float] Dictionary containing statistics for the current source. required df DataFrame The grouped by sources dataframe of the measurements containing all the flux and flux error information, required peak bool Whether to use peak_flux for the calculation. If False then the integrated flux is used. False Returns: Type Description float The calculated eta value. Source code in vast_pipeline/pipeline/utils.py def get_eta_metric ( row : Dict [ str , float ], df : pd . DataFrame , peak : bool = False ) -> float : ''' Calculates the eta variability metric of a source. Works on the grouped by dataframe using the fluxes of the associated measurements. Args: row: Dictionary containing statistics for the current source. df: The grouped by sources dataframe of the measurements containing all the flux and flux error information, peak: Whether to use peak_flux for the calculation. If False then the integrated flux is used. Returns: The calculated eta value. ''' if row [ 'n_meas' ] == 1 : return 0. suffix = 'peak' if peak else 'int' weights = 1. / df [ f 'flux_ { suffix } _err' ] . values ** 2 fluxes = df [ f 'flux_ { suffix } ' ] . values eta = ( row [ 'n_meas' ] / ( row [ 'n_meas' ] - 1 )) * ( ( weights * fluxes ** 2 ) . mean () - ( ( weights * fluxes ) . mean () ** 2 / weights . mean () ) ) return eta get_image_list_diff ( row ) \u00b6 Calculate the difference between the ideal coverage image list of a source and the actual observed image list. Also checks whether an epoch does in fact contain a detection but is not in the expected 'ideal' image for that epoch. Parameters: Name Type Description Default row Series The row from the sources dataframe that is being iterated over. required Returns: Type Description Union[List[str], int] A list of the images missing from the observed image list. Will be returned as '-1' integer value if there are no missing images. Source code in vast_pipeline/pipeline/utils.py def get_image_list_diff ( row : pd . Series ) -> Union [ List [ str ], int ]: \"\"\" Calculate the difference between the ideal coverage image list of a source and the actual observed image list. Also checks whether an epoch does in fact contain a detection but is not in the expected 'ideal' image for that epoch. Args: row: The row from the sources dataframe that is being iterated over. Returns: A list of the images missing from the observed image list. Will be returned as '-1' integer value if there are no missing images. \"\"\" out = list ( filter ( lambda arg : arg not in row [ 'img_list' ], row [ 'skyreg_img_list' ]) ) # set empty list to -1 if not out : return - 1 # Check that an epoch has not already been seen (just not in the 'ideal' # image) out_epochs = [ row [ 'skyreg_epoch' ][ pair [ 0 ]] for pair in enumerate ( row [ 'skyreg_img_list' ] ) if pair [ 1 ] in out ] out = [ out [ pair [ 0 ]] for pair in enumerate ( out_epochs ) if pair [ 1 ] not in row [ 'epoch_list' ] ] if not out : return - 1 return out get_names_and_epochs ( grp ) \u00b6 Convenience function to group together the image names, epochs and datetimes into one list object which is then returned as a pandas series. This is necessary for easier processing in the ideal coverage analysis. Parameters: Name Type Description Default grp DataFrame A group from the grouped by sources DataFrame. required Returns: Type Description Series Pandas series containing the list object that contains the lists of the image names, epochs and datetimes. Source code in vast_pipeline/pipeline/utils.py def get_names_and_epochs ( grp : pd . DataFrame ) -> pd . Series : \"\"\" Convenience function to group together the image names, epochs and datetimes into one list object which is then returned as a pandas series. This is necessary for easier processing in the ideal coverage analysis. Args: grp: A group from the grouped by sources DataFrame. Returns: Pandas series containing the list object that contains the lists of the image names, epochs and datetimes. \"\"\" d = {} d [ 'skyreg_img_epoch_list' ] = [[[ x , ], y , z ] for x , y , z in zip ( grp [ 'name' ] . values . tolist (), grp [ 'epoch' ] . values . tolist (), grp [ 'datetime' ] . values . tolist () )] return pd . Series ( d ) get_parallel_assoc_image_df ( images , skyregion_groups ) \u00b6 Merge the sky region groups with the images and skyreg_ids. Parameters: Name Type Description Default images List[vast_pipeline.models.Image] A list of the Image objects. required skyregion_groups DataFrame The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ required Returns: Type Description DataFrame Dataframe containing the merged images and skyreg_id and skyreg_group. +----+-------------------------------+-------------+----------------+ | | image | skyreg_id | skyreg_group | |----+-------------------------------+-------------+----------------| | 0 | VAST_2118+00A.EPOCH01.I.fits | 2 | 1 | | 1 | VAST_2118-06A.EPOCH01.I.fits | 3 | 1 | | 2 | VAST_0127-73A.EPOCH01.I.fits | 1 | 2 | | 3 | VAST_2118-06A.EPOCH03x.I.fits | 3 | 1 | | 4 | VAST_2118-06A.EPOCH02.I.fits | 3 | 1 | | 5 | VAST_2118-06A.EPOCH05x.I.fits | 3 | 1 | | 6 | VAST_2118-06A.EPOCH06x.I.fits | 3 | 1 | | 7 | VAST_0127-73A.EPOCH08.I.fits | 1 | 2 | +----+-------------------------------+-------------+----------------+ Source code in vast_pipeline/pipeline/utils.py def get_parallel_assoc_image_df ( images : List [ Image ], skyregion_groups : pd . DataFrame ) -> pd . DataFrame : \"\"\" Merge the sky region groups with the images and skyreg_ids. Args: images: A list of the Image objects. skyregion_groups: The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ Returns: Dataframe containing the merged images and skyreg_id and skyreg_group. +----+-------------------------------+-------------+----------------+ | | image | skyreg_id | skyreg_group | |----+-------------------------------+-------------+----------------| | 0 | VAST_2118+00A.EPOCH01.I.fits | 2 | 1 | | 1 | VAST_2118-06A.EPOCH01.I.fits | 3 | 1 | | 2 | VAST_0127-73A.EPOCH01.I.fits | 1 | 2 | | 3 | VAST_2118-06A.EPOCH03x.I.fits | 3 | 1 | | 4 | VAST_2118-06A.EPOCH02.I.fits | 3 | 1 | | 5 | VAST_2118-06A.EPOCH05x.I.fits | 3 | 1 | | 6 | VAST_2118-06A.EPOCH06x.I.fits | 3 | 1 | | 7 | VAST_0127-73A.EPOCH08.I.fits | 1 | 2 | +----+-------------------------------+-------------+----------------+ \"\"\" skyreg_ids = [ i . skyreg_id for i in images ] images_df = pd . DataFrame ({ 'image_dj' : images , 'skyreg_id' : skyreg_ids , }) images_df = images_df . merge ( skyregion_groups , how = 'left' , left_on = 'skyreg_id' , right_index = True ) images_df [ 'image_name' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . name ) images_df [ 'image_datetime' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . datetime ) return images_df get_rms_noise_image_values ( rms_path ) \u00b6 Open the RMS noise FITS file and compute the median, max and min rms values to be added to the image model and then used in the calculations. Parameters: Name Type Description Default rms_path str The system path to the RMS FITS image. required Returns: Type Description Tuple[float, float, float] The median, minimum and maximum values of the RMS image. Exceptions: Type Description IOError Raised when the RMS FITS file cannot be found. Source code in vast_pipeline/pipeline/utils.py def get_rms_noise_image_values ( rms_path : str ) -> Tuple [ float , float , float ]: ''' Open the RMS noise FITS file and compute the median, max and min rms values to be added to the image model and then used in the calculations. Args: rms_path: The system path to the RMS FITS image. Returns: The median, minimum and maximum values of the RMS image. Raises: IOError: Raised when the RMS FITS file cannot be found. ''' logger . debug ( 'Extracting Image RMS values from Noise file...' ) med_val = min_val = max_val = 0. try : with fits . open ( rms_path ) as f : data = f [ 0 ] . data data = data [ np . logical_not ( np . isnan ( data ))] data = data [ data != 0 ] med_val = np . median ( data ) * 1e+3 min_val = np . min ( data ) * 1e+3 max_val = np . max ( data ) * 1e+3 del data except Exception : raise IOError ( f 'Could not read this RMS FITS file: { rms_path } ' ) return med_val , min_val , max_val get_src_skyregion_merged_df ( sources_df , images_df , skyreg_df ) \u00b6 Analyses the current sources_df to determine what the 'ideal coverage' for each source should be. In other words, what images is the source missing in when it should have been seen. Parameters: Name Type Description Default sources_df DataFrame The output of the association step containing the measurements associated into sources. required images_df DataFrame Contains the images of the pipeline run. I.e. all image objects for the run loaded into a dataframe. required skyreg_df DataFrame Contains the sky regions of the pipeline run. I.e. all sky region objects for the run loaded into a dataframe. required Returns: Type Description DataFrame DataFrame containing missing image information. Output format: +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | | 1290 | ['VAST_0127-73A.EPOCH01.I.fits'] | 20.8455 | -76.8269 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+------------------------------+ img_diff | primary | ----------------------------------+------------------------------+ ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ----------------------------------+------------------------------+ ------------------------------+--------------+ detection | in_primary | ------------------------------+--------------| VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | ------------------------------+--------------+ Source code in vast_pipeline/pipeline/utils.py def get_src_skyregion_merged_df ( sources_df : pd . DataFrame , images_df : pd . DataFrame , skyreg_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Analyses the current sources_df to determine what the 'ideal coverage' for each source should be. In other words, what images is the source missing in when it should have been seen. Args: sources_df: The output of the association step containing the measurements associated into sources. images_df: Contains the images of the pipeline run. I.e. all image objects for the run loaded into a dataframe. skyreg_df: Contains the sky regions of the pipeline run. I.e. all sky region objects for the run loaded into a dataframe. Returns: DataFrame containing missing image information. Output format: +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | | 1290 | ['VAST_0127-73A.EPOCH01.I.fits'] | 20.8455 | -76.8269 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+------------------------------+ img_diff | primary | ----------------------------------+------------------------------+ ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ----------------------------------+------------------------------+ ------------------------------+--------------+ detection | in_primary | ------------------------------+--------------| VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | ------------------------------+--------------+ \"\"\" logger . info ( \"Creating ideal source coverage df...\" ) merged_timer = StopWatch () skyreg_df = skyreg_df . drop ( [ 'x' , 'y' , 'z' , 'width_ra' , 'width_dec' ], axis = 1 ) images_df [ 'name' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . name ) images_df [ 'datetime' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . datetime ) skyreg_df = skyreg_df . join ( pd . DataFrame ( images_df . groupby ( 'skyreg_id' ) . apply ( get_names_and_epochs ) ), on = 'id' ) sources_df = sources_df . sort_values ( by = 'datetime' ) # calculate some metrics on sources # compute only some necessary metrics in the groupby timer = StopWatch () srcs_df = parallel_groupby_coord ( sources_df ) logger . debug ( 'Groupby-apply time: %.2f seconds' , timer . reset ()) del sources_df # crossmatch sources with sky regions up to the max sky region radius skyreg_coords = SkyCoord ( ra = skyreg_df . centre_ra , dec = skyreg_df . centre_dec , unit = \"deg\" ) srcs_coords = SkyCoord ( ra = srcs_df . wavg_ra , dec = srcs_df . wavg_dec , unit = \"deg\" ) skyreg_idx , srcs_idx , sep , _ = srcs_coords . search_around_sky ( skyreg_coords , skyreg_df . xtr_radius . max () * u . deg ) skyreg_df = skyreg_df . drop ( columns = [ \"centre_ra\" , \"centre_dec\" ]) . set_index ( \"id\" ) # select rows where separation is less than sky region radius # drop not more useful columns and groupby source id # compute list of images src_skyrg_df = ( pd . DataFrame ( { \"source\" : srcs_df . iloc [ srcs_idx ] . index , \"id\" : skyreg_df . iloc [ skyreg_idx ] . index , \"sep\" : sep . to ( \"deg\" ) . value , } ) . merge ( skyreg_df , left_on = \"id\" , right_index = True ) . query ( \"sep < xtr_radius\" ) . drop ( columns = [ \"id\" , \"xtr_radius\" ]) . explode ( \"skyreg_img_epoch_list\" ) ) del skyreg_df src_skyrg_df [ [ 'skyreg_img_list' , 'skyreg_epoch' , 'skyreg_datetime' ] ] = pd . DataFrame ( src_skyrg_df [ 'skyreg_img_epoch_list' ] . tolist (), index = src_skyrg_df . index ) src_skyrg_df = src_skyrg_df . drop ( 'skyreg_img_epoch_list' , axis = 1 ) src_skyrg_df = ( src_skyrg_df . sort_values ( [ 'source' , 'sep' ] ) . drop_duplicates ([ 'source' , 'skyreg_epoch' ]) . sort_values ( by = 'skyreg_datetime' ) . drop ( [ 'sep' , 'skyreg_datetime' ], axis = 1 ) ) # annoyingly epoch needs to be not a list to drop duplicates # but then we need to sum the epochs into a list src_skyrg_df [ 'skyreg_epoch' ] = src_skyrg_df [ 'skyreg_epoch' ] . apply ( lambda x : [ x , ] ) src_skyrg_df = ( src_skyrg_df . groupby ( 'source' ) . sum ( numeric_only = False ) # sum because we need to preserve order ) # merge into main df and compare the images srcs_df = srcs_df . merge ( src_skyrg_df , left_index = True , right_index = True ) del src_skyrg_df srcs_df [ 'img_diff' ] = srcs_df [ [ 'img_list' , 'skyreg_img_list' , 'epoch_list' , 'skyreg_epoch' ] ] . apply ( get_image_list_diff , axis = 1 ) srcs_df = srcs_df . loc [ srcs_df [ 'img_diff' ] != - 1 ] srcs_df = srcs_df . drop ( [ 'epoch_list' , 'skyreg_epoch' ], axis = 1 ) srcs_df [ 'primary' ] = srcs_df [ 'skyreg_img_list' ] . apply ( lambda x : x [ 0 ]) srcs_df [ 'detection' ] = srcs_df [ 'img_list' ] . apply ( lambda x : x [ 0 ]) srcs_df [ 'in_primary' ] = srcs_df [ [ 'primary' , 'img_list' ] ] . apply ( check_primary_image , axis = 1 ) srcs_df = srcs_df . drop ([ 'img_list' , 'skyreg_img_list' , 'primary' ], axis = 1 ) logger . info ( 'Ideal source coverage time: %.2f seconds' , merged_timer . reset () ) return srcs_df group_skyregions ( df ) \u00b6 Logic to group sky regions into overlapping groups. Returns a dataframe containing the sky region id as the index and a column containing a list of the sky region group number it belongs to. Parameters: Name Type Description Default df DataFrame A dataframe containing all the sky regions of the run. Only the 'id', 'centre_ra', 'centre_dec' and 'xtr_radius' columns are required. +------+-------------+--------------+--------------+ | id | centre_ra | centre_dec | xtr_radius | |------+-------------+--------------+--------------| | 2 | 319.652 | 0.0030765 | 6.72488 | | 3 | 319.652 | -6.2989 | 6.7401 | | 1 | 21.8361 | -73.121 | 7.24662 | +------+-------------+--------------+--------------+ required Returns: Type Description DataFrame The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ Source code in vast_pipeline/pipeline/utils.py def group_skyregions ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Logic to group sky regions into overlapping groups. Returns a dataframe containing the sky region id as the index and a column containing a list of the sky region group number it belongs to. Args: df: A dataframe containing all the sky regions of the run. Only the 'id', 'centre_ra', 'centre_dec' and 'xtr_radius' columns are required. +------+-------------+--------------+--------------+ | id | centre_ra | centre_dec | xtr_radius | |------+-------------+--------------+--------------| | 2 | 319.652 | 0.0030765 | 6.72488 | | 3 | 319.652 | -6.2989 | 6.7401 | | 1 | 21.8361 | -73.121 | 7.24662 | +------+-------------+--------------+--------------+ Returns: The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ \"\"\" sr_coords = SkyCoord ( df [ 'centre_ra' ], df [ 'centre_dec' ], unit = ( u . deg , u . deg ) ) df = df . set_index ( 'id' ) results = df . apply ( _get_skyregion_relations , args = ( sr_coords , df . index ), axis = 1 ) skyreg_groups : Dict [ int , List [ Any ]] = {} master_done = [] # keep track of all checked ids in master done for skyreg_id , neighbours in results . iteritems (): if skyreg_id not in master_done : local_done = [] # a local done list for the sky region group. # add the current skyreg_id to both master and local done. master_done . append ( skyreg_id ) local_done . append ( skyreg_id ) # Define the new group number based on the existing ones. skyreg_group = len ( skyreg_groups ) + 1 # Add all the ones that we know are neighbours that were obtained # from _get_skyregion_relations. skyreg_groups [ skyreg_group ] = list ( neighbours ) # Now the sky region group is extended out to include all those sky # regions that overlap with the neighbours. # Each neighbour is checked and added to the local done list. # Checked means that for each neighbour, it's own neighbours are # added to the current group if not in already. # When the local done is equal to the skyreg group we know that # we have exhausted all possible neighbours and that results in a # sky region group. while sorted ( local_done ) != sorted ( skyreg_groups [ skyreg_group ]): # Loop over each neighbour for other_skyreg_id in skyreg_groups [ skyreg_group ]: # If we haven't checked this neighbour locally proceed. if other_skyreg_id not in local_done : # Add it to the local checked. local_done . append ( other_skyreg_id ) # Get the neighbours neighbour and add these. new_vals = results . loc [ other_skyreg_id ] for k in new_vals : if k not in skyreg_groups [ skyreg_group ]: skyreg_groups [ skyreg_group ] . append ( k ) # Reached the end of the group so append all to the master # done list for j in skyreg_groups [ skyreg_group ]: master_done . append ( j ) else : # continue if already placed in group continue # flip the dictionary around skyreg_group_ids = {} for i in skyreg_groups : for j in skyreg_groups [ i ]: skyreg_group_ids [ j ] = i skyreg_group_ids = pd . DataFrame . from_dict ( skyreg_group_ids , orient = 'index' ) . rename ( columns = { 0 : 'skyreg_group' }) return skyreg_group_ids groupby_funcs ( df ) \u00b6 Performs calculations on the unique sources to get the lightcurve properties. Works on the grouped by source dataframe. Parameters: Name Type Description Default df DataFrame The current iteration dataframe of the grouped by sources dataframe. required Returns: Type Description Series Pandas series containing the calculated metrics of the source. Source code in vast_pipeline/pipeline/utils.py def groupby_funcs ( df : pd . DataFrame ) -> pd . Series : ''' Performs calculations on the unique sources to get the lightcurve properties. Works on the grouped by source dataframe. Args: df: The current iteration dataframe of the grouped by sources dataframe. Returns: Pandas series containing the calculated metrics of the source. ''' # calculated average ra, dec, fluxes and metrics d = {} d [ 'img_list' ] = df [ 'image' ] . values . tolist () d [ 'n_meas_forced' ] = df [ 'forced' ] . sum () d [ 'n_meas' ] = df [ 'id' ] . count () d [ 'n_meas_sel' ] = d [ 'n_meas' ] - d [ 'n_meas_forced' ] d [ 'n_sibl' ] = df [ 'has_siblings' ] . sum () if d [ 'n_meas_forced' ] > 0 : non_forced_sel = ~ df [ 'forced' ] d [ 'wavg_ra' ] = ( df . loc [ non_forced_sel , 'interim_ew' ] . sum () / df . loc [ non_forced_sel , 'weight_ew' ] . sum () ) d [ 'wavg_dec' ] = ( df . loc [ non_forced_sel , 'interim_ns' ] . sum () / df . loc [ non_forced_sel , 'weight_ns' ] . sum () ) d [ 'avg_compactness' ] = df . loc [ non_forced_sel , 'compactness' ] . mean () d [ 'min_snr' ] = df . loc [ non_forced_sel , 'snr' ] . min () d [ 'max_snr' ] = df . loc [ non_forced_sel , 'snr' ] . max () else : d [ 'wavg_ra' ] = df [ 'interim_ew' ] . sum () / df [ 'weight_ew' ] . sum () d [ 'wavg_dec' ] = df [ 'interim_ns' ] . sum () / df [ 'weight_ns' ] . sum () d [ 'avg_compactness' ] = df [ 'compactness' ] . mean () d [ 'min_snr' ] = df [ 'snr' ] . min () d [ 'max_snr' ] = df [ 'snr' ] . max () d [ 'wavg_uncertainty_ew' ] = 1. / np . sqrt ( df [ 'weight_ew' ] . sum ()) d [ 'wavg_uncertainty_ns' ] = 1. / np . sqrt ( df [ 'weight_ns' ] . sum ()) for col in [ 'avg_flux_int' , 'avg_flux_peak' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . mean () for col in [ 'max_flux_peak' , 'max_flux_int' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . max () for col in [ 'min_flux_peak' , 'min_flux_int' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . min () for col in [ 'min_flux_peak_isl_ratio' , 'min_flux_int_isl_ratio' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . min () for col in [ 'flux_int' , 'flux_peak' ]: d [ f ' { col } _sq' ] = ( df [ col ] ** 2 ) . mean () d [ 'v_int' ] = df [ 'flux_int' ] . std () / df [ 'flux_int' ] . mean () d [ 'v_peak' ] = df [ 'flux_peak' ] . std () / df [ 'flux_peak' ] . mean () d [ 'eta_int' ] = get_eta_metric ( d , df ) d [ 'eta_peak' ] = get_eta_metric ( d , df , peak = True ) # remove not used cols for col in [ 'flux_int_sq' , 'flux_peak_sq' ]: d . pop ( col ) # get unique related sources list_uniq_related = list ( set ( chain . from_iterable ( lst for lst in df [ 'related' ] if isinstance ( lst , list ) ) )) d [ 'related_list' ] = list_uniq_related if list_uniq_related else - 1 return pd . Series ( d ) . fillna ( value = { \"v_int\" : 0.0 , \"v_peak\" : 0.0 }) parallel_groupby ( df ) \u00b6 Performs the parallel source dataframe operations to calculate the source metrics using Dask and returns the resulting dataframe. Parameters: Name Type Description Default df DataFrame The sources dataframe produced by the previous pipeline stages. required Returns: Type Description DataFrame The source dataframe with the calculated metric columns. Source code in vast_pipeline/pipeline/utils.py def parallel_groupby ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Performs the parallel source dataframe operations to calculate the source metrics using Dask and returns the resulting dataframe. Args: df: The sources dataframe produced by the previous pipeline stages. Returns: The source dataframe with the calculated metric columns. \"\"\" col_dtype = { 'img_list' : 'O' , 'n_meas_forced' : 'i' , 'n_meas' : 'i' , 'n_meas_sel' : 'i' , 'n_sibl' : 'i' , 'wavg_ra' : 'f' , 'wavg_dec' : 'f' , 'avg_compactness' : 'f' , 'min_snr' : 'f' , 'max_snr' : 'f' , 'wavg_uncertainty_ew' : 'f' , 'wavg_uncertainty_ns' : 'f' , 'avg_flux_int' : 'f' , 'avg_flux_peak' : 'f' , 'max_flux_peak' : 'f' , 'max_flux_int' : 'f' , 'min_flux_peak' : 'f' , 'min_flux_int' : 'f' , 'min_flux_peak_isl_ratio' : 'f' , 'min_flux_int_isl_ratio' : 'f' , 'v_int' : 'f' , 'v_peak' : 'f' , 'eta_int' : 'f' , 'eta_peak' : 'f' , 'related_list' : 'O' } n_cpu = cpu_count () - 1 out = dd . from_pandas ( df , n_cpu ) out = ( out . groupby ( 'source' ) . apply ( groupby_funcs , meta = col_dtype ) . compute ( num_workers = n_cpu , scheduler = 'processes' ) ) out [ 'n_rel' ] = out [ 'related_list' ] . apply ( lambda x : 0 if x == - 1 else len ( x )) return out parallel_groupby_coord ( df ) \u00b6 This function uses Dask to perform the average coordinate and unique image and epoch lists calculation. The result from the Dask compute is returned which is a dataframe containing the results for each source. Parameters: Name Type Description Default df DataFrame The sources dataframe produced by the pipeline. required Returns: Type Description DataFrame The resulting average coordinate values and unique image and epoch lists for each unique source (group). Source code in vast_pipeline/pipeline/utils.py def parallel_groupby_coord ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" This function uses Dask to perform the average coordinate and unique image and epoch lists calculation. The result from the Dask compute is returned which is a dataframe containing the results for each source. Args: df: The sources dataframe produced by the pipeline. Returns: The resulting average coordinate values and unique image and epoch lists for each unique source (group). \"\"\" col_dtype = { 'img_list' : 'O' , 'epoch_list' : 'O' , 'wavg_ra' : 'f' , 'wavg_dec' : 'f' , } n_cpu = cpu_count () - 1 out = dd . from_pandas ( df , n_cpu ) out = ( out . groupby ( 'source' ) . apply ( calc_ave_coord , meta = col_dtype ) . compute ( num_workers = n_cpu , scheduler = 'processes' ) ) return out prep_skysrc_df ( images , perc_error = 0.0 , duplicate_limit = None , ini_df = False ) \u00b6 Initialise the source dataframe to use in association logic by reading the measurement parquet file and creating columns. When epoch based association is used it will also remove duplicate measurements from the list of sources. Parameters: Name Type Description Default images List[vast_pipeline.models.Image] A list holding the Image objects of the images to load measurements for. required perc_error float A percentage flux error to apply to the flux errors of the measurements. Defaults to 0. 0.0 duplicate_limit Optional[astropy.coordinates.angles.Angle] The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used in the 'remove_duplicate_measurements' function (usual ASKAP pixel size). None ini_df bool Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. False Returns: Type Description DataFrame The measurements of the image(s) with some extra values set ready for association and duplicates removed if necessary. Source code in vast_pipeline/pipeline/utils.py def prep_skysrc_df ( images : List [ Image ], perc_error : float = 0. , duplicate_limit : Optional [ Angle ] = None , ini_df : bool = False ) -> pd . DataFrame : ''' Initialise the source dataframe to use in association logic by reading the measurement parquet file and creating columns. When epoch based association is used it will also remove duplicate measurements from the list of sources. Args: images: A list holding the Image objects of the images to load measurements for. perc_error: A percentage flux error to apply to the flux errors of the measurements. Defaults to 0. duplicate_limit: The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used in the 'remove_duplicate_measurements' function (usual ASKAP pixel size). ini_df: Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. Returns: The measurements of the image(s) with some extra values set ready for association and duplicates removed if necessary. ''' cols = [ 'id' , 'ra' , 'uncertainty_ew' , 'weight_ew' , 'dec' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' ] df = _load_measurements ( images [ 0 ], cols , ini_df = ini_df ) if len ( images ) > 1 : for img in images [ 1 :]: df = df . append ( _load_measurements ( img , cols , df . source . max (), ini_df = ini_df ), ignore_index = True ) df = remove_duplicate_measurements ( df , dup_lim = duplicate_limit , ini_df = ini_df ) df = df . drop ( 'dist_from_centre' , axis = 1 ) if perc_error != 0.0 : logger . info ( 'Correcting flux errors with config error setting...' ) for col in [ 'flux_int' , 'flux_peak' ]: df [ f ' { col } _err' ] = np . hypot ( df [ f ' { col } _err' ] . values , perc_error * df [ col ] . values ) return df reconstruct_associtaion_dfs ( images_df_done , previous_parquet_paths ) \u00b6 This function is used with add image mode and performs the necessary manipulations to reconstruct the sources_df and skyc1_srcs required by association. Parameters: Name Type Description Default images_df_done DataFrame The images_df output from the existing run (from the parquet). required previous_parquet_paths Dict[str, str] Dictionary that contains the paths for the previous run parquet files. Keys are 'images', 'associations', 'sources', 'relations' and 'measurement_pairs'. required Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] The reconstructed sources_df and skyc1_srs dataframes. Source code in vast_pipeline/pipeline/utils.py def reconstruct_associtaion_dfs ( images_df_done : pd . DataFrame , previous_parquet_paths : Dict [ str , str ] ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" This function is used with add image mode and performs the necessary manipulations to reconstruct the sources_df and skyc1_srcs required by association. Args: images_df_done: The images_df output from the existing run (from the parquet). previous_parquet_paths: Dictionary that contains the paths for the previous run parquet files. Keys are 'images', 'associations', 'sources', 'relations' and 'measurement_pairs'. Returns: The reconstructed sources_df and skyc1_srs dataframes. \"\"\" prev_associations = pd . read_parquet ( previous_parquet_paths [ 'associations' ]) # Get the parquet paths from the image objects img_meas_paths = ( images_df_done [ 'image_dj' ] . apply ( lambda x : x . measurements_path ) . to_list () ) # Obtain the pipeline run path in order to fetch forced measurements. run_path = previous_parquet_paths [ 'sources' ] . replace ( 'sources.parquet.bak' , '' ) # Get the forced measurement paths. img_fmeas_paths = [] for i in images_df_done . image_name . values : forced_parquet = os . path . join ( run_path , \"forced_measurements_ {} .parquet\" . format ( i . replace ( \".\" , \"_\" ) ) ) if os . path . isfile ( forced_parquet ): img_fmeas_paths . append ( forced_parquet ) # Create union of paths. img_meas_paths += img_fmeas_paths # Define the columns that are required cols = [ 'id' , 'ra' , 'uncertainty_ew' , 'weight_ew' , 'dec' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image_id' , 'time' , ] # Open all the parquets logger . debug ( \"Opening all measurement parquet files to use in reconstruction...\" ) measurements = pd . concat ( [ pd . read_parquet ( f , columns = cols ) for f in img_meas_paths ] ) # Create mask to drop measurements for epoch mode (epoch based mode). measurements_mask = measurements [ 'id' ] . isin ( prev_associations [ 'meas_id' ]) measurements = measurements . loc [ measurements_mask ] . set_index ( 'id' ) # Set the index on images_df for faster merging. images_df_done [ 'image_id' ] = images_df_done [ 'image_dj' ] . apply ( lambda x : x . id ) . values images_df_done = images_df_done . set_index ( 'image_id' ) # Merge image information to measurements measurements = ( measurements . merge ( images_df_done [[ 'image_name' , 'epoch' ]], left_on = 'image_id' , right_index = True ) . rename ( columns = { 'image_name' : 'image' }) ) # Drop any associations that are not used in this sky region group. associations_mask = prev_associations [ 'meas_id' ] . isin ( measurements . index . values ) prev_associations = prev_associations . loc [ associations_mask ] # Merge measurements into the associations to form the sources_df. sources_df = ( prev_associations . merge ( measurements , left_on = 'meas_id' , right_index = True ) . rename ( columns = { 'source_id' : 'source' , 'time' : 'datetime' , 'meas_id' : 'id' , 'ra' : 'ra_source' , 'dec' : 'dec_source' , 'uncertainty_ew' : 'uncertainty_ew_source' , 'uncertainty_ns' : 'uncertainty_ns_source' , }) ) # Load up the previous unique sources. prev_sources = pd . read_parquet ( previous_parquet_paths [ 'sources' ], columns = [ 'wavg_ra' , 'wavg_dec' , 'wavg_uncertainty_ew' , 'wavg_uncertainty_ns' , ] ) # Merge the wavg ra and dec to the sources_df - this is required to # create the skyc1_srcs below (but MUST be converted back to the source # ra and dec) sources_df = ( sources_df . merge ( prev_sources , left_on = 'source' , right_index = True ) . rename ( columns = { 'wavg_ra' : 'ra' , 'wavg_dec' : 'dec' , 'wavg_uncertainty_ew' : 'uncertainty_ew' , 'wavg_uncertainty_ns' : 'uncertainty_ns' , }) ) # Load the previous relations prev_relations = pd . read_parquet ( previous_parquet_paths [ 'relations' ]) # Form relation lists to merge in. prev_relations = pd . DataFrame ( prev_relations . groupby ( 'from_source_id' )[ 'to_source_id' ] . apply ( lambda x : x . values . tolist ()) ) . rename ( columns = { 'to_source_id' : 'related' }) # Append the relations to only the last instance of each source # First get the ids of the sources relation_ids = sources_df [ sources_df . source . isin ( prev_relations . index . values )] . drop_duplicates ( 'source' , keep = 'last' ) . index . values # Make sure we attach the correct source id source_ids = sources_df . loc [ relation_ids ] . source . values sources_df [ 'related' ] = np . nan relations_to_update = prev_relations . loc [ source_ids ] . to_numpy () . copy () relations_to_update = np . reshape ( relations_to_update , relations_to_update . shape [ 0 ]) sources_df . loc [ relation_ids , 'related' ] = relations_to_update # Reorder so we don't mess up the dask metas. sources_df = sources_df [[ 'id' , 'uncertainty_ew' , 'weight_ew' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image' , 'datetime' , 'source' , 'ra' , 'dec' , 'ra_source' , 'dec_source' , 'd2d' , 'dr' , 'related' , 'epoch' , 'uncertainty_ew_source' , 'uncertainty_ns_source' ]] # Create the unique skyc1_srcs dataframe. skyc1_srcs = ( sources_df [ ~ sources_df [ 'forced' ]] . sort_values ( by = 'id' ) . drop ( 'related' , axis = 1 ) . drop_duplicates ( 'source' ) ) . copy ( deep = True ) # Get relations into the skyc1_srcs (as we only keep the first instance # which does not have the relation information) skyc1_srcs = skyc1_srcs . merge ( prev_relations , how = 'left' , left_on = 'source' , right_index = True ) # Need to break the pointer relationship between the related sources ( # deep=True copy does not truly copy mutable type objects) relation_mask = skyc1_srcs . related . notna () relation_vals = skyc1_srcs . loc [ relation_mask , 'related' ] . to_list () new_relation_vals = [ x . copy () for x in relation_vals ] skyc1_srcs . loc [ relation_mask , 'related' ] = new_relation_vals # Reorder so we don't mess up the dask metas. skyc1_srcs = skyc1_srcs [[ 'id' , 'ra' , 'uncertainty_ew' , 'weight_ew' , 'dec' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image' , 'datetime' , 'source' , 'ra_source' , 'dec_source' , 'd2d' , 'dr' , 'related' , 'epoch' ]] . reset_index ( drop = True ) # Finally move the source ra and dec back to the sources_df ra and dec # columns sources_df [ 'ra' ] = sources_df [ 'ra_source' ] sources_df [ 'dec' ] = sources_df [ 'dec_source' ] sources_df [ 'uncertainty_ew' ] = sources_df [ 'uncertainty_ew_source' ] sources_df [ 'uncertainty_ns' ] = sources_df [ 'uncertainty_ns_source' ] # Drop not needed columns for the sources_df. sources_df = sources_df . drop ([ 'uncertainty_ew_source' , 'uncertainty_ns_source' ], axis = 1 ) . reset_index ( drop = True ) return sources_df , skyc1_srcs remove_duplicate_measurements ( sources_df , dup_lim = None , ini_df = False ) \u00b6 Remove perceived duplicate sources from a dataframe of loaded measurements. Duplicates are determined by their separation and whether this distances is within the 'dup_lim'. Parameters: Name Type Description Default sources_df DataFrame The loaded measurements from two or more images. required dup_lim Optional[astropy.coordinates.angles.Angle] The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used (usual ASKAP pixel size). None ini_df bool Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. False Returns: Type Description DataFrame The input sources_df with duplicate sources removed. Source code in vast_pipeline/pipeline/utils.py def remove_duplicate_measurements ( sources_df : pd . DataFrame , dup_lim : Optional [ Angle ] = None , ini_df : bool = False ) -> pd . DataFrame : \"\"\" Remove perceived duplicate sources from a dataframe of loaded measurements. Duplicates are determined by their separation and whether this distances is within the 'dup_lim'. Args: sources_df: The loaded measurements from two or more images. dup_lim: The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used (usual ASKAP pixel size). ini_df: Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. Returns: The input sources_df with duplicate sources removed. \"\"\" logger . debug ( 'Cleaning duplicate sources from epoch...' ) if dup_lim is None : dup_lim = Angle ( 2.5 * u . arcsec ) logger . debug ( 'Using duplicate crossmatch radius of %.2f arcsec.' , dup_lim . arcsec ) # sort by the distance from the image centre so we know # that the first source is always the one to keep sources_df = sources_df . sort_values ( by = 'dist_from_centre' ) sources_sc = SkyCoord ( sources_df [ 'ra' ], sources_df [ 'dec' ], unit = ( u . deg , u . deg ) ) # perform search around sky to get all self matches idxc , idxcatalog , * _ = sources_sc . search_around_sky ( sources_sc , dup_lim ) # create df from results results = pd . DataFrame ( data = { 'source_id' : idxc , 'match_id' : idxcatalog , 'source_image' : sources_df . iloc [ idxc ][ 'image' ] . tolist (), 'match_image' : sources_df . iloc [ idxcatalog ][ 'image' ] . tolist () } ) # Drop those that are matched from the same image matching_image_mask = ( results [ 'source_image' ] != results [ 'match_image' ] ) results = ( results . loc [ matching_image_mask ] . drop ([ 'source_image' , 'match_image' ], axis = 1 ) ) # create a pair column defining each pair ith index results [ 'pair' ] = results . apply ( tuple , 1 ) . apply ( sorted ) . apply ( tuple ) # Drop the duplicate pairs (pairs are sorted so this works) results = results . drop_duplicates ( 'pair' ) # No longer need pair results = results . drop ( 'pair' , axis = 1 ) # Drop all self matches and we are left with those to drop # in the match id column. to_drop = results . loc [ results [ 'source_id' ] != results [ 'match_id' ], 'match_id' ] # Get the index values from the ith values to_drop_indexes = sources_df . iloc [ to_drop ] . index . values logger . debug ( \"Dropping %i duplicate measurements.\" , to_drop_indexes . shape [ 0 ] ) # Drop them from sources sources_df = sources_df . drop ( to_drop_indexes ) . sort_values ( by = 'ra' ) # reset the source_df index sources_df = sources_df . reset_index ( drop = True ) # Reset the source number if ini_df : sources_df [ 'source' ] = sources_df . index + 1 del results return sources_df write_parquets ( images , skyregions , bands , run_path ) \u00b6 This function saves images, skyregions and bands to parquet files. It also returns a DataFrame containing containing the information of the sky regions associated with the current run. Parameters: Name Type Description Default images List[vast_pipeline.models.Image] list of image Django ORM objects. required skyregions List[vast_pipeline.models.SkyRegion] list sky region Django ORM objects. required bands List[vast_pipeline.models.Band] list of band Django ORM objects. required run_path str directory to save parquets to. required Returns: Type Description DataFrame Sky regions as pandas DataFrame Source code in vast_pipeline/pipeline/utils.py def write_parquets ( images : List [ Image ], skyregions : List [ SkyRegion ], bands : List [ Band ], run_path : str ) -> pd . DataFrame : \"\"\" This function saves images, skyregions and bands to parquet files. It also returns a DataFrame containing containing the information of the sky regions associated with the current run. Args: images: list of image Django ORM objects. skyregions: list sky region Django ORM objects. bands: list of band Django ORM objects. run_path: directory to save parquets to. Returns: Sky regions as pandas DataFrame \"\"\" # write images parquet file under pipeline run folder images_df = pd . DataFrame ( map ( lambda x : x . __dict__ , images )) images_df = images_df . drop ( '_state' , axis = 1 ) images_df . to_parquet ( os . path . join ( run_path , 'images.parquet' ), index = False ) # write skyregions parquet file under pipeline run folder skyregs_df = pd . DataFrame ( map ( lambda x : x . __dict__ , skyregions )) skyregs_df = skyregs_df . drop ( '_state' , axis = 1 ) skyregs_df . to_parquet ( os . path . join ( run_path , 'skyregions.parquet' ), index = False ) # write skyregions parquet file under pipeline run folder bands_df = pd . DataFrame ( map ( lambda x : x . __dict__ , bands )) bands_df = bands_df . drop ( '_state' , axis = 1 ) bands_df . to_parquet ( os . path . join ( run_path , 'bands.parquet' ), index = False ) return skyregs_df","title":"utils.py"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.add_new_many_to_one_relations","text":"This handles the relation information being created from the many_to_one function in advanced association. It is a lot simpler than the one_to_many case as it purely just adds the new relations to the relation column, taking into account if it is already a list of relations or not (i.e. no previous relations). Parameters: Name Type Description Default row Series The relation information Series from the association dataframe. Only the columns ['related_skyc1', 'new_relations'] are required. required Returns: Type Description List[int] The new related field for the source in question, containing the appended ids. Source code in vast_pipeline/pipeline/utils.py def add_new_many_to_one_relations ( row : pd . Series ) -> List [ int ]: \"\"\" This handles the relation information being created from the many_to_one function in advanced association. It is a lot simpler than the one_to_many case as it purely just adds the new relations to the relation column, taking into account if it is already a list of relations or not (i.e. no previous relations). Args: row: The relation information Series from the association dataframe. Only the columns ['related_skyc1', 'new_relations'] are required. Returns: The new related field for the source in question, containing the appended ids. \"\"\" out = row [ 'new_relations' ] . copy () if isinstance ( row [ 'related_skyc1' ], list ): out += row [ 'related_skyc1' ] . copy () return out","title":"add_new_many_to_one_relations()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.add_new_one_to_many_relations","text":"This handles the relation information being created from the one_to_many functions in association. Parameters: Name Type Description Default row Series The relation information Series from the association dataframe. Only the columns ['related_skyc1', 'source_skyc1'] are required for advanced, these are instead called ['related', 'source'] for basic. required advanced bool Whether advanced association is being used which changes the names of the columns involved. False source_ids Optional[pandas.core.frame.DataFrame] A dataframe that contains the other ids to append to related for each original source. +----------------+--------+ | source_skyc1 | 0 | |----------------+--------| | 122 | [5542] | | 254 | [5543] | | 262 | [5544] | | 405 | [5545] | | 656 | [5546] | +----------------+--------+ None Returns: Type Description List[int] The new related field for the source in question, containing the appended ids. Source code in vast_pipeline/pipeline/utils.py def add_new_one_to_many_relations ( row : pd . Series , advanced : bool = False , source_ids : Optional [ pd . DataFrame ] = None ) -> List [ int ]: \"\"\" This handles the relation information being created from the one_to_many functions in association. Args: row: The relation information Series from the association dataframe. Only the columns ['related_skyc1', 'source_skyc1'] are required for advanced, these are instead called ['related', 'source'] for basic. advanced: Whether advanced association is being used which changes the names of the columns involved. source_ids: A dataframe that contains the other ids to append to related for each original source. +----------------+--------+ | source_skyc1 | 0 | |----------------+--------| | 122 | [5542] | | 254 | [5543] | | 262 | [5544] | | 405 | [5545] | | 656 | [5546] | +----------------+--------+ Returns: The new related field for the source in question, containing the appended ids. \"\"\" if source_ids is None : source_ids = pd . DataFrame () related_col = 'related_skyc1' if advanced else 'related' source_col = 'source_skyc1' if advanced else 'source' # this is the not_original case where the original source id is appended. if source_ids . empty : if isinstance ( row [ related_col ], list ): out = row [ related_col ] out . append ( row [ source_col ]) else : out = [ row [ source_col ], ] else : # the original case to append all the new ids. source_ids = source_ids . loc [ row [ source_col ]] . iloc [ 0 ] if isinstance ( row [ related_col ], list ): out = row [ related_col ] + source_ids else : out = source_ids return out","title":"add_new_one_to_many_relations()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.add_run_to_img","text":"Add a pipeline run to an Image (and corresponding SkyRegion) in the db Parameters: Name Type Description Default pipeline_run Run Pipeline run object you want to add. required img Image Image object you want to add to. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def add_run_to_img ( pipeline_run : Run , img : Image ) -> None : \"\"\" Add a pipeline run to an Image (and corresponding SkyRegion) in the db Args: pipeline_run: Pipeline run object you want to add. img: Image object you want to add to. Returns: None \"\"\" skyreg = img . skyreg # check and add the many to many if not existent if not Image . objects . filter ( id = img . id , run__id = pipeline_run . id ) . exists (): logger . info ( 'Adding %s to image %s ' , pipeline_run , img . name ) img . run . add ( pipeline_run ) if pipeline_run not in skyreg . run . all (): logger . info ( 'Adding %s to sky region %s ' , pipeline_run , skyreg ) skyreg . run . add ( pipeline_run )","title":"add_run_to_img()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.backup_parquets","text":"Backups up all the existing parquet files in a pipeline run directory. Backups are named with a '.bak' suffix in the pipeline run directory. Parameters: Name Type Description Default p_run_path str The path of the pipeline run where the parquets are stored. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def backup_parquets ( p_run_path : str ) -> None : \"\"\" Backups up all the existing parquet files in a pipeline run directory. Backups are named with a '.bak' suffix in the pipeline run directory. Args: p_run_path: The path of the pipeline run where the parquets are stored. Returns: None \"\"\" parquets = ( glob . glob ( os . path . join ( p_run_path , \"*.parquet\" )) # TODO Remove arrow when arrow files are no longer required. + glob . glob ( os . path . join ( p_run_path , \"*.arrow\" ))) for i in parquets : backup_name = i + '.bak' if os . path . isfile ( backup_name ): logger . debug ( f 'Removing old backup file: { backup_name } .' ) os . remove ( backup_name ) shutil . copyfile ( i , backup_name )","title":"backup_parquets()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.calc_ave_coord","text":"Calculates the average coordinate of the grouped by sources dataframe for each unique group, along with defining the image and epoch list for each unique source (group). Parameters: Name Type Description Default grp DataFrame The current group dataframe (unique source) of the grouped by dataframe being acted upon. required Returns: Type Description Series A pandas series containing the average coordinate along with the image and epoch lists. Source code in vast_pipeline/pipeline/utils.py def calc_ave_coord ( grp : pd . DataFrame ) -> pd . Series : \"\"\" Calculates the average coordinate of the grouped by sources dataframe for each unique group, along with defining the image and epoch list for each unique source (group). Args: grp: The current group dataframe (unique source) of the grouped by dataframe being acted upon. Returns: A pandas series containing the average coordinate along with the image and epoch lists. \"\"\" d = {} grp = grp . sort_values ( by = 'datetime' ) d [ 'img_list' ] = grp [ 'image' ] . values . tolist () d [ 'epoch_list' ] = grp [ 'epoch' ] . values . tolist () d [ 'wavg_ra' ] = grp [ 'interim_ew' ] . sum () / grp [ 'weight_ew' ] . sum () d [ 'wavg_dec' ] = grp [ 'interim_ns' ] . sum () / grp [ 'weight_ns' ] . sum () return pd . Series ( d )","title":"calc_ave_coord()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.check_primary_image","text":"Checks whether the primary image of the ideal source dataframe is in the image list for the source. Parameters: Name Type Description Default row Series Input dataframe row, with columns ['primary'] and ['img_list']. required Returns: Type Description bool True if primary in image list else False. Source code in vast_pipeline/pipeline/utils.py def check_primary_image ( row : pd . Series ) -> bool : \"\"\" Checks whether the primary image of the ideal source dataframe is in the image list for the source. Args: row: Input dataframe row, with columns ['primary'] and ['img_list']. Returns: True if primary in image list else False. \"\"\" return row [ 'primary' ] in row [ 'img_list' ]","title":"check_primary_image()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.create_measurement_pairs_arrow_file","text":"Creates a measurement_pairs.arrow file using the parquet outputs of a pipeline run. Parameters: Name Type Description Default p_run Run Pipeline model instance. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def create_measurement_pairs_arrow_file ( p_run : Run ) -> None : \"\"\" Creates a measurement_pairs.arrow file using the parquet outputs of a pipeline run. Args: p_run: Pipeline model instance. Returns: None \"\"\" logger . info ( 'Creating measurement_pairs.arrow for run %s .' , p_run . name ) measurement_pairs_df = pd . read_parquet ( os . path . join ( p_run . path , 'measurement_pairs.parquet' ) ) logger . debug ( 'Optimising dataframe.' ) measurement_pairs_df = optimize_ints ( optimize_floats ( measurement_pairs_df )) logger . debug ( \"Loading to pyarrow table.\" ) measurement_pairs_df = pa . Table . from_pandas ( measurement_pairs_df ) logger . debug ( \"Exporting to arrow file.\" ) outname = os . path . join ( p_run . path , 'measurement_pairs.arrow' ) local = pa . fs . LocalFileSystem () with local . open_output_stream ( outname ) as file : with pa . RecordBatchFileWriter ( file , measurement_pairs_df . schema ) as writer : writer . write_table ( measurement_pairs_df )","title":"create_measurement_pairs_arrow_file()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.create_measurements_arrow_file","text":"Creates a measurements.arrow file using the parquet outputs of a pipeline run. Parameters: Name Type Description Default p_run Run Pipeline model instance. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def create_measurements_arrow_file ( p_run : Run ) -> None : \"\"\" Creates a measurements.arrow file using the parquet outputs of a pipeline run. Args: p_run: Pipeline model instance. Returns: None \"\"\" logger . info ( 'Creating measurements.arrow for run %s .' , p_run . name ) associations = pd . read_parquet ( os . path . join ( p_run . path , 'associations.parquet' ) ) images = pd . read_parquet ( os . path . join ( p_run . path , 'images.parquet' ) ) m_files = images [ 'measurements_path' ] . tolist () m_files += glob . glob ( os . path . join ( p_run . path , 'forced*.parquet' )) logger . debug ( 'Loading %i files...' , len ( m_files )) measurements = dd . read_parquet ( m_files , engine = 'pyarrow' ) . compute () measurements = measurements . loc [ measurements [ 'id' ] . isin ( associations [ 'meas_id' ] . values ) ] measurements = ( associations . loc [:, [ 'meas_id' , 'source_id' ]] . set_index ( 'meas_id' ) . merge ( measurements , left_index = True , right_on = 'id' ) . rename ( columns = { 'source_id' : 'source' }) ) # drop timezone from datetime for vaex compatibility # TODO: Look to keep the timezone if/when vaex is compatible. measurements [ 'time' ] = measurements [ 'time' ] . dt . tz_localize ( None ) logger . debug ( 'Optimising dataframes.' ) measurements = optimize_ints ( optimize_floats ( measurements )) logger . debug ( \"Loading to pyarrow table.\" ) measurements = pa . Table . from_pandas ( measurements ) logger . debug ( \"Exporting to arrow file.\" ) outname = os . path . join ( p_run . path , 'measurements.arrow' ) local = pa . fs . LocalFileSystem () with local . open_output_stream ( outname ) as file : with pa . RecordBatchFileWriter ( file , measurements . schema ) as writer : writer . write_table ( measurements )","title":"create_measurements_arrow_file()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.create_temp_config_file","text":"Creates the temp config file which is saved at the beginning of each run. It is to avoid issues created by users changing the config while the run is running. Parameters: Name Type Description Default p_run_path str The path of the pipeline run of the config to be copied. required Returns: Type Description None None Source code in vast_pipeline/pipeline/utils.py def create_temp_config_file ( p_run_path : str ) -> None : \"\"\" Creates the temp config file which is saved at the beginning of each run. It is to avoid issues created by users changing the config while the run is running. Args: p_run_path: The path of the pipeline run of the config to be copied. Returns: None \"\"\" config_name = 'config.yaml' temp_config_name = 'config_temp.yaml' shutil . copyfile ( os . path . join ( p_run_path , config_name ), os . path . join ( p_run_path , temp_config_name ) )","title":"create_temp_config_file()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.cross_join","text":"A convenience function to merge two dataframes. Parameters: Name Type Description Default left DataFrame The base pandas DataFrame to merge. required right DataFrame The pandas DataFrame to merge to the left. required Returns: Type Description DataFrame The resultant merged DataFrame. Source code in vast_pipeline/pipeline/utils.py def cross_join ( left : pd . DataFrame , right : pd . DataFrame ) -> pd . DataFrame : \"\"\" A convenience function to merge two dataframes. Args: left: The base pandas DataFrame to merge. right: The pandas DataFrame to merge to the left. Returns: The resultant merged DataFrame. \"\"\" return ( left . assign ( key = 1 ) . merge ( right . assign ( key = 1 ), on = 'key' ) . drop ( 'key' , axis = 1 ) )","title":"cross_join()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_create_img","text":"Function to fetch or create the Image and Sky Region objects for an image. Parameters: Name Type Description Default band_id int The integer database id value of the frequency band of the image. required image SelavyImage The image object. required Returns: Type Description Tuple[vast_pipeline.models.Image, bool] The resulting image django ORM object, and a bool value denoting if the image already existed in the database. Source code in vast_pipeline/pipeline/utils.py def get_create_img ( band_id : int , image : SelavyImage ) -> Tuple [ Image , bool ]: \"\"\" Function to fetch or create the Image and Sky Region objects for an image. Args: band_id: The integer database id value of the frequency band of the image. image: The image object. Returns: The resulting image django ORM object, and a bool value denoting if the image already existed in the database. \"\"\" images = Image . objects . filter ( name__exact = image . name ) exists = images . exists () if exists : img : Image = images . get () # Add background path if not originally provided if image . background_path and not img . background_path : img . background_path = image . background_path img . save () else : # at this stage, measurement parquet file is not created but # assume location img_folder_name = image . name . replace ( '.' , '_' ) measurements_path = os . path . join ( settings . PIPELINE_WORKING_DIR , 'images' , img_folder_name , 'measurements.parquet' ) img = Image ( band_id = band_id , measurements_path = measurements_path ) # set the attributes and save the image, # by selecting only valid (not hidden) attributes # FYI attributs and/or method starting with _ are hidden # and with __ can't be modified/called for fld in img . _meta . get_fields (): if getattr ( fld , 'attname' , None ) and ( getattr ( image , fld . attname , None ) is not None ): setattr ( img , fld . attname , getattr ( image , fld . attname )) img . rms_median , img . rms_min , img . rms_max = get_rms_noise_image_values ( img . noise_path ) # get create the sky region and associate with image img . skyreg = get_create_skyreg ( img ) img . save () return ( img , exists )","title":"get_create_img()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_create_img_band","text":"Return the existing Band row for the given FitsImage. An image is considered to belong to a band if its frequency is within some tolerance of the band's frequency. Returns a Band row or None if no matching band. Parameters: Name Type Description Default image FitsImage The image Django ORM object. required Returns: Type Description Band The band Django ORM object. Source code in vast_pipeline/pipeline/utils.py def get_create_img_band ( image : FitsImage ) -> Band : ''' Return the existing Band row for the given FitsImage. An image is considered to belong to a band if its frequency is within some tolerance of the band's frequency. Returns a Band row or None if no matching band. Args: image: The image Django ORM object. Returns: The band Django ORM object. ''' # For now we match bands using the central frequency. # This assumes that every band has a unique frequency, # which is true for the data we've used so far. freq = int ( image . freq_eff * 1.e-6 ) freq_band = int ( image . freq_bw * 1.e-6 ) # TODO: refine the band query for band in Band . objects . all (): diff = abs ( freq - band . frequency ) / float ( band . frequency ) if diff < 0.02 : return band # no band has been found so create it band = Band ( name = str ( freq ), frequency = freq , bandwidth = freq_band ) logger . info ( 'Adding new frequency band: %s ' , band ) band . save () return band","title":"get_create_img_band()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_create_p_run","text":"Get or create a pipeline run in db, return the run django object and a flag True/False if has been created or already exists. Parameters: Name Type Description Default name str The name of the pipeline run. required path str The system path to the pipeline run folder which contains the configuration file and where outputs will be saved. required description str An optional description of the pipeline run. None user User The Django user that launched the pipeline run. None Returns: Type Description Tuple[vast_pipeline.models.Run, bool] The pipeline run object and a boolean object representing whether the pipeline run already existed ('True') or not ('False'). Source code in vast_pipeline/pipeline/utils.py def get_create_p_run ( name : str , path : str , description : str = None , user : User = None ) -> Tuple [ Run , bool ]: ''' Get or create a pipeline run in db, return the run django object and a flag True/False if has been created or already exists. Args: name: The name of the pipeline run. path: The system path to the pipeline run folder which contains the configuration file and where outputs will be saved. description: An optional description of the pipeline run. user: The Django user that launched the pipeline run. Returns: The pipeline run object and a boolean object representing whether the pipeline run already existed ('True') or not ('False'). ''' p_run = Run . objects . filter ( name__exact = name ) if p_run : return p_run . get (), True description = \"\" if description is None else description p_run = Run ( name = name , description = description , path = path ) if user : p_run . user = user p_run . save () return p_run , False","title":"get_create_p_run()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_create_skyreg","text":"This creates a Sky Region object in Django ORM given the related image object. Parameters: Name Type Description Default image Image The image Django ORM object. required Returns: Type Description SkyRegion The sky region Django ORM object. Source code in vast_pipeline/pipeline/utils.py def get_create_skyreg ( image : Image ) -> SkyRegion : ''' This creates a Sky Region object in Django ORM given the related image object. Args: image: The image Django ORM object. Returns: The sky region Django ORM object. ''' # In the calculations below, it is assumed the image has square # pixels (this pipeline has been designed for ASKAP images, so it # should always be square). It will likely give wrong results if not skyregions = SkyRegion . objects . filter ( centre_ra = image . ra , centre_dec = image . dec , xtr_radius = image . fov_bmin ) if skyregions : skyr = skyregions . get () logger . info ( 'Found sky region %s ' , skyr ) else : x , y , z = eq_to_cart ( image . ra , image . dec ) skyr = SkyRegion ( centre_ra = image . ra , centre_dec = image . dec , width_ra = image . physical_bmin , width_dec = image . physical_bmaj , xtr_radius = image . fov_bmin , x = x , y = y , z = z , ) skyr . save () logger . info ( 'Created sky region %s ' , skyr ) return skyr","title":"get_create_skyreg()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_eta_metric","text":"Calculates the eta variability metric of a source. Works on the grouped by dataframe using the fluxes of the associated measurements. Parameters: Name Type Description Default row Dict[str, float] Dictionary containing statistics for the current source. required df DataFrame The grouped by sources dataframe of the measurements containing all the flux and flux error information, required peak bool Whether to use peak_flux for the calculation. If False then the integrated flux is used. False Returns: Type Description float The calculated eta value. Source code in vast_pipeline/pipeline/utils.py def get_eta_metric ( row : Dict [ str , float ], df : pd . DataFrame , peak : bool = False ) -> float : ''' Calculates the eta variability metric of a source. Works on the grouped by dataframe using the fluxes of the associated measurements. Args: row: Dictionary containing statistics for the current source. df: The grouped by sources dataframe of the measurements containing all the flux and flux error information, peak: Whether to use peak_flux for the calculation. If False then the integrated flux is used. Returns: The calculated eta value. ''' if row [ 'n_meas' ] == 1 : return 0. suffix = 'peak' if peak else 'int' weights = 1. / df [ f 'flux_ { suffix } _err' ] . values ** 2 fluxes = df [ f 'flux_ { suffix } ' ] . values eta = ( row [ 'n_meas' ] / ( row [ 'n_meas' ] - 1 )) * ( ( weights * fluxes ** 2 ) . mean () - ( ( weights * fluxes ) . mean () ** 2 / weights . mean () ) ) return eta","title":"get_eta_metric()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_image_list_diff","text":"Calculate the difference between the ideal coverage image list of a source and the actual observed image list. Also checks whether an epoch does in fact contain a detection but is not in the expected 'ideal' image for that epoch. Parameters: Name Type Description Default row Series The row from the sources dataframe that is being iterated over. required Returns: Type Description Union[List[str], int] A list of the images missing from the observed image list. Will be returned as '-1' integer value if there are no missing images. Source code in vast_pipeline/pipeline/utils.py def get_image_list_diff ( row : pd . Series ) -> Union [ List [ str ], int ]: \"\"\" Calculate the difference between the ideal coverage image list of a source and the actual observed image list. Also checks whether an epoch does in fact contain a detection but is not in the expected 'ideal' image for that epoch. Args: row: The row from the sources dataframe that is being iterated over. Returns: A list of the images missing from the observed image list. Will be returned as '-1' integer value if there are no missing images. \"\"\" out = list ( filter ( lambda arg : arg not in row [ 'img_list' ], row [ 'skyreg_img_list' ]) ) # set empty list to -1 if not out : return - 1 # Check that an epoch has not already been seen (just not in the 'ideal' # image) out_epochs = [ row [ 'skyreg_epoch' ][ pair [ 0 ]] for pair in enumerate ( row [ 'skyreg_img_list' ] ) if pair [ 1 ] in out ] out = [ out [ pair [ 0 ]] for pair in enumerate ( out_epochs ) if pair [ 1 ] not in row [ 'epoch_list' ] ] if not out : return - 1 return out","title":"get_image_list_diff()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_names_and_epochs","text":"Convenience function to group together the image names, epochs and datetimes into one list object which is then returned as a pandas series. This is necessary for easier processing in the ideal coverage analysis. Parameters: Name Type Description Default grp DataFrame A group from the grouped by sources DataFrame. required Returns: Type Description Series Pandas series containing the list object that contains the lists of the image names, epochs and datetimes. Source code in vast_pipeline/pipeline/utils.py def get_names_and_epochs ( grp : pd . DataFrame ) -> pd . Series : \"\"\" Convenience function to group together the image names, epochs and datetimes into one list object which is then returned as a pandas series. This is necessary for easier processing in the ideal coverage analysis. Args: grp: A group from the grouped by sources DataFrame. Returns: Pandas series containing the list object that contains the lists of the image names, epochs and datetimes. \"\"\" d = {} d [ 'skyreg_img_epoch_list' ] = [[[ x , ], y , z ] for x , y , z in zip ( grp [ 'name' ] . values . tolist (), grp [ 'epoch' ] . values . tolist (), grp [ 'datetime' ] . values . tolist () )] return pd . Series ( d )","title":"get_names_and_epochs()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_parallel_assoc_image_df","text":"Merge the sky region groups with the images and skyreg_ids. Parameters: Name Type Description Default images List[vast_pipeline.models.Image] A list of the Image objects. required skyregion_groups DataFrame The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ required Returns: Type Description DataFrame Dataframe containing the merged images and skyreg_id and skyreg_group. +----+-------------------------------+-------------+----------------+ | | image | skyreg_id | skyreg_group | |----+-------------------------------+-------------+----------------| | 0 | VAST_2118+00A.EPOCH01.I.fits | 2 | 1 | | 1 | VAST_2118-06A.EPOCH01.I.fits | 3 | 1 | | 2 | VAST_0127-73A.EPOCH01.I.fits | 1 | 2 | | 3 | VAST_2118-06A.EPOCH03x.I.fits | 3 | 1 | | 4 | VAST_2118-06A.EPOCH02.I.fits | 3 | 1 | | 5 | VAST_2118-06A.EPOCH05x.I.fits | 3 | 1 | | 6 | VAST_2118-06A.EPOCH06x.I.fits | 3 | 1 | | 7 | VAST_0127-73A.EPOCH08.I.fits | 1 | 2 | +----+-------------------------------+-------------+----------------+ Source code in vast_pipeline/pipeline/utils.py def get_parallel_assoc_image_df ( images : List [ Image ], skyregion_groups : pd . DataFrame ) -> pd . DataFrame : \"\"\" Merge the sky region groups with the images and skyreg_ids. Args: images: A list of the Image objects. skyregion_groups: The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ Returns: Dataframe containing the merged images and skyreg_id and skyreg_group. +----+-------------------------------+-------------+----------------+ | | image | skyreg_id | skyreg_group | |----+-------------------------------+-------------+----------------| | 0 | VAST_2118+00A.EPOCH01.I.fits | 2 | 1 | | 1 | VAST_2118-06A.EPOCH01.I.fits | 3 | 1 | | 2 | VAST_0127-73A.EPOCH01.I.fits | 1 | 2 | | 3 | VAST_2118-06A.EPOCH03x.I.fits | 3 | 1 | | 4 | VAST_2118-06A.EPOCH02.I.fits | 3 | 1 | | 5 | VAST_2118-06A.EPOCH05x.I.fits | 3 | 1 | | 6 | VAST_2118-06A.EPOCH06x.I.fits | 3 | 1 | | 7 | VAST_0127-73A.EPOCH08.I.fits | 1 | 2 | +----+-------------------------------+-------------+----------------+ \"\"\" skyreg_ids = [ i . skyreg_id for i in images ] images_df = pd . DataFrame ({ 'image_dj' : images , 'skyreg_id' : skyreg_ids , }) images_df = images_df . merge ( skyregion_groups , how = 'left' , left_on = 'skyreg_id' , right_index = True ) images_df [ 'image_name' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . name ) images_df [ 'image_datetime' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . datetime ) return images_df","title":"get_parallel_assoc_image_df()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_rms_noise_image_values","text":"Open the RMS noise FITS file and compute the median, max and min rms values to be added to the image model and then used in the calculations. Parameters: Name Type Description Default rms_path str The system path to the RMS FITS image. required Returns: Type Description Tuple[float, float, float] The median, minimum and maximum values of the RMS image. Exceptions: Type Description IOError Raised when the RMS FITS file cannot be found. Source code in vast_pipeline/pipeline/utils.py def get_rms_noise_image_values ( rms_path : str ) -> Tuple [ float , float , float ]: ''' Open the RMS noise FITS file and compute the median, max and min rms values to be added to the image model and then used in the calculations. Args: rms_path: The system path to the RMS FITS image. Returns: The median, minimum and maximum values of the RMS image. Raises: IOError: Raised when the RMS FITS file cannot be found. ''' logger . debug ( 'Extracting Image RMS values from Noise file...' ) med_val = min_val = max_val = 0. try : with fits . open ( rms_path ) as f : data = f [ 0 ] . data data = data [ np . logical_not ( np . isnan ( data ))] data = data [ data != 0 ] med_val = np . median ( data ) * 1e+3 min_val = np . min ( data ) * 1e+3 max_val = np . max ( data ) * 1e+3 del data except Exception : raise IOError ( f 'Could not read this RMS FITS file: { rms_path } ' ) return med_val , min_val , max_val","title":"get_rms_noise_image_values()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.get_src_skyregion_merged_df","text":"Analyses the current sources_df to determine what the 'ideal coverage' for each source should be. In other words, what images is the source missing in when it should have been seen. Parameters: Name Type Description Default sources_df DataFrame The output of the association step containing the measurements associated into sources. required images_df DataFrame Contains the images of the pipeline run. I.e. all image objects for the run loaded into a dataframe. required skyreg_df DataFrame Contains the sky regions of the pipeline run. I.e. all sky region objects for the run loaded into a dataframe. required Returns: Type Description DataFrame DataFrame containing missing image information. Output format: +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | | 1290 | ['VAST_0127-73A.EPOCH01.I.fits'] | 20.8455 | -76.8269 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+------------------------------+ img_diff | primary | ----------------------------------+------------------------------+ ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ----------------------------------+------------------------------+ ------------------------------+--------------+ detection | in_primary | ------------------------------+--------------| VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | ------------------------------+--------------+ Source code in vast_pipeline/pipeline/utils.py def get_src_skyregion_merged_df ( sources_df : pd . DataFrame , images_df : pd . DataFrame , skyreg_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Analyses the current sources_df to determine what the 'ideal coverage' for each source should be. In other words, what images is the source missing in when it should have been seen. Args: sources_df: The output of the association step containing the measurements associated into sources. images_df: Contains the images of the pipeline run. I.e. all image objects for the run loaded into a dataframe. skyreg_df: Contains the sky regions of the pipeline run. I.e. all sky region objects for the run loaded into a dataframe. Returns: DataFrame containing missing image information. Output format: +----------+----------------------------------+-----------+------------+ | source | img_list | wavg_ra | wavg_dec | |----------+----------------------------------+-----------+------------+ | 278 | ['VAST_0127-73A.EPOCH01.I.fits'] | 22.2929 | -71.8717 | | 702 | ['VAST_0127-73A.EPOCH01.I.fits'] | 28.8125 | -69.3547 | | 844 | ['VAST_0127-73A.EPOCH01.I.fits'] | 17.3152 | -72.346 | | 934 | ['VAST_0127-73A.EPOCH01.I.fits'] | 9.75754 | -72.9629 | | 1290 | ['VAST_0127-73A.EPOCH01.I.fits'] | 20.8455 | -76.8269 | +----------+----------------------------------+-----------+------------+ ------------------------------------------------------------------+ skyreg_img_list | ------------------------------------------------------------------+ ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ['VAST_0127-73A.EPOCH01.I.fits', 'VAST_0127-73A.EPOCH08.I.fits'] | ------------------------------------------------------------------+ ----------------------------------+------------------------------+ img_diff | primary | ----------------------------------+------------------------------+ ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ['VAST_0127-73A.EPOCH08.I.fits'] | VAST_0127-73A.EPOCH01.I.fits | ----------------------------------+------------------------------+ ------------------------------+--------------+ detection | in_primary | ------------------------------+--------------| VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | VAST_0127-73A.EPOCH01.I.fits | True | ------------------------------+--------------+ \"\"\" logger . info ( \"Creating ideal source coverage df...\" ) merged_timer = StopWatch () skyreg_df = skyreg_df . drop ( [ 'x' , 'y' , 'z' , 'width_ra' , 'width_dec' ], axis = 1 ) images_df [ 'name' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . name ) images_df [ 'datetime' ] = images_df [ 'image_dj' ] . apply ( lambda x : x . datetime ) skyreg_df = skyreg_df . join ( pd . DataFrame ( images_df . groupby ( 'skyreg_id' ) . apply ( get_names_and_epochs ) ), on = 'id' ) sources_df = sources_df . sort_values ( by = 'datetime' ) # calculate some metrics on sources # compute only some necessary metrics in the groupby timer = StopWatch () srcs_df = parallel_groupby_coord ( sources_df ) logger . debug ( 'Groupby-apply time: %.2f seconds' , timer . reset ()) del sources_df # crossmatch sources with sky regions up to the max sky region radius skyreg_coords = SkyCoord ( ra = skyreg_df . centre_ra , dec = skyreg_df . centre_dec , unit = \"deg\" ) srcs_coords = SkyCoord ( ra = srcs_df . wavg_ra , dec = srcs_df . wavg_dec , unit = \"deg\" ) skyreg_idx , srcs_idx , sep , _ = srcs_coords . search_around_sky ( skyreg_coords , skyreg_df . xtr_radius . max () * u . deg ) skyreg_df = skyreg_df . drop ( columns = [ \"centre_ra\" , \"centre_dec\" ]) . set_index ( \"id\" ) # select rows where separation is less than sky region radius # drop not more useful columns and groupby source id # compute list of images src_skyrg_df = ( pd . DataFrame ( { \"source\" : srcs_df . iloc [ srcs_idx ] . index , \"id\" : skyreg_df . iloc [ skyreg_idx ] . index , \"sep\" : sep . to ( \"deg\" ) . value , } ) . merge ( skyreg_df , left_on = \"id\" , right_index = True ) . query ( \"sep < xtr_radius\" ) . drop ( columns = [ \"id\" , \"xtr_radius\" ]) . explode ( \"skyreg_img_epoch_list\" ) ) del skyreg_df src_skyrg_df [ [ 'skyreg_img_list' , 'skyreg_epoch' , 'skyreg_datetime' ] ] = pd . DataFrame ( src_skyrg_df [ 'skyreg_img_epoch_list' ] . tolist (), index = src_skyrg_df . index ) src_skyrg_df = src_skyrg_df . drop ( 'skyreg_img_epoch_list' , axis = 1 ) src_skyrg_df = ( src_skyrg_df . sort_values ( [ 'source' , 'sep' ] ) . drop_duplicates ([ 'source' , 'skyreg_epoch' ]) . sort_values ( by = 'skyreg_datetime' ) . drop ( [ 'sep' , 'skyreg_datetime' ], axis = 1 ) ) # annoyingly epoch needs to be not a list to drop duplicates # but then we need to sum the epochs into a list src_skyrg_df [ 'skyreg_epoch' ] = src_skyrg_df [ 'skyreg_epoch' ] . apply ( lambda x : [ x , ] ) src_skyrg_df = ( src_skyrg_df . groupby ( 'source' ) . sum ( numeric_only = False ) # sum because we need to preserve order ) # merge into main df and compare the images srcs_df = srcs_df . merge ( src_skyrg_df , left_index = True , right_index = True ) del src_skyrg_df srcs_df [ 'img_diff' ] = srcs_df [ [ 'img_list' , 'skyreg_img_list' , 'epoch_list' , 'skyreg_epoch' ] ] . apply ( get_image_list_diff , axis = 1 ) srcs_df = srcs_df . loc [ srcs_df [ 'img_diff' ] != - 1 ] srcs_df = srcs_df . drop ( [ 'epoch_list' , 'skyreg_epoch' ], axis = 1 ) srcs_df [ 'primary' ] = srcs_df [ 'skyreg_img_list' ] . apply ( lambda x : x [ 0 ]) srcs_df [ 'detection' ] = srcs_df [ 'img_list' ] . apply ( lambda x : x [ 0 ]) srcs_df [ 'in_primary' ] = srcs_df [ [ 'primary' , 'img_list' ] ] . apply ( check_primary_image , axis = 1 ) srcs_df = srcs_df . drop ([ 'img_list' , 'skyreg_img_list' , 'primary' ], axis = 1 ) logger . info ( 'Ideal source coverage time: %.2f seconds' , merged_timer . reset () ) return srcs_df","title":"get_src_skyregion_merged_df()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.group_skyregions","text":"Logic to group sky regions into overlapping groups. Returns a dataframe containing the sky region id as the index and a column containing a list of the sky region group number it belongs to. Parameters: Name Type Description Default df DataFrame A dataframe containing all the sky regions of the run. Only the 'id', 'centre_ra', 'centre_dec' and 'xtr_radius' columns are required. +------+-------------+--------------+--------------+ | id | centre_ra | centre_dec | xtr_radius | |------+-------------+--------------+--------------| | 2 | 319.652 | 0.0030765 | 6.72488 | | 3 | 319.652 | -6.2989 | 6.7401 | | 1 | 21.8361 | -73.121 | 7.24662 | +------+-------------+--------------+--------------+ required Returns: Type Description DataFrame The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ Source code in vast_pipeline/pipeline/utils.py def group_skyregions ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Logic to group sky regions into overlapping groups. Returns a dataframe containing the sky region id as the index and a column containing a list of the sky region group number it belongs to. Args: df: A dataframe containing all the sky regions of the run. Only the 'id', 'centre_ra', 'centre_dec' and 'xtr_radius' columns are required. +------+-------------+--------------+--------------+ | id | centre_ra | centre_dec | xtr_radius | |------+-------------+--------------+--------------| | 2 | 319.652 | 0.0030765 | 6.72488 | | 3 | 319.652 | -6.2989 | 6.7401 | | 1 | 21.8361 | -73.121 | 7.24662 | +------+-------------+--------------+--------------+ Returns: The sky region group of each skyregion id. +----+----------------+ | | skyreg_group | |----+----------------| | 2 | 1 | | 3 | 1 | | 1 | 2 | +----+----------------+ \"\"\" sr_coords = SkyCoord ( df [ 'centre_ra' ], df [ 'centre_dec' ], unit = ( u . deg , u . deg ) ) df = df . set_index ( 'id' ) results = df . apply ( _get_skyregion_relations , args = ( sr_coords , df . index ), axis = 1 ) skyreg_groups : Dict [ int , List [ Any ]] = {} master_done = [] # keep track of all checked ids in master done for skyreg_id , neighbours in results . iteritems (): if skyreg_id not in master_done : local_done = [] # a local done list for the sky region group. # add the current skyreg_id to both master and local done. master_done . append ( skyreg_id ) local_done . append ( skyreg_id ) # Define the new group number based on the existing ones. skyreg_group = len ( skyreg_groups ) + 1 # Add all the ones that we know are neighbours that were obtained # from _get_skyregion_relations. skyreg_groups [ skyreg_group ] = list ( neighbours ) # Now the sky region group is extended out to include all those sky # regions that overlap with the neighbours. # Each neighbour is checked and added to the local done list. # Checked means that for each neighbour, it's own neighbours are # added to the current group if not in already. # When the local done is equal to the skyreg group we know that # we have exhausted all possible neighbours and that results in a # sky region group. while sorted ( local_done ) != sorted ( skyreg_groups [ skyreg_group ]): # Loop over each neighbour for other_skyreg_id in skyreg_groups [ skyreg_group ]: # If we haven't checked this neighbour locally proceed. if other_skyreg_id not in local_done : # Add it to the local checked. local_done . append ( other_skyreg_id ) # Get the neighbours neighbour and add these. new_vals = results . loc [ other_skyreg_id ] for k in new_vals : if k not in skyreg_groups [ skyreg_group ]: skyreg_groups [ skyreg_group ] . append ( k ) # Reached the end of the group so append all to the master # done list for j in skyreg_groups [ skyreg_group ]: master_done . append ( j ) else : # continue if already placed in group continue # flip the dictionary around skyreg_group_ids = {} for i in skyreg_groups : for j in skyreg_groups [ i ]: skyreg_group_ids [ j ] = i skyreg_group_ids = pd . DataFrame . from_dict ( skyreg_group_ids , orient = 'index' ) . rename ( columns = { 0 : 'skyreg_group' }) return skyreg_group_ids","title":"group_skyregions()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.groupby_funcs","text":"Performs calculations on the unique sources to get the lightcurve properties. Works on the grouped by source dataframe. Parameters: Name Type Description Default df DataFrame The current iteration dataframe of the grouped by sources dataframe. required Returns: Type Description Series Pandas series containing the calculated metrics of the source. Source code in vast_pipeline/pipeline/utils.py def groupby_funcs ( df : pd . DataFrame ) -> pd . Series : ''' Performs calculations on the unique sources to get the lightcurve properties. Works on the grouped by source dataframe. Args: df: The current iteration dataframe of the grouped by sources dataframe. Returns: Pandas series containing the calculated metrics of the source. ''' # calculated average ra, dec, fluxes and metrics d = {} d [ 'img_list' ] = df [ 'image' ] . values . tolist () d [ 'n_meas_forced' ] = df [ 'forced' ] . sum () d [ 'n_meas' ] = df [ 'id' ] . count () d [ 'n_meas_sel' ] = d [ 'n_meas' ] - d [ 'n_meas_forced' ] d [ 'n_sibl' ] = df [ 'has_siblings' ] . sum () if d [ 'n_meas_forced' ] > 0 : non_forced_sel = ~ df [ 'forced' ] d [ 'wavg_ra' ] = ( df . loc [ non_forced_sel , 'interim_ew' ] . sum () / df . loc [ non_forced_sel , 'weight_ew' ] . sum () ) d [ 'wavg_dec' ] = ( df . loc [ non_forced_sel , 'interim_ns' ] . sum () / df . loc [ non_forced_sel , 'weight_ns' ] . sum () ) d [ 'avg_compactness' ] = df . loc [ non_forced_sel , 'compactness' ] . mean () d [ 'min_snr' ] = df . loc [ non_forced_sel , 'snr' ] . min () d [ 'max_snr' ] = df . loc [ non_forced_sel , 'snr' ] . max () else : d [ 'wavg_ra' ] = df [ 'interim_ew' ] . sum () / df [ 'weight_ew' ] . sum () d [ 'wavg_dec' ] = df [ 'interim_ns' ] . sum () / df [ 'weight_ns' ] . sum () d [ 'avg_compactness' ] = df [ 'compactness' ] . mean () d [ 'min_snr' ] = df [ 'snr' ] . min () d [ 'max_snr' ] = df [ 'snr' ] . max () d [ 'wavg_uncertainty_ew' ] = 1. / np . sqrt ( df [ 'weight_ew' ] . sum ()) d [ 'wavg_uncertainty_ns' ] = 1. / np . sqrt ( df [ 'weight_ns' ] . sum ()) for col in [ 'avg_flux_int' , 'avg_flux_peak' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . mean () for col in [ 'max_flux_peak' , 'max_flux_int' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . max () for col in [ 'min_flux_peak' , 'min_flux_int' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . min () for col in [ 'min_flux_peak_isl_ratio' , 'min_flux_int_isl_ratio' ]: d [ col ] = df [ col . split ( '_' , 1 )[ 1 ]] . min () for col in [ 'flux_int' , 'flux_peak' ]: d [ f ' { col } _sq' ] = ( df [ col ] ** 2 ) . mean () d [ 'v_int' ] = df [ 'flux_int' ] . std () / df [ 'flux_int' ] . mean () d [ 'v_peak' ] = df [ 'flux_peak' ] . std () / df [ 'flux_peak' ] . mean () d [ 'eta_int' ] = get_eta_metric ( d , df ) d [ 'eta_peak' ] = get_eta_metric ( d , df , peak = True ) # remove not used cols for col in [ 'flux_int_sq' , 'flux_peak_sq' ]: d . pop ( col ) # get unique related sources list_uniq_related = list ( set ( chain . from_iterable ( lst for lst in df [ 'related' ] if isinstance ( lst , list ) ) )) d [ 'related_list' ] = list_uniq_related if list_uniq_related else - 1 return pd . Series ( d ) . fillna ( value = { \"v_int\" : 0.0 , \"v_peak\" : 0.0 })","title":"groupby_funcs()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.parallel_groupby","text":"Performs the parallel source dataframe operations to calculate the source metrics using Dask and returns the resulting dataframe. Parameters: Name Type Description Default df DataFrame The sources dataframe produced by the previous pipeline stages. required Returns: Type Description DataFrame The source dataframe with the calculated metric columns. Source code in vast_pipeline/pipeline/utils.py def parallel_groupby ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Performs the parallel source dataframe operations to calculate the source metrics using Dask and returns the resulting dataframe. Args: df: The sources dataframe produced by the previous pipeline stages. Returns: The source dataframe with the calculated metric columns. \"\"\" col_dtype = { 'img_list' : 'O' , 'n_meas_forced' : 'i' , 'n_meas' : 'i' , 'n_meas_sel' : 'i' , 'n_sibl' : 'i' , 'wavg_ra' : 'f' , 'wavg_dec' : 'f' , 'avg_compactness' : 'f' , 'min_snr' : 'f' , 'max_snr' : 'f' , 'wavg_uncertainty_ew' : 'f' , 'wavg_uncertainty_ns' : 'f' , 'avg_flux_int' : 'f' , 'avg_flux_peak' : 'f' , 'max_flux_peak' : 'f' , 'max_flux_int' : 'f' , 'min_flux_peak' : 'f' , 'min_flux_int' : 'f' , 'min_flux_peak_isl_ratio' : 'f' , 'min_flux_int_isl_ratio' : 'f' , 'v_int' : 'f' , 'v_peak' : 'f' , 'eta_int' : 'f' , 'eta_peak' : 'f' , 'related_list' : 'O' } n_cpu = cpu_count () - 1 out = dd . from_pandas ( df , n_cpu ) out = ( out . groupby ( 'source' ) . apply ( groupby_funcs , meta = col_dtype ) . compute ( num_workers = n_cpu , scheduler = 'processes' ) ) out [ 'n_rel' ] = out [ 'related_list' ] . apply ( lambda x : 0 if x == - 1 else len ( x )) return out","title":"parallel_groupby()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.parallel_groupby_coord","text":"This function uses Dask to perform the average coordinate and unique image and epoch lists calculation. The result from the Dask compute is returned which is a dataframe containing the results for each source. Parameters: Name Type Description Default df DataFrame The sources dataframe produced by the pipeline. required Returns: Type Description DataFrame The resulting average coordinate values and unique image and epoch lists for each unique source (group). Source code in vast_pipeline/pipeline/utils.py def parallel_groupby_coord ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" This function uses Dask to perform the average coordinate and unique image and epoch lists calculation. The result from the Dask compute is returned which is a dataframe containing the results for each source. Args: df: The sources dataframe produced by the pipeline. Returns: The resulting average coordinate values and unique image and epoch lists for each unique source (group). \"\"\" col_dtype = { 'img_list' : 'O' , 'epoch_list' : 'O' , 'wavg_ra' : 'f' , 'wavg_dec' : 'f' , } n_cpu = cpu_count () - 1 out = dd . from_pandas ( df , n_cpu ) out = ( out . groupby ( 'source' ) . apply ( calc_ave_coord , meta = col_dtype ) . compute ( num_workers = n_cpu , scheduler = 'processes' ) ) return out","title":"parallel_groupby_coord()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.prep_skysrc_df","text":"Initialise the source dataframe to use in association logic by reading the measurement parquet file and creating columns. When epoch based association is used it will also remove duplicate measurements from the list of sources. Parameters: Name Type Description Default images List[vast_pipeline.models.Image] A list holding the Image objects of the images to load measurements for. required perc_error float A percentage flux error to apply to the flux errors of the measurements. Defaults to 0. 0.0 duplicate_limit Optional[astropy.coordinates.angles.Angle] The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used in the 'remove_duplicate_measurements' function (usual ASKAP pixel size). None ini_df bool Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. False Returns: Type Description DataFrame The measurements of the image(s) with some extra values set ready for association and duplicates removed if necessary. Source code in vast_pipeline/pipeline/utils.py def prep_skysrc_df ( images : List [ Image ], perc_error : float = 0. , duplicate_limit : Optional [ Angle ] = None , ini_df : bool = False ) -> pd . DataFrame : ''' Initialise the source dataframe to use in association logic by reading the measurement parquet file and creating columns. When epoch based association is used it will also remove duplicate measurements from the list of sources. Args: images: A list holding the Image objects of the images to load measurements for. perc_error: A percentage flux error to apply to the flux errors of the measurements. Defaults to 0. duplicate_limit: The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used in the 'remove_duplicate_measurements' function (usual ASKAP pixel size). ini_df: Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. Returns: The measurements of the image(s) with some extra values set ready for association and duplicates removed if necessary. ''' cols = [ 'id' , 'ra' , 'uncertainty_ew' , 'weight_ew' , 'dec' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' ] df = _load_measurements ( images [ 0 ], cols , ini_df = ini_df ) if len ( images ) > 1 : for img in images [ 1 :]: df = df . append ( _load_measurements ( img , cols , df . source . max (), ini_df = ini_df ), ignore_index = True ) df = remove_duplicate_measurements ( df , dup_lim = duplicate_limit , ini_df = ini_df ) df = df . drop ( 'dist_from_centre' , axis = 1 ) if perc_error != 0.0 : logger . info ( 'Correcting flux errors with config error setting...' ) for col in [ 'flux_int' , 'flux_peak' ]: df [ f ' { col } _err' ] = np . hypot ( df [ f ' { col } _err' ] . values , perc_error * df [ col ] . values ) return df","title":"prep_skysrc_df()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.reconstruct_associtaion_dfs","text":"This function is used with add image mode and performs the necessary manipulations to reconstruct the sources_df and skyc1_srcs required by association. Parameters: Name Type Description Default images_df_done DataFrame The images_df output from the existing run (from the parquet). required previous_parquet_paths Dict[str, str] Dictionary that contains the paths for the previous run parquet files. Keys are 'images', 'associations', 'sources', 'relations' and 'measurement_pairs'. required Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] The reconstructed sources_df and skyc1_srs dataframes. Source code in vast_pipeline/pipeline/utils.py def reconstruct_associtaion_dfs ( images_df_done : pd . DataFrame , previous_parquet_paths : Dict [ str , str ] ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" This function is used with add image mode and performs the necessary manipulations to reconstruct the sources_df and skyc1_srcs required by association. Args: images_df_done: The images_df output from the existing run (from the parquet). previous_parquet_paths: Dictionary that contains the paths for the previous run parquet files. Keys are 'images', 'associations', 'sources', 'relations' and 'measurement_pairs'. Returns: The reconstructed sources_df and skyc1_srs dataframes. \"\"\" prev_associations = pd . read_parquet ( previous_parquet_paths [ 'associations' ]) # Get the parquet paths from the image objects img_meas_paths = ( images_df_done [ 'image_dj' ] . apply ( lambda x : x . measurements_path ) . to_list () ) # Obtain the pipeline run path in order to fetch forced measurements. run_path = previous_parquet_paths [ 'sources' ] . replace ( 'sources.parquet.bak' , '' ) # Get the forced measurement paths. img_fmeas_paths = [] for i in images_df_done . image_name . values : forced_parquet = os . path . join ( run_path , \"forced_measurements_ {} .parquet\" . format ( i . replace ( \".\" , \"_\" ) ) ) if os . path . isfile ( forced_parquet ): img_fmeas_paths . append ( forced_parquet ) # Create union of paths. img_meas_paths += img_fmeas_paths # Define the columns that are required cols = [ 'id' , 'ra' , 'uncertainty_ew' , 'weight_ew' , 'dec' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image_id' , 'time' , ] # Open all the parquets logger . debug ( \"Opening all measurement parquet files to use in reconstruction...\" ) measurements = pd . concat ( [ pd . read_parquet ( f , columns = cols ) for f in img_meas_paths ] ) # Create mask to drop measurements for epoch mode (epoch based mode). measurements_mask = measurements [ 'id' ] . isin ( prev_associations [ 'meas_id' ]) measurements = measurements . loc [ measurements_mask ] . set_index ( 'id' ) # Set the index on images_df for faster merging. images_df_done [ 'image_id' ] = images_df_done [ 'image_dj' ] . apply ( lambda x : x . id ) . values images_df_done = images_df_done . set_index ( 'image_id' ) # Merge image information to measurements measurements = ( measurements . merge ( images_df_done [[ 'image_name' , 'epoch' ]], left_on = 'image_id' , right_index = True ) . rename ( columns = { 'image_name' : 'image' }) ) # Drop any associations that are not used in this sky region group. associations_mask = prev_associations [ 'meas_id' ] . isin ( measurements . index . values ) prev_associations = prev_associations . loc [ associations_mask ] # Merge measurements into the associations to form the sources_df. sources_df = ( prev_associations . merge ( measurements , left_on = 'meas_id' , right_index = True ) . rename ( columns = { 'source_id' : 'source' , 'time' : 'datetime' , 'meas_id' : 'id' , 'ra' : 'ra_source' , 'dec' : 'dec_source' , 'uncertainty_ew' : 'uncertainty_ew_source' , 'uncertainty_ns' : 'uncertainty_ns_source' , }) ) # Load up the previous unique sources. prev_sources = pd . read_parquet ( previous_parquet_paths [ 'sources' ], columns = [ 'wavg_ra' , 'wavg_dec' , 'wavg_uncertainty_ew' , 'wavg_uncertainty_ns' , ] ) # Merge the wavg ra and dec to the sources_df - this is required to # create the skyc1_srcs below (but MUST be converted back to the source # ra and dec) sources_df = ( sources_df . merge ( prev_sources , left_on = 'source' , right_index = True ) . rename ( columns = { 'wavg_ra' : 'ra' , 'wavg_dec' : 'dec' , 'wavg_uncertainty_ew' : 'uncertainty_ew' , 'wavg_uncertainty_ns' : 'uncertainty_ns' , }) ) # Load the previous relations prev_relations = pd . read_parquet ( previous_parquet_paths [ 'relations' ]) # Form relation lists to merge in. prev_relations = pd . DataFrame ( prev_relations . groupby ( 'from_source_id' )[ 'to_source_id' ] . apply ( lambda x : x . values . tolist ()) ) . rename ( columns = { 'to_source_id' : 'related' }) # Append the relations to only the last instance of each source # First get the ids of the sources relation_ids = sources_df [ sources_df . source . isin ( prev_relations . index . values )] . drop_duplicates ( 'source' , keep = 'last' ) . index . values # Make sure we attach the correct source id source_ids = sources_df . loc [ relation_ids ] . source . values sources_df [ 'related' ] = np . nan relations_to_update = prev_relations . loc [ source_ids ] . to_numpy () . copy () relations_to_update = np . reshape ( relations_to_update , relations_to_update . shape [ 0 ]) sources_df . loc [ relation_ids , 'related' ] = relations_to_update # Reorder so we don't mess up the dask metas. sources_df = sources_df [[ 'id' , 'uncertainty_ew' , 'weight_ew' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image' , 'datetime' , 'source' , 'ra' , 'dec' , 'ra_source' , 'dec_source' , 'd2d' , 'dr' , 'related' , 'epoch' , 'uncertainty_ew_source' , 'uncertainty_ns_source' ]] # Create the unique skyc1_srcs dataframe. skyc1_srcs = ( sources_df [ ~ sources_df [ 'forced' ]] . sort_values ( by = 'id' ) . drop ( 'related' , axis = 1 ) . drop_duplicates ( 'source' ) ) . copy ( deep = True ) # Get relations into the skyc1_srcs (as we only keep the first instance # which does not have the relation information) skyc1_srcs = skyc1_srcs . merge ( prev_relations , how = 'left' , left_on = 'source' , right_index = True ) # Need to break the pointer relationship between the related sources ( # deep=True copy does not truly copy mutable type objects) relation_mask = skyc1_srcs . related . notna () relation_vals = skyc1_srcs . loc [ relation_mask , 'related' ] . to_list () new_relation_vals = [ x . copy () for x in relation_vals ] skyc1_srcs . loc [ relation_mask , 'related' ] = new_relation_vals # Reorder so we don't mess up the dask metas. skyc1_srcs = skyc1_srcs [[ 'id' , 'ra' , 'uncertainty_ew' , 'weight_ew' , 'dec' , 'uncertainty_ns' , 'weight_ns' , 'flux_int' , 'flux_int_err' , 'flux_int_isl_ratio' , 'flux_peak' , 'flux_peak_err' , 'flux_peak_isl_ratio' , 'forced' , 'compactness' , 'has_siblings' , 'snr' , 'image' , 'datetime' , 'source' , 'ra_source' , 'dec_source' , 'd2d' , 'dr' , 'related' , 'epoch' ]] . reset_index ( drop = True ) # Finally move the source ra and dec back to the sources_df ra and dec # columns sources_df [ 'ra' ] = sources_df [ 'ra_source' ] sources_df [ 'dec' ] = sources_df [ 'dec_source' ] sources_df [ 'uncertainty_ew' ] = sources_df [ 'uncertainty_ew_source' ] sources_df [ 'uncertainty_ns' ] = sources_df [ 'uncertainty_ns_source' ] # Drop not needed columns for the sources_df. sources_df = sources_df . drop ([ 'uncertainty_ew_source' , 'uncertainty_ns_source' ], axis = 1 ) . reset_index ( drop = True ) return sources_df , skyc1_srcs","title":"reconstruct_associtaion_dfs()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.remove_duplicate_measurements","text":"Remove perceived duplicate sources from a dataframe of loaded measurements. Duplicates are determined by their separation and whether this distances is within the 'dup_lim'. Parameters: Name Type Description Default sources_df DataFrame The loaded measurements from two or more images. required dup_lim Optional[astropy.coordinates.angles.Angle] The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used (usual ASKAP pixel size). None ini_df bool Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. False Returns: Type Description DataFrame The input sources_df with duplicate sources removed. Source code in vast_pipeline/pipeline/utils.py def remove_duplicate_measurements ( sources_df : pd . DataFrame , dup_lim : Optional [ Angle ] = None , ini_df : bool = False ) -> pd . DataFrame : \"\"\" Remove perceived duplicate sources from a dataframe of loaded measurements. Duplicates are determined by their separation and whether this distances is within the 'dup_lim'. Args: sources_df: The loaded measurements from two or more images. dup_lim: The separation limit of when a source is considered a duplicate. Defaults to None in which case 2.5 arcsec is used (usual ASKAP pixel size). ini_df: Boolean to indicate whether these sources are part of the initial source list creation for association. If 'True' the source ids are reset ready for the first iteration. Defaults to 'False'. Returns: The input sources_df with duplicate sources removed. \"\"\" logger . debug ( 'Cleaning duplicate sources from epoch...' ) if dup_lim is None : dup_lim = Angle ( 2.5 * u . arcsec ) logger . debug ( 'Using duplicate crossmatch radius of %.2f arcsec.' , dup_lim . arcsec ) # sort by the distance from the image centre so we know # that the first source is always the one to keep sources_df = sources_df . sort_values ( by = 'dist_from_centre' ) sources_sc = SkyCoord ( sources_df [ 'ra' ], sources_df [ 'dec' ], unit = ( u . deg , u . deg ) ) # perform search around sky to get all self matches idxc , idxcatalog , * _ = sources_sc . search_around_sky ( sources_sc , dup_lim ) # create df from results results = pd . DataFrame ( data = { 'source_id' : idxc , 'match_id' : idxcatalog , 'source_image' : sources_df . iloc [ idxc ][ 'image' ] . tolist (), 'match_image' : sources_df . iloc [ idxcatalog ][ 'image' ] . tolist () } ) # Drop those that are matched from the same image matching_image_mask = ( results [ 'source_image' ] != results [ 'match_image' ] ) results = ( results . loc [ matching_image_mask ] . drop ([ 'source_image' , 'match_image' ], axis = 1 ) ) # create a pair column defining each pair ith index results [ 'pair' ] = results . apply ( tuple , 1 ) . apply ( sorted ) . apply ( tuple ) # Drop the duplicate pairs (pairs are sorted so this works) results = results . drop_duplicates ( 'pair' ) # No longer need pair results = results . drop ( 'pair' , axis = 1 ) # Drop all self matches and we are left with those to drop # in the match id column. to_drop = results . loc [ results [ 'source_id' ] != results [ 'match_id' ], 'match_id' ] # Get the index values from the ith values to_drop_indexes = sources_df . iloc [ to_drop ] . index . values logger . debug ( \"Dropping %i duplicate measurements.\" , to_drop_indexes . shape [ 0 ] ) # Drop them from sources sources_df = sources_df . drop ( to_drop_indexes ) . sort_values ( by = 'ra' ) # reset the source_df index sources_df = sources_df . reset_index ( drop = True ) # Reset the source number if ini_df : sources_df [ 'source' ] = sources_df . index + 1 del results return sources_df","title":"remove_duplicate_measurements()"},{"location":"reference/pipeline/utils/#vast_pipeline.pipeline.utils.write_parquets","text":"This function saves images, skyregions and bands to parquet files. It also returns a DataFrame containing containing the information of the sky regions associated with the current run. Parameters: Name Type Description Default images List[vast_pipeline.models.Image] list of image Django ORM objects. required skyregions List[vast_pipeline.models.SkyRegion] list sky region Django ORM objects. required bands List[vast_pipeline.models.Band] list of band Django ORM objects. required run_path str directory to save parquets to. required Returns: Type Description DataFrame Sky regions as pandas DataFrame Source code in vast_pipeline/pipeline/utils.py def write_parquets ( images : List [ Image ], skyregions : List [ SkyRegion ], bands : List [ Band ], run_path : str ) -> pd . DataFrame : \"\"\" This function saves images, skyregions and bands to parquet files. It also returns a DataFrame containing containing the information of the sky regions associated with the current run. Args: images: list of image Django ORM objects. skyregions: list sky region Django ORM objects. bands: list of band Django ORM objects. run_path: directory to save parquets to. Returns: Sky regions as pandas DataFrame \"\"\" # write images parquet file under pipeline run folder images_df = pd . DataFrame ( map ( lambda x : x . __dict__ , images )) images_df = images_df . drop ( '_state' , axis = 1 ) images_df . to_parquet ( os . path . join ( run_path , 'images.parquet' ), index = False ) # write skyregions parquet file under pipeline run folder skyregs_df = pd . DataFrame ( map ( lambda x : x . __dict__ , skyregions )) skyregs_df = skyregs_df . drop ( '_state' , axis = 1 ) skyregs_df . to_parquet ( os . path . join ( run_path , 'skyregions.parquet' ), index = False ) # write skyregions parquet file under pipeline run folder bands_df = pd . DataFrame ( map ( lambda x : x . __dict__ , bands )) bands_df = bands_df . drop ( '_state' , axis = 1 ) bands_df . to_parquet ( os . path . join ( run_path , 'bands.parquet' ), index = False ) return skyregs_df","title":"write_parquets()"},{"location":"reference/survey/translators/","text":"Translators to map the table column names in the input files into the required column names in our db. The flux/ang scale needs to be the multiplicative factor that converts the input flux into mJy and a/b into arcsec. tr_aegean \u00b6 The translator dictionary for a Agean catalogue input. Not complete. tr_gleam \u00b6 The translator dictionary for a GLEAM catalogue input. Not complete. tr_mwacs \u00b6 The translator dictionary for a MWACS catalogue input. Not complete. tr_nvss \u00b6 The translator dictionary for a NVSS catalogue input. Not complete. tr_selavy \u00b6 The translator dictionary for a Selavy catalogue input. tr_sumss \u00b6 The translator dictionary for a SUMSS catalogue input. Not complete. translators \u00b6 Dictionary containing all the translators defined in this module.","title":"translators.py"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_aegean","text":"The translator dictionary for a Agean catalogue input. Not complete.","title":"tr_aegean"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_gleam","text":"The translator dictionary for a GLEAM catalogue input. Not complete.","title":"tr_gleam"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_mwacs","text":"The translator dictionary for a MWACS catalogue input. Not complete.","title":"tr_mwacs"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_nvss","text":"The translator dictionary for a NVSS catalogue input. Not complete.","title":"tr_nvss"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_selavy","text":"The translator dictionary for a Selavy catalogue input.","title":"tr_selavy"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.tr_sumss","text":"The translator dictionary for a SUMSS catalogue input. Not complete.","title":"tr_sumss"},{"location":"reference/survey/translators/#vast_pipeline.survey.translators.translators","text":"Dictionary containing all the translators defined in this module.","title":"translators"},{"location":"reference/utils/auth/","text":"create_admin_user ( uid , response , details , user , social , * args , ** kwargs ) \u00b6 Give Django admin privileges to a user who login via GitHub and belong to a specific team. The parameters are as per python-social-auth docs https://python-social-auth.readthedocs.io/en/latest/pipeline.html#extending-the-pipeline Parameters: Name Type Description Default uid int user id required response Dict request dictionary required details Dict user details generated by the backend required user User Django user model object required social UserSocialAuth Social auth user model object required Returns: Type Description Dict return a dictionary with the Django User object in it or empty if no action are taken Source code in vast_pipeline/utils/auth.py def create_admin_user ( uid : int , response : Dict , details : Dict , user : User , social : UserSocialAuth , * args , ** kwargs ) -> Dict : \"\"\" Give Django admin privileges to a user who login via GitHub and belong to a specific team. The parameters are as per python-social-auth docs https://python-social-auth.readthedocs.io/en/latest/pipeline.html#extending-the-pipeline Args: uid: user id response: request dictionary details: user details generated by the backend user: Django user model object social: Social auth user model object Returns: return a dictionary with the Django User object in it or empty if no action are taken \"\"\" # assume github-org backend, add <if backend.name == 'github-org'> # if other backend are implemented admin_team = settings . SOCIAL_AUTH_GITHUB_ADMIN_TEAM usr = response . get ( 'login' , '' ) if ( usr != '' and admin_team != '' and user and not user . is_staff and not user . is_superuser ): logger . info ( 'Trying to add Django admin privileges to user' ) # check if github user belong to admin team org = settings . SOCIAL_AUTH_GITHUB_ORG_NAME header = { 'Authorization' : f \"token { response . get ( 'access_token' , '' ) } \" } url = ( f 'https://api.github.com/orgs/ { org } /teams/ { admin_team } ' f '/memberships/ { usr } ' ) resp = requests . get ( url , headers = header ) if resp . ok : # add user to admin user . is_superuser = True user . is_staff = True user . save () logger . info ( 'Django admin privileges successfully added to user' ) return { 'user' : user } logger . info ( f 'GitHub request failed, reason: { resp . reason } ' ) return {} return {} load_github_avatar ( response , social , * args , ** kwargs ) \u00b6 Add GitHub avatar url to the extra data stored by social_django app Parameters: Name Type Description Default response Dict request dictionary required social UserSocialAuth Social auth user model object required *args Variable length argument list. () **kwargs Arbitrary keyword arguments. {} Returns: Type Description Dict return a dictionary with the Social auth user object in it or empty if no action are taken Source code in vast_pipeline/utils/auth.py def load_github_avatar ( response : Dict , social : UserSocialAuth , * args , ** kwargs ) -> Dict : \"\"\" Add GitHub avatar url to the extra data stored by social_django app Args: response: request dictionary social: Social auth user model object *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. Returns: return a dictionary with the Social auth user object in it or empty if no action are taken \"\"\" # assume github-org backend, add <if backend.name == 'github-org'> # if other backend are implemented # if social and social.get('extra_data', None) # print(vars(social)) if 'avatar_url' not in social . extra_data : logger . info ( 'Adding GitHub avatar url to user extra data' ) social . extra_data [ 'avatar_url' ] = response [ 'avatar_url' ] social . save () return { 'social' : social } return {}","title":"auth.py"},{"location":"reference/utils/auth/#vast_pipeline.utils.auth.create_admin_user","text":"Give Django admin privileges to a user who login via GitHub and belong to a specific team. The parameters are as per python-social-auth docs https://python-social-auth.readthedocs.io/en/latest/pipeline.html#extending-the-pipeline Parameters: Name Type Description Default uid int user id required response Dict request dictionary required details Dict user details generated by the backend required user User Django user model object required social UserSocialAuth Social auth user model object required Returns: Type Description Dict return a dictionary with the Django User object in it or empty if no action are taken Source code in vast_pipeline/utils/auth.py def create_admin_user ( uid : int , response : Dict , details : Dict , user : User , social : UserSocialAuth , * args , ** kwargs ) -> Dict : \"\"\" Give Django admin privileges to a user who login via GitHub and belong to a specific team. The parameters are as per python-social-auth docs https://python-social-auth.readthedocs.io/en/latest/pipeline.html#extending-the-pipeline Args: uid: user id response: request dictionary details: user details generated by the backend user: Django user model object social: Social auth user model object Returns: return a dictionary with the Django User object in it or empty if no action are taken \"\"\" # assume github-org backend, add <if backend.name == 'github-org'> # if other backend are implemented admin_team = settings . SOCIAL_AUTH_GITHUB_ADMIN_TEAM usr = response . get ( 'login' , '' ) if ( usr != '' and admin_team != '' and user and not user . is_staff and not user . is_superuser ): logger . info ( 'Trying to add Django admin privileges to user' ) # check if github user belong to admin team org = settings . SOCIAL_AUTH_GITHUB_ORG_NAME header = { 'Authorization' : f \"token { response . get ( 'access_token' , '' ) } \" } url = ( f 'https://api.github.com/orgs/ { org } /teams/ { admin_team } ' f '/memberships/ { usr } ' ) resp = requests . get ( url , headers = header ) if resp . ok : # add user to admin user . is_superuser = True user . is_staff = True user . save () logger . info ( 'Django admin privileges successfully added to user' ) return { 'user' : user } logger . info ( f 'GitHub request failed, reason: { resp . reason } ' ) return {} return {}","title":"create_admin_user()"},{"location":"reference/utils/auth/#vast_pipeline.utils.auth.load_github_avatar","text":"Add GitHub avatar url to the extra data stored by social_django app Parameters: Name Type Description Default response Dict request dictionary required social UserSocialAuth Social auth user model object required *args Variable length argument list. () **kwargs Arbitrary keyword arguments. {} Returns: Type Description Dict return a dictionary with the Social auth user object in it or empty if no action are taken Source code in vast_pipeline/utils/auth.py def load_github_avatar ( response : Dict , social : UserSocialAuth , * args , ** kwargs ) -> Dict : \"\"\" Add GitHub avatar url to the extra data stored by social_django app Args: response: request dictionary social: Social auth user model object *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. Returns: return a dictionary with the Social auth user object in it or empty if no action are taken \"\"\" # assume github-org backend, add <if backend.name == 'github-org'> # if other backend are implemented # if social and social.get('extra_data', None) # print(vars(social)) if 'avatar_url' not in social . extra_data : logger . info ( 'Adding GitHub avatar url to user extra data' ) social . extra_data [ 'avatar_url' ] = response [ 'avatar_url' ] social . save () return { 'social' : social } return {}","title":"load_github_avatar()"},{"location":"reference/utils/external_query/","text":"ned ( coord , radius ) \u00b6 Perform a cone search for sources with NED. Parameters: Name Type Description Default coord SkyCoord The coordinate of the centre of the cone. required radius Angle The radius of the cone in angular units. required Returns: Type Description List[Dict[str, Any]] A list of dicts, where each dict is a query result row with the following keys: object_name: the name of the astronomical object. database: the source of the result, i.e. NED. separation_arcsec: separation to the query coordinate in arcsec. otype: object type. otype_long: long form of the object type. ra_hms: RA coordinate string in hms format. dec_dms: Dec coordinate string in \u00b1dms format. Source code in vast_pipeline/utils/external_query.py def ned ( coord : SkyCoord , radius : Angle ) -> List [ Dict [ str , Any ]]: \"\"\"Perform a cone search for sources with NED. Args: coord: The coordinate of the centre of the cone. radius: The radius of the cone in angular units. Returns: A list of dicts, where each dict is a query result row with the following keys: - object_name: the name of the astronomical object. - database: the source of the result, i.e. NED. - separation_arcsec: separation to the query coordinate in arcsec. - otype: object type. - otype_long: long form of the object type. - ra_hms: RA coordinate string in hms format. - dec_dms: Dec coordinate string in \u00b1dms format. \"\"\" # NED API doesn't supply the long-form object types. # Copied from https://ned.ipac.caltech.edu/Documents/Guides/Database NED_OTYPES = { \"*\" : \"Star or Point Source\" , \"**\" : \"Double star\" , \"*Ass\" : \"Stellar association\" , \"*Cl\" : \"Star cluster\" , \"AbLS\" : \"Absorption line system\" , \"Blue*\" : \"Blue star\" , \"C*\" : \"Carbon star\" , \"EmLS\" : \"Emission line source\" , \"EmObj\" : \"Emission object\" , \"exG*\" : \"Extragalactic star (not a member of an identified galaxy)\" , \"Flare*\" : \"Flare star\" , \"G\" : \"Galaxy\" , \"GammaS\" : \"Gamma ray source\" , \"GClstr\" : \"Cluster of galaxies\" , \"GGroup\" : \"Group of galaxies\" , \"GPair\" : \"Galaxy pair\" , \"GTrpl\" : \"Galaxy triple\" , \"G_Lens\" : \"Lensed image of a galaxy\" , \"HII\" : \"HII region\" , \"IrS\" : \"Infrared source\" , \"MCld\" : \"Molecular cloud\" , \"Neb\" : \"Nebula\" , \"Nova\" : \"Nova\" , \"Other\" : \"Other classification (e.g. comet; plate defect)\" , \"PN\" : \"Planetary nebula\" , \"PofG\" : \"Part of galaxy\" , \"Psr\" : \"Pulsar\" , \"QGroup\" : \"Group of QSOs\" , \"QSO\" : \"Quasi-stellar object\" , \"Q_Lens\" : \"Lensed image of a QSO\" , \"RadioS\" : \"Radio source\" , \"Red*\" : \"Red star\" , \"RfN\" : \"Reflection nebula\" , \"SN\" : \"Supernova\" , \"SNR\" : \"Supernova remnant\" , \"UvES\" : \"Ultraviolet excess source\" , \"UvS\" : \"Ultraviolet source\" , \"V*\" : \"Variable star\" , \"VisS\" : \"Visual source\" , \"WD*\" : \"White dwarf\" , \"WR*\" : \"Wolf-Rayet star\" , \"XrayS\" : \"X-ray source\" , \"!*\" : \"Galactic star\" , \"!**\" : \"Galactic double star\" , \"!*Ass\" : \"Galactic star association\" , \"!*Cl\" : \"Galactic Star cluster\" , \"!Blue*\" : \"Galactic blue star\" , \"!C*\" : \"Galactic carbon star\" , \"!EmObj\" : \"Galactic emission line object\" , \"!Flar*\" : \"Galactic flare star\" , \"!HII\" : \"Galactic HII region\" , \"!MCld\" : \"Galactic molecular cloud\" , \"!Neb\" : \"Galactic nebula\" , \"!Nova\" : \"Galactic nova\" , \"!PN\" : \"Galactic planetary nebula\" , \"!Psr\" : \"Galactic pulsar\" , \"!RfN\" : \"Galactic reflection nebula\" , \"!Red*\" : \"Galactic red star\" , \"!SN\" : \"Galactic supernova\" , \"!SNR\" : \"Galactic supernova remnant\" , \"!V*\" : \"Galactic variable star\" , \"!WD*\" : \"Galactic white dwarf\" , \"!WR*\" : \"Galactic Wolf-Rayet star\" , } ned_result_table = Ned . query_region ( coord , radius = radius ) if ned_result_table is None or len ( ned_result_table ) == 0 : ned_results_dict_list = [] else : ned_results_df = ned_result_table [ [ \"Object Name\" , \"Separation\" , \"Type\" , \"RA\" , \"DEC\" ] ] . to_pandas () ned_results_df = ned_results_df . rename ( columns = { \"Object Name\" : \"object_name\" , \"Separation\" : \"separation_arcsec\" , \"Type\" : \"otype\" , \"RA\" : \"ra_hms\" , \"DEC\" : \"dec_dms\" , } ) ned_results_df [ \"otype_long\" ] = ned_results_df . otype . replace ( NED_OTYPES ) # convert NED result separation (arcmin) to arcsec ned_results_df [ \"separation_arcsec\" ] = ned_results_df [ \"separation_arcsec\" ] * 60 # convert coordinates to RA (hms) Dec (dms) strings ned_results_df [ \"ra_hms\" ] = Longitude ( ned_results_df [ \"ra_hms\" ], unit = \"deg\" ) . to_string ( unit = \"hourangle\" ) ned_results_df [ \"dec_dms\" ] = Latitude ( ned_results_df [ \"dec_dms\" ], unit = \"deg\" ) . to_string ( unit = \"deg\" ) ned_results_df [ \"database\" ] = \"NED\" # convert dataframe to dict and replace float NaNs with None for JSON encoding ned_results_dict_list = ned_results_df . sort_values ( \"separation_arcsec\" ) . to_dict ( orient = \"records\" ) return ned_results_dict_list simbad ( coord , radius ) \u00b6 Perform a cone search for sources with SIMBAD. Parameters: Name Type Description Default coord SkyCoord The coordinate of the centre of the cone. required radius Angle The radius of the cone in angular units. required Returns: Type Description List[Dict[str, Any]] A list of dicts, where each dict is a query result row with the following keys: object_name: the name of the astronomical object. database: the source of the result, i.e. SIMBAD. separation_arcsec: separation to the query coordinate in arcsec. otype: object type. otype_long: long form of the object type. ra_hms: RA coordinate string in hms format. dec_dms: Dec coordinate string in \u00b1dms format. Source code in vast_pipeline/utils/external_query.py def simbad ( coord : SkyCoord , radius : Angle ) -> List [ Dict [ str , Any ]]: \"\"\"Perform a cone search for sources with SIMBAD. Args: coord: The coordinate of the centre of the cone. radius: The radius of the cone in angular units. Returns: A list of dicts, where each dict is a query result row with the following keys: - object_name: the name of the astronomical object. - database: the source of the result, i.e. SIMBAD. - separation_arcsec: separation to the query coordinate in arcsec. - otype: object type. - otype_long: long form of the object type. - ra_hms: RA coordinate string in hms format. - dec_dms: Dec coordinate string in \u00b1dms format. \"\"\" CustomSimbad = Simbad () CustomSimbad . add_votable_fields ( \"distance_result\" , \"otype(S)\" , \"otype(V)\" , \"otypes\" , ) try : simbad_result_table = CustomSimbad . query_region ( coord , radius = radius ) except requests . HTTPError : # try the Harvard mirror CustomSimbad . SIMBAD_URL = \"https://simbad.harvard.edu/simbad/sim-script\" simbad_result_table = CustomSimbad . query_region ( coord , radius = radius ) if simbad_result_table is None : simbad_results_dict_list = [] else : simbad_results_df = simbad_result_table [ [ \"MAIN_ID\" , \"DISTANCE_RESULT\" , \"OTYPE_S\" , \"OTYPE_V\" , \"RA\" , \"DEC\" ] ] . to_pandas () simbad_results_df = simbad_results_df . rename ( columns = { \"MAIN_ID\" : \"object_name\" , \"DISTANCE_RESULT\" : \"separation_arcsec\" , \"OTYPE_S\" : \"otype\" , \"OTYPE_V\" : \"otype_long\" , \"RA\" : \"ra_hms\" , \"DEC\" : \"dec_dms\" , } ) simbad_results_df [ \"database\" ] = \"SIMBAD\" # convert coordinates to RA (hms) Dec (dms) strings simbad_results_df [ \"ra_hms\" ] = Longitude ( simbad_results_df [ \"ra_hms\" ], unit = \"hourangle\" ) . to_string ( unit = \"hourangle\" ) simbad_results_df [ \"dec_dms\" ] = Latitude ( simbad_results_df [ \"dec_dms\" ], unit = \"deg\" ) . to_string ( unit = \"deg\" ) simbad_results_dict_list = simbad_results_df . to_dict ( orient = \"records\" ) return simbad_results_dict_list tns ( coord , radius ) \u00b6 Perform a cone search for sources with the Transient Name Server (TNS). Parameters: Name Type Description Default coord SkyCoord The coordinate of the centre of the cone. required radius Angle The radius of the cone in angular units. required Returns: Type Description List[Dict[str, Any]] A list of dicts, where each dict is a query result row with the following keys: object_name: the name of the transient. database: the source of the result, i.e. TNS. separation_arcsec: separation to the query coordinate in arcsec. otype: object type. otype_long: long form of the object type. Not given by TNS, will always be an empty string. ra_hms: RA coordinate string in hms format. dec_dms: Dec coordinate string in \u00b1dms format. Source code in vast_pipeline/utils/external_query.py def tns ( coord : SkyCoord , radius : Angle ) -> List [ Dict [ str , Any ]]: \"\"\"Perform a cone search for sources with the Transient Name Server (TNS). Args: coord: The coordinate of the centre of the cone. radius: The radius of the cone in angular units. Returns: A list of dicts, where each dict is a query result row with the following keys: - object_name: the name of the transient. - database: the source of the result, i.e. TNS. - separation_arcsec: separation to the query coordinate in arcsec. - otype: object type. - otype_long: long form of the object type. Not given by TNS, will always be an empty string. - ra_hms: RA coordinate string in hms format. - dec_dms: Dec coordinate string in \u00b1dms format. \"\"\" TNS_API_URL = \"https://www.wis-tns.org/api/\" headers = { \"user-agent\" : settings . TNS_USER_AGENT , } search_dict = { \"ra\" : coord . ra . to_string ( unit = \"hourangle\" , sep = \":\" , pad = True ), \"dec\" : coord . dec . to_string ( unit = \"deg\" , sep = \":\" , alwayssign = True , pad = True ), \"radius\" : str ( radius . value ), \"units\" : radius . unit . name , } r = requests . post ( urljoin ( TNS_API_URL , \"get/search\" ), data = { \"api_key\" : settings . TNS_API_KEY , \"data\" : json . dumps ( search_dict )}, headers = headers , ) tns_results_dict_list : List [ Dict [ str , Any ]] if r . ok : tns_results_dict_list = r . json ()[ \"data\" ][ \"reply\" ] # Get details for each object result. TNS API doesn't support doing this in one # request, so we iterate. for result in tns_results_dict_list : search_dict = { \"objname\" : result [ \"objname\" ], } r = requests . post ( urljoin ( TNS_API_URL , \"get/object\" ), data = { \"api_key\" : settings . TNS_API_KEY , \"data\" : json . dumps ( search_dict )}, headers = headers , ) if r . ok : object_dict = r . json ()[ \"data\" ][ \"reply\" ] object_coord = SkyCoord ( ra = object_dict [ \"radeg\" ], dec = object_dict [ \"decdeg\" ], unit = \"deg\" ) result [ \"otype\" ] = object_dict [ \"object_type\" ][ \"name\" ] if result [ \"otype\" ] is None : result [ \"otype\" ] = \"\" result [ \"otype_long\" ] = \"\" result [ \"separation_arcsec\" ] = coord . separation ( object_coord ) . arcsec result [ \"ra_hms\" ] = object_coord . ra . to_string ( unit = \"hourangle\" ) result [ \"dec_dms\" ] = object_coord . dec . to_string ( unit = \"deg\" ) result [ \"database\" ] = \"TNS\" result [ \"object_name\" ] = object_dict [ \"objname\" ] return tns_results_dict_list","title":"external_query.py"},{"location":"reference/utils/external_query/#vast_pipeline.utils.external_query.ned","text":"Perform a cone search for sources with NED. Parameters: Name Type Description Default coord SkyCoord The coordinate of the centre of the cone. required radius Angle The radius of the cone in angular units. required Returns: Type Description List[Dict[str, Any]] A list of dicts, where each dict is a query result row with the following keys: object_name: the name of the astronomical object. database: the source of the result, i.e. NED. separation_arcsec: separation to the query coordinate in arcsec. otype: object type. otype_long: long form of the object type. ra_hms: RA coordinate string in hms format. dec_dms: Dec coordinate string in \u00b1dms format. Source code in vast_pipeline/utils/external_query.py def ned ( coord : SkyCoord , radius : Angle ) -> List [ Dict [ str , Any ]]: \"\"\"Perform a cone search for sources with NED. Args: coord: The coordinate of the centre of the cone. radius: The radius of the cone in angular units. Returns: A list of dicts, where each dict is a query result row with the following keys: - object_name: the name of the astronomical object. - database: the source of the result, i.e. NED. - separation_arcsec: separation to the query coordinate in arcsec. - otype: object type. - otype_long: long form of the object type. - ra_hms: RA coordinate string in hms format. - dec_dms: Dec coordinate string in \u00b1dms format. \"\"\" # NED API doesn't supply the long-form object types. # Copied from https://ned.ipac.caltech.edu/Documents/Guides/Database NED_OTYPES = { \"*\" : \"Star or Point Source\" , \"**\" : \"Double star\" , \"*Ass\" : \"Stellar association\" , \"*Cl\" : \"Star cluster\" , \"AbLS\" : \"Absorption line system\" , \"Blue*\" : \"Blue star\" , \"C*\" : \"Carbon star\" , \"EmLS\" : \"Emission line source\" , \"EmObj\" : \"Emission object\" , \"exG*\" : \"Extragalactic star (not a member of an identified galaxy)\" , \"Flare*\" : \"Flare star\" , \"G\" : \"Galaxy\" , \"GammaS\" : \"Gamma ray source\" , \"GClstr\" : \"Cluster of galaxies\" , \"GGroup\" : \"Group of galaxies\" , \"GPair\" : \"Galaxy pair\" , \"GTrpl\" : \"Galaxy triple\" , \"G_Lens\" : \"Lensed image of a galaxy\" , \"HII\" : \"HII region\" , \"IrS\" : \"Infrared source\" , \"MCld\" : \"Molecular cloud\" , \"Neb\" : \"Nebula\" , \"Nova\" : \"Nova\" , \"Other\" : \"Other classification (e.g. comet; plate defect)\" , \"PN\" : \"Planetary nebula\" , \"PofG\" : \"Part of galaxy\" , \"Psr\" : \"Pulsar\" , \"QGroup\" : \"Group of QSOs\" , \"QSO\" : \"Quasi-stellar object\" , \"Q_Lens\" : \"Lensed image of a QSO\" , \"RadioS\" : \"Radio source\" , \"Red*\" : \"Red star\" , \"RfN\" : \"Reflection nebula\" , \"SN\" : \"Supernova\" , \"SNR\" : \"Supernova remnant\" , \"UvES\" : \"Ultraviolet excess source\" , \"UvS\" : \"Ultraviolet source\" , \"V*\" : \"Variable star\" , \"VisS\" : \"Visual source\" , \"WD*\" : \"White dwarf\" , \"WR*\" : \"Wolf-Rayet star\" , \"XrayS\" : \"X-ray source\" , \"!*\" : \"Galactic star\" , \"!**\" : \"Galactic double star\" , \"!*Ass\" : \"Galactic star association\" , \"!*Cl\" : \"Galactic Star cluster\" , \"!Blue*\" : \"Galactic blue star\" , \"!C*\" : \"Galactic carbon star\" , \"!EmObj\" : \"Galactic emission line object\" , \"!Flar*\" : \"Galactic flare star\" , \"!HII\" : \"Galactic HII region\" , \"!MCld\" : \"Galactic molecular cloud\" , \"!Neb\" : \"Galactic nebula\" , \"!Nova\" : \"Galactic nova\" , \"!PN\" : \"Galactic planetary nebula\" , \"!Psr\" : \"Galactic pulsar\" , \"!RfN\" : \"Galactic reflection nebula\" , \"!Red*\" : \"Galactic red star\" , \"!SN\" : \"Galactic supernova\" , \"!SNR\" : \"Galactic supernova remnant\" , \"!V*\" : \"Galactic variable star\" , \"!WD*\" : \"Galactic white dwarf\" , \"!WR*\" : \"Galactic Wolf-Rayet star\" , } ned_result_table = Ned . query_region ( coord , radius = radius ) if ned_result_table is None or len ( ned_result_table ) == 0 : ned_results_dict_list = [] else : ned_results_df = ned_result_table [ [ \"Object Name\" , \"Separation\" , \"Type\" , \"RA\" , \"DEC\" ] ] . to_pandas () ned_results_df = ned_results_df . rename ( columns = { \"Object Name\" : \"object_name\" , \"Separation\" : \"separation_arcsec\" , \"Type\" : \"otype\" , \"RA\" : \"ra_hms\" , \"DEC\" : \"dec_dms\" , } ) ned_results_df [ \"otype_long\" ] = ned_results_df . otype . replace ( NED_OTYPES ) # convert NED result separation (arcmin) to arcsec ned_results_df [ \"separation_arcsec\" ] = ned_results_df [ \"separation_arcsec\" ] * 60 # convert coordinates to RA (hms) Dec (dms) strings ned_results_df [ \"ra_hms\" ] = Longitude ( ned_results_df [ \"ra_hms\" ], unit = \"deg\" ) . to_string ( unit = \"hourangle\" ) ned_results_df [ \"dec_dms\" ] = Latitude ( ned_results_df [ \"dec_dms\" ], unit = \"deg\" ) . to_string ( unit = \"deg\" ) ned_results_df [ \"database\" ] = \"NED\" # convert dataframe to dict and replace float NaNs with None for JSON encoding ned_results_dict_list = ned_results_df . sort_values ( \"separation_arcsec\" ) . to_dict ( orient = \"records\" ) return ned_results_dict_list","title":"ned()"},{"location":"reference/utils/external_query/#vast_pipeline.utils.external_query.simbad","text":"Perform a cone search for sources with SIMBAD. Parameters: Name Type Description Default coord SkyCoord The coordinate of the centre of the cone. required radius Angle The radius of the cone in angular units. required Returns: Type Description List[Dict[str, Any]] A list of dicts, where each dict is a query result row with the following keys: object_name: the name of the astronomical object. database: the source of the result, i.e. SIMBAD. separation_arcsec: separation to the query coordinate in arcsec. otype: object type. otype_long: long form of the object type. ra_hms: RA coordinate string in hms format. dec_dms: Dec coordinate string in \u00b1dms format. Source code in vast_pipeline/utils/external_query.py def simbad ( coord : SkyCoord , radius : Angle ) -> List [ Dict [ str , Any ]]: \"\"\"Perform a cone search for sources with SIMBAD. Args: coord: The coordinate of the centre of the cone. radius: The radius of the cone in angular units. Returns: A list of dicts, where each dict is a query result row with the following keys: - object_name: the name of the astronomical object. - database: the source of the result, i.e. SIMBAD. - separation_arcsec: separation to the query coordinate in arcsec. - otype: object type. - otype_long: long form of the object type. - ra_hms: RA coordinate string in hms format. - dec_dms: Dec coordinate string in \u00b1dms format. \"\"\" CustomSimbad = Simbad () CustomSimbad . add_votable_fields ( \"distance_result\" , \"otype(S)\" , \"otype(V)\" , \"otypes\" , ) try : simbad_result_table = CustomSimbad . query_region ( coord , radius = radius ) except requests . HTTPError : # try the Harvard mirror CustomSimbad . SIMBAD_URL = \"https://simbad.harvard.edu/simbad/sim-script\" simbad_result_table = CustomSimbad . query_region ( coord , radius = radius ) if simbad_result_table is None : simbad_results_dict_list = [] else : simbad_results_df = simbad_result_table [ [ \"MAIN_ID\" , \"DISTANCE_RESULT\" , \"OTYPE_S\" , \"OTYPE_V\" , \"RA\" , \"DEC\" ] ] . to_pandas () simbad_results_df = simbad_results_df . rename ( columns = { \"MAIN_ID\" : \"object_name\" , \"DISTANCE_RESULT\" : \"separation_arcsec\" , \"OTYPE_S\" : \"otype\" , \"OTYPE_V\" : \"otype_long\" , \"RA\" : \"ra_hms\" , \"DEC\" : \"dec_dms\" , } ) simbad_results_df [ \"database\" ] = \"SIMBAD\" # convert coordinates to RA (hms) Dec (dms) strings simbad_results_df [ \"ra_hms\" ] = Longitude ( simbad_results_df [ \"ra_hms\" ], unit = \"hourangle\" ) . to_string ( unit = \"hourangle\" ) simbad_results_df [ \"dec_dms\" ] = Latitude ( simbad_results_df [ \"dec_dms\" ], unit = \"deg\" ) . to_string ( unit = \"deg\" ) simbad_results_dict_list = simbad_results_df . to_dict ( orient = \"records\" ) return simbad_results_dict_list","title":"simbad()"},{"location":"reference/utils/external_query/#vast_pipeline.utils.external_query.tns","text":"Perform a cone search for sources with the Transient Name Server (TNS). Parameters: Name Type Description Default coord SkyCoord The coordinate of the centre of the cone. required radius Angle The radius of the cone in angular units. required Returns: Type Description List[Dict[str, Any]] A list of dicts, where each dict is a query result row with the following keys: object_name: the name of the transient. database: the source of the result, i.e. TNS. separation_arcsec: separation to the query coordinate in arcsec. otype: object type. otype_long: long form of the object type. Not given by TNS, will always be an empty string. ra_hms: RA coordinate string in hms format. dec_dms: Dec coordinate string in \u00b1dms format. Source code in vast_pipeline/utils/external_query.py def tns ( coord : SkyCoord , radius : Angle ) -> List [ Dict [ str , Any ]]: \"\"\"Perform a cone search for sources with the Transient Name Server (TNS). Args: coord: The coordinate of the centre of the cone. radius: The radius of the cone in angular units. Returns: A list of dicts, where each dict is a query result row with the following keys: - object_name: the name of the transient. - database: the source of the result, i.e. TNS. - separation_arcsec: separation to the query coordinate in arcsec. - otype: object type. - otype_long: long form of the object type. Not given by TNS, will always be an empty string. - ra_hms: RA coordinate string in hms format. - dec_dms: Dec coordinate string in \u00b1dms format. \"\"\" TNS_API_URL = \"https://www.wis-tns.org/api/\" headers = { \"user-agent\" : settings . TNS_USER_AGENT , } search_dict = { \"ra\" : coord . ra . to_string ( unit = \"hourangle\" , sep = \":\" , pad = True ), \"dec\" : coord . dec . to_string ( unit = \"deg\" , sep = \":\" , alwayssign = True , pad = True ), \"radius\" : str ( radius . value ), \"units\" : radius . unit . name , } r = requests . post ( urljoin ( TNS_API_URL , \"get/search\" ), data = { \"api_key\" : settings . TNS_API_KEY , \"data\" : json . dumps ( search_dict )}, headers = headers , ) tns_results_dict_list : List [ Dict [ str , Any ]] if r . ok : tns_results_dict_list = r . json ()[ \"data\" ][ \"reply\" ] # Get details for each object result. TNS API doesn't support doing this in one # request, so we iterate. for result in tns_results_dict_list : search_dict = { \"objname\" : result [ \"objname\" ], } r = requests . post ( urljoin ( TNS_API_URL , \"get/object\" ), data = { \"api_key\" : settings . TNS_API_KEY , \"data\" : json . dumps ( search_dict )}, headers = headers , ) if r . ok : object_dict = r . json ()[ \"data\" ][ \"reply\" ] object_coord = SkyCoord ( ra = object_dict [ \"radeg\" ], dec = object_dict [ \"decdeg\" ], unit = \"deg\" ) result [ \"otype\" ] = object_dict [ \"object_type\" ][ \"name\" ] if result [ \"otype\" ] is None : result [ \"otype\" ] = \"\" result [ \"otype_long\" ] = \"\" result [ \"separation_arcsec\" ] = coord . separation ( object_coord ) . arcsec result [ \"ra_hms\" ] = object_coord . ra . to_string ( unit = \"hourangle\" ) result [ \"dec_dms\" ] = object_coord . dec . to_string ( unit = \"deg\" ) result [ \"database\" ] = \"TNS\" result [ \"object_name\" ] = object_dict [ \"objname\" ] return tns_results_dict_list","title":"tns()"},{"location":"reference/utils/unit_tags/","text":"deg_to_arcmin ( angle ) \u00b6 Convert degrees to arcminutes. Parameters: Name Type Description Default angle float Angle in units of degrees. required Returns: Type Description float Angle in units of arcminutes. Source code in vast_pipeline/utils/unit_tags.py @register . filter def deg_to_arcmin ( angle : float ) -> float : \"\"\" Convert degrees to arcminutes. Args: angle: Angle in units of degrees. Returns: Angle in units of arcminutes. \"\"\" return float ( angle ) * 60. deg_to_arcsec ( angle ) \u00b6 Convert degrees to arcseconds. Parameters: Name Type Description Default angle float Angle in units of degrees. required Returns: Type Description float angle: Angle in units of arcseconds. Source code in vast_pipeline/utils/unit_tags.py @register . filter def deg_to_arcsec ( angle : float ) -> float : \"\"\" Convert degrees to arcseconds. Args: angle: Angle in units of degrees. Returns: angle: Angle in units of arcseconds. \"\"\" return float ( angle ) * 3600.","title":"unit_tags.py"},{"location":"reference/utils/unit_tags/#vast_pipeline.utils.unit_tags.deg_to_arcmin","text":"Convert degrees to arcminutes. Parameters: Name Type Description Default angle float Angle in units of degrees. required Returns: Type Description float Angle in units of arcminutes. Source code in vast_pipeline/utils/unit_tags.py @register . filter def deg_to_arcmin ( angle : float ) -> float : \"\"\" Convert degrees to arcminutes. Args: angle: Angle in units of degrees. Returns: Angle in units of arcminutes. \"\"\" return float ( angle ) * 60.","title":"deg_to_arcmin()"},{"location":"reference/utils/unit_tags/#vast_pipeline.utils.unit_tags.deg_to_arcsec","text":"Convert degrees to arcseconds. Parameters: Name Type Description Default angle float Angle in units of degrees. required Returns: Type Description float angle: Angle in units of arcseconds. Source code in vast_pipeline/utils/unit_tags.py @register . filter def deg_to_arcsec ( angle : float ) -> float : \"\"\" Convert degrees to arcseconds. Args: angle: Angle in units of degrees. Returns: angle: Angle in units of arcseconds. \"\"\" return float ( angle ) * 3600.","title":"deg_to_arcsec()"},{"location":"reference/utils/utils/","text":"This module contains general pipeline utility functions. StopWatch \u00b6 A simple stopwatch to simplify timing code. __init__ ( self ) special \u00b6 Initialise the StopWatch Returns: Type Description None None. Source code in vast_pipeline/utils/utils.py def __init__ ( self ) -> None : \"\"\" Initialise the StopWatch Returns: None. \"\"\" self . _init = datetime . now () self . _last = self . _init reset ( self ) \u00b6 Reset the stopwatch and return the time since last reset (seconds). Returns: Type Description float The time in seconds since the last reset. Source code in vast_pipeline/utils/utils.py def reset ( self ) -> float : \"\"\" Reset the stopwatch and return the time since last reset (seconds). Returns: The time in seconds since the last reset. \"\"\" now = datetime . now () diff = ( now - self . _last ) . total_seconds () self . _last = now return diff reset_init ( self ) \u00b6 Reset the stopwatch and return the total time since initialisation. Returns: Type Description float The time in seconds since the initialisation. Source code in vast_pipeline/utils/utils.py def reset_init ( self ) -> float : \"\"\" Reset the stopwatch and return the total time since initialisation. Returns: The time in seconds since the initialisation. \"\"\" now = datetime . now () diff = ( now - self . _init ) . total_seconds () self . _last = self . _init = now return diff check_read_write_perm ( path , perm = 'W' ) \u00b6 Assess the file permission on a path. Parameters: Name Type Description Default path str The system path to assess. required perm str The permission to check for. 'W' Returns: Type Description None None Exceptions: Type Description IOError The permission is not valid on the checked directory. Source code in vast_pipeline/utils/utils.py def check_read_write_perm ( path : str , perm : str = \"W\" ) -> None : \"\"\" Assess the file permission on a path. Args: path: The system path to assess. perm: The permission to check for. Returns: None Raises: IOError: The permission is not valid on the checked directory. \"\"\" assert perm in ( \"R\" , \"W\" , \"X\" ), \"permission not supported\" perm_map = { \"R\" : os . R_OK , \"W\" : os . W_OK , \"X\" : os . X_OK } if not os . access ( path , perm_map [ perm ]): msg = f \"permission not valid on folder: { path } \" logger . error ( msg ) raise IOError ( msg ) pass deg2dms ( deg , dms_format = False , precision = 2 , truncate = False , latitude = True ) \u00b6 Convert angle in degrees into a DMS formatted string. Parameters: Name Type Description Default deg float The angle to convert in degrees. required dms_format bool If True , use \"d\", \"m\", and \"s\" as the coorindate separator, otherwise use \":\". Defaults to False. False precision int Floating point precision of the arcseconds component. Can be 0 or a positive integer. Negative values will be interpreted as 0. Defaults to 2. 2 truncate bool Truncate values after the decimal point instead of rounding. Defaults to False (rounding). False latitude bool The input deg value should be intrepreted as a latitude. Otherwise, it will be interpreted as a longitude. Defaults to True (latitude). True Returns: Type Description str deg formatted as a DMS string. Examples: >>> deg2dms ( 12.582438888888889 ) '+12:34:56.78' >>> deg2dms ( 2.582438888888889 , dms_format = True ) '+02d34m56.78s' >>> deg2dms ( - 12.582438888888889 , precision = 1 ) '-12:34:56.8' >>> deg2dms ( - 12.582438888888889 , precision = 1 , truncate = True ) '-12:34:56.7' Source code in vast_pipeline/utils/utils.py def deg2dms ( deg : float , dms_format : bool = False , precision : int = 2 , truncate : bool = False , latitude : bool = True , ) -> str : \"\"\"Convert angle in degrees into a DMS formatted string. Args: deg: The angle to convert in degrees. dms_format (optional): If `True`, use \"d\", \"m\", and \"s\" as the coorindate separator, otherwise use \":\". Defaults to False. precision (optional): Floating point precision of the arcseconds component. Can be 0 or a positive integer. Negative values will be interpreted as 0. Defaults to 2. truncate (optional): Truncate values after the decimal point instead of rounding. Defaults to False (rounding). latitude (optional): The input `deg` value should be intrepreted as a latitude. Otherwise, it will be interpreted as a longitude. Defaults to True (latitude). Returns: `deg` formatted as a DMS string. Example: >>> deg2dms(12.582438888888889) '+12:34:56.78' >>> deg2dms(2.582438888888889, dms_format=True) '+02d34m56.78s' >>> deg2dms(-12.582438888888889, precision=1) '-12:34:56.8' >>> deg2dms(-12.582438888888889, precision=1, truncate=True) '-12:34:56.7' \"\"\" AngleClass = Latitude if latitude else Longitude angle = AngleClass ( deg , unit = \"deg\" ) precision = precision if precision >= 0 else 0 output_str : str = angle . to_string ( unit = \"deg\" , sep = \"fromunit\" if dms_format else \":\" , precision = precision if not truncate else None , alwayssign = True , pad = True , ) if truncate : # find the decimal point char position and the number of decimal places in the # rendered input coordinate (in DMS format, not decimal deg) dp_pos = output_str . find ( \".\" ) n_dp = len ( output_str [ dp_pos + 1 :]) if dp_pos >= 0 else 0 # if the input coordinate precision is less than the requsted output precision, # pad the end with zeroes if n_dp < precision : seconds_str = \"\" # account for rendered input coord having precision = 0 if dp_pos < 0 : seconds_str += \".\" seconds_str += \"0\" * ( precision - n_dp ) output_str += seconds_str # otherwise, cut off the excess decimal places elif n_dp > precision : if precision > 0 : # account for the decimal point char precision += 1 output_str = output_str [: dp_pos + precision ] # in the n_dp == precision case, do nothing return output_str deg2hms ( deg , hms_format = False , precision = 2 , truncate = False , longitude = True ) \u00b6 Convert angle in degrees into a HMS formatted string. Parameters: Name Type Description Default deg float The angle to convert in degrees. required hms_format bool If True , use \"h\", \"m\", and \"s\" as the coorindate separator, otherwise use \":\". Defaults to False. False precision int Floating point precision of the seconds component. Can be 0 or a positive integer. Negative values will be interpreted as 0. Defaults to 2. 2 truncate bool Truncate values after the decimal point instead of rounding. Defaults to False (rounding). False longitude bool The input deg value should be intrepreted as a longitude. Otherwise, it will be interpreted as a latitude. Defaults to True (longitude). True Returns: Type Description str deg formatted as an HMS string. Examples: >>> deg2hms ( 188.73658333333333 ) '12:34:56.78' >>> deg2hms ( - 188.73658333333333 , hms_format = True ) '12h34m56.78s' >>> deg2hms ( 188.73658333333333 , precision = 1 ) '12:34:56.8' >>> deg2hms ( 188.73658333333333 , precision = 1 , truncate = True ) '12:34:56.7' Source code in vast_pipeline/utils/utils.py def deg2hms ( deg : float , hms_format : bool = False , precision : int = 2 , truncate : bool = False , longitude : bool = True , ) -> str : \"\"\"Convert angle in degrees into a HMS formatted string. Args: deg: The angle to convert in degrees. hms_format (optional): If `True`, use \"h\", \"m\", and \"s\" as the coorindate separator, otherwise use \":\". Defaults to False. precision (optional): Floating point precision of the seconds component. Can be 0 or a positive integer. Negative values will be interpreted as 0. Defaults to 2. truncate (optional): Truncate values after the decimal point instead of rounding. Defaults to False (rounding). longitude (optional): The input `deg` value should be intrepreted as a longitude. Otherwise, it will be interpreted as a latitude. Defaults to True (longitude). Returns: `deg` formatted as an HMS string. Example: >>> deg2hms(188.73658333333333) '12:34:56.78' >>> deg2hms(-188.73658333333333, hms_format=True) '12h34m56.78s' >>> deg2hms(188.73658333333333, precision=1) '12:34:56.8' >>> deg2hms(188.73658333333333, precision=1, truncate=True) '12:34:56.7' \"\"\" # use the deg2dms formatter, replace d with h, and cut off the leading \u00b1 sign return deg2dms ( deg / 15.0 , dms_format = hms_format , precision = precision , truncate = truncate , latitude = not longitude , ) . replace ( \"d\" , \"h\" )[ 1 :] dict_merge ( dct , merge_dct , add_keys = True ) \u00b6 Recursive dict merge. Inspired by dict.update(), instead of updating only top-level keys, dict_merge recurses down into dicts nested to an arbitrary depth, updating keys. The merge_dct is merged into dct . This version will return a copy of the dictionary and leave the original arguments untouched. The optional argument add_keys , determines whether keys which are present in merge_dict but not dct should be included in the new dict. Parameters: Name Type Description Default dct Dict[Any, Any] onto which the merge is executed required merge_dct Dict[Any, Any] dct merged into dct required add_keys bool whether to add new keys True Returns: Type Description Dict[Any, Any] dict: updated dict Source code in vast_pipeline/utils/utils.py def dict_merge ( dct : Dict [ Any , Any ], merge_dct : Dict [ Any , Any ], add_keys = True ) -> Dict [ Any , Any ]: \"\"\"Recursive dict merge. Inspired by dict.update(), instead of updating only top-level keys, dict_merge recurses down into dicts nested to an arbitrary depth, updating keys. The `merge_dct` is merged into `dct`. This version will return a copy of the dictionary and leave the original arguments untouched. The optional argument `add_keys`, determines whether keys which are present in `merge_dict` but not `dct` should be included in the new dict. Args: dct (dict): onto which the merge is executed merge_dct (dict): dct merged into dct add_keys (bool): whether to add new keys Returns: dict: updated dict \"\"\" dct = dct . copy () if not add_keys : merge_dct = { k : merge_dct [ k ] for k in set ( dct ) . intersection ( set ( merge_dct ))} for k , v in merge_dct . items (): if ( k in dct and isinstance ( dct [ k ], dict ) and isinstance ( merge_dct [ k ], collections . Mapping ) ): dct [ k ] = dict_merge ( dct [ k ], merge_dct [ k ], add_keys = add_keys ) else : dct [ k ] = merge_dct [ k ] return dct eq_to_cart ( ra , dec ) \u00b6 Find the cartesian co-ordinates on the unit sphere given the eq. co-ords. ra, dec should be in degrees. Parameters: Name Type Description Default ra float The right ascension coordinate, in degrees, to convert. required dec float The declination coordinate, in degrees, to convert. required Returns: Type Description Tuple[float, float, float] The cartesian coordinates. Source code in vast_pipeline/utils/utils.py def eq_to_cart ( ra : float , dec : float ) -> Tuple [ float , float , float ]: \"\"\" Find the cartesian co-ordinates on the unit sphere given the eq. co-ords. ra, dec should be in degrees. Args: ra: The right ascension coordinate, in degrees, to convert. dec: The declination coordinate, in degrees, to convert. Returns: The cartesian coordinates. \"\"\" # TODO: This part of the code can probably be removed along with the # storage of these coodinates on the image. return ( m . cos ( m . radians ( dec )) * m . cos ( m . radians ( ra )), # Cartesian x m . cos ( m . radians ( dec )) * m . sin ( m . radians ( ra )), # Cartesian y m . sin ( m . radians ( dec )), # Cartesian z ) equ2gal ( ra , dec ) \u00b6 Convert equatorial coordinates to galactic Parameters: Name Type Description Default ra float Right ascension in units of degrees. required dec float Declination in units of degrees. required Returns: Type Description Tuple[float, float] Tuple (float, float): Galactic longitude and latitude in degrees. Source code in vast_pipeline/utils/utils.py def equ2gal ( ra : float , dec : float ) -> Tuple [ float , float ]: \"\"\" Convert equatorial coordinates to galactic Args: ra (float): Right ascension in units of degrees. dec (float): Declination in units of degrees. Returns: Tuple (float, float): Galactic longitude and latitude in degrees. \"\"\" c = SkyCoord ( np . float ( ra ), np . float ( dec ), unit = ( u . deg , u . deg ), frame = \"icrs\" ) l = c . galactic . l . deg b = c . galactic . b . deg return l , b gal2equ ( l , b ) \u00b6 Convert galactic coordinates to equatorial. Parameters: Name Type Description Default l float Galactic longitude in degrees. required b float Galactic latitude in degrees. required Returns: Type Description Tuple[float, float] Tuple (float, float): Right ascension and declination in units of degrees. Source code in vast_pipeline/utils/utils.py def gal2equ ( l : float , b : float ) -> Tuple [ float , float ]: \"\"\" Convert galactic coordinates to equatorial. Args: l (float): Galactic longitude in degrees. b (float): Galactic latitude in degrees. Returns: Tuple (float, float): Right ascension and declination in units of degrees. \"\"\" c = SkyCoord ( l = np . float ( l ) * u . deg , b = np . float ( b ) * u . deg , frame = \"galactic\" ) ra = c . icrs . ra . deg dec = c . icrs . dec . deg return ra , dec optimize_floats ( df ) \u00b6 Downcast float columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Parameters: Name Type Description Default df DataFrame input dataframe, no specific columns. required Returns: Type Description DataFrame The input dataframe with the float64 type columns downcasted. Source code in vast_pipeline/utils/utils.py def optimize_floats ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Downcast float columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Args: df: input dataframe, no specific columns. Returns: The input dataframe with the `float64` type columns downcasted. \"\"\" floats = df . select_dtypes ( include = [ \"float64\" ]) . columns . tolist () df [ floats ] = df [ floats ] . apply ( pd . to_numeric , downcast = \"float\" ) return df optimize_ints ( df ) \u00b6 Downcast integer columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Parameters: Name Type Description Default df DataFrame Input dataframe, no specific columns. required Returns: Type Description DataFrame The input dataframe with the int64 type columns downcasted. Source code in vast_pipeline/utils/utils.py def optimize_ints ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Downcast integer columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Args: df: Input dataframe, no specific columns. Returns: The input dataframe with the `int64` type columns downcasted. \"\"\" ints = df . select_dtypes ( include = [ \"int64\" ]) . columns . tolist () df [ ints ] = df [ ints ] . apply ( pd . to_numeric , downcast = \"integer\" ) return df parse_coord ( coord_string , coord_frame = 'icrs' ) \u00b6 Parse a coordinate string and return a SkyCoord. The units may be expressed within coord_string e.g. \"21h52m03.1s -62d08m19.7s\", \"18.4d +43.1d\". If no units are given, the following assumptions are made: - if both coordinate components are decimals, they are assumed to be in degrees. - if a sexagesimal coordinate is given and the frame is galactic, both components are assumed to be in degrees. For any other frame, the first component is assumed to be in hourangles and the second in degrees. Will raise a ValueError if SkyCoord is unable to parse coord_string . Parameters: Name Type Description Default coord_string str The coordinate string to parse. required coord_frame str The frame of coord_string . Defaults to \"icrs\". 'icrs' Returns: Type Description SkyCoord SkyCoord Source code in vast_pipeline/utils/utils.py def parse_coord ( coord_string : str , coord_frame : str = \"icrs\" ) -> SkyCoord : \"\"\"Parse a coordinate string and return a SkyCoord. The units may be expressed within `coord_string` e.g. \"21h52m03.1s -62d08m19.7s\", \"18.4d +43.1d\". If no units are given, the following assumptions are made: - if both coordinate components are decimals, they are assumed to be in degrees. - if a sexagesimal coordinate is given and the frame is galactic, both components are assumed to be in degrees. For any other frame, the first component is assumed to be in hourangles and the second in degrees. Will raise a ValueError if SkyCoord is unable to parse `coord_string`. Args: coord_string (str): The coordinate string to parse. coord_frame (str, optional): The frame of `coord_string`. Defaults to \"icrs\". Returns: SkyCoord \"\"\" # if both coord components are decimals, assume they're in degrees, otherwise assume # hourangles and degrees. Note that the unit parameter is ignored if the units are # not ambiguous i.e. if coord_string contains the units (e.g. 18.4d, 5h35m, etc) try : _ = [ float ( x ) for x in coord_string . split ()] unit = \"deg\" except ValueError : if coord_frame == \"galactic\" : unit = \"deg\" else : unit = \"hourangle,deg\" coord = SkyCoord ( coord_string , unit = unit , frame = coord_frame ) return coord","title":"utils.py"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.StopWatch","text":"A simple stopwatch to simplify timing code.","title":"StopWatch"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.StopWatch.__init__","text":"Initialise the StopWatch Returns: Type Description None None. Source code in vast_pipeline/utils/utils.py def __init__ ( self ) -> None : \"\"\" Initialise the StopWatch Returns: None. \"\"\" self . _init = datetime . now () self . _last = self . _init","title":"__init__()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.StopWatch.reset","text":"Reset the stopwatch and return the time since last reset (seconds). Returns: Type Description float The time in seconds since the last reset. Source code in vast_pipeline/utils/utils.py def reset ( self ) -> float : \"\"\" Reset the stopwatch and return the time since last reset (seconds). Returns: The time in seconds since the last reset. \"\"\" now = datetime . now () diff = ( now - self . _last ) . total_seconds () self . _last = now return diff","title":"reset()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.StopWatch.reset_init","text":"Reset the stopwatch and return the total time since initialisation. Returns: Type Description float The time in seconds since the initialisation. Source code in vast_pipeline/utils/utils.py def reset_init ( self ) -> float : \"\"\" Reset the stopwatch and return the total time since initialisation. Returns: The time in seconds since the initialisation. \"\"\" now = datetime . now () diff = ( now - self . _init ) . total_seconds () self . _last = self . _init = now return diff","title":"reset_init()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.check_read_write_perm","text":"Assess the file permission on a path. Parameters: Name Type Description Default path str The system path to assess. required perm str The permission to check for. 'W' Returns: Type Description None None Exceptions: Type Description IOError The permission is not valid on the checked directory. Source code in vast_pipeline/utils/utils.py def check_read_write_perm ( path : str , perm : str = \"W\" ) -> None : \"\"\" Assess the file permission on a path. Args: path: The system path to assess. perm: The permission to check for. Returns: None Raises: IOError: The permission is not valid on the checked directory. \"\"\" assert perm in ( \"R\" , \"W\" , \"X\" ), \"permission not supported\" perm_map = { \"R\" : os . R_OK , \"W\" : os . W_OK , \"X\" : os . X_OK } if not os . access ( path , perm_map [ perm ]): msg = f \"permission not valid on folder: { path } \" logger . error ( msg ) raise IOError ( msg ) pass","title":"check_read_write_perm()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.deg2dms","text":"Convert angle in degrees into a DMS formatted string. Parameters: Name Type Description Default deg float The angle to convert in degrees. required dms_format bool If True , use \"d\", \"m\", and \"s\" as the coorindate separator, otherwise use \":\". Defaults to False. False precision int Floating point precision of the arcseconds component. Can be 0 or a positive integer. Negative values will be interpreted as 0. Defaults to 2. 2 truncate bool Truncate values after the decimal point instead of rounding. Defaults to False (rounding). False latitude bool The input deg value should be intrepreted as a latitude. Otherwise, it will be interpreted as a longitude. Defaults to True (latitude). True Returns: Type Description str deg formatted as a DMS string. Examples: >>> deg2dms ( 12.582438888888889 ) '+12:34:56.78' >>> deg2dms ( 2.582438888888889 , dms_format = True ) '+02d34m56.78s' >>> deg2dms ( - 12.582438888888889 , precision = 1 ) '-12:34:56.8' >>> deg2dms ( - 12.582438888888889 , precision = 1 , truncate = True ) '-12:34:56.7' Source code in vast_pipeline/utils/utils.py def deg2dms ( deg : float , dms_format : bool = False , precision : int = 2 , truncate : bool = False , latitude : bool = True , ) -> str : \"\"\"Convert angle in degrees into a DMS formatted string. Args: deg: The angle to convert in degrees. dms_format (optional): If `True`, use \"d\", \"m\", and \"s\" as the coorindate separator, otherwise use \":\". Defaults to False. precision (optional): Floating point precision of the arcseconds component. Can be 0 or a positive integer. Negative values will be interpreted as 0. Defaults to 2. truncate (optional): Truncate values after the decimal point instead of rounding. Defaults to False (rounding). latitude (optional): The input `deg` value should be intrepreted as a latitude. Otherwise, it will be interpreted as a longitude. Defaults to True (latitude). Returns: `deg` formatted as a DMS string. Example: >>> deg2dms(12.582438888888889) '+12:34:56.78' >>> deg2dms(2.582438888888889, dms_format=True) '+02d34m56.78s' >>> deg2dms(-12.582438888888889, precision=1) '-12:34:56.8' >>> deg2dms(-12.582438888888889, precision=1, truncate=True) '-12:34:56.7' \"\"\" AngleClass = Latitude if latitude else Longitude angle = AngleClass ( deg , unit = \"deg\" ) precision = precision if precision >= 0 else 0 output_str : str = angle . to_string ( unit = \"deg\" , sep = \"fromunit\" if dms_format else \":\" , precision = precision if not truncate else None , alwayssign = True , pad = True , ) if truncate : # find the decimal point char position and the number of decimal places in the # rendered input coordinate (in DMS format, not decimal deg) dp_pos = output_str . find ( \".\" ) n_dp = len ( output_str [ dp_pos + 1 :]) if dp_pos >= 0 else 0 # if the input coordinate precision is less than the requsted output precision, # pad the end with zeroes if n_dp < precision : seconds_str = \"\" # account for rendered input coord having precision = 0 if dp_pos < 0 : seconds_str += \".\" seconds_str += \"0\" * ( precision - n_dp ) output_str += seconds_str # otherwise, cut off the excess decimal places elif n_dp > precision : if precision > 0 : # account for the decimal point char precision += 1 output_str = output_str [: dp_pos + precision ] # in the n_dp == precision case, do nothing return output_str","title":"deg2dms()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.deg2hms","text":"Convert angle in degrees into a HMS formatted string. Parameters: Name Type Description Default deg float The angle to convert in degrees. required hms_format bool If True , use \"h\", \"m\", and \"s\" as the coorindate separator, otherwise use \":\". Defaults to False. False precision int Floating point precision of the seconds component. Can be 0 or a positive integer. Negative values will be interpreted as 0. Defaults to 2. 2 truncate bool Truncate values after the decimal point instead of rounding. Defaults to False (rounding). False longitude bool The input deg value should be intrepreted as a longitude. Otherwise, it will be interpreted as a latitude. Defaults to True (longitude). True Returns: Type Description str deg formatted as an HMS string. Examples: >>> deg2hms ( 188.73658333333333 ) '12:34:56.78' >>> deg2hms ( - 188.73658333333333 , hms_format = True ) '12h34m56.78s' >>> deg2hms ( 188.73658333333333 , precision = 1 ) '12:34:56.8' >>> deg2hms ( 188.73658333333333 , precision = 1 , truncate = True ) '12:34:56.7' Source code in vast_pipeline/utils/utils.py def deg2hms ( deg : float , hms_format : bool = False , precision : int = 2 , truncate : bool = False , longitude : bool = True , ) -> str : \"\"\"Convert angle in degrees into a HMS formatted string. Args: deg: The angle to convert in degrees. hms_format (optional): If `True`, use \"h\", \"m\", and \"s\" as the coorindate separator, otherwise use \":\". Defaults to False. precision (optional): Floating point precision of the seconds component. Can be 0 or a positive integer. Negative values will be interpreted as 0. Defaults to 2. truncate (optional): Truncate values after the decimal point instead of rounding. Defaults to False (rounding). longitude (optional): The input `deg` value should be intrepreted as a longitude. Otherwise, it will be interpreted as a latitude. Defaults to True (longitude). Returns: `deg` formatted as an HMS string. Example: >>> deg2hms(188.73658333333333) '12:34:56.78' >>> deg2hms(-188.73658333333333, hms_format=True) '12h34m56.78s' >>> deg2hms(188.73658333333333, precision=1) '12:34:56.8' >>> deg2hms(188.73658333333333, precision=1, truncate=True) '12:34:56.7' \"\"\" # use the deg2dms formatter, replace d with h, and cut off the leading \u00b1 sign return deg2dms ( deg / 15.0 , dms_format = hms_format , precision = precision , truncate = truncate , latitude = not longitude , ) . replace ( \"d\" , \"h\" )[ 1 :]","title":"deg2hms()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.dict_merge","text":"Recursive dict merge. Inspired by dict.update(), instead of updating only top-level keys, dict_merge recurses down into dicts nested to an arbitrary depth, updating keys. The merge_dct is merged into dct . This version will return a copy of the dictionary and leave the original arguments untouched. The optional argument add_keys , determines whether keys which are present in merge_dict but not dct should be included in the new dict. Parameters: Name Type Description Default dct Dict[Any, Any] onto which the merge is executed required merge_dct Dict[Any, Any] dct merged into dct required add_keys bool whether to add new keys True Returns: Type Description Dict[Any, Any] dict: updated dict Source code in vast_pipeline/utils/utils.py def dict_merge ( dct : Dict [ Any , Any ], merge_dct : Dict [ Any , Any ], add_keys = True ) -> Dict [ Any , Any ]: \"\"\"Recursive dict merge. Inspired by dict.update(), instead of updating only top-level keys, dict_merge recurses down into dicts nested to an arbitrary depth, updating keys. The `merge_dct` is merged into `dct`. This version will return a copy of the dictionary and leave the original arguments untouched. The optional argument `add_keys`, determines whether keys which are present in `merge_dict` but not `dct` should be included in the new dict. Args: dct (dict): onto which the merge is executed merge_dct (dict): dct merged into dct add_keys (bool): whether to add new keys Returns: dict: updated dict \"\"\" dct = dct . copy () if not add_keys : merge_dct = { k : merge_dct [ k ] for k in set ( dct ) . intersection ( set ( merge_dct ))} for k , v in merge_dct . items (): if ( k in dct and isinstance ( dct [ k ], dict ) and isinstance ( merge_dct [ k ], collections . Mapping ) ): dct [ k ] = dict_merge ( dct [ k ], merge_dct [ k ], add_keys = add_keys ) else : dct [ k ] = merge_dct [ k ] return dct","title":"dict_merge()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.eq_to_cart","text":"Find the cartesian co-ordinates on the unit sphere given the eq. co-ords. ra, dec should be in degrees. Parameters: Name Type Description Default ra float The right ascension coordinate, in degrees, to convert. required dec float The declination coordinate, in degrees, to convert. required Returns: Type Description Tuple[float, float, float] The cartesian coordinates. Source code in vast_pipeline/utils/utils.py def eq_to_cart ( ra : float , dec : float ) -> Tuple [ float , float , float ]: \"\"\" Find the cartesian co-ordinates on the unit sphere given the eq. co-ords. ra, dec should be in degrees. Args: ra: The right ascension coordinate, in degrees, to convert. dec: The declination coordinate, in degrees, to convert. Returns: The cartesian coordinates. \"\"\" # TODO: This part of the code can probably be removed along with the # storage of these coodinates on the image. return ( m . cos ( m . radians ( dec )) * m . cos ( m . radians ( ra )), # Cartesian x m . cos ( m . radians ( dec )) * m . sin ( m . radians ( ra )), # Cartesian y m . sin ( m . radians ( dec )), # Cartesian z )","title":"eq_to_cart()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.equ2gal","text":"Convert equatorial coordinates to galactic Parameters: Name Type Description Default ra float Right ascension in units of degrees. required dec float Declination in units of degrees. required Returns: Type Description Tuple[float, float] Tuple (float, float): Galactic longitude and latitude in degrees. Source code in vast_pipeline/utils/utils.py def equ2gal ( ra : float , dec : float ) -> Tuple [ float , float ]: \"\"\" Convert equatorial coordinates to galactic Args: ra (float): Right ascension in units of degrees. dec (float): Declination in units of degrees. Returns: Tuple (float, float): Galactic longitude and latitude in degrees. \"\"\" c = SkyCoord ( np . float ( ra ), np . float ( dec ), unit = ( u . deg , u . deg ), frame = \"icrs\" ) l = c . galactic . l . deg b = c . galactic . b . deg return l , b","title":"equ2gal()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.gal2equ","text":"Convert galactic coordinates to equatorial. Parameters: Name Type Description Default l float Galactic longitude in degrees. required b float Galactic latitude in degrees. required Returns: Type Description Tuple[float, float] Tuple (float, float): Right ascension and declination in units of degrees. Source code in vast_pipeline/utils/utils.py def gal2equ ( l : float , b : float ) -> Tuple [ float , float ]: \"\"\" Convert galactic coordinates to equatorial. Args: l (float): Galactic longitude in degrees. b (float): Galactic latitude in degrees. Returns: Tuple (float, float): Right ascension and declination in units of degrees. \"\"\" c = SkyCoord ( l = np . float ( l ) * u . deg , b = np . float ( b ) * u . deg , frame = \"galactic\" ) ra = c . icrs . ra . deg dec = c . icrs . dec . deg return ra , dec","title":"gal2equ()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.optimize_floats","text":"Downcast float columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Parameters: Name Type Description Default df DataFrame input dataframe, no specific columns. required Returns: Type Description DataFrame The input dataframe with the float64 type columns downcasted. Source code in vast_pipeline/utils/utils.py def optimize_floats ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Downcast float columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Args: df: input dataframe, no specific columns. Returns: The input dataframe with the `float64` type columns downcasted. \"\"\" floats = df . select_dtypes ( include = [ \"float64\" ]) . columns . tolist () df [ floats ] = df [ floats ] . apply ( pd . to_numeric , downcast = \"float\" ) return df","title":"optimize_floats()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.optimize_ints","text":"Downcast integer columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Parameters: Name Type Description Default df DataFrame Input dataframe, no specific columns. required Returns: Type Description DataFrame The input dataframe with the int64 type columns downcasted. Source code in vast_pipeline/utils/utils.py def optimize_ints ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Downcast integer columns in a pd.DataFrame to the smallest data type without losing any information. Credit to Robbert van der Gugten. Args: df: Input dataframe, no specific columns. Returns: The input dataframe with the `int64` type columns downcasted. \"\"\" ints = df . select_dtypes ( include = [ \"int64\" ]) . columns . tolist () df [ ints ] = df [ ints ] . apply ( pd . to_numeric , downcast = \"integer\" ) return df","title":"optimize_ints()"},{"location":"reference/utils/utils/#vast_pipeline.utils.utils.parse_coord","text":"Parse a coordinate string and return a SkyCoord. The units may be expressed within coord_string e.g. \"21h52m03.1s -62d08m19.7s\", \"18.4d +43.1d\". If no units are given, the following assumptions are made: - if both coordinate components are decimals, they are assumed to be in degrees. - if a sexagesimal coordinate is given and the frame is galactic, both components are assumed to be in degrees. For any other frame, the first component is assumed to be in hourangles and the second in degrees. Will raise a ValueError if SkyCoord is unable to parse coord_string . Parameters: Name Type Description Default coord_string str The coordinate string to parse. required coord_frame str The frame of coord_string . Defaults to \"icrs\". 'icrs' Returns: Type Description SkyCoord SkyCoord Source code in vast_pipeline/utils/utils.py def parse_coord ( coord_string : str , coord_frame : str = \"icrs\" ) -> SkyCoord : \"\"\"Parse a coordinate string and return a SkyCoord. The units may be expressed within `coord_string` e.g. \"21h52m03.1s -62d08m19.7s\", \"18.4d +43.1d\". If no units are given, the following assumptions are made: - if both coordinate components are decimals, they are assumed to be in degrees. - if a sexagesimal coordinate is given and the frame is galactic, both components are assumed to be in degrees. For any other frame, the first component is assumed to be in hourangles and the second in degrees. Will raise a ValueError if SkyCoord is unable to parse `coord_string`. Args: coord_string (str): The coordinate string to parse. coord_frame (str, optional): The frame of `coord_string`. Defaults to \"icrs\". Returns: SkyCoord \"\"\" # if both coord components are decimals, assume they're in degrees, otherwise assume # hourangles and degrees. Note that the unit parameter is ignored if the units are # not ambiguous i.e. if coord_string contains the units (e.g. 18.4d, 5h35m, etc) try : _ = [ float ( x ) for x in coord_string . split ()] unit = \"deg\" except ValueError : if coord_frame == \"galactic\" : unit = \"deg\" else : unit = \"hourangle,deg\" coord = SkyCoord ( coord_string , unit = unit , frame = coord_frame ) return coord","title":"parse_coord()"},{"location":"reference/utils/view/","text":"Functions and variables used in pipeline/views.py. generate_colsfields ( fields , url_prefix_dict , not_orderable_col = None , not_searchable_col = None ) \u00b6 Generate data to be included in context for datatables. Example of url_prefix_dict format: api_col_dict = { 'source.name': reverse('vast_pipeline:source_detail', args=[1])[:-2], 'source.run.name': reverse('vast_pipeline:run_detail', args=[1])[:-2] } Parameters: Name Type Description Default fields List[str] List of fields to include as columns. required url_prefix_dict Dict[str, str] Dict containing the url prefix to form href links in the datatables. required not_orderable_col Optional[List[str]] List of columns that should be set to be not orderable in the final table. None not_searchable_col Optional[List[str]] List of columns that should be set to be not searchable in the final table. None Returns: Type Description List[Dict[str, Any]] colsfields (list): List containing JSON object. Source code in vast_pipeline/utils/view.py def generate_colsfields ( fields : List [ str ], url_prefix_dict : Dict [ str , str ], not_orderable_col : Optional [ List [ str ]] = None , not_searchable_col : Optional [ List [ str ]] = None , ) -> List [ Dict [ str , Any ]]: \"\"\" Generate data to be included in context for datatables. Example of url_prefix_dict format: api_col_dict = { 'source.name': reverse('vast_pipeline:source_detail', args=[1])[:-2], 'source.run.name': reverse('vast_pipeline:run_detail', args=[1])[:-2] } Args: fields (list): List of fields to include as columns. url_prefix_dict (dict): Dict containing the url prefix to form href links in the datatables. not_orderable_col (list): List of columns that should be set to be not orderable in the final table. not_searchable_col (list): List of columns that should be set to be not searchable in the final table. Returns: colsfields (list): List containing JSON object. \"\"\" colsfields = [] if not_orderable_col is None : not_orderable_col = [] if not_searchable_col is None : not_searchable_col = [] for col in fields : field2append = {} if col == 'name' : field2append = { 'data' : col , 'render' : { 'url' : { 'prefix' : url_prefix_dict [ col ], 'col' : 'name' } } } elif '.name' in col : # this is for nested fields to build a render with column name # and id in url. The API results should look like: # {... , main_col : {'name': value, 'id': value, ... }} main_col = col . rsplit ( '.' , 1 )[ 0 ] field2append = { 'data' : col , 'render' : { 'url' : { 'prefix' : url_prefix_dict [ col ], 'col' : main_col , 'nested' : True , } } } elif col == 'n_sibl' : field2append = { 'data' : col , 'render' : { 'contains_sibl' : { 'col' : col } } } elif col in FLOAT_FIELDS : field2append = { 'data' : col , 'render' : { 'float' : { 'col' : col , 'precision' : FLOAT_FIELDS [ col ][ 'precision' ], 'scale' : FLOAT_FIELDS [ col ][ 'scale' ], } } } else : field2append = { 'data' : col } if col in not_orderable_col : field2append [ 'orderable' ] = False if col in not_searchable_col : field2append [ 'searchable' ] = False colsfields . append ( field2append ) return colsfields get_skyregions_collection ( run_id = None ) \u00b6 Produce Sky region geometry shapes JSON object for d3-celestial. Parameters: Name Type Description Default run_id Optional[int] Run ID to filter on if not None. None Returns: Type Description Dict[str, Any] skyregions_collection (dict): Dictionary representing a JSON obejct containing the sky regions. Source code in vast_pipeline/utils/view.py def get_skyregions_collection ( run_id : Optional [ int ] = None ) -> Dict [ str , Any ]: \"\"\" Produce Sky region geometry shapes JSON object for d3-celestial. Args: run_id (int, optional): Run ID to filter on if not None. Returns: skyregions_collection (dict): Dictionary representing a JSON obejct containing the sky regions. \"\"\" skyregions = SkyRegion . objects . all () if run_id is not None : skyregions = skyregions . filter ( run = run_id ) features = [] for skr in skyregions : ra_fix = 360. if skr . centre_ra > 180. else 0. ra = skr . centre_ra - ra_fix dec = skr . centre_dec width_ra = skr . width_ra / 2. width_dec = skr . width_dec / 2. id = skr . id features . append ( { \"type\" : \"Feature\" , \"id\" : f \"SkyRegion { id } \" , \"properties\" : { \"n\" : f \" { id : 02d } \" , \"loc\" : [ ra , dec ] }, \"geometry\" : { \"type\" : \"MultiLineString\" , \"coordinates\" : [[ [ ra + width_ra , dec + width_dec ], [ ra + width_ra , dec - width_dec ], [ ra - width_ra , dec - width_dec ], [ ra - width_ra , dec + width_dec ], [ ra + width_ra , dec + width_dec ] ]] } } ) skyregions_collection = { \"type\" : \"FeatureCollection\" , \"features\" : features } return skyregions_collection","title":"view.py"},{"location":"reference/utils/view/#vast_pipeline.utils.view.generate_colsfields","text":"Generate data to be included in context for datatables. Example of url_prefix_dict format: api_col_dict = { 'source.name': reverse('vast_pipeline:source_detail', args=[1])[:-2], 'source.run.name': reverse('vast_pipeline:run_detail', args=[1])[:-2] } Parameters: Name Type Description Default fields List[str] List of fields to include as columns. required url_prefix_dict Dict[str, str] Dict containing the url prefix to form href links in the datatables. required not_orderable_col Optional[List[str]] List of columns that should be set to be not orderable in the final table. None not_searchable_col Optional[List[str]] List of columns that should be set to be not searchable in the final table. None Returns: Type Description List[Dict[str, Any]] colsfields (list): List containing JSON object. Source code in vast_pipeline/utils/view.py def generate_colsfields ( fields : List [ str ], url_prefix_dict : Dict [ str , str ], not_orderable_col : Optional [ List [ str ]] = None , not_searchable_col : Optional [ List [ str ]] = None , ) -> List [ Dict [ str , Any ]]: \"\"\" Generate data to be included in context for datatables. Example of url_prefix_dict format: api_col_dict = { 'source.name': reverse('vast_pipeline:source_detail', args=[1])[:-2], 'source.run.name': reverse('vast_pipeline:run_detail', args=[1])[:-2] } Args: fields (list): List of fields to include as columns. url_prefix_dict (dict): Dict containing the url prefix to form href links in the datatables. not_orderable_col (list): List of columns that should be set to be not orderable in the final table. not_searchable_col (list): List of columns that should be set to be not searchable in the final table. Returns: colsfields (list): List containing JSON object. \"\"\" colsfields = [] if not_orderable_col is None : not_orderable_col = [] if not_searchable_col is None : not_searchable_col = [] for col in fields : field2append = {} if col == 'name' : field2append = { 'data' : col , 'render' : { 'url' : { 'prefix' : url_prefix_dict [ col ], 'col' : 'name' } } } elif '.name' in col : # this is for nested fields to build a render with column name # and id in url. The API results should look like: # {... , main_col : {'name': value, 'id': value, ... }} main_col = col . rsplit ( '.' , 1 )[ 0 ] field2append = { 'data' : col , 'render' : { 'url' : { 'prefix' : url_prefix_dict [ col ], 'col' : main_col , 'nested' : True , } } } elif col == 'n_sibl' : field2append = { 'data' : col , 'render' : { 'contains_sibl' : { 'col' : col } } } elif col in FLOAT_FIELDS : field2append = { 'data' : col , 'render' : { 'float' : { 'col' : col , 'precision' : FLOAT_FIELDS [ col ][ 'precision' ], 'scale' : FLOAT_FIELDS [ col ][ 'scale' ], } } } else : field2append = { 'data' : col } if col in not_orderable_col : field2append [ 'orderable' ] = False if col in not_searchable_col : field2append [ 'searchable' ] = False colsfields . append ( field2append ) return colsfields","title":"generate_colsfields()"},{"location":"reference/utils/view/#vast_pipeline.utils.view.get_skyregions_collection","text":"Produce Sky region geometry shapes JSON object for d3-celestial. Parameters: Name Type Description Default run_id Optional[int] Run ID to filter on if not None. None Returns: Type Description Dict[str, Any] skyregions_collection (dict): Dictionary representing a JSON obejct containing the sky regions. Source code in vast_pipeline/utils/view.py def get_skyregions_collection ( run_id : Optional [ int ] = None ) -> Dict [ str , Any ]: \"\"\" Produce Sky region geometry shapes JSON object for d3-celestial. Args: run_id (int, optional): Run ID to filter on if not None. Returns: skyregions_collection (dict): Dictionary representing a JSON obejct containing the sky regions. \"\"\" skyregions = SkyRegion . objects . all () if run_id is not None : skyregions = skyregions . filter ( run = run_id ) features = [] for skr in skyregions : ra_fix = 360. if skr . centre_ra > 180. else 0. ra = skr . centre_ra - ra_fix dec = skr . centre_dec width_ra = skr . width_ra / 2. width_dec = skr . width_dec / 2. id = skr . id features . append ( { \"type\" : \"Feature\" , \"id\" : f \"SkyRegion { id } \" , \"properties\" : { \"n\" : f \" { id : 02d } \" , \"loc\" : [ ra , dec ] }, \"geometry\" : { \"type\" : \"MultiLineString\" , \"coordinates\" : [[ [ ra + width_ra , dec + width_dec ], [ ra + width_ra , dec - width_dec ], [ ra - width_ra , dec - width_dec ], [ ra - width_ra , dec + width_dec ], [ ra + width_ra , dec + width_dec ] ]] } } ) skyregions_collection = { \"type\" : \"FeatureCollection\" , \"features\" : features } return skyregions_collection","title":"get_skyregions_collection()"},{"location":"using/access/","text":"Accessing the Pipeline \u00b6 Access to the pipeline website is done using GitHub as the authentification method. In particular it checks organisation membership to confirm that the user is allowed access. For example for those wanting to access the VAST instance hosted on Nimbus must make sure they are a member of the askap-vast GitHub organisation. Note If you are attempting to access an instance of the VAST pipeline not hosted by the VAST group, confirm with the administrator what GitHub organisation membership is required. Adminstrators can refer to Pipeline Login for information on how to configure the login system. Logging In \u00b6 Navigate to the VAST Pipeline (or other hosted instance) and the following page will appear. Click the Login with GitHub button and you will be presented with the following page to enter you GitHub details. Note that if you are already logged into GitHub in your browser then you will likely not see this page. After a successful login you will then be redirected to the Pipeline homepage. Troubleshooting \u00b6 Failures will commonly be caused by the user not being a member of the correct GitHub organisation or an error in the configuration of the login system by the administrator. Contact the administrator of the pipeline instance if you encounter problems logging in.","title":"Accessing the Pipeline"},{"location":"using/access/#accessing-the-pipeline","text":"Access to the pipeline website is done using GitHub as the authentification method. In particular it checks organisation membership to confirm that the user is allowed access. For example for those wanting to access the VAST instance hosted on Nimbus must make sure they are a member of the askap-vast GitHub organisation. Note If you are attempting to access an instance of the VAST pipeline not hosted by the VAST group, confirm with the administrator what GitHub organisation membership is required. Adminstrators can refer to Pipeline Login for information on how to configure the login system.","title":"Accessing the Pipeline"},{"location":"using/access/#logging-in","text":"Navigate to the VAST Pipeline (or other hosted instance) and the following page will appear. Click the Login with GitHub button and you will be presented with the following page to enter you GitHub details. Note that if you are already logged into GitHub in your browser then you will likely not see this page. After a successful login you will then be redirected to the Pipeline homepage.","title":"Logging In"},{"location":"using/access/#troubleshooting","text":"Failures will commonly be caused by the user not being a member of the correct GitHub organisation or an error in the configuration of the login system by the administrator. Contact the administrator of the pipeline instance if you encounter problems logging in.","title":"Troubleshooting"},{"location":"using/addtorun/","text":"Adding Images to a Run \u00b6 This page describes how to add images to a completed run, including how to restore the run to the previous state if an addition goes wrong. Note Adding images to an existing run will update the sources already present from the respective run, such that the existing source IDs, comments, and tags are retained. If a full re-run was used instead then new IDs would be created and the comments and tags lost. There is no limit on how many times images may be added to a run. Step-by-step Guide \u00b6 Warning A run must have a Completed status before images can be added to it. No other settings other than the input data can be changed in the config. 1. Navigate to the Run Detail Page \u00b6 Navigate to the detail page of the run you wish to process, and confirm that the job is marked as Completed . 2. Add the New Images to the Configuration \u00b6 Scroll to the configuration editor, enter edit mode and add the new images to the existing data inputs. If using epoch mode notation, add a new epoch(s) to the file. Once all the images, selavy files, rms images and background images have been added, select the Write Current Config option to save the file. Warning Do not remove the previous images from the configuration inputs or change any other options! Remember to make sure the order of the new input data is consistent between types! 3. Perform a Config Validation \u00b6 Check that the configuration file is still valid by selecting Validate Config from the same menu as shown in the previous step 2 screenshot. 4. Process the Run \u00b6 Select the Add Images or Re-Process Run button at the top right of the run detail page to open the processing modal window. Select whether to turn debugging log output On or Off and when ready select the Schedule Run . Warning Do not toggle Full Re-Run to On ! You can refresh the page to check the status of the run. You can confirm that the images have been added correctly by consulting the log output found below the configuration file. New images should have been ingested and output similar to the following should be present: 2021-04-02-21-14-31_log.txt 2021-04-02 21:17:00,628 association INFO Starting association. 2021-04-02 21:17:00,628 association INFO Association mode selected: basic. 2021-04-02 21:17:00,708 association INFO Found 7 images to add to the run. Once the processing has Completed the run detail page will now show the updated statistics and information of the run. Restore Run to Pre-Add Version \u00b6 When images are added to a run, a backup is made of the run before proceeding which can be used to restore the run to the pre-addition version. For example, perhaps the wrong images were added or an error occurred mid-addition that could not be resolved. For full details see the documentation page for restoring a run here . Warning Do not add any further images if you wish to restore otherwise the backup version will be lost!","title":"Adding Images to a Run"},{"location":"using/addtorun/#adding-images-to-a-run","text":"This page describes how to add images to a completed run, including how to restore the run to the previous state if an addition goes wrong. Note Adding images to an existing run will update the sources already present from the respective run, such that the existing source IDs, comments, and tags are retained. If a full re-run was used instead then new IDs would be created and the comments and tags lost. There is no limit on how many times images may be added to a run.","title":"Adding Images to a Run"},{"location":"using/addtorun/#step-by-step-guide","text":"Warning A run must have a Completed status before images can be added to it. No other settings other than the input data can be changed in the config.","title":"Step-by-step Guide"},{"location":"using/addtorun/#1-navigate-to-the-run-detail-page","text":"Navigate to the detail page of the run you wish to process, and confirm that the job is marked as Completed .","title":"1. Navigate to the Run Detail Page"},{"location":"using/addtorun/#2-add-the-new-images-to-the-configuration","text":"Scroll to the configuration editor, enter edit mode and add the new images to the existing data inputs. If using epoch mode notation, add a new epoch(s) to the file. Once all the images, selavy files, rms images and background images have been added, select the Write Current Config option to save the file. Warning Do not remove the previous images from the configuration inputs or change any other options! Remember to make sure the order of the new input data is consistent between types!","title":"2. Add the New Images to the Configuration"},{"location":"using/addtorun/#3-perform-a-config-validation","text":"Check that the configuration file is still valid by selecting Validate Config from the same menu as shown in the previous step 2 screenshot.","title":"3. Perform a Config Validation"},{"location":"using/addtorun/#4-process-the-run","text":"Select the Add Images or Re-Process Run button at the top right of the run detail page to open the processing modal window. Select whether to turn debugging log output On or Off and when ready select the Schedule Run . Warning Do not toggle Full Re-Run to On ! You can refresh the page to check the status of the run. You can confirm that the images have been added correctly by consulting the log output found below the configuration file. New images should have been ingested and output similar to the following should be present: 2021-04-02-21-14-31_log.txt 2021-04-02 21:17:00,628 association INFO Starting association. 2021-04-02 21:17:00,628 association INFO Association mode selected: basic. 2021-04-02 21:17:00,708 association INFO Found 7 images to add to the run. Once the processing has Completed the run detail page will now show the updated statistics and information of the run.","title":"4. Process the Run"},{"location":"using/addtorun/#restore-run-to-pre-add-version","text":"When images are added to a run, a backup is made of the run before proceeding which can be used to restore the run to the pre-addition version. For example, perhaps the wrong images were added or an error occurred mid-addition that could not be resolved. For full details see the documentation page for restoring a run here . Warning Do not add any further images if you wish to restore otherwise the backup version will be lost!","title":"Restore Run to Pre-Add Version"},{"location":"using/deleterun/","text":"Deleting a Run \u00b6 This page describes how to delete a pipeline run through the website interface. Deleting a run means that all outputs such as sources and associations are deleted from the database and the run itself is also removed. Images and the accompanying measurements are only removed if they were used solely by the deleted run. The run directory is also deleted that contains the output files and configuration files. A pipeline run can only be deleted by the creator or an administrator. Warning As stated above, deleting a run through the website will also delete the full run directory, which includes the configuration files. Please manually back up the configuration file if you think you are likely to revisit that particular run configuration in the future. Admin Tip Administrators can refer to the clearpiperun command for details on how to reset a pipeline run via the command line. Step-by-step Guide \u00b6 1. Navigate to the Run Detail Page \u00b6 Navigate to the detail page of the run you wish to delete. 2. Click on the Delete Run Button \u00b6 Click on the Delete Run button at the top right of the page, to open the confirmation modal. 3. Confirm Deletion \u00b6 To confirm the deletion click on the Delete Run button in the modal. Once pressed the website will direct back to the Pipeline Runs page and a confirmation message will appear in the top right. Note Runs with lots of database entries may take a short time to delete. Hence, the run may still appear in the pipeline runs list for a short time following the request with a status of Deleting . Refreshing the page will show a deleting status if the process is still running:","title":"Deleting a Run"},{"location":"using/deleterun/#deleting-a-run","text":"This page describes how to delete a pipeline run through the website interface. Deleting a run means that all outputs such as sources and associations are deleted from the database and the run itself is also removed. Images and the accompanying measurements are only removed if they were used solely by the deleted run. The run directory is also deleted that contains the output files and configuration files. A pipeline run can only be deleted by the creator or an administrator. Warning As stated above, deleting a run through the website will also delete the full run directory, which includes the configuration files. Please manually back up the configuration file if you think you are likely to revisit that particular run configuration in the future. Admin Tip Administrators can refer to the clearpiperun command for details on how to reset a pipeline run via the command line.","title":"Deleting a Run"},{"location":"using/deleterun/#step-by-step-guide","text":"","title":"Step-by-step Guide"},{"location":"using/deleterun/#1-navigate-to-the-run-detail-page","text":"Navigate to the detail page of the run you wish to delete.","title":"1. Navigate to the Run Detail Page"},{"location":"using/deleterun/#2-click-on-the-delete-run-button","text":"Click on the Delete Run button at the top right of the page, to open the confirmation modal.","title":"2. Click on the Delete Run Button"},{"location":"using/deleterun/#3-confirm-deletion","text":"To confirm the deletion click on the Delete Run button in the modal. Once pressed the website will direct back to the Pipeline Runs page and a confirmation message will appear in the top right. Note Runs with lots of database entries may take a short time to delete. Hence, the run may still appear in the pipeline runs list for a short time following the request with a status of Deleting . Refreshing the page will show a deleting status if the process is still running:","title":"3. Confirm Deletion"},{"location":"using/genarrow/","text":"Generating Arrow Files \u00b6 This page describes how to generate the measurement arrow files for a pipeline run if the option in the configuration file to create them was turned off. Arrow files only be generated by the creator or an administrator. Two files are produced by the method: File Description measurements.arrow An Apache Arrow format file containing all the measurements associated with the pipeline run (see Arrow Files ). Extra processing is performed in the creation of this file such that source ids are already in place for the measurements. measurement_pairs.arrow An Apache Arrow format file containing all the measurement pair metrics (see Arrow Files ). Arrow Files Available Users can see if arrow files are present for the run of interest by checking the respective run detail page. Admin Tip The arrow files can be generated using the command line using the command createmaeasarrow ). Why Create Arrow Files? \u00b6 Large pipeline runs (hundreds of images) mean that to read the measurements, hundreds of parquet files need to be read in, and can contain millions of rows. This can be slow using libraries such as pandas, and also consumes a lot of system memory. Instead, if the measurements are saved in the Apache Arrow format, libraries such as vaex are able to open .arrow files in an out-of-core context so the memory footprint is hugely reduced along with the reading of the file being very fast. The two-epoch measurement pairs are also saved to arrow format due to the same reasons. See Reading with vaex for further details on using vaex . Step-by-step Guide \u00b6 1. Navigate to the Run Detail Page \u00b6 Navigate to the detail page of the run you wish to generate arrow files for. 2. Select the Generate Arrow Files Option \u00b6 Click the Generate Arrow Files option at the top-right of the page. This will open the generate arrow files modal. 3. Submit Generate Arrow Files Request \u00b6 It is possible to overwrite existing arrow files by toggling the Overwrite Current Files option. When ready, click the Generate Arrow Files button on the modal to submit the generate request. A notification will show to indicate whether the submission was successful. 4. Refresh and Check the Generate Arrow Files Log File \u00b6 It is possible to check the progress by looking at the Generate Arrow Files Log File which can be found on the run detail page. The log will not be refreshed automatically and instead the page needs to be manually refreshed. Once completed the arrow files will be available for use.","title":"Generating Arrow Files"},{"location":"using/genarrow/#generating-arrow-files","text":"This page describes how to generate the measurement arrow files for a pipeline run if the option in the configuration file to create them was turned off. Arrow files only be generated by the creator or an administrator. Two files are produced by the method: File Description measurements.arrow An Apache Arrow format file containing all the measurements associated with the pipeline run (see Arrow Files ). Extra processing is performed in the creation of this file such that source ids are already in place for the measurements. measurement_pairs.arrow An Apache Arrow format file containing all the measurement pair metrics (see Arrow Files ). Arrow Files Available Users can see if arrow files are present for the run of interest by checking the respective run detail page. Admin Tip The arrow files can be generated using the command line using the command createmaeasarrow ).","title":"Generating Arrow Files"},{"location":"using/genarrow/#why-create-arrow-files","text":"Large pipeline runs (hundreds of images) mean that to read the measurements, hundreds of parquet files need to be read in, and can contain millions of rows. This can be slow using libraries such as pandas, and also consumes a lot of system memory. Instead, if the measurements are saved in the Apache Arrow format, libraries such as vaex are able to open .arrow files in an out-of-core context so the memory footprint is hugely reduced along with the reading of the file being very fast. The two-epoch measurement pairs are also saved to arrow format due to the same reasons. See Reading with vaex for further details on using vaex .","title":"Why Create Arrow Files?"},{"location":"using/genarrow/#step-by-step-guide","text":"","title":"Step-by-step Guide"},{"location":"using/genarrow/#1-navigate-to-the-run-detail-page","text":"Navigate to the detail page of the run you wish to generate arrow files for.","title":"1. Navigate to the Run Detail Page"},{"location":"using/genarrow/#2-select-the-generate-arrow-files-option","text":"Click the Generate Arrow Files option at the top-right of the page. This will open the generate arrow files modal.","title":"2. Select the Generate Arrow Files Option"},{"location":"using/genarrow/#3-submit-generate-arrow-files-request","text":"It is possible to overwrite existing arrow files by toggling the Overwrite Current Files option. When ready, click the Generate Arrow Files button on the modal to submit the generate request. A notification will show to indicate whether the submission was successful.","title":"3. Submit Generate Arrow Files Request"},{"location":"using/genarrow/#4-refresh-and-check-the-generate-arrow-files-log-file","text":"It is possible to check the progress by looking at the Generate Arrow Files Log File which can be found on the run detail page. The log will not be refreshed automatically and instead the page needs to be manually refreshed. Once completed the arrow files will be available for use.","title":"4. Refresh and Check the Generate Arrow Files Log File"},{"location":"using/initrun/","text":"Initialising a Pipeline Run \u00b6 This page outlines the steps required to create a pipeline run through the web interface. A description of the run configuration options can be found in the next section . Note Administrators please refer to this section in the admin documentation for instructions on how to initialise a pipeline run via the command line interface. Warning No data quality control is performed by the pipeline. Make sure your input data is clean and error free before processing using your preferred method. Step-by-step Guide \u00b6 1. Navigate to the Pipeline Runs Overview Page \u00b6 Navigate to the Pipeline Runs overview page by clicking on the Pipeline Runs option in the left hand side navigation bar, as highlighted below. 2. Select the New Pipeline Run Option \u00b6 From the Pipeline Runs overview page, select the New Pipeline Run button as highlighted in the screenshot below. This will open up a modal window to begin the run initialisation process. 3. Fill in the Run Details \u00b6 Fill in the name and description of the run and then press next to navigate to the next form to enter and select the configuration options. For full details on how to configure a pipeline run see the Run Configuration page, but a few notes here: Any settings entered here are not final, they can still be changed once the run is created. The order of the input files must match between the data types - i.e. the first selavy file, rms image and background image must all be the products of the first image, and so on. If you have a high number of images, selavy files, rms images and background images, it may be easier to leave these empty and instead use the text editor on the run detail page to directly enter the list to the configuration file. By default, non-admin users have a 200 image limit. Once you have finished filling in the configuration options, press the create button. 4. The Run Detail Page \u00b6 After pressing create, the run will be initialised in the pipeline, which means the required configuration files have been created and the run is ready to be processed. You will be navigated to the detail page of the created run (shown below). On this page you can: View details of the run. View and edit the configuration file. Leave a comment about the run. View tables of associated images and measurements (once processed). Submit the run to be processed. For full details on: how to use the text editor to edit the configuration file see Run Configuration , how to submit the job to be processed see Processing a Run , and how to add images to a run that has already been processed Adding Images to a Run .","title":"Initialising a Run"},{"location":"using/initrun/#initialising-a-pipeline-run","text":"This page outlines the steps required to create a pipeline run through the web interface. A description of the run configuration options can be found in the next section . Note Administrators please refer to this section in the admin documentation for instructions on how to initialise a pipeline run via the command line interface. Warning No data quality control is performed by the pipeline. Make sure your input data is clean and error free before processing using your preferred method.","title":"Initialising a Pipeline Run"},{"location":"using/initrun/#step-by-step-guide","text":"","title":"Step-by-step Guide"},{"location":"using/initrun/#1-navigate-to-the-pipeline-runs-overview-page","text":"Navigate to the Pipeline Runs overview page by clicking on the Pipeline Runs option in the left hand side navigation bar, as highlighted below.","title":"1. Navigate to the Pipeline Runs Overview Page"},{"location":"using/initrun/#2-select-the-new-pipeline-run-option","text":"From the Pipeline Runs overview page, select the New Pipeline Run button as highlighted in the screenshot below. This will open up a modal window to begin the run initialisation process.","title":"2. Select the New Pipeline Run Option"},{"location":"using/initrun/#3-fill-in-the-run-details","text":"Fill in the name and description of the run and then press next to navigate to the next form to enter and select the configuration options. For full details on how to configure a pipeline run see the Run Configuration page, but a few notes here: Any settings entered here are not final, they can still be changed once the run is created. The order of the input files must match between the data types - i.e. the first selavy file, rms image and background image must all be the products of the first image, and so on. If you have a high number of images, selavy files, rms images and background images, it may be easier to leave these empty and instead use the text editor on the run detail page to directly enter the list to the configuration file. By default, non-admin users have a 200 image limit. Once you have finished filling in the configuration options, press the create button.","title":"3. Fill in the Run Details"},{"location":"using/initrun/#4-the-run-detail-page","text":"After pressing create, the run will be initialised in the pipeline, which means the required configuration files have been created and the run is ready to be processed. You will be navigated to the detail page of the created run (shown below). On this page you can: View details of the run. View and edit the configuration file. Leave a comment about the run. View tables of associated images and measurements (once processed). Submit the run to be processed. For full details on: how to use the text editor to edit the configuration file see Run Configuration , how to submit the job to be processed see Processing a Run , and how to add images to a run that has already been processed Adding Images to a Run .","title":"4. The Run Detail Page"},{"location":"using/processrun/","text":"Processing a Run \u00b6 This page describes how to submit a pipeline run for processing. Admin Tip Administrators please refer to this section in the admin documentation for instructions on how to process a pipeline run via the command line interface. Admin Warning The Django Q service must be running in order for pipeline runs to be processed. See the Deployment page for further details. Tip Use the editor window on the run detail page to make adjustments to the run configuration file before processing. Step-by-step Guide \u00b6 1. Navigate to the Run Detail Page \u00b6 Navigate to the detail page of the run you wish to process. 2. Run a config validation \u00b6 Before processing it is recommended to check that the configuration file is valid. This is done by scrolling down to the config file card on the page and selecting the Validate Config option accessed by clicking the three dots menu button. Doing this will check if the configuration contains any errors prior to processing. Feedback will be provided on whether the configuration file is valid. In the event of an error, this can be corrected by using the edit option found in the same menu. 3. Confirm Processing \u00b6 With a successful configuration validation, scroll back up to the top of the page and click the Process Run button. This will open a modal window for you to confirm processing. For a newly initialised run, the only option that requires attention is whether to toggle the Debug Log Output on. This can be helpful if processing a new set of images which the pipeline hasn't seen before. If you are processing a run that has errored in the initial processing then the Full Re-Run option should be toggled to On . Once ready, press the Schedule Run button which will send the run to the queue for processing. Warning For non-admin users, by default there is a run image limit of 200. Monitoring the Run \u00b6 You can check the status of the run by refreshing the run detail page and seeing if the Run Status field has been updated. You can also check the log output by scrolling down to the log file card found below the configuration file. There is currently no automated notification on completion or errors. Full Re-Run \u00b6 A full re-run will be required if the run configuration needs to be changed, or in the event that an initial run has errored. Warning The Full Re-Run option will remove all associated existing data for that run. Note If images have been added to a run and the processing errors, there is a one time undo option that may avoid having to use the Full Re-Run command. Adding Images to a Run \u00b6 See the dedicated documentation page here .","title":"Processing a Run"},{"location":"using/processrun/#processing-a-run","text":"This page describes how to submit a pipeline run for processing. Admin Tip Administrators please refer to this section in the admin documentation for instructions on how to process a pipeline run via the command line interface. Admin Warning The Django Q service must be running in order for pipeline runs to be processed. See the Deployment page for further details. Tip Use the editor window on the run detail page to make adjustments to the run configuration file before processing.","title":"Processing a Run"},{"location":"using/processrun/#step-by-step-guide","text":"","title":"Step-by-step Guide"},{"location":"using/processrun/#1-navigate-to-the-run-detail-page","text":"Navigate to the detail page of the run you wish to process.","title":"1. Navigate to the Run Detail Page"},{"location":"using/processrun/#2-run-a-config-validation","text":"Before processing it is recommended to check that the configuration file is valid. This is done by scrolling down to the config file card on the page and selecting the Validate Config option accessed by clicking the three dots menu button. Doing this will check if the configuration contains any errors prior to processing. Feedback will be provided on whether the configuration file is valid. In the event of an error, this can be corrected by using the edit option found in the same menu.","title":"2. Run a config validation"},{"location":"using/processrun/#3-confirm-processing","text":"With a successful configuration validation, scroll back up to the top of the page and click the Process Run button. This will open a modal window for you to confirm processing. For a newly initialised run, the only option that requires attention is whether to toggle the Debug Log Output on. This can be helpful if processing a new set of images which the pipeline hasn't seen before. If you are processing a run that has errored in the initial processing then the Full Re-Run option should be toggled to On . Once ready, press the Schedule Run button which will send the run to the queue for processing. Warning For non-admin users, by default there is a run image limit of 200.","title":"3. Confirm Processing"},{"location":"using/processrun/#monitoring-the-run","text":"You can check the status of the run by refreshing the run detail page and seeing if the Run Status field has been updated. You can also check the log output by scrolling down to the log file card found below the configuration file. There is currently no automated notification on completion or errors.","title":"Monitoring the Run"},{"location":"using/processrun/#full-re-run","text":"A full re-run will be required if the run configuration needs to be changed, or in the event that an initial run has errored. Warning The Full Re-Run option will remove all associated existing data for that run. Note If images have been added to a run and the processing errors, there is a one time undo option that may avoid having to use the Full Re-Run command.","title":"Full Re-Run"},{"location":"using/processrun/#adding-images-to-a-run","text":"See the dedicated documentation page here .","title":"Adding Images to a Run"},{"location":"using/requireddata/","text":"Required Data \u00b6 This page gives an overview of what data is required to run the pipeline along with how to obtain the data. Acquiring ASKAP Data \u00b6 Note: VAST Data Releases If you are a member of the VAST collaboration you will have access to the VAST data release which contains the data for the VAST Pilot Survey. Refer to the VAST wiki for more details. Data produced by the ASKAP telescope can be accessed by using The CSIRO ASKAP Science Data Archive (CASDA) . CASDA provides a web form for users to search for the data they are interested in and request for the data to be staged for download. Note that a form of account or registration is required to download image cube products. All data products from CASDA should be compatible without any modifications. Please report an issue if you find this to not be the case. Tip: CASDA Data Products Descriptions of the data products available on CASDA can be found on this page . Pipeline Required Data \u00b6 Warning: Correcting Data Currently, the pipeline does not contain any processes to correct the data as it is ingested by the pipeline. For example, if corrections to the flux or positions need to be applied, these should be done to the data directly before they are processed by the pipeline. If an image has been previously ingested before corrections were applied, the filename of the corrected image must be changed. Doing so will make the pipeline see the corrected image as a new image and reingest the corrected data. A pipeline run requires a minimum of two ASKAP observational images to process. For each image, the following table provides a summary of the files that are required. Data File Type Description Primary image .fits The primary, Stokes I, taylor 0, image of the observation. Component Catalogue .xml, .csv, .txt The component source catalogue produced by the source finder selavy from the primary image. Noise map of primary image .fits The noise (or rms) map produced by the source finder selavy from the primary image. Background map of primary image (optional) .fits The background (or mean) map produced by the source finder selavy from the primary image. The background image is only required when source monitoring is enabled. Refer to the respective sections on this page for more details on these inputs. Image File \u00b6 This is the primary image file produced by the ASKAP pipeline. The pipeline requires the Stokes I, total intensity (taylor 0) image file. It is also recommended to use the convolved final image, where each of the 36 individual beams have been convolved to a common resolution prior to the creation of the combined mosaic of the field. These files are denoted by a .conv in the file name. Must be in the FITS file format. Example CASDA Filename image.i.SB25597.cont.taylor.0.restored.conv.fits Component Catalogues \u00b6 Currently, the pipeline does not contain any source finding capabilities so the source catalogue must be provided. In particular, the pipeline requires the continuum component catalogue produced by the selavy source finder. The files can be one of three formats: XML The VOTable XML format of the selavy components catalogue. Provided by CASDA. Must have a file extension of .xml . CSV The csv format of the selavy components catalogue. Provided by CASDA. Must have a file extension of .csv . TXT 'Fixed-width' formatted output produced by the selavy source finder directly. These are not available through CASDA. Must have a file extension of .txt . Example CASDA Filenames xml selavy-image.i.SB25597.cont.taylor.0.restored.conv.components.xml csv AS107_Continuum_Component_Catalogue_25597_3792.csv Note that CASDA does not provide the fixed width text format selavy output. File format examples xml <?xml version=\"1.0\"?> <VOTABLE version= \"1.3\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xmlns= \"http://www.ivoa.net/xml/VOTable/v1.3\" xmlns:stc= \"http://www.ivoa.net/xml/STC/v1.30\" > <COOSYS ID= \"J2000\" equinox= \"J2000\" system= \"eq_FK5\" /> <RESOURCE name= \"Component catalogue from Selavy source finding\" > <TABLE name= \"Component catalogue\" > <DESCRIPTION></DESCRIPTION> <PARAM name= \"table_version\" ucd= \"meta.version\" datatype= \"char\" arraysize= \"43\" value= \"casda.continuum_component_description_v1.9\" /> <PARAM name= \"imageFile\" ucd= \"meta.file;meta.fits\" datatype= \"char\" arraysize= \"48\" value= \"image.i.SB25600.cont.taylor.0.restored.conv.fits\" /> <PARAM name= \"flagSubsection\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"subsection\" ucd= \"\" datatype= \"char\" arraysize= \"25\" value= \"[1:16449,1:14759,1:1,1:1]\" /> <PARAM name= \"flagStatSec\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"StatSec\" ucd= \"\" datatype= \"char\" arraysize= \"7\" value= \"[*,*,*]\" /> <PARAM name= \"searchType\" ucd= \"meta.note\" datatype= \"char\" arraysize= \"7\" value= \"spatial\" /> <PARAM name= \"flagNegative\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagBaseline\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagRobustStats\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"flagFDR\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"threshold\" ucd= \"phot.flux;stat.min\" datatype= \"float\" value= \"5\" /> <PARAM name= \"flagGrowth\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"growthThreshold\" ucd= \"phot.flux;stat.min\" datatype= \"float\" value= \"3\" /> <PARAM name= \"minPix\" ucd= \"\" datatype= \"int\" value= \"3\" /> <PARAM name= \"minChannels\" ucd= \"\" datatype= \"int\" value= \"0\" /> <PARAM name= \"minVoxels\" ucd= \"\" datatype= \"int\" value= \"3\" /> <PARAM name= \"flagAdjacent\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"threshVelocity\" ucd= \"\" datatype= \"float\" value= \"7\" /> <PARAM name= \"flagRejectBeforeMerge\" ucd= \"\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagTwoStageMerging\" ucd= \"\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"pixelCentre\" ucd= \"\" datatype= \"char\" arraysize= \"8\" value= \"centroid\" /> <PARAM name= \"flagSmooth\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagATrous\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"Reference frequency\" ucd= \"em.freq;meta.main\" datatype= \"float\" unit= \"Hz\" value= \"1.36749e+09\" /> <PARAM name= \"thresholdActual\" ucd= \"\" datatype= \"float\" unit= \"Jy/beam\" value= \"5\" /> <FIELD name= \"island_id\" ID= \"col_island_id\" ucd= \"meta.id.parent\" datatype= \"char\" unit= \"--\" arraysize= \"20\" /> <FIELD name= \"component_id\" ID= \"col_component_id\" ucd= \"meta.id;meta.main\" datatype= \"char\" unit= \"--\" arraysize= \"24\" /> <FIELD name= \"component_name\" ID= \"col_component_name\" ucd= \"meta.id\" datatype= \"char\" unit= \"\" arraysize= \"26\" /> <FIELD name= \"ra_hms_cont\" ID= \"col_ra_hms_cont\" ucd= \"pos.eq.ra\" ref= \"J2000\" datatype= \"char\" unit= \"\" arraysize= \"12\" /> <FIELD name= \"dec_dms_cont\" ID= \"col_dec_dms_cont\" ucd= \"pos.eq.dec\" ref= \"J2000\" datatype= \"char\" unit= \"\" arraysize= \"13\" /> <FIELD name= \"ra_deg_cont\" ID= \"col_ra_deg_cont\" ucd= \"pos.eq.ra;meta.main\" ref= \"J2000\" datatype= \"double\" unit= \"deg\" width= \"12\" precision= \"6\" /> <FIELD name= \"dec_deg_cont\" ID= \"col_dec_deg_cont\" ucd= \"pos.eq.dec;meta.main\" ref= \"J2000\" datatype= \"double\" unit= \"deg\" width= \"13\" precision= \"6\" /> <FIELD name= \"ra_err\" ID= \"col_ra_err\" ucd= \"stat.error;pos.eq.ra\" ref= \"J2000\" datatype= \"float\" unit= \"arcsec\" width= \"11\" precision= \"2\" /> <FIELD name= \"dec_err\" ID= \"col_dec_err\" ucd= \"stat.error;pos.eq.dec\" ref= \"J2000\" datatype= \"float\" unit= \"arcsec\" width= \"11\" precision= \"2\" /> <FIELD name= \"freq\" ID= \"col_freq\" ucd= \"em.freq\" datatype= \"float\" unit= \"MHz\" width= \"11\" precision= \"1\" /> <FIELD name= \"flux_peak\" ID= \"col_flux_peak\" ucd= \"phot.flux.density;stat.max;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy/beam\" width= \"11\" precision= \"3\" /> <FIELD name= \"flux_peak_err\" ID= \"col_flux_peak_err\" ucd= \"stat.error;phot.flux.density;stat.max;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy/beam\" width= \"14\" precision= \"3\" /> <FIELD name= \"flux_int\" ID= \"col_flux_int\" ucd= \"phot.flux.density;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy\" width= \"10\" precision= \"3\" /> <FIELD name= \"flux_int_err\" ID= \"col_flux_int_err\" ucd= \"stat.error;phot.flux.density;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy\" width= \"13\" precision= \"3\" /> <FIELD name= \"maj_axis\" ID= \"col_maj_axis\" ucd= \"phys.angSize.smajAxis;em.radio;stat.fit\" datatype= \"float\" unit= \"arcsec\" width= \"9\" precision= \"2\" /> <FIELD name= \"min_axis\" ID= \"col_min_axis\" ucd= \"phys.angSize.sminAxis;em.radio;stat.fit\" datatype= \"float\" unit= \"arcsec\" width= \"9\" precision= \"2\" /> <FIELD name= \"pos_ang\" ID= \"col_pos_ang\" ucd= \"phys.angSize;pos.posAng;em.radio;stat.fit\" datatype= \"float\" unit= \"deg\" width= \"8\" precision= \"2\" /> <FIELD name= \"maj_axis_err\" ID= \"col_maj_axis_err\" ucd= \"stat.error;phys.angSize.smajAxis;em.radio\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"min_axis_err\" ID= \"col_min_axis_err\" ucd= \"stat.error;phys.angSize.sminAxis;em.radio\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"pos_ang_err\" ID= \"col_pos_ang_err\" ucd= \"stat.error;phys.angSize;pos.posAng;em.radio\" datatype= \"float\" unit= \"deg\" width= \"12\" precision= \"2\" /> <FIELD name= \"maj_axis_deconv\" ID= \"col_maj_axis_deconv\" ucd= \"phys.angSize.smajAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"18\" precision= \"2\" /> <FIELD name= \"min_axis_deconv\" ID= \"col_min_axis_deconv\" ucd= \"phys.angSize.sminAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"16\" precision= \"2\" /> <FIELD name= \"pos_ang_deconv\" ID= \"col_pos_ang_deconv\" ucd= \"phys.angSize;pos.posAng;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"deg\" width= \"15\" precision= \"2\" /> <FIELD name= \"maj_axis_deconv_err\" ID= \"col_maj_axis_deconv_err\" ucd= \"stat.error;phys.angSize.smajAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"min_axis_deconv_err\" ID= \"col_min_axis_deconv_err\" ucd= \"stat.error;phys.angSize.sminAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"pos_ang_deconv_err\" ID= \"col_pos_ang_deconv_err\" ucd= \"stat.error;phys.angSize;pos.posAng;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"deg\" width= \"12\" precision= \"2\" /> <FIELD name= \"chi_squared_fit\" ID= \"col_chi_squared_fit\" ucd= \"stat.fit.chi2\" datatype= \"float\" unit= \"--\" width= \"17\" precision= \"3\" /> <FIELD name= \"rms_fit_gauss\" ID= \"col_rms_fit_gauss\" ucd= \"stat.stdev;stat.fit\" datatype= \"float\" unit= \"mJy/beam\" width= \"15\" precision= \"3\" /> <FIELD name= \"spectral_index\" ID= \"col_spectral_index\" ucd= \"spect.index;em.radio\" datatype= \"float\" unit= \"--\" width= \"15\" precision= \"2\" /> <FIELD name= \"spectral_curvature\" ID= \"col_spectral_curvature\" ucd= \"askap:spect.curvature;em.radio\" datatype= \"float\" unit= \"--\" width= \"19\" precision= \"2\" /> <FIELD name= \"spectral_index_err\" ID= \"col_spectral_index_err\" ucd= \"stat.error;spect.index;em.radio\" datatype= \"float\" unit= \"--\" width= \"15\" precision= \"2\" /> <FIELD name= \"spectral_curvature_err\" ID= \"col_spectral_curvature_err\" ucd= \"stat.error;askap:spect.curvature;em.radio\" datatype= \"float\" unit= \"--\" width= \"19\" precision= \"2\" /> <FIELD name= \"rms_image\" ID= \"col_rms_image\" ucd= \"stat.stdev;phot.flux.density\" datatype= \"float\" unit= \"mJy/beam\" width= \"12\" precision= \"3\" /> <FIELD name= \"has_siblings\" ID= \"col_has_siblings\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"fit_is_estimate\" ID= \"col_fit_is_estimate\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"spectral_index_from_TT\" ID= \"col_spectral_index_from_TT\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"flag_c4\" ID= \"col_flag_c4\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"comment\" ID= \"col_comment\" ucd= \"meta.note\" datatype= \"char\" unit= \"\" arraysize= \"100\" /> <DATA> <TABLEDATA> <TR> <TD> SB25600_island_1 </TD><TD> SB25600_component_1a </TD><TD> J053527-691611 </TD><TD> 05:35:27.9 </TD><TD> -69:16:11 </TD><TD> 83.866441 </TD><TD> -69.269927 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 1367.5 </TD><TD> 780.068 </TD><TD> 0.721 </TD><TD> 843.563 </TD><TD> 1.692 </TD><TD> 11.80 </TD><TD> 8.40 </TD><TD> 176.22 </TD><TD> 0.01 </TD><TD> 0.01 </TD><TD> 0.11 </TD><TD> 3.09 </TD><TD> 1.82 </TD><TD> 56.44 </TD><TD> 0.78 </TD><TD> 2.35 </TD><TD> 1.49 </TD><TD> 596.911 </TD><TD> 1811.003 </TD><TD> -0.93 </TD><TD> -99.00 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 1.038 </TD><TD> 0 </TD><TD> 0 </TD><TD> 1 </TD><TD> 0 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_2 </TD><TD> SB25600_component_2a </TD><TD> J060004-703833 </TD><TD> 06:00:04.9 </TD><TD> -70:38:33 </TD><TD> 90.020608 </TD><TD> -70.642664 </TD><TD> 0.01 </TD><TD> 0.01 </TD><TD> 1367.5 </TD><TD> 438.885 </TD><TD> 1.202 </TD><TD> 494.222 </TD><TD> 2.918 </TD><TD> 11.65 </TD><TD> 8.86 </TD><TD> 9.51 </TD><TD> 0.03 </TD><TD> 0.04 </TD><TD> 0.40 </TD><TD> 5.22 </TD><TD> 0.00 </TD><TD> 58.20 </TD><TD> 0.09 </TD><TD> 0.00 </TD><TD> 0.82 </TD><TD> 5351.118 </TD><TD> 4419.234 </TD><TD> -1.06 </TD><TD> -99.00 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 0.715 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 1 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_2 </TD><TD> SB25600_component_2b </TD><TD> J060006-703854 </TD><TD> 06:00:06.1 </TD><TD> -70:38:54 </TD><TD> 90.025359 </TD><TD> -70.648364 </TD><TD> 0.19 </TD><TD> 0.24 </TD><TD> 1367.5 </TD><TD> 24.719 </TD><TD> 1.326 </TD><TD> 24.048 </TD><TD> 2.986 </TD><TD> 10.66 </TD><TD> 8.37 </TD><TD> 167.45 </TD><TD> 0.59 </TD><TD> 0.80 </TD><TD> 9.41 </TD><TD> 3.00 </TD><TD> 0.00 </TD><TD> -86.64 </TD><TD> 5.43 </TD><TD> 0.00 </TD><TD> 14.18 </TD><TD> 5351.118 </TD><TD> 4419.234 </TD><TD> -99.00 </TD><TD> -99.00 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 0.715 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 1 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_3 </TD><TD> SB25600_component_3a </TD><TD> J061931-681533 </TD><TD> 06:19:31.4 </TD><TD> -68:15:33 </TD><TD> 94.880875 </TD><TD> -68.259404 </TD><TD> 0.49 </TD><TD> 0.65 </TD><TD> 1367.5 </TD><TD> 258.844 </TD><TD> 72.505 </TD><TD> 302.138 </TD><TD> 84.967 </TD><TD> 11.83 </TD><TD> 9.04 </TD><TD> 0.90 </TD><TD> 0.21 </TD><TD> 0.18 </TD><TD> 3.84 </TD><TD> 4.77 </TD><TD> 1.40 </TD><TD> 63.44 </TD><TD> 1.39 </TD><TD> 19.02 </TD><TD> 9.15 </TD><TD> 1284.683 </TD><TD> 2467.498 </TD><TD> -0.74 </TD><TD> -99.00 </TD><TD> 0.01 </TD><TD> 0.00 </TD><TD> 0.277 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 0 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_3 </TD><TD> SB25600_component_3b </TD><TD> J061931-681531 </TD><TD> 06:19:31.0 </TD><TD> -68:15:31 </TD><TD> 94.879037 </TD><TD> -68.258688 </TD><TD> 0.21 </TD><TD> 0.57 </TD><TD> 1367.5 </TD><TD> 153.160 </TD><TD> 84.478 </TD><TD> 157.853 </TD><TD> 87.200 </TD><TD> 11.40 </TD><TD> 8.28 </TD><TD> 6.90 </TD><TD> 0.20 </TD><TD> 0.21 </TD><TD> 1.40 </TD><TD> 4.07 </TD><TD> 0.00 </TD><TD> 55.21 </TD><TD> 1.06 </TD><TD> 0.00 </TD><TD> 5.16 </TD><TD> 1284.683 </TD><TD> 2467.498 </TD><TD> -1.17 </TD><TD> -99.00 </TD><TD> 0.02 </TD><TD> 0.00 </TD><TD> 0.277 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 0 </TD><TD> </TD> </TR> </TABLEDATA> </DATA> </TABLE> </RESOURCE> </VOTABLE> csv id,catalogue_id,first_sbid,other_sbids,project_id,island_id,component_id,component_name,ra_hms_cont,dec_dms_cont,ra_deg_cont,dec_deg_cont,ra_err,dec_err,freq,flux_peak,flux_peak_err,flux_int,flux_int_err,maj_axis,min_axis,pos_ang,maj_axis_err,min_axis_err,pos_ang_err,maj_axis_deconv,min_axis_deconv,maj_axis_deconv_err,pos_ang_deconv,min_axis_deconv_err,pos_ang_deconv_err,chi_squared_fit,rms_fit_gauss,spectral_index,spectral_index_err,spectral_curvature,spectral_curvature_err,rms_image,has_siblings,fit_is_estimate,spectral_index_from_tt,flag_c4,comment,quality_level,released_date 20793260,3790,25600,,18,SB25600_island_2768,SB25600_component_2768a,J055644-690942,05:56:44.5,-69:09:42,89.185558,-69.161889,0.04,0.05,1367.5,0.834,0.009,0.68,0.037,9.63,7.76,165.12,0.23,0.33,3.9,0.0,0.0,0.0,-89.09,0.0,3.48,0.034,52.957,-99.0,0.0,-99.0,0.0,0.166,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z 20793259,3790,25600,,18,SB25600_island_2767,SB25600_component_2767a,J060047-692500,06:00:47.7,-69:25:00,90.198913,-69.416437,0.09,0.37,1367.5,0.889,0.016,2.642,0.2,29.12,9.35,2.08,2.0,0.99,1.35,26.73,4.84,0.03,2.91,6.03,1.5,1.869,234.431,-99.0,0.0,-99.0,0.0,0.175,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z 20793258,3790,25600,,18,SB25600_island_2766,SB25600_component_2766a,J055849-690051,05:58:49.4,-69:00:51,89.705808,-69.014253,0.14,0.24,1367.5,0.885,0.025,1.181,0.191,13.04,9.38,6.04,1.3,1.45,8.83,6.87,3.72,3.2,36.6,14.03,34.31,0.561,193.467,-99.0,0.0,-99.0,0.0,0.177,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z 20793257,3790,25600,,18,SB25600_island_2765,SB25600_component_2765a,J055339-683120,05:53:39.5,-68:31:20,88.414758,-68.522436,0.08,0.1,1367.5,0.906,0.014,0.895,0.063,12.78,7.08,10.0,0.43,0.43,2.33,6.37,0.0,0.54,28.11,0.0,4.65,0.138,99.302,-99.0,0.0,-99.0,0.0,0.173,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z txt # island_id component_id component_name ra_hms_cont dec_dms_cont ra_deg_cont dec_deg_cont ra_err dec_err freq flux_peak flux_peak_err flux_int flux_int_err maj_axis min_axis pos_ang maj_axis_err min_axis_err pos_ang_err maj_axis_deconv min_axis_deconv pos_ang_deconv maj_axis_deconv_err min_axis_deconv_err pos_ang_deconv_err chi_squared_fit rms_fit_gauss spectral_index spectral_curvature spectral_index_err spectral_curvature_err rms_image has_siblings fit_is_estimate spectral_index_from_TT flag_c4 comment # -- -- [deg] [deg] [arcsec] [arcsec] [MHz] [mJy/beam] [mJy/beam] [mJy] [mJy] [arcsec] [arcsec] [deg] [arcsec] [arcsec] [deg] [arcsec] [arcsec] [deg] [arcsec] [arcsec] [deg] -- [mJy/beam] -- -- -- -- [mJy/beam] SB10342_island_1000 SB10342_component_1000a B2337-0423 23:37:09.9 -04:23:13 354.291208 -4.387204 0.03 0.02 -0.0 15.907 0.067 19.273 0.112 17.29 13.32 86.15 0.08 0.01 0.66 7.68 2.86 -35.37 0.08 0.72 0.71 57.119 678.701 -0.79 -99.00 0.00 0.00 0.304 0 0 1 0 SB10342_island_1001 SB10342_component_1001a B0001-0346 00:01:57.6 -03:46:12 0.490036 -3.770075 0.19 0.15 -0.0 12.139 0.425 24.396 0.914 24.14 15.82 61.05 0.45 0.04 2.12 17.96 10.07 50.17 0.06 0.24 4.91 690.545 1639.191 -0.58 -99.00 0.00 0.00 0.339 1 0 1 0 SB10342_island_1001 SB10342_component_1001b B0001-0346 00:01:57.8 -03:46:04 0.491002 -3.767781 1.13 1.35 -0.0 4.044 0.387 10.072 1.053 50.40 9.39 42.19 3.33 0.04 0.73 48.09 0.00 40.49 0.08 0.00 31.48 690.545 1639.191 -0.36 -99.00 0.00 0.00 0.339 1 0 1 0 SB10342_island_1002 SB10342_component_1002a B2333-0141 23:33:29.4 -01:41:04 353.372617 -1.684465 0.13 0.17 -0.0 14.516 0.128 47.232 0.561 35.56 17.39 140.82 0.51 0.02 0.52 33.30 7.00 -35.89 0.02 0.39 3.49 926.099 1383.267 -0.53 -99.00 0.00 0.00 0.459 1 0 1 0 SB10342_island_1002 SB10342_component_1002b B2333-0141 23:33:30.8 -01:41:38 353.378227 -1.693919 0.20 0.26 -0.0 8.051 0.134 25.654 0.550 28.02 21.61 133.92 0.67 0.06 3.37 24.95 14.85 -35.94 0.05 0.13 4.34 926.099 1383.267 -0.21 -99.00 0.00 0.00 0.459 1 0 1 0 Noise Image File \u00b6 This is the noise map that is created from the primary image file during source finding by selavy . Must be in the FITS file format. Example CASDA Filename noiseMap.image.i.SB25600.cont.taylor.0.restored.conv.fits Background Image File \u00b6 This is the mean map that is created from the primary image file during source finding by selavy . The background image files are only required when source monitoring is enabled in the pipeline run. Must be in the FITS file format. Example CASDA Filename meanMap.image.i.SB25600.cont.taylor.0.restored.conv.fits Data Location \u00b6 Data should be placed in the directories denoted by RAW_IMAGE_DIR and HOME_DATA_DIR in the pipeline configuration. The HOME_DATA_DIR is designed to allow users to upload their own data to their home directory on the system where the pipeline is installed. By default, the HOME_DATA_DIR is set to scan the directory called vast-pipeline-extra-data in the users home area. Refer to the Pipeline Configuration section for more information.","title":"Required Data"},{"location":"using/requireddata/#required-data","text":"This page gives an overview of what data is required to run the pipeline along with how to obtain the data.","title":"Required Data"},{"location":"using/requireddata/#acquiring-askap-data","text":"Note: VAST Data Releases If you are a member of the VAST collaboration you will have access to the VAST data release which contains the data for the VAST Pilot Survey. Refer to the VAST wiki for more details. Data produced by the ASKAP telescope can be accessed by using The CSIRO ASKAP Science Data Archive (CASDA) . CASDA provides a web form for users to search for the data they are interested in and request for the data to be staged for download. Note that a form of account or registration is required to download image cube products. All data products from CASDA should be compatible without any modifications. Please report an issue if you find this to not be the case. Tip: CASDA Data Products Descriptions of the data products available on CASDA can be found on this page .","title":"Acquiring ASKAP Data"},{"location":"using/requireddata/#pipeline-required-data","text":"Warning: Correcting Data Currently, the pipeline does not contain any processes to correct the data as it is ingested by the pipeline. For example, if corrections to the flux or positions need to be applied, these should be done to the data directly before they are processed by the pipeline. If an image has been previously ingested before corrections were applied, the filename of the corrected image must be changed. Doing so will make the pipeline see the corrected image as a new image and reingest the corrected data. A pipeline run requires a minimum of two ASKAP observational images to process. For each image, the following table provides a summary of the files that are required. Data File Type Description Primary image .fits The primary, Stokes I, taylor 0, image of the observation. Component Catalogue .xml, .csv, .txt The component source catalogue produced by the source finder selavy from the primary image. Noise map of primary image .fits The noise (or rms) map produced by the source finder selavy from the primary image. Background map of primary image (optional) .fits The background (or mean) map produced by the source finder selavy from the primary image. The background image is only required when source monitoring is enabled. Refer to the respective sections on this page for more details on these inputs.","title":"Pipeline Required Data"},{"location":"using/requireddata/#image-file","text":"This is the primary image file produced by the ASKAP pipeline. The pipeline requires the Stokes I, total intensity (taylor 0) image file. It is also recommended to use the convolved final image, where each of the 36 individual beams have been convolved to a common resolution prior to the creation of the combined mosaic of the field. These files are denoted by a .conv in the file name. Must be in the FITS file format. Example CASDA Filename image.i.SB25597.cont.taylor.0.restored.conv.fits","title":"Image File"},{"location":"using/requireddata/#component-catalogues","text":"Currently, the pipeline does not contain any source finding capabilities so the source catalogue must be provided. In particular, the pipeline requires the continuum component catalogue produced by the selavy source finder. The files can be one of three formats: XML The VOTable XML format of the selavy components catalogue. Provided by CASDA. Must have a file extension of .xml . CSV The csv format of the selavy components catalogue. Provided by CASDA. Must have a file extension of .csv . TXT 'Fixed-width' formatted output produced by the selavy source finder directly. These are not available through CASDA. Must have a file extension of .txt . Example CASDA Filenames xml selavy-image.i.SB25597.cont.taylor.0.restored.conv.components.xml csv AS107_Continuum_Component_Catalogue_25597_3792.csv Note that CASDA does not provide the fixed width text format selavy output. File format examples xml <?xml version=\"1.0\"?> <VOTABLE version= \"1.3\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xmlns= \"http://www.ivoa.net/xml/VOTable/v1.3\" xmlns:stc= \"http://www.ivoa.net/xml/STC/v1.30\" > <COOSYS ID= \"J2000\" equinox= \"J2000\" system= \"eq_FK5\" /> <RESOURCE name= \"Component catalogue from Selavy source finding\" > <TABLE name= \"Component catalogue\" > <DESCRIPTION></DESCRIPTION> <PARAM name= \"table_version\" ucd= \"meta.version\" datatype= \"char\" arraysize= \"43\" value= \"casda.continuum_component_description_v1.9\" /> <PARAM name= \"imageFile\" ucd= \"meta.file;meta.fits\" datatype= \"char\" arraysize= \"48\" value= \"image.i.SB25600.cont.taylor.0.restored.conv.fits\" /> <PARAM name= \"flagSubsection\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"subsection\" ucd= \"\" datatype= \"char\" arraysize= \"25\" value= \"[1:16449,1:14759,1:1,1:1]\" /> <PARAM name= \"flagStatSec\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"StatSec\" ucd= \"\" datatype= \"char\" arraysize= \"7\" value= \"[*,*,*]\" /> <PARAM name= \"searchType\" ucd= \"meta.note\" datatype= \"char\" arraysize= \"7\" value= \"spatial\" /> <PARAM name= \"flagNegative\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagBaseline\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagRobustStats\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"flagFDR\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"threshold\" ucd= \"phot.flux;stat.min\" datatype= \"float\" value= \"5\" /> <PARAM name= \"flagGrowth\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"growthThreshold\" ucd= \"phot.flux;stat.min\" datatype= \"float\" value= \"3\" /> <PARAM name= \"minPix\" ucd= \"\" datatype= \"int\" value= \"3\" /> <PARAM name= \"minChannels\" ucd= \"\" datatype= \"int\" value= \"0\" /> <PARAM name= \"minVoxels\" ucd= \"\" datatype= \"int\" value= \"3\" /> <PARAM name= \"flagAdjacent\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"threshVelocity\" ucd= \"\" datatype= \"float\" value= \"7\" /> <PARAM name= \"flagRejectBeforeMerge\" ucd= \"\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagTwoStageMerging\" ucd= \"\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"pixelCentre\" ucd= \"\" datatype= \"char\" arraysize= \"8\" value= \"centroid\" /> <PARAM name= \"flagSmooth\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagATrous\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"Reference frequency\" ucd= \"em.freq;meta.main\" datatype= \"float\" unit= \"Hz\" value= \"1.36749e+09\" /> <PARAM name= \"thresholdActual\" ucd= \"\" datatype= \"float\" unit= \"Jy/beam\" value= \"5\" /> <FIELD name= \"island_id\" ID= \"col_island_id\" ucd= \"meta.id.parent\" datatype= \"char\" unit= \"--\" arraysize= \"20\" /> <FIELD name= \"component_id\" ID= \"col_component_id\" ucd= \"meta.id;meta.main\" datatype= \"char\" unit= \"--\" arraysize= \"24\" /> <FIELD name= \"component_name\" ID= \"col_component_name\" ucd= \"meta.id\" datatype= \"char\" unit= \"\" arraysize= \"26\" /> <FIELD name= \"ra_hms_cont\" ID= \"col_ra_hms_cont\" ucd= \"pos.eq.ra\" ref= \"J2000\" datatype= \"char\" unit= \"\" arraysize= \"12\" /> <FIELD name= \"dec_dms_cont\" ID= \"col_dec_dms_cont\" ucd= \"pos.eq.dec\" ref= \"J2000\" datatype= \"char\" unit= \"\" arraysize= \"13\" /> <FIELD name= \"ra_deg_cont\" ID= \"col_ra_deg_cont\" ucd= \"pos.eq.ra;meta.main\" ref= \"J2000\" datatype= \"double\" unit= \"deg\" width= \"12\" precision= \"6\" /> <FIELD name= \"dec_deg_cont\" ID= \"col_dec_deg_cont\" ucd= \"pos.eq.dec;meta.main\" ref= \"J2000\" datatype= \"double\" unit= \"deg\" width= \"13\" precision= \"6\" /> <FIELD name= \"ra_err\" ID= \"col_ra_err\" ucd= \"stat.error;pos.eq.ra\" ref= \"J2000\" datatype= \"float\" unit= \"arcsec\" width= \"11\" precision= \"2\" /> <FIELD name= \"dec_err\" ID= \"col_dec_err\" ucd= \"stat.error;pos.eq.dec\" ref= \"J2000\" datatype= \"float\" unit= \"arcsec\" width= \"11\" precision= \"2\" /> <FIELD name= \"freq\" ID= \"col_freq\" ucd= \"em.freq\" datatype= \"float\" unit= \"MHz\" width= \"11\" precision= \"1\" /> <FIELD name= \"flux_peak\" ID= \"col_flux_peak\" ucd= \"phot.flux.density;stat.max;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy/beam\" width= \"11\" precision= \"3\" /> <FIELD name= \"flux_peak_err\" ID= \"col_flux_peak_err\" ucd= \"stat.error;phot.flux.density;stat.max;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy/beam\" width= \"14\" precision= \"3\" /> <FIELD name= \"flux_int\" ID= \"col_flux_int\" ucd= \"phot.flux.density;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy\" width= \"10\" precision= \"3\" /> <FIELD name= \"flux_int_err\" ID= \"col_flux_int_err\" ucd= \"stat.error;phot.flux.density;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy\" width= \"13\" precision= \"3\" /> <FIELD name= \"maj_axis\" ID= \"col_maj_axis\" ucd= \"phys.angSize.smajAxis;em.radio;stat.fit\" datatype= \"float\" unit= \"arcsec\" width= \"9\" precision= \"2\" /> <FIELD name= \"min_axis\" ID= \"col_min_axis\" ucd= \"phys.angSize.sminAxis;em.radio;stat.fit\" datatype= \"float\" unit= \"arcsec\" width= \"9\" precision= \"2\" /> <FIELD name= \"pos_ang\" ID= \"col_pos_ang\" ucd= \"phys.angSize;pos.posAng;em.radio;stat.fit\" datatype= \"float\" unit= \"deg\" width= \"8\" precision= \"2\" /> <FIELD name= \"maj_axis_err\" ID= \"col_maj_axis_err\" ucd= \"stat.error;phys.angSize.smajAxis;em.radio\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"min_axis_err\" ID= \"col_min_axis_err\" ucd= \"stat.error;phys.angSize.sminAxis;em.radio\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"pos_ang_err\" ID= \"col_pos_ang_err\" ucd= \"stat.error;phys.angSize;pos.posAng;em.radio\" datatype= \"float\" unit= \"deg\" width= \"12\" precision= \"2\" /> <FIELD name= \"maj_axis_deconv\" ID= \"col_maj_axis_deconv\" ucd= \"phys.angSize.smajAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"18\" precision= \"2\" /> <FIELD name= \"min_axis_deconv\" ID= \"col_min_axis_deconv\" ucd= \"phys.angSize.sminAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"16\" precision= \"2\" /> <FIELD name= \"pos_ang_deconv\" ID= \"col_pos_ang_deconv\" ucd= \"phys.angSize;pos.posAng;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"deg\" width= \"15\" precision= \"2\" /> <FIELD name= \"maj_axis_deconv_err\" ID= \"col_maj_axis_deconv_err\" ucd= \"stat.error;phys.angSize.smajAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"min_axis_deconv_err\" ID= \"col_min_axis_deconv_err\" ucd= \"stat.error;phys.angSize.sminAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"pos_ang_deconv_err\" ID= \"col_pos_ang_deconv_err\" ucd= \"stat.error;phys.angSize;pos.posAng;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"deg\" width= \"12\" precision= \"2\" /> <FIELD name= \"chi_squared_fit\" ID= \"col_chi_squared_fit\" ucd= \"stat.fit.chi2\" datatype= \"float\" unit= \"--\" width= \"17\" precision= \"3\" /> <FIELD name= \"rms_fit_gauss\" ID= \"col_rms_fit_gauss\" ucd= \"stat.stdev;stat.fit\" datatype= \"float\" unit= \"mJy/beam\" width= \"15\" precision= \"3\" /> <FIELD name= \"spectral_index\" ID= \"col_spectral_index\" ucd= \"spect.index;em.radio\" datatype= \"float\" unit= \"--\" width= \"15\" precision= \"2\" /> <FIELD name= \"spectral_curvature\" ID= \"col_spectral_curvature\" ucd= \"askap:spect.curvature;em.radio\" datatype= \"float\" unit= \"--\" width= \"19\" precision= \"2\" /> <FIELD name= \"spectral_index_err\" ID= \"col_spectral_index_err\" ucd= \"stat.error;spect.index;em.radio\" datatype= \"float\" unit= \"--\" width= \"15\" precision= \"2\" /> <FIELD name= \"spectral_curvature_err\" ID= \"col_spectral_curvature_err\" ucd= \"stat.error;askap:spect.curvature;em.radio\" datatype= \"float\" unit= \"--\" width= \"19\" precision= \"2\" /> <FIELD name= \"rms_image\" ID= \"col_rms_image\" ucd= \"stat.stdev;phot.flux.density\" datatype= \"float\" unit= \"mJy/beam\" width= \"12\" precision= \"3\" /> <FIELD name= \"has_siblings\" ID= \"col_has_siblings\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"fit_is_estimate\" ID= \"col_fit_is_estimate\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"spectral_index_from_TT\" ID= \"col_spectral_index_from_TT\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"flag_c4\" ID= \"col_flag_c4\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"comment\" ID= \"col_comment\" ucd= \"meta.note\" datatype= \"char\" unit= \"\" arraysize= \"100\" /> <DATA> <TABLEDATA> <TR> <TD> SB25600_island_1 </TD><TD> SB25600_component_1a </TD><TD> J053527-691611 </TD><TD> 05:35:27.9 </TD><TD> -69:16:11 </TD><TD> 83.866441 </TD><TD> -69.269927 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 1367.5 </TD><TD> 780.068 </TD><TD> 0.721 </TD><TD> 843.563 </TD><TD> 1.692 </TD><TD> 11.80 </TD><TD> 8.40 </TD><TD> 176.22 </TD><TD> 0.01 </TD><TD> 0.01 </TD><TD> 0.11 </TD><TD> 3.09 </TD><TD> 1.82 </TD><TD> 56.44 </TD><TD> 0.78 </TD><TD> 2.35 </TD><TD> 1.49 </TD><TD> 596.911 </TD><TD> 1811.003 </TD><TD> -0.93 </TD><TD> -99.00 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 1.038 </TD><TD> 0 </TD><TD> 0 </TD><TD> 1 </TD><TD> 0 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_2 </TD><TD> SB25600_component_2a </TD><TD> J060004-703833 </TD><TD> 06:00:04.9 </TD><TD> -70:38:33 </TD><TD> 90.020608 </TD><TD> -70.642664 </TD><TD> 0.01 </TD><TD> 0.01 </TD><TD> 1367.5 </TD><TD> 438.885 </TD><TD> 1.202 </TD><TD> 494.222 </TD><TD> 2.918 </TD><TD> 11.65 </TD><TD> 8.86 </TD><TD> 9.51 </TD><TD> 0.03 </TD><TD> 0.04 </TD><TD> 0.40 </TD><TD> 5.22 </TD><TD> 0.00 </TD><TD> 58.20 </TD><TD> 0.09 </TD><TD> 0.00 </TD><TD> 0.82 </TD><TD> 5351.118 </TD><TD> 4419.234 </TD><TD> -1.06 </TD><TD> -99.00 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 0.715 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 1 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_2 </TD><TD> SB25600_component_2b </TD><TD> J060006-703854 </TD><TD> 06:00:06.1 </TD><TD> -70:38:54 </TD><TD> 90.025359 </TD><TD> -70.648364 </TD><TD> 0.19 </TD><TD> 0.24 </TD><TD> 1367.5 </TD><TD> 24.719 </TD><TD> 1.326 </TD><TD> 24.048 </TD><TD> 2.986 </TD><TD> 10.66 </TD><TD> 8.37 </TD><TD> 167.45 </TD><TD> 0.59 </TD><TD> 0.80 </TD><TD> 9.41 </TD><TD> 3.00 </TD><TD> 0.00 </TD><TD> -86.64 </TD><TD> 5.43 </TD><TD> 0.00 </TD><TD> 14.18 </TD><TD> 5351.118 </TD><TD> 4419.234 </TD><TD> -99.00 </TD><TD> -99.00 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 0.715 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 1 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_3 </TD><TD> SB25600_component_3a </TD><TD> J061931-681533 </TD><TD> 06:19:31.4 </TD><TD> -68:15:33 </TD><TD> 94.880875 </TD><TD> -68.259404 </TD><TD> 0.49 </TD><TD> 0.65 </TD><TD> 1367.5 </TD><TD> 258.844 </TD><TD> 72.505 </TD><TD> 302.138 </TD><TD> 84.967 </TD><TD> 11.83 </TD><TD> 9.04 </TD><TD> 0.90 </TD><TD> 0.21 </TD><TD> 0.18 </TD><TD> 3.84 </TD><TD> 4.77 </TD><TD> 1.40 </TD><TD> 63.44 </TD><TD> 1.39 </TD><TD> 19.02 </TD><TD> 9.15 </TD><TD> 1284.683 </TD><TD> 2467.498 </TD><TD> -0.74 </TD><TD> -99.00 </TD><TD> 0.01 </TD><TD> 0.00 </TD><TD> 0.277 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 0 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_3 </TD><TD> SB25600_component_3b </TD><TD> J061931-681531 </TD><TD> 06:19:31.0 </TD><TD> -68:15:31 </TD><TD> 94.879037 </TD><TD> -68.258688 </TD><TD> 0.21 </TD><TD> 0.57 </TD><TD> 1367.5 </TD><TD> 153.160 </TD><TD> 84.478 </TD><TD> 157.853 </TD><TD> 87.200 </TD><TD> 11.40 </TD><TD> 8.28 </TD><TD> 6.90 </TD><TD> 0.20 </TD><TD> 0.21 </TD><TD> 1.40 </TD><TD> 4.07 </TD><TD> 0.00 </TD><TD> 55.21 </TD><TD> 1.06 </TD><TD> 0.00 </TD><TD> 5.16 </TD><TD> 1284.683 </TD><TD> 2467.498 </TD><TD> -1.17 </TD><TD> -99.00 </TD><TD> 0.02 </TD><TD> 0.00 </TD><TD> 0.277 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 0 </TD><TD> </TD> </TR> </TABLEDATA> </DATA> </TABLE> </RESOURCE> </VOTABLE> csv id,catalogue_id,first_sbid,other_sbids,project_id,island_id,component_id,component_name,ra_hms_cont,dec_dms_cont,ra_deg_cont,dec_deg_cont,ra_err,dec_err,freq,flux_peak,flux_peak_err,flux_int,flux_int_err,maj_axis,min_axis,pos_ang,maj_axis_err,min_axis_err,pos_ang_err,maj_axis_deconv,min_axis_deconv,maj_axis_deconv_err,pos_ang_deconv,min_axis_deconv_err,pos_ang_deconv_err,chi_squared_fit,rms_fit_gauss,spectral_index,spectral_index_err,spectral_curvature,spectral_curvature_err,rms_image,has_siblings,fit_is_estimate,spectral_index_from_tt,flag_c4,comment,quality_level,released_date 20793260,3790,25600,,18,SB25600_island_2768,SB25600_component_2768a,J055644-690942,05:56:44.5,-69:09:42,89.185558,-69.161889,0.04,0.05,1367.5,0.834,0.009,0.68,0.037,9.63,7.76,165.12,0.23,0.33,3.9,0.0,0.0,0.0,-89.09,0.0,3.48,0.034,52.957,-99.0,0.0,-99.0,0.0,0.166,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z 20793259,3790,25600,,18,SB25600_island_2767,SB25600_component_2767a,J060047-692500,06:00:47.7,-69:25:00,90.198913,-69.416437,0.09,0.37,1367.5,0.889,0.016,2.642,0.2,29.12,9.35,2.08,2.0,0.99,1.35,26.73,4.84,0.03,2.91,6.03,1.5,1.869,234.431,-99.0,0.0,-99.0,0.0,0.175,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z 20793258,3790,25600,,18,SB25600_island_2766,SB25600_component_2766a,J055849-690051,05:58:49.4,-69:00:51,89.705808,-69.014253,0.14,0.24,1367.5,0.885,0.025,1.181,0.191,13.04,9.38,6.04,1.3,1.45,8.83,6.87,3.72,3.2,36.6,14.03,34.31,0.561,193.467,-99.0,0.0,-99.0,0.0,0.177,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z 20793257,3790,25600,,18,SB25600_island_2765,SB25600_component_2765a,J055339-683120,05:53:39.5,-68:31:20,88.414758,-68.522436,0.08,0.1,1367.5,0.906,0.014,0.895,0.063,12.78,7.08,10.0,0.43,0.43,2.33,6.37,0.0,0.54,28.11,0.0,4.65,0.138,99.302,-99.0,0.0,-99.0,0.0,0.173,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z txt # island_id component_id component_name ra_hms_cont dec_dms_cont ra_deg_cont dec_deg_cont ra_err dec_err freq flux_peak flux_peak_err flux_int flux_int_err maj_axis min_axis pos_ang maj_axis_err min_axis_err pos_ang_err maj_axis_deconv min_axis_deconv pos_ang_deconv maj_axis_deconv_err min_axis_deconv_err pos_ang_deconv_err chi_squared_fit rms_fit_gauss spectral_index spectral_curvature spectral_index_err spectral_curvature_err rms_image has_siblings fit_is_estimate spectral_index_from_TT flag_c4 comment # -- -- [deg] [deg] [arcsec] [arcsec] [MHz] [mJy/beam] [mJy/beam] [mJy] [mJy] [arcsec] [arcsec] [deg] [arcsec] [arcsec] [deg] [arcsec] [arcsec] [deg] [arcsec] [arcsec] [deg] -- [mJy/beam] -- -- -- -- [mJy/beam] SB10342_island_1000 SB10342_component_1000a B2337-0423 23:37:09.9 -04:23:13 354.291208 -4.387204 0.03 0.02 -0.0 15.907 0.067 19.273 0.112 17.29 13.32 86.15 0.08 0.01 0.66 7.68 2.86 -35.37 0.08 0.72 0.71 57.119 678.701 -0.79 -99.00 0.00 0.00 0.304 0 0 1 0 SB10342_island_1001 SB10342_component_1001a B0001-0346 00:01:57.6 -03:46:12 0.490036 -3.770075 0.19 0.15 -0.0 12.139 0.425 24.396 0.914 24.14 15.82 61.05 0.45 0.04 2.12 17.96 10.07 50.17 0.06 0.24 4.91 690.545 1639.191 -0.58 -99.00 0.00 0.00 0.339 1 0 1 0 SB10342_island_1001 SB10342_component_1001b B0001-0346 00:01:57.8 -03:46:04 0.491002 -3.767781 1.13 1.35 -0.0 4.044 0.387 10.072 1.053 50.40 9.39 42.19 3.33 0.04 0.73 48.09 0.00 40.49 0.08 0.00 31.48 690.545 1639.191 -0.36 -99.00 0.00 0.00 0.339 1 0 1 0 SB10342_island_1002 SB10342_component_1002a B2333-0141 23:33:29.4 -01:41:04 353.372617 -1.684465 0.13 0.17 -0.0 14.516 0.128 47.232 0.561 35.56 17.39 140.82 0.51 0.02 0.52 33.30 7.00 -35.89 0.02 0.39 3.49 926.099 1383.267 -0.53 -99.00 0.00 0.00 0.459 1 0 1 0 SB10342_island_1002 SB10342_component_1002b B2333-0141 23:33:30.8 -01:41:38 353.378227 -1.693919 0.20 0.26 -0.0 8.051 0.134 25.654 0.550 28.02 21.61 133.92 0.67 0.06 3.37 24.95 14.85 -35.94 0.05 0.13 4.34 926.099 1383.267 -0.21 -99.00 0.00 0.00 0.459 1 0 1 0","title":"Component Catalogues"},{"location":"using/requireddata/#noise-image-file","text":"This is the noise map that is created from the primary image file during source finding by selavy . Must be in the FITS file format. Example CASDA Filename noiseMap.image.i.SB25600.cont.taylor.0.restored.conv.fits","title":"Noise Image File"},{"location":"using/requireddata/#background-image-file","text":"This is the mean map that is created from the primary image file during source finding by selavy . The background image files are only required when source monitoring is enabled in the pipeline run. Must be in the FITS file format. Example CASDA Filename meanMap.image.i.SB25600.cont.taylor.0.restored.conv.fits","title":"Background Image File"},{"location":"using/requireddata/#data-location","text":"Data should be placed in the directories denoted by RAW_IMAGE_DIR and HOME_DATA_DIR in the pipeline configuration. The HOME_DATA_DIR is designed to allow users to upload their own data to their home directory on the system where the pipeline is installed. By default, the HOME_DATA_DIR is set to scan the directory called vast-pipeline-extra-data in the users home area. Refer to the Pipeline Configuration section for more information.","title":"Data Location"},{"location":"using/restorerun/","text":"Restoring a Run \u00b6 This page details the process of restoring a pipeline run to the previous successful version. When images are added to a run, a backup is made of the run before proceeding which can be used to restore the run to the pre-addition version. For example, perhaps the wrong images were added or an error occurred mid-addition that could not be resolved. A pipeline run can only be restored by the creator or an administrator. Admin Tip This process can also be launched via the command line using the restorepiperun command. It is described in the admin section here . Warning: One time use This process can only be used to restore the run once. I.e. it is not possible to restore the run to an even earlier version. Step-by-step Guide \u00b6 In this example, the docs_example_run will be restored to the state before the images were added in the Adding Images to a Run example. 1. Navigate to the Run Detail Page \u00b6 Navigate to the detail page of the run you wish to restore. 2. Select the Restore Run Option \u00b6 Click the Restore Run option at the top-right of the page. This will open the restore confirmation modal. 3. Check the Restore Configuration \u00b6 Shown in the modal is the configuration file of the previous successful run. This can be used to check that the images listed here are those that are expected. Debug level logging can also be turned on using the toggle button. When ready, click the Restore Run button on the modal to submit the restore request. A notification will show to indicate whether the submission was successful. 4. Refresh and Check the Restore Log File \u00b6 While restoring the pipeline run will show a status of Restoring . It is possible to check the progress by looking at the Restore Log File which can be found on the run detail page. The log will not be refreshed automatically and instead the page needs to be manually refreshed. Upon a successful restoration the status will be changed back to Completed .","title":"Restoring a Run"},{"location":"using/restorerun/#restoring-a-run","text":"This page details the process of restoring a pipeline run to the previous successful version. When images are added to a run, a backup is made of the run before proceeding which can be used to restore the run to the pre-addition version. For example, perhaps the wrong images were added or an error occurred mid-addition that could not be resolved. A pipeline run can only be restored by the creator or an administrator. Admin Tip This process can also be launched via the command line using the restorepiperun command. It is described in the admin section here . Warning: One time use This process can only be used to restore the run once. I.e. it is not possible to restore the run to an even earlier version.","title":"Restoring a Run"},{"location":"using/restorerun/#step-by-step-guide","text":"In this example, the docs_example_run will be restored to the state before the images were added in the Adding Images to a Run example.","title":"Step-by-step Guide"},{"location":"using/restorerun/#1-navigate-to-the-run-detail-page","text":"Navigate to the detail page of the run you wish to restore.","title":"1. Navigate to the Run Detail Page"},{"location":"using/restorerun/#2-select-the-restore-run-option","text":"Click the Restore Run option at the top-right of the page. This will open the restore confirmation modal.","title":"2. Select the Restore Run Option"},{"location":"using/restorerun/#3-check-the-restore-configuration","text":"Shown in the modal is the configuration file of the previous successful run. This can be used to check that the images listed here are those that are expected. Debug level logging can also be turned on using the toggle button. When ready, click the Restore Run button on the modal to submit the restore request. A notification will show to indicate whether the submission was successful.","title":"3. Check the Restore Configuration"},{"location":"using/restorerun/#4-refresh-and-check-the-restore-log-file","text":"While restoring the pipeline run will show a status of Restoring . It is possible to check the progress by looking at the Restore Log File which can be found on the run detail page. The log will not be refreshed automatically and instead the page needs to be manually refreshed. Upon a successful restoration the status will be changed back to Completed .","title":"4. Refresh and Check the Restore Log File"},{"location":"using/runconfig/","text":"Run Configuration \u00b6 This page gives an overview of the configuration options available for a pipeline run. Default Configuration File \u00b6 Below is an example of a default config.yaml file. Note that no images or other input files have been provided. The file can be either edited directly or through the editor available on the run detail page. Warning Similarly to Python files, the indentation in the run configuration YAML file is important as it defines nested parameters. config.yaml # This file specifies the pipeline configuration for the current pipeline run. # You should review these settings before processing any images - some of the default # values will probably not be appropriate. run : # Path of the pipeline run path : ... # auto-filled by pipeline initpiperun command # Hide astropy warnings during the run execution. suppress_astropy_warnings : True inputs : # NOTE: all the inputs must match with each other, i.e. the catalogue for the first # input image (inputs.image[0]) must be the first input catalogue (inputs.selavy[0]) # and so on. image : # list input images here, e.g. (note the leading hyphens) # - /path/to/image1.fits # - /path/to/image2.fits selavy : # list input selavy catalogues here, as above with the images noise : # list input noise (rms) images here, as above with the images # Required only if source_monitoring.monitor is true, otherwise optional. If not providing # background images, remove the entire background section below. background : # list input background images here, as above with the images source_monitoring : # Source monitoring can be done both forward and backward in 'time'. # Monitoring backward means re-opening files that were previously processed and can be slow. monitor : True # Minimum SNR ratio a source has to be if it was placed in the area of minimum rms in # the image from which it is to be extracted from. If lower than this value it is skipped min_sigma : 3.0 # Multiplicative scaling factor to the buffer size of the forced photometry from the # image edge edge_buffer_scale : 1.2 # Passed to forced-phot as `cluster_threshold`. See docs for details. If unsure, leave # as default. cluster_threshold : 3.0 # Attempt forced-phot fit even if there are NaN's present in the rms or background maps. allow_nan : False source_association : # basic, advanced, or deruiter method : basic # Maximum source separation allowed during basic and advanced association in arcsec radius : 10.0 # Options that apply only to deruiter association deruiter_radius : 5.68 # unitless deruiter_beamwidth_limit : 1.5 # multiplicative factor # Split input images into sky region groups and run the association on these groups in # parallel. Best used when there are a large number of input images with multiple # non-overlapping patches of the sky. # Not recommended for smaller searches of <= 3 sky regions. parallel : False # If images have been submitted in epoch dictionaries then an attempt will be made by # the pipeline to remove duplicate sources. To do this a crossmatch is made between # catalgoues to match 'the same' measurements from different catalogues. This # parameter governs the distance for which a match is made in arcsec. Default is 2.5 # arcsec which is typically 1 pixel in ASKAP images. epoch_duplicate_radius : 2.5 # arcsec new_sources : # Controls when a source is labelled as a new source. The source in question must meet # the requirement of: min sigma > (source_peak_flux / lowest_previous_image_min_rms) min_sigma : 5.0 measurements : # Source finder used to produce input catalogues. Only selavy is currently supported. source_finder : selavy # Minimum error to apply to all flux measurements. The actual value used will either # be the catalogued value or this value, whichever is greater. This is a fraction, e.g. # 0.05 = 5% error, 0 = no minimum error. flux_fractional_error : 0.0 # Replace the selavy errors with Condon (1997) errors. condon_errors : True # Sometimes the local rms for a source is reported as 0 by selavy. # Choose a value to use for the local rms in these cases in mJy/beam. selavy_local_rms_fill_value : 0.2 # Create 'measurements.arrow' and 'measurement_pairs.arrow' files at the end of # a successful run. write_arrow_files : False # The positional uncertainty of a measurement is in reality the fitting errors and the # astrometric uncertainty of the image/survey/instrument combined in quadrature. # These two parameters are the astrometric uncertainty in RA/Dec and they may be different. ra_uncertainty : 1.0 # arcsec dec_uncertainty : 1.0 # arcsec variability : # Only measurement pairs where the Vs metric exceeds this value are selected for the # aggregate pair metrics that are stored in Source objects. source_aggregate_pair_metrics_min_abs_vs : 4.3 Note Throughout the documentation we use dot-notation to refer to nested parameters, for example inputs.image refers to the list of input images. This page on YAML syntax from the Ansible documentation is a good brief primer on the basics. Configuration Options \u00b6 General Run Options \u00b6 run.path Path to the directory for the pipeline run. This parameter will be automatically filled if the configuration file is generate with the initpiperun management command or if the run was created with the web interface. run.suppress_astropy_warnings Boolean. Astropy warnings are suppressed in the logging output if set to True . Defaults to True . Input Images and Selavy Files \u00b6 Warning: Entry Order The order of the the inputs must be consistent between the different input types. I.e. if image1.fits is the first listed image then image1_selavy.txt must be the first selavy input listed. inputs.image Line entries or epoch headed entries. The full paths to the image FITS files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. config.yaml Normal mode inputs : image : - /full/path/to/image1.fits - /full/path/to/image2.fits - /full/path/to/image3.fits Epoch mode inputs : image : epoch01 : - /full/path/to/image1.fits - /full/path/to/image2.fits epoch02 : - /full/path/to/image3.fits Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10: inputs.selavy Line entries or epoch headed entries. The full paths to the selavy text files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. config.yaml Normal mode inputs : selavy : - /full/path/to/image1_selavy.txt - /full/path/to/image2_selavy.txt - /full/path/to/image3_selavy.txt Epoch mode inputs : selavy : epoch01 : - /full/path/to/image1_selavy.txt - /full/path/to/image2_selavy.txt epoch02 : - /full/path/to/image3_selavy.txt Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10: inputs.noise Line entries or epoch headed entries. The full paths to the image noise (RMS) FITS files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. config.yaml Normal mode inputs : noise : - /full/path/to/image1_rms.fits - /full/path/to/image2_rms.fits - /full/path/to/image3_rms.fits Epoch mode inputs : noise : epoch01 : - /full/path/to/image1_rms.fits - /full/path/to/image2_rms.fits epoch02 : - /full/path/to/image3_rms.fits Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10: inputs.background Line entries or epoch headed entries. The full paths to the image background (mean) FITS files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. The background images are only required to be defined if source_monitoring.monitor is set to True . config.yaml Normal mode inputs : background : - /full/path/to/image1_bkg.fits - /full/path/to/image2_bkg.fits - /full/path/to/image3_bkg.fits Epoch mode inputs : background : epoch01 : - /full/path/to/image1_bkg.fits - /full/path/to/image2_bkg.fits epoch02 : - /full/path/to/image3_bkg.fits Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10: Using glob expressions \u00b6 Instead of providing each input file explicitly, the inputs can be given as glob expressions which are resolved and sorted. Glob expressions must be provided as a mapping with the key glob . Both normal and epoch mode are supported. For example, the image input examples given above can be equivalently specified with the following glob expressions. config.yaml Normal mode inputs : image : glob : /full/path/to/image*.fits Epoch mode inputs : image : epoch01 : glob : /full/path/to/image[12].fits epoch02 : - /full/path/to/image3.fits Multiple glob expressions can also be provided as a list, in which case they are resolved and sorted in the order they are given. For example: config.yaml inputs : image : glob : - /full/path/to/A/image*.fits - /full/path/to/B/image*.fits Note that it is not valid YAML to mix a sequence/list and a mapping/dictionary, meaning that for each input type (or epoch if using epoch mode), the files may be given either as glob expressions or explicit file paths. For example, the following is invalid : Invalid config.yaml inputs : image : # Invalid! Thou shalt not mix sequences and mappings in YAML - /full/path/to/A/image1.fits glob : /full/path/to/B/image*.fits However, an explicit file path is a valid glob expression, so adding explicit paths alongside glob expressions is still possible by simply including the path in a list of glob expressions. For example, the following is valid: config.yaml inputs : image : glob : - /full/path/to/A/image1.fits - /full/path/to/B/image*.fits In the above example, the final resolved image input list would contain the image /full/path/to/A/image1.fits , followed by all files matching image*.fits in /full/path/to/B . Source Monitoring \u00b6 source_monitoring.monitor Boolean. Turns on or off forced extractions for non detections. If set to True then inputs.background must also be defined. Defaults to False . source_monitoring.min_sigma Float. For forced extractions to be performed they must meet a minimum signal-to-noise threshold with respect to the minimum rms value of the respective image. If the proposed forced measurement does not meet the threshold then it is not performed. I.e. \\[ \\frac{f_{peak}}{\\text{rms}_{image, min}} \\geq \\small{\\text{source_monitoring.min_sigma}}\\text{,} \\] where \\(f_{peak}\\) is the initial detection peak flux measurement of the source in question and \\(\\text{rms}_{image, min}\\) is the minimum RMS value of the image where the forced extraction is to take place. Defaults to 3.0 . source_monitoring.edge_buffer_scale Float. Monitor forced extractions are not performed when the location is within 3 beamwidths of the image edge. This parameter scales this distance by the value set, which can help avoid errors when the 3 beamwidth limit is insufficient to avoid extraction failures. Defaults to 1.2. source_monitoring.cluster_threshold Float. A argument directly passed to the forced photometry package used by the pipeline. It defines the multiple of major_axes to use for identifying clusters. Defaults to 3.0. source_monitoring.allow_nan Boolean. A argument directly passed to the forced photometry package used by the pipeline. It defines whether NaN values are allowed to be present in the extraction area in the rms or background maps. True would mean that NaN values are allowed. Defaults to False. Association \u00b6 Tip Refer to the association documentation for full details on the association methods. source_association.method String. Select whether to use the basic , advanced or deruiter association method, entered as a string of the method name. Defaults to \"basic\" . source_association.radius Float. The distance limit to use during basic and advanced association. Unit is arcseconds. Defaults to 10.0 . source_association.deruiter_radius Float. The de Ruiter radius limit to use during deruiter association only. The parameter is unitless. Defaults to 5.68 . source_association.deruiter_beamwidth_limit Float. The beamwidth limit to use during deruiter association only. Multiplicative factor. Defaults to 1.5 . source_association.parallel Boolean. When True , association is performed in parallel on non-overlapping groups of sky regions. Defaults to False . source_association.epoch_duplicate_radius Float. Applies to epoch based association only. Defines the limit at which a duplicate source is identified. Unit is arcseconds. Defaults to 2.5 (commonly one pixel for ASKAP images). New Sources \u00b6 new_sources.min_sigma Float. Defines the limit at which a source is classed as a new source based upon the would-be significance of detections in previous images where no detection was made. i.e. \\[ \\frac{f_{peak}}{\\text{rms}_{image, min}} \\geq \\small{\\text{new_sources.min_sigma}}\\text{,} \\] where \\(f_{peak}\\) is the initial detection peak flux measurement of the source in question and \\(\\text{rms}_{image, min}\\) is the minimum RMS value of the previous image(s) where no detection was made. If the requirement is met in any previous image then the source is flagged as new. Defaults to 5.0 . Measurements \u00b6 measurements.source_finder String. Signifies the format of the source finder text file read by the pipeline. Currently only supports \"selavy\" . Warning Source finding is not performed by the pipeline and must be completed prior to processing. measurements.flux_fractional_error Define a fractional flux error that will be added in quadrature to the extracted sources. Note that this will be reflected in the final source statistics and will not be applied directly to the measurements. Entered as a float between 0 - 1.0 which represents 0 - 100%. Defaults to 0.0 . measurements.condon_errors Boolean. Calculate the Condon errors of the extractions when read in from the source extraction file. If False then the errors directly from the source finder output are used. Recommended to set to True for selavy extractions. Defaults to True . measurements.selavy_local_rms_fill_value Float. Value to substitute for the local_rms parameter in selavy extractions if a 0.0 value is found. Unit is mJy. Defaults to 0.2 . measurements.write_arrow_files Boolean. When True then two arrow format files are produced: measurements.arrow - an arrow file containing all the measurements associated with the run. measurement_pairs.arrow - an arrow file containing the measurement pairs information pre-merged with extra information from the measurements. Producing these files for large runs (200+ images) is recommended for post-processing. Defaults to False . Note The arrow files can optionally be produced after the run has completed. See the Generating Arrow Files page . measurements.ra_uncertainty Float. Defines an uncertainty error to the RA that will be added in quadrature to the existing source extraction error. Used to represent a systematic positional error. Unit is arcseconds. Defaults to 1.0. measurements.dec_uncertainty Float. Defines an uncertainty error to the Dec that will be added in quadrature to the existing source extraction error. Used to represent systematic positional error. Unit is arcseconds. Defaults to 1.0. Variability \u00b6 variability.source_aggregate_pair_metrics_min_abs_vs Float. Defines the minimum \\(V_s\\) two-epoch metric value threshold used to attach the most significant pair value to the source. Defaults to 4.3 .","title":"Run Configuration"},{"location":"using/runconfig/#run-configuration","text":"This page gives an overview of the configuration options available for a pipeline run.","title":"Run Configuration"},{"location":"using/runconfig/#default-configuration-file","text":"Below is an example of a default config.yaml file. Note that no images or other input files have been provided. The file can be either edited directly or through the editor available on the run detail page. Warning Similarly to Python files, the indentation in the run configuration YAML file is important as it defines nested parameters. config.yaml # This file specifies the pipeline configuration for the current pipeline run. # You should review these settings before processing any images - some of the default # values will probably not be appropriate. run : # Path of the pipeline run path : ... # auto-filled by pipeline initpiperun command # Hide astropy warnings during the run execution. suppress_astropy_warnings : True inputs : # NOTE: all the inputs must match with each other, i.e. the catalogue for the first # input image (inputs.image[0]) must be the first input catalogue (inputs.selavy[0]) # and so on. image : # list input images here, e.g. (note the leading hyphens) # - /path/to/image1.fits # - /path/to/image2.fits selavy : # list input selavy catalogues here, as above with the images noise : # list input noise (rms) images here, as above with the images # Required only if source_monitoring.monitor is true, otherwise optional. If not providing # background images, remove the entire background section below. background : # list input background images here, as above with the images source_monitoring : # Source monitoring can be done both forward and backward in 'time'. # Monitoring backward means re-opening files that were previously processed and can be slow. monitor : True # Minimum SNR ratio a source has to be if it was placed in the area of minimum rms in # the image from which it is to be extracted from. If lower than this value it is skipped min_sigma : 3.0 # Multiplicative scaling factor to the buffer size of the forced photometry from the # image edge edge_buffer_scale : 1.2 # Passed to forced-phot as `cluster_threshold`. See docs for details. If unsure, leave # as default. cluster_threshold : 3.0 # Attempt forced-phot fit even if there are NaN's present in the rms or background maps. allow_nan : False source_association : # basic, advanced, or deruiter method : basic # Maximum source separation allowed during basic and advanced association in arcsec radius : 10.0 # Options that apply only to deruiter association deruiter_radius : 5.68 # unitless deruiter_beamwidth_limit : 1.5 # multiplicative factor # Split input images into sky region groups and run the association on these groups in # parallel. Best used when there are a large number of input images with multiple # non-overlapping patches of the sky. # Not recommended for smaller searches of <= 3 sky regions. parallel : False # If images have been submitted in epoch dictionaries then an attempt will be made by # the pipeline to remove duplicate sources. To do this a crossmatch is made between # catalgoues to match 'the same' measurements from different catalogues. This # parameter governs the distance for which a match is made in arcsec. Default is 2.5 # arcsec which is typically 1 pixel in ASKAP images. epoch_duplicate_radius : 2.5 # arcsec new_sources : # Controls when a source is labelled as a new source. The source in question must meet # the requirement of: min sigma > (source_peak_flux / lowest_previous_image_min_rms) min_sigma : 5.0 measurements : # Source finder used to produce input catalogues. Only selavy is currently supported. source_finder : selavy # Minimum error to apply to all flux measurements. The actual value used will either # be the catalogued value or this value, whichever is greater. This is a fraction, e.g. # 0.05 = 5% error, 0 = no minimum error. flux_fractional_error : 0.0 # Replace the selavy errors with Condon (1997) errors. condon_errors : True # Sometimes the local rms for a source is reported as 0 by selavy. # Choose a value to use for the local rms in these cases in mJy/beam. selavy_local_rms_fill_value : 0.2 # Create 'measurements.arrow' and 'measurement_pairs.arrow' files at the end of # a successful run. write_arrow_files : False # The positional uncertainty of a measurement is in reality the fitting errors and the # astrometric uncertainty of the image/survey/instrument combined in quadrature. # These two parameters are the astrometric uncertainty in RA/Dec and they may be different. ra_uncertainty : 1.0 # arcsec dec_uncertainty : 1.0 # arcsec variability : # Only measurement pairs where the Vs metric exceeds this value are selected for the # aggregate pair metrics that are stored in Source objects. source_aggregate_pair_metrics_min_abs_vs : 4.3 Note Throughout the documentation we use dot-notation to refer to nested parameters, for example inputs.image refers to the list of input images. This page on YAML syntax from the Ansible documentation is a good brief primer on the basics.","title":"Default Configuration File"},{"location":"using/runconfig/#configuration-options","text":"","title":"Configuration Options"},{"location":"using/runconfig/#general-run-options","text":"run.path Path to the directory for the pipeline run. This parameter will be automatically filled if the configuration file is generate with the initpiperun management command or if the run was created with the web interface. run.suppress_astropy_warnings Boolean. Astropy warnings are suppressed in the logging output if set to True . Defaults to True .","title":"General Run Options"},{"location":"using/runconfig/#input-images-and-selavy-files","text":"Warning: Entry Order The order of the the inputs must be consistent between the different input types. I.e. if image1.fits is the first listed image then image1_selavy.txt must be the first selavy input listed. inputs.image Line entries or epoch headed entries. The full paths to the image FITS files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. config.yaml Normal mode inputs : image : - /full/path/to/image1.fits - /full/path/to/image2.fits - /full/path/to/image3.fits Epoch mode inputs : image : epoch01 : - /full/path/to/image1.fits - /full/path/to/image2.fits epoch02 : - /full/path/to/image3.fits Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10: inputs.selavy Line entries or epoch headed entries. The full paths to the selavy text files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. config.yaml Normal mode inputs : selavy : - /full/path/to/image1_selavy.txt - /full/path/to/image2_selavy.txt - /full/path/to/image3_selavy.txt Epoch mode inputs : selavy : epoch01 : - /full/path/to/image1_selavy.txt - /full/path/to/image2_selavy.txt epoch02 : - /full/path/to/image3_selavy.txt Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10: inputs.noise Line entries or epoch headed entries. The full paths to the image noise (RMS) FITS files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. config.yaml Normal mode inputs : noise : - /full/path/to/image1_rms.fits - /full/path/to/image2_rms.fits - /full/path/to/image3_rms.fits Epoch mode inputs : noise : epoch01 : - /full/path/to/image1_rms.fits - /full/path/to/image2_rms.fits epoch02 : - /full/path/to/image3_rms.fits Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10: inputs.background Line entries or epoch headed entries. The full paths to the image background (mean) FITS files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. The background images are only required to be defined if source_monitoring.monitor is set to True . config.yaml Normal mode inputs : background : - /full/path/to/image1_bkg.fits - /full/path/to/image2_bkg.fits - /full/path/to/image3_bkg.fits Epoch mode inputs : background : epoch01 : - /full/path/to/image1_bkg.fits - /full/path/to/image2_bkg.fits epoch02 : - /full/path/to/image3_bkg.fits Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10:","title":"Input Images and Selavy Files"},{"location":"using/runconfig/#using-glob-expressions","text":"Instead of providing each input file explicitly, the inputs can be given as glob expressions which are resolved and sorted. Glob expressions must be provided as a mapping with the key glob . Both normal and epoch mode are supported. For example, the image input examples given above can be equivalently specified with the following glob expressions. config.yaml Normal mode inputs : image : glob : /full/path/to/image*.fits Epoch mode inputs : image : epoch01 : glob : /full/path/to/image[12].fits epoch02 : - /full/path/to/image3.fits Multiple glob expressions can also be provided as a list, in which case they are resolved and sorted in the order they are given. For example: config.yaml inputs : image : glob : - /full/path/to/A/image*.fits - /full/path/to/B/image*.fits Note that it is not valid YAML to mix a sequence/list and a mapping/dictionary, meaning that for each input type (or epoch if using epoch mode), the files may be given either as glob expressions or explicit file paths. For example, the following is invalid : Invalid config.yaml inputs : image : # Invalid! Thou shalt not mix sequences and mappings in YAML - /full/path/to/A/image1.fits glob : /full/path/to/B/image*.fits However, an explicit file path is a valid glob expression, so adding explicit paths alongside glob expressions is still possible by simply including the path in a list of glob expressions. For example, the following is valid: config.yaml inputs : image : glob : - /full/path/to/A/image1.fits - /full/path/to/B/image*.fits In the above example, the final resolved image input list would contain the image /full/path/to/A/image1.fits , followed by all files matching image*.fits in /full/path/to/B .","title":"Using glob expressions"},{"location":"using/runconfig/#source-monitoring","text":"source_monitoring.monitor Boolean. Turns on or off forced extractions for non detections. If set to True then inputs.background must also be defined. Defaults to False . source_monitoring.min_sigma Float. For forced extractions to be performed they must meet a minimum signal-to-noise threshold with respect to the minimum rms value of the respective image. If the proposed forced measurement does not meet the threshold then it is not performed. I.e. \\[ \\frac{f_{peak}}{\\text{rms}_{image, min}} \\geq \\small{\\text{source_monitoring.min_sigma}}\\text{,} \\] where \\(f_{peak}\\) is the initial detection peak flux measurement of the source in question and \\(\\text{rms}_{image, min}\\) is the minimum RMS value of the image where the forced extraction is to take place. Defaults to 3.0 . source_monitoring.edge_buffer_scale Float. Monitor forced extractions are not performed when the location is within 3 beamwidths of the image edge. This parameter scales this distance by the value set, which can help avoid errors when the 3 beamwidth limit is insufficient to avoid extraction failures. Defaults to 1.2. source_monitoring.cluster_threshold Float. A argument directly passed to the forced photometry package used by the pipeline. It defines the multiple of major_axes to use for identifying clusters. Defaults to 3.0. source_monitoring.allow_nan Boolean. A argument directly passed to the forced photometry package used by the pipeline. It defines whether NaN values are allowed to be present in the extraction area in the rms or background maps. True would mean that NaN values are allowed. Defaults to False.","title":"Source Monitoring"},{"location":"using/runconfig/#association","text":"Tip Refer to the association documentation for full details on the association methods. source_association.method String. Select whether to use the basic , advanced or deruiter association method, entered as a string of the method name. Defaults to \"basic\" . source_association.radius Float. The distance limit to use during basic and advanced association. Unit is arcseconds. Defaults to 10.0 . source_association.deruiter_radius Float. The de Ruiter radius limit to use during deruiter association only. The parameter is unitless. Defaults to 5.68 . source_association.deruiter_beamwidth_limit Float. The beamwidth limit to use during deruiter association only. Multiplicative factor. Defaults to 1.5 . source_association.parallel Boolean. When True , association is performed in parallel on non-overlapping groups of sky regions. Defaults to False . source_association.epoch_duplicate_radius Float. Applies to epoch based association only. Defines the limit at which a duplicate source is identified. Unit is arcseconds. Defaults to 2.5 (commonly one pixel for ASKAP images).","title":"Association"},{"location":"using/runconfig/#new-sources","text":"new_sources.min_sigma Float. Defines the limit at which a source is classed as a new source based upon the would-be significance of detections in previous images where no detection was made. i.e. \\[ \\frac{f_{peak}}{\\text{rms}_{image, min}} \\geq \\small{\\text{new_sources.min_sigma}}\\text{,} \\] where \\(f_{peak}\\) is the initial detection peak flux measurement of the source in question and \\(\\text{rms}_{image, min}\\) is the minimum RMS value of the previous image(s) where no detection was made. If the requirement is met in any previous image then the source is flagged as new. Defaults to 5.0 .","title":"New Sources"},{"location":"using/runconfig/#measurements","text":"measurements.source_finder String. Signifies the format of the source finder text file read by the pipeline. Currently only supports \"selavy\" . Warning Source finding is not performed by the pipeline and must be completed prior to processing. measurements.flux_fractional_error Define a fractional flux error that will be added in quadrature to the extracted sources. Note that this will be reflected in the final source statistics and will not be applied directly to the measurements. Entered as a float between 0 - 1.0 which represents 0 - 100%. Defaults to 0.0 . measurements.condon_errors Boolean. Calculate the Condon errors of the extractions when read in from the source extraction file. If False then the errors directly from the source finder output are used. Recommended to set to True for selavy extractions. Defaults to True . measurements.selavy_local_rms_fill_value Float. Value to substitute for the local_rms parameter in selavy extractions if a 0.0 value is found. Unit is mJy. Defaults to 0.2 . measurements.write_arrow_files Boolean. When True then two arrow format files are produced: measurements.arrow - an arrow file containing all the measurements associated with the run. measurement_pairs.arrow - an arrow file containing the measurement pairs information pre-merged with extra information from the measurements. Producing these files for large runs (200+ images) is recommended for post-processing. Defaults to False . Note The arrow files can optionally be produced after the run has completed. See the Generating Arrow Files page . measurements.ra_uncertainty Float. Defines an uncertainty error to the RA that will be added in quadrature to the existing source extraction error. Used to represent a systematic positional error. Unit is arcseconds. Defaults to 1.0. measurements.dec_uncertainty Float. Defines an uncertainty error to the Dec that will be added in quadrature to the existing source extraction error. Used to represent systematic positional error. Unit is arcseconds. Defaults to 1.0.","title":"Measurements"},{"location":"using/runconfig/#variability","text":"variability.source_aggregate_pair_metrics_min_abs_vs Float. Defines the minimum \\(V_s\\) two-epoch metric value threshold used to attach the most significant pair value to the source. Defaults to 4.3 .","title":"Variability"}]}