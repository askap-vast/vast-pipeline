{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , with an added List of PRs section and links to the relevant PRs on the individual updates. This project adheres to Semantic Versioning . Unreleased \u00b6 Added \u00b6 Added documentation versioning #627 . Added a ZTF cone search button on the source detail page #626 . Added the 0-based index of each measurement to the image cutout card headers #625 . Added Bokeh hover tooltip to measurement pair graph to display pair metrics #625 . Added new VAST surveys (13-21) to the Aladin Lite panel #622 . Added eta-V plot analysis page along with documentation #586 . Added thumbnails to light curve tooltips #586 . Added logfile dropdown selection to run detail page #595 . Added datetime stamps to all log files #595 . Added new log files for arrow file creation and restore run and added to run detail page #580 . Added restore run test #580 . Added new run status of DEL , Deleting #580 . Added documentation pages on new action buttons #580 . Added UI action buttons to run-detail page to allow arrow file generation, deletion and restoration #580 . Added try-except error capture on pre-run checks to correctly assign pipeline run as failed if an error occurs #576 . Added support for ingesting Selavy catalogues in VOTable (XML) and CSV format #565 Added new commands: initingest and ingestimages #544 Added documentation on the data required to run the pipeline #572 . Added support for ingesting Selavy catalogues in VOTable (XML) and CSV format #565 . Added new commands: initingest and ingestimages #544 . Added TNS cone search to the external search results on the source detail page #557 . Added HOME_DATA_ROOT to the pipeline settings to override the OS default home directory location #559 . Added processing spinner to source query table #551 . Added site_url to the mkdocs config so static asset URLs have the correct base URL #543 . Added basic linter to CI/CD #546 Changed \u00b6 Fixed markupsafe to v2.0.1 for Jinja2 compatibility #627 . Bumped all documentation dependancies to latest versions #627 . Changed GitHub workflows for new documentation versioning #627 . Bumped Jinja2 to 3.0.3 to fix a Markupsafe error caused by a removed function #634 . Dependancies updated using npm audit fix (non-breaking) #620 . Refactored adding source to favourites button to use ajax to avoid page reload #614 . Bumped test python versions to 3.7.12, 3.8.12 and 3.9.10 #586 . Bumped various dependencies using a fresh poetry.lock file #586 . Bumped bokeh packages to 2.3.3 #586 . Django-Q config variable max_attempts is configurable in the .env file #595 . Replaced models.MeasurementPair model with a dataclass #590 . Django-Q config variables timeout and retry are configurable in the .env file #589 . Changed restore run command to only allow one run as input #580 . Changed existing documentation pages to reflect new buttons #580 . Moved creation of output backup files to occur before the config check #576 . Association test data updated with d2d fix #574 . Removed the timezone from the Timestamps being written to the the arrow file as this causes problems with vaex #571 . Reduced the memory footprint for computing the ideal source coverages by sky regions #555 . Gulp will only read webinterface/.env if the required vars are undefined in the current environment #548 . Fixed \u00b6 Fixed docs issue that stopped serializers and views being shown in the code reference #627 . Fixed broken links to external search results from NED by URI encoding source names #633 . Fixed a regression from pandas==1.4.0 that caused empty groups to be passed to an apply function #632 . Fixed source names to be IAU compliant #618 . Fixed broken NED links for coordinates with many decimal places #623 . Added an error handler for the external source queries (e.g. SIMBAD) #616 . Stopped JS9 from changing the page titles #597 . Fixed regression issues with pandas 1.4 #586 . Fixed config being copied before run was confirmed to actually go ahead for existing runs #595 . Fixed forced measurements being removed from associations during the restore run process #600 . Fixed measurement FITS cutout bug #588 . Fixed removal of image and sky region objects when a run is deleted #585 . Fixed testing pandas equal deprecation warning #580 . Fixed restore run relations issue #580 . Fixed logic for full re-run requirement when UI run is being re-run from an error status #576 . Fixed d2d not being carried through the advanced association process #574 . Fixed old dictionary references in the documentation run config page #572 . Fixed a regression from pandas=1.3.0 that caused non-numeric columns to be dropped after a groupby sum operation #567 . Fixed permission error for regular users when trying to launch an initialised run #563 . Fixed outdated installation link in README #543 . Removed \u00b6 Removed the unique constraint on models.Measurement.name #583 . List of PRs \u00b6 #627 : dep, docs: Documentation update and versioning. #633 : fix: URI encode NED object names. #632 : fix: skip empty groups in new sources groupby-apply. #634 : dep: bump Jinja2 to v3. #626 : feat: Add a ZTF cone search link. #618 : fix: Produce IAU compliant source names. #625 : feat: Pair metrics hover tooltip. #620 : dep: Non-breaking npm audit fix update. #622 : feat: Updated Aladin VAST surveys. #623 : fix: fixed NED links. #616 : fix: added error handling to external queries. #614 : feat: Refactored add to favourites button to avoid refresh. #597 : fix: Update detail page titles. #586 : feat, dep, doc: Add an eta-v analysis page for the source query. #595 : fix: Add date and time stamp to log files. #600 : fix: Fixed restore run forced measurements associations. #590 : fix: Remove MeasurementPair model. #589 : fix: expose django-q timeout and retry to env vars. #588 : fix: change cutout endpoint to use measurement ID. #585 : fix: Clean up m2m related objects when deleting a run. #583 : fix: remove unique constraint from Measurement.name. #580 : feat, fix, doc: Added UI run action buttons. #576 : fix: Fixed UI re-run from errored status. #574 : fix: Fixed d2d assignment in advanced association. #572 : doc: Added required data page to documentation. #571 : fix: Removed timezone from measurements arrow file time column #565 : feat: added support for reading selavy VOTables and CSVs. #567 : fix: fixed pandas=1.3.0 groupby sum regression. #563 : fix: fixed launch run user permission bug. #544 : feat: new command to ingest images without running the full pipeline. #557 : feat: Add TNS external search for sources. #559 : feat: added HOME_DATA_ROOT setting. #555 : fix: compute ideal source coverage with astropy xmatch. #551 : feat: added processing spinner to source query table. #550 : fix: missing changelog entry #548 : fix: only read .env if required vars are undefined. #546 : feat, fix: remove unused imports, and added basic linter during CI/CD. #543 : fix, doc: Fix README link and documentation 404 assets. 1.0.0 (2021-05-21) \u00b6 Added \u00b6 When searching by source names, any \"VAST\" prefix on the name will be silently removed to make searching for published VAST sources easier #536 . Added acknowledgements and help section to docs #535 . Added vast_pipeline/_version.py to store the current software version and updated release documentation #532 . Added created and last updated dates to doc pages using mkdocs-git-revision-date-localized-plugin #514 . Added support for glob expressions when specifying input files in the run config file #504 Added DEFAULT_AUTO_FIELD to settings.py to silence Django 3.2 warnings #507 Added lightgallery support for all images in the documentation #494 . Added new entries in the documentation contributing section #494 . Added new entries in the documentation FAQ section #491 . Added new home page for documentation #491 . Added dark mode switch on documentation #487 . Added .env file information to documentation #487 . Added further epoch based association information to documentation page #487 . Added script to auto-generate code reference documentation pages #480 . Added code reference section to documentation #480 . Added new pages and sections to documentation #471 Added requirements/environment.yml so make it easier for Miniconda users to get the non-Python dependencies #472 . Added pyproject.toml and poetry.lock #472 . Added init-tools/init-db.py #472 . Added image add mode run restore command 'restorepiperun' #463 Added documentation folder and files for mkdocs and CI #433 Added add image to existing run feature #443 Added networkx to base reqiurements #460 . Added CI/CD workflow to run tests on pull requests #446 Added basic regression tests #425 Added image length validation for config #425 Changed \u00b6 Changed source naming convention to Jhhmmss.s(+/-)ddmmss to match VAST-P1 paper (Murphy, et al. 2021) convention #536 Updated npm packages to resolve dependabot security alert #533 . Updated homepage text to reflect new features and documentation #534 . Changed layout of source detail page #526 . Updated mkdocs-material to 7.1.4 for native creation date support #518 . Updated developing docs to specify the main development branch as dev instead of master #521 . Updated tests to account for relation fix #510 . All file examples in docs are now enclosed in an example admonition #494 . Further changes to layout of documentation #494 . Changed arrow file generation from vaex to pyarrow #503 . Changed layout of documentation to use tabs #491 . Dependabot: Bump y18n from 3.2.1 to 3.2.2 #482 . Replaced run config .py format with .yaml #483 . Changed docs VAST logo to icon format to avoid stretched appearence #487 . Bumped Browsersync from 2.26.13 to 2.26.14 #481 . Dependabot: Bump prismjs from 1.22.0 to 1.23.0 #469 . Changed non-google format docstrings to google format #480 . Changed some documentation layout and updated content #471 . Changed the vaex dependency to vaex-arrow #472 . Set CREATE_MEASUREMENTS_ARROW_FILES = True in the basic association test config #472 . Bumped minimum Python version to 3.7.1 #472 . Replaced npm package gulp-sass with @mr-hope/gulp-sass , a fork which drops the dependency on the deprecated node-sass which is difficult to install #472 . Changed the installation documentation to instruct users to use a PostgreSQL Docker image with Q3C already installed #472 . Changed 'cmd' flag in run pipeline to 'cli' #466 . Changed CONTRIBUTING.md and README.md #433 Changed forced extraction name suffix to run id rather than datetime #443 Changed tests to run on smaller cutouts #443 Changed particles style on login page #459 . Dependabot: Bump ini from 1.3.5 to 1.3.8 #436 Fixed \u00b6 Fixed the broken link to the image detail page on measurement detail pages #528 . Fixed simbad and ned external search results table nan values #523 . Fixed inaccurate total results reported by some paginators #517 . Removed excess whitespace from coordinates that get copied to the clipboard #515 Fixed rogue relations being created during one-to-many functions #510 . Fixed JS9 regions so that the selected source components are always on top #508 Fixed docstring in config.py #494 . Fixed arrow files being generated via the website #503 . Fixed a bug that returned all sources when performing a cone search where one of the coords = 0 #501 Fixed the missing hover tool for lightcurve plots of non-variable sources #493 Fixed the default Dask multiprocessing context to \"fork\" #472 . Fixed Selavy catalogue ingest to discard the unit row before reading the data #473 . Fixed initial job processing from the UI #466 . Fixed links in README.md #464 . Fixed basic association new sources created through relations #443 Fixed tests running pipeline multiple times #443 Fixed particles canvas sizing on login page #459 . Fixed breadcrumb new line on small resolutitons #459 . Fixed config files in tests #430 Fixed sources table on measurement detail page #429 . Fixed missing meta columns in parallel association #427 . Removed \u00b6 Removed SURVEYS_WORKING_DIR from settings and env file #538 . Removed default_survey from run configuration file #538 . Removed importsurvey command and catalogue.py #538 . Removed SurveySource, Survey and SurveySourceQuerySet models #538 . Removed email and Slack links from docs footer #535 . Removed bootstrap as the required version is bundled with startbootstrap-sb-admin-2 #533 . Removed docs/readme.md softlink as it is no longer used #494 . Removed vaex-arrow from the dependancies #503 . Removed requirements/*.txt files. Development dependency management moved to Poetry #472 . Removed init-tools/init-db.sh #472 . Removed INSTALL.md , PROFILE.md and static/README.md #433 Removed aplpy from base requirements #460 . List of PRs \u00b6 #538 feat: Removed survey source models, commands and references. #536 feat: changed source naming convention. #535 doc: added help and acknowledgement doc page. #534 feat: Update homepage text. #532 feat, doc: Versioning. #533 dep: updated npm deps; removed bootstrap. #528 fix: fixed broken image detail link. #526 feat: Updated source detail page layout. #518 dep: Updated mkdocs-material for native creation date support. #523 fix: Fixed external search results table nan values. #521 doc: update doc related to default dev branch. #517 fix: pin djangorestframework-datatables to 0.5.1. #515 fix: remove linebreaks from coordinates. #514 dep: Added created and updated dates to doc pages. #510 fix: Fix rogue relations. #508 fix: Draw selected source components on top in JS9. #504 feat: Add glob expression support to yaml run config. #507 fix: set default auto field model. #494 doc, dep: Docs: Added lightgallery support, layout update, minor fixes and additions. #503 fix, dep: Change arrow file generation from vaex to pyarrow. #501 fix: fix broken cone search when coord = 0 #491 doc: Updated the docs layout, home page and FAQs. #493 fix: Fix bokeh hover tool for lightcurve plots. #482 dep: Bump y18n from 3.2.1 to 3.2.2. #483 feat: replace run config .py files with .yaml. #487 doc: Minor documentation improvements. #481 dep: Bump Browsersync from 2.26.13 to 2.26.14. #469 dep: Bump prismjs from 1.22.0 to 1.23.0. #480 feat: Code reference documentation update. #471 feat: Documentation update. #472 feat: Simplify install. #473 fix: discard the selavy unit row before reading. #466 fix: Fixed initial job processing from the UI. #463 feat: Added image add mode run restore command. #433 doc: add documentation GitHub pages website with CI. #443 feat, fix: Adds the ability to add images to an existing run. #460 dep: Removed aplpy from base requirements. #446 feat: CI/CD workflow. #459 fix: Fix particles and breadcrumb issues on mobile. #436 dep: Bump ini from 1.3.5 to 1.3.8. #430 fix: Test config files. #425 feat: Basic regression tests. #429 fix: Fixed sources table on measurement detail page. #427 fix: Fixed missing meta columns in parallel association. 0.2.0 (2020-11-30) \u00b6 Added \u00b6 Added a check in the UI running that the job is not already running or queued #421 . Added the deletion of all parquet and arrow files upon a re-run #421 . Added source selection by name or ID on source query page #401 . Added test cases #412 Added askap-vast/forced_phot to pip requirements #408 . Added pipeline configuration parameter, SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS , to filter measurement pairs before calculating aggregate metrics #407 . Added custom 404.html and 500.html templates for error pages #415 Added ability to export measurement_pairs.parqyet as an arrow file #393 . Added new fields to detail pages and source and measurement tables #406 . Added new fields to source query page (island flux ratio, min and max fluxes) #406 . Added min, max flux values to sources and agg min island flux ratio field #406 . Added island flux ratio column to measurements, component flux divided by total island flux (peak and int) #406 . Added a maximum number of images for runs through the UI #404 . Added the ability to run a pipeline run through the UI #404 . Added Queued status to the list of pipeline run statuses #404 . Added the dependancy django-q that enables scheduled tasks to be processed #404 . Added source tagging #396 . Added link to measurement table from the lightcurve to source detail page #387 . Added 'epoch based' parameter to pipeline run detail page #387 . Adds basic commenting functionality for sources, measurements, images, and runs #368 . Custom CSS now processed with Sass: Bootstrap and sb-admin-2 theme are compiled into a single stylesheet #370 . Added vast_pipeline/pipeline/generators.py which contains generator functions #382 . Range and NaN check on new source analysis to match forced extraction #374 . Added the ability for the pipeline to read in groups of images which are defined as a single epoch #277 . Added the ability of the pipeline to remove duplicated measurements from an epoch #277 . Added option to control separation measurements which are defined as a duplicate #277 . Added the ability of the pipeline to separate images to associate into unique sky region groups #277 . Added option to perform assocication of separate sky region groups in parallel #277 . Added new options to webinterface pipeline run creation #277 . Added epoch_based column run model #277 . Added links to tables and postage stamps on source detail page #379 . Updates image background_path from current run when not originally provided #377 . Added csv export button to datatables on webinterface #363 . Added support for Excel export button to datatables on webinterface (waiting on datatables buttons fix) #363 . Added column visibility button to datatables on webinterface #363 . Added dependancy datatables-buttons 1.6.4 #363 . Added dependancy jszip (required for Excel export) #363 . Adds n_selavy_measurements and n_forced_measurements to run model #362 . Adds steps to populate new measurement count fields in pipeline run #362 . Source order from the query is preserved on source detail view #364 . Setting HOME_DATA_DIR to specify a directory relative to the user's home directory to scan for FITS and text files to use in a Run initialised with the UI #361 . Adds a node graph to accompany the lightcurve that shows which measurement pairs exceed the default variability metric thresholds ( Vs >= 4.3 , |m| >= 0.26 ) #305 . Adds the MeasurementPair model to store two variability metrics for each flux type: Vs, the t-statistic; and m, the modulation index. The maximum of these metrics are also added to the Source model for joinless queries. These metrics are calculated during the pipeline run #305 . Adds radio buttons to change the lightcurve data points between peak and integrated fluxes #305 . Fills out information on all webinterface detail pages #345 . Adds frequency information the measurements and images webinterface tables. #345 . Adds celestial plot and tables to webinterface pipeline detail page #345 . Adds useful links to webinterface navbar #345 . Adds tool tips to webinterface source query #345 . Adds hash reading to webinterface source query to allow filling from URL hash parameters #345 . Add links to number cards on webinterface #345 . Added home icon on hover on webinterface #345 . Added copy-to-clipboard functionality on coordinates on webinterface #345 . Changed \u00b6 Renamed 'alert-wrapper' container to 'toast-wrapper' #419 . Changed alerts to use the Bootstrap toasts system #419 . Bumped some npm package versions to address dependabot security alerts #411 . Images table on pipeline run detail page changed to order by datetime by default #417 . Changed config argument CREATE_MEASUREMENTS_ARROW_FILE -> CREATE_MEASUREMENTS_ARROW_FILES #393 . Naming of average flux query fields to account for other min max flux fields #406 . Expanded README.md to include DjangoQ and UI job scheduling information #404 . Shifted alerts location to the top right #404 . Log file card now expanded by default on pipeline run detail page #404 . Changed user comments on source detail pages to incorporate tagging feature #396 . Updated RACS HiPS URL in Aladin #399 . Changed home page changelog space to welcome/help messages #387 . The comment field in the Run model has been renamed to description . A comment many-to-many relationship was added to permit user comments on Run instances #368 . Moved sb-admin-2 Bootstrap theme static assets to NPM package dependency #370 . Refactored bulk uploading to use iterable generator objects #382 . Updated validation of config file to check that all options are present and valid #373 . Rewritten relation functions to improve speed #307 . Minor changes to association to increase speed #307 . Changes to decrease memory usage during the calculation of the ideal coverage dataframe #307 . Updated the get_src_skyregion_merged_df logic to account for epochs #277 . Updated the job creation modal layout #277 . Bumped datatables-buttons to 1.6.5 and enabled excel export buttton #380 . Bumped datatables to 1.10.22 #363 . Changed dom layout on datatables #363 . Changed external results table pagination buttons on source detail webinterface page pagination to include less numbers to avoid overlap #363 . Changes measurement counts view on website to use new model parameters #362 . Lightcurve plot now generated using Bokeh #305 . Multiple changes to webinterface page layouts #345 . Changes source names to the format ASKAP_hhmmss.ss(+/-)ddmmss.ss #345 . Simplified webinterface navbar #345 . Excludes sources and pipeline runs from being listed in the source query page that are not complete on the webinterface #345 . Clarifies number of measurements on webinterface detail pages #345 . Changed N.A. labels to N/A on the webinterface #345 . Fixed \u00b6 Fixed pipeline run DB loading in command line runpipeline command #401 . Fixed nodejs version #412 Fixed npm start failure #412 All queries using the 2-epoch metric Vs now operate on abs(Vs) . The original Vs stored in MeasurementPair objects is still signed #407 . Changed aggregate 2-epoch metric calculation for Source objects to ensure they come from the same pair #407 . Fixed new sources rms measurement returns when no measurements are valid #417 . Fixed measuring rms values from selavy created NAXIS=3 FITS images #417 . Fixed rms value calculation in non-cluster forced extractions #402 . Increase request limit for gunicorn #398 . Fixed max source Vs metric to being an absolute value #391 . Fixed misalignment of lightcurve card header text and the flux type radio buttons #386 . Fixes incorrently named GitHub social-auth settings variable that prevented users from logging in with GitHub #372 . Fixes webinterface navbar overspill at small sizes #345 . Fixes webinterface favourite source table #345 . Removed \u00b6 Removed/Disabled obsolete test cases #412 Removed vast_pipeline/pipeline/forced_phot.py #408 . Removed 'selavy' from homepage measurements count label #391 . Removed leftover pipeline/plots.py file #391 . Removed static/css/pipeline.css , this file is now produced by compiling the Sass ( scss/**/*.scss ) files with Gulp #370 . Removed any storage of meas_dj_obj or src_dj_obj in the pipeline #382 . Removed static/vendor/chart-js package #305 . Removed static/css/collapse-box.css , content moved to pipeline.css #345 . List of PRs \u00b6 #421 feat: Delete output files on re-run & UI run check. #401 feat: Added source selection by name or id to query page. #412 feat: added some unit tests. #419 feat: Update alerts to use toasts. #408 feat: use forced_phot dependency instead of copied code. #407 fix, model: modified 2-epoch metric calculation. #411 fix: updated npm deps to fix security vulnerabilities. #415 feat: Added custom 404 and 500 templates. #393 feat: Added measurement_pairs arrow export. #406 feat, model: Added island flux ratio columns. #402 fix: Fixed rms value calculation in non-cluster forced extractions. #404 feat, dep, model: Completed schedule pipe run. #396 feat: added source tagging. #398 fix: gunicorn request limit #399 fix: Updated RACS HiPS path. #391 fix: Vs metric fix and removed pipeline/plots.py. #387 feat: Minor website updates. #386 fix: fix lightcurve header floats. #368 feat: vast-candidates merger: Add user commenting #370 feat: moved sb-admin-2 assets to dependencies. #382 feat: Refactored bulk uploading of objects. #374 feat, fix: Bring new source checks inline with forced extraction. #373 fix: Check all options are valid and present in validate_cfg. #307 feat: Improve relation functions and general association speed ups. #277 feat,model: Parallel and epoch based association. #380 feat, dep: Enable Excel export button. #379 feat: Add links to source detail template. #377 fix: Update image bkg path when not originally provided. #363 feat, dep: Add export and column visibility buttons to tables. #362 feat, model: Added number of measurements to Run DB model. #364 feat: preserve source query order on detail view. #361 feat, fix: restrict home dir scan to specified directory. #372 fix: fix social auth scope setting name. #305 feat: 2 epoch metrics #345 feat, fix: Website improvements. 0.1.0 (2020-09-27) \u00b6 First release of the Vast Pipeline. This was able to process 707 images (EPOCH01 to EPOCH11x) on a machine with 64 GB of RAM. List of PRs \u00b6 #347 feat: Towards first release #354 fix, model: Updated Band model fields to floats #346 fix: fix JS9 overflow in measurement detail view #349 dep: Bump lodash from 4.17.15 to 4.17.20 #348 dep: Bump django from 3.0.5 to 3.0.7 in /requirements #344 fix: fixed aladin init for all pages #340 break: rename pipeline folder to vast_pipeline #342 fix: Hotfix - fixed parquet path on job detail view #336 feat: Simbad/NED async cone search #284 fix: Update Aladin surveys with RACS and VAST #333 feat: auth to GitHub org, add logging and docstring #325 fix, feat: fix forced extraction using Dask bags backend #334 doc: better migration management explanation #332 fix: added clean to build task, removed commented lines #322 fix, model: add unique to image name, remove timestamp from image folder #321 feat: added css and js sourcemaps #314 feat: query form redesign, sesame resolver, coord validator #318 feat: Suppress astropy warnings #317 fix: Forced photometry fixes for #298 and #312 #316 fix: fix migration file 0001_initial.py #310 fix: Fix run detail number of measurements display #309 fix: Added JS9 overlay filters and changed JS9 overlay behaviour on sources and measurements #303 fix: Fix write config feedback and validation #306 feat: Add config validation checks #302 fix: Fix RA correction for d3 celestial #300 fix: increase line limit for gunicorn server #299 fix: fix admin \"view site\" redirect #294 fix: Make lightcurves start at zero #268 feat: Production set up with static files and command #291 fix: Bug fix for forced_photom cluster allow_nan #289 fix: Fix broken UI run creation #287 fix: Fix forced measurement parquet files write #286 fix: compile JS9 without helper option #285 fix: Fix removing forced parquet and clear images from piperun","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , with an added List of PRs section and links to the relevant PRs on the individual updates. This project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"Unreleased"},{"location":"changelog/#added","text":"Added documentation versioning #627 . Added a ZTF cone search button on the source detail page #626 . Added the 0-based index of each measurement to the image cutout card headers #625 . Added Bokeh hover tooltip to measurement pair graph to display pair metrics #625 . Added new VAST surveys (13-21) to the Aladin Lite panel #622 . Added eta-V plot analysis page along with documentation #586 . Added thumbnails to light curve tooltips #586 . Added logfile dropdown selection to run detail page #595 . Added datetime stamps to all log files #595 . Added new log files for arrow file creation and restore run and added to run detail page #580 . Added restore run test #580 . Added new run status of DEL , Deleting #580 . Added documentation pages on new action buttons #580 . Added UI action buttons to run-detail page to allow arrow file generation, deletion and restoration #580 . Added try-except error capture on pre-run checks to correctly assign pipeline run as failed if an error occurs #576 . Added support for ingesting Selavy catalogues in VOTable (XML) and CSV format #565 Added new commands: initingest and ingestimages #544 Added documentation on the data required to run the pipeline #572 . Added support for ingesting Selavy catalogues in VOTable (XML) and CSV format #565 . Added new commands: initingest and ingestimages #544 . Added TNS cone search to the external search results on the source detail page #557 . Added HOME_DATA_ROOT to the pipeline settings to override the OS default home directory location #559 . Added processing spinner to source query table #551 . Added site_url to the mkdocs config so static asset URLs have the correct base URL #543 . Added basic linter to CI/CD #546","title":"Added"},{"location":"changelog/#changed","text":"Fixed markupsafe to v2.0.1 for Jinja2 compatibility #627 . Bumped all documentation dependancies to latest versions #627 . Changed GitHub workflows for new documentation versioning #627 . Bumped Jinja2 to 3.0.3 to fix a Markupsafe error caused by a removed function #634 . Dependancies updated using npm audit fix (non-breaking) #620 . Refactored adding source to favourites button to use ajax to avoid page reload #614 . Bumped test python versions to 3.7.12, 3.8.12 and 3.9.10 #586 . Bumped various dependencies using a fresh poetry.lock file #586 . Bumped bokeh packages to 2.3.3 #586 . Django-Q config variable max_attempts is configurable in the .env file #595 . Replaced models.MeasurementPair model with a dataclass #590 . Django-Q config variables timeout and retry are configurable in the .env file #589 . Changed restore run command to only allow one run as input #580 . Changed existing documentation pages to reflect new buttons #580 . Moved creation of output backup files to occur before the config check #576 . Association test data updated with d2d fix #574 . Removed the timezone from the Timestamps being written to the the arrow file as this causes problems with vaex #571 . Reduced the memory footprint for computing the ideal source coverages by sky regions #555 . Gulp will only read webinterface/.env if the required vars are undefined in the current environment #548 .","title":"Changed"},{"location":"changelog/#fixed","text":"Fixed docs issue that stopped serializers and views being shown in the code reference #627 . Fixed broken links to external search results from NED by URI encoding source names #633 . Fixed a regression from pandas==1.4.0 that caused empty groups to be passed to an apply function #632 . Fixed source names to be IAU compliant #618 . Fixed broken NED links for coordinates with many decimal places #623 . Added an error handler for the external source queries (e.g. SIMBAD) #616 . Stopped JS9 from changing the page titles #597 . Fixed regression issues with pandas 1.4 #586 . Fixed config being copied before run was confirmed to actually go ahead for existing runs #595 . Fixed forced measurements being removed from associations during the restore run process #600 . Fixed measurement FITS cutout bug #588 . Fixed removal of image and sky region objects when a run is deleted #585 . Fixed testing pandas equal deprecation warning #580 . Fixed restore run relations issue #580 . Fixed logic for full re-run requirement when UI run is being re-run from an error status #576 . Fixed d2d not being carried through the advanced association process #574 . Fixed old dictionary references in the documentation run config page #572 . Fixed a regression from pandas=1.3.0 that caused non-numeric columns to be dropped after a groupby sum operation #567 . Fixed permission error for regular users when trying to launch an initialised run #563 . Fixed outdated installation link in README #543 .","title":"Fixed"},{"location":"changelog/#removed","text":"Removed the unique constraint on models.Measurement.name #583 .","title":"Removed"},{"location":"changelog/#list-of-prs","text":"#627 : dep, docs: Documentation update and versioning. #633 : fix: URI encode NED object names. #632 : fix: skip empty groups in new sources groupby-apply. #634 : dep: bump Jinja2 to v3. #626 : feat: Add a ZTF cone search link. #618 : fix: Produce IAU compliant source names. #625 : feat: Pair metrics hover tooltip. #620 : dep: Non-breaking npm audit fix update. #622 : feat: Updated Aladin VAST surveys. #623 : fix: fixed NED links. #616 : fix: added error handling to external queries. #614 : feat: Refactored add to favourites button to avoid refresh. #597 : fix: Update detail page titles. #586 : feat, dep, doc: Add an eta-v analysis page for the source query. #595 : fix: Add date and time stamp to log files. #600 : fix: Fixed restore run forced measurements associations. #590 : fix: Remove MeasurementPair model. #589 : fix: expose django-q timeout and retry to env vars. #588 : fix: change cutout endpoint to use measurement ID. #585 : fix: Clean up m2m related objects when deleting a run. #583 : fix: remove unique constraint from Measurement.name. #580 : feat, fix, doc: Added UI run action buttons. #576 : fix: Fixed UI re-run from errored status. #574 : fix: Fixed d2d assignment in advanced association. #572 : doc: Added required data page to documentation. #571 : fix: Removed timezone from measurements arrow file time column #565 : feat: added support for reading selavy VOTables and CSVs. #567 : fix: fixed pandas=1.3.0 groupby sum regression. #563 : fix: fixed launch run user permission bug. #544 : feat: new command to ingest images without running the full pipeline. #557 : feat: Add TNS external search for sources. #559 : feat: added HOME_DATA_ROOT setting. #555 : fix: compute ideal source coverage with astropy xmatch. #551 : feat: added processing spinner to source query table. #550 : fix: missing changelog entry #548 : fix: only read .env if required vars are undefined. #546 : feat, fix: remove unused imports, and added basic linter during CI/CD. #543 : fix, doc: Fix README link and documentation 404 assets.","title":"List of PRs"},{"location":"changelog/#100-2021-05-21","text":"","title":"1.0.0 (2021-05-21)"},{"location":"changelog/#added_1","text":"When searching by source names, any \"VAST\" prefix on the name will be silently removed to make searching for published VAST sources easier #536 . Added acknowledgements and help section to docs #535 . Added vast_pipeline/_version.py to store the current software version and updated release documentation #532 . Added created and last updated dates to doc pages using mkdocs-git-revision-date-localized-plugin #514 . Added support for glob expressions when specifying input files in the run config file #504 Added DEFAULT_AUTO_FIELD to settings.py to silence Django 3.2 warnings #507 Added lightgallery support for all images in the documentation #494 . Added new entries in the documentation contributing section #494 . Added new entries in the documentation FAQ section #491 . Added new home page for documentation #491 . Added dark mode switch on documentation #487 . Added .env file information to documentation #487 . Added further epoch based association information to documentation page #487 . Added script to auto-generate code reference documentation pages #480 . Added code reference section to documentation #480 . Added new pages and sections to documentation #471 Added requirements/environment.yml so make it easier for Miniconda users to get the non-Python dependencies #472 . Added pyproject.toml and poetry.lock #472 . Added init-tools/init-db.py #472 . Added image add mode run restore command 'restorepiperun' #463 Added documentation folder and files for mkdocs and CI #433 Added add image to existing run feature #443 Added networkx to base reqiurements #460 . Added CI/CD workflow to run tests on pull requests #446 Added basic regression tests #425 Added image length validation for config #425","title":"Added"},{"location":"changelog/#changed_1","text":"Changed source naming convention to Jhhmmss.s(+/-)ddmmss to match VAST-P1 paper (Murphy, et al. 2021) convention #536 Updated npm packages to resolve dependabot security alert #533 . Updated homepage text to reflect new features and documentation #534 . Changed layout of source detail page #526 . Updated mkdocs-material to 7.1.4 for native creation date support #518 . Updated developing docs to specify the main development branch as dev instead of master #521 . Updated tests to account for relation fix #510 . All file examples in docs are now enclosed in an example admonition #494 . Further changes to layout of documentation #494 . Changed arrow file generation from vaex to pyarrow #503 . Changed layout of documentation to use tabs #491 . Dependabot: Bump y18n from 3.2.1 to 3.2.2 #482 . Replaced run config .py format with .yaml #483 . Changed docs VAST logo to icon format to avoid stretched appearence #487 . Bumped Browsersync from 2.26.13 to 2.26.14 #481 . Dependabot: Bump prismjs from 1.22.0 to 1.23.0 #469 . Changed non-google format docstrings to google format #480 . Changed some documentation layout and updated content #471 . Changed the vaex dependency to vaex-arrow #472 . Set CREATE_MEASUREMENTS_ARROW_FILES = True in the basic association test config #472 . Bumped minimum Python version to 3.7.1 #472 . Replaced npm package gulp-sass with @mr-hope/gulp-sass , a fork which drops the dependency on the deprecated node-sass which is difficult to install #472 . Changed the installation documentation to instruct users to use a PostgreSQL Docker image with Q3C already installed #472 . Changed 'cmd' flag in run pipeline to 'cli' #466 . Changed CONTRIBUTING.md and README.md #433 Changed forced extraction name suffix to run id rather than datetime #443 Changed tests to run on smaller cutouts #443 Changed particles style on login page #459 . Dependabot: Bump ini from 1.3.5 to 1.3.8 #436","title":"Changed"},{"location":"changelog/#fixed_1","text":"Fixed the broken link to the image detail page on measurement detail pages #528 . Fixed simbad and ned external search results table nan values #523 . Fixed inaccurate total results reported by some paginators #517 . Removed excess whitespace from coordinates that get copied to the clipboard #515 Fixed rogue relations being created during one-to-many functions #510 . Fixed JS9 regions so that the selected source components are always on top #508 Fixed docstring in config.py #494 . Fixed arrow files being generated via the website #503 . Fixed a bug that returned all sources when performing a cone search where one of the coords = 0 #501 Fixed the missing hover tool for lightcurve plots of non-variable sources #493 Fixed the default Dask multiprocessing context to \"fork\" #472 . Fixed Selavy catalogue ingest to discard the unit row before reading the data #473 . Fixed initial job processing from the UI #466 . Fixed links in README.md #464 . Fixed basic association new sources created through relations #443 Fixed tests running pipeline multiple times #443 Fixed particles canvas sizing on login page #459 . Fixed breadcrumb new line on small resolutitons #459 . Fixed config files in tests #430 Fixed sources table on measurement detail page #429 . Fixed missing meta columns in parallel association #427 .","title":"Fixed"},{"location":"changelog/#removed_1","text":"Removed SURVEYS_WORKING_DIR from settings and env file #538 . Removed default_survey from run configuration file #538 . Removed importsurvey command and catalogue.py #538 . Removed SurveySource, Survey and SurveySourceQuerySet models #538 . Removed email and Slack links from docs footer #535 . Removed bootstrap as the required version is bundled with startbootstrap-sb-admin-2 #533 . Removed docs/readme.md softlink as it is no longer used #494 . Removed vaex-arrow from the dependancies #503 . Removed requirements/*.txt files. Development dependency management moved to Poetry #472 . Removed init-tools/init-db.sh #472 . Removed INSTALL.md , PROFILE.md and static/README.md #433 Removed aplpy from base requirements #460 .","title":"Removed"},{"location":"changelog/#list-of-prs_1","text":"#538 feat: Removed survey source models, commands and references. #536 feat: changed source naming convention. #535 doc: added help and acknowledgement doc page. #534 feat: Update homepage text. #532 feat, doc: Versioning. #533 dep: updated npm deps; removed bootstrap. #528 fix: fixed broken image detail link. #526 feat: Updated source detail page layout. #518 dep: Updated mkdocs-material for native creation date support. #523 fix: Fixed external search results table nan values. #521 doc: update doc related to default dev branch. #517 fix: pin djangorestframework-datatables to 0.5.1. #515 fix: remove linebreaks from coordinates. #514 dep: Added created and updated dates to doc pages. #510 fix: Fix rogue relations. #508 fix: Draw selected source components on top in JS9. #504 feat: Add glob expression support to yaml run config. #507 fix: set default auto field model. #494 doc, dep: Docs: Added lightgallery support, layout update, minor fixes and additions. #503 fix, dep: Change arrow file generation from vaex to pyarrow. #501 fix: fix broken cone search when coord = 0 #491 doc: Updated the docs layout, home page and FAQs. #493 fix: Fix bokeh hover tool for lightcurve plots. #482 dep: Bump y18n from 3.2.1 to 3.2.2. #483 feat: replace run config .py files with .yaml. #487 doc: Minor documentation improvements. #481 dep: Bump Browsersync from 2.26.13 to 2.26.14. #469 dep: Bump prismjs from 1.22.0 to 1.23.0. #480 feat: Code reference documentation update. #471 feat: Documentation update. #472 feat: Simplify install. #473 fix: discard the selavy unit row before reading. #466 fix: Fixed initial job processing from the UI. #463 feat: Added image add mode run restore command. #433 doc: add documentation GitHub pages website with CI. #443 feat, fix: Adds the ability to add images to an existing run. #460 dep: Removed aplpy from base requirements. #446 feat: CI/CD workflow. #459 fix: Fix particles and breadcrumb issues on mobile. #436 dep: Bump ini from 1.3.5 to 1.3.8. #430 fix: Test config files. #425 feat: Basic regression tests. #429 fix: Fixed sources table on measurement detail page. #427 fix: Fixed missing meta columns in parallel association.","title":"List of PRs"},{"location":"changelog/#020-2020-11-30","text":"","title":"0.2.0 (2020-11-30)"},{"location":"changelog/#added_2","text":"Added a check in the UI running that the job is not already running or queued #421 . Added the deletion of all parquet and arrow files upon a re-run #421 . Added source selection by name or ID on source query page #401 . Added test cases #412 Added askap-vast/forced_phot to pip requirements #408 . Added pipeline configuration parameter, SOURCE_AGGREGATE_PAIR_METRICS_MIN_ABS_VS , to filter measurement pairs before calculating aggregate metrics #407 . Added custom 404.html and 500.html templates for error pages #415 Added ability to export measurement_pairs.parqyet as an arrow file #393 . Added new fields to detail pages and source and measurement tables #406 . Added new fields to source query page (island flux ratio, min and max fluxes) #406 . Added min, max flux values to sources and agg min island flux ratio field #406 . Added island flux ratio column to measurements, component flux divided by total island flux (peak and int) #406 . Added a maximum number of images for runs through the UI #404 . Added the ability to run a pipeline run through the UI #404 . Added Queued status to the list of pipeline run statuses #404 . Added the dependancy django-q that enables scheduled tasks to be processed #404 . Added source tagging #396 . Added link to measurement table from the lightcurve to source detail page #387 . Added 'epoch based' parameter to pipeline run detail page #387 . Adds basic commenting functionality for sources, measurements, images, and runs #368 . Custom CSS now processed with Sass: Bootstrap and sb-admin-2 theme are compiled into a single stylesheet #370 . Added vast_pipeline/pipeline/generators.py which contains generator functions #382 . Range and NaN check on new source analysis to match forced extraction #374 . Added the ability for the pipeline to read in groups of images which are defined as a single epoch #277 . Added the ability of the pipeline to remove duplicated measurements from an epoch #277 . Added option to control separation measurements which are defined as a duplicate #277 . Added the ability of the pipeline to separate images to associate into unique sky region groups #277 . Added option to perform assocication of separate sky region groups in parallel #277 . Added new options to webinterface pipeline run creation #277 . Added epoch_based column run model #277 . Added links to tables and postage stamps on source detail page #379 . Updates image background_path from current run when not originally provided #377 . Added csv export button to datatables on webinterface #363 . Added support for Excel export button to datatables on webinterface (waiting on datatables buttons fix) #363 . Added column visibility button to datatables on webinterface #363 . Added dependancy datatables-buttons 1.6.4 #363 . Added dependancy jszip (required for Excel export) #363 . Adds n_selavy_measurements and n_forced_measurements to run model #362 . Adds steps to populate new measurement count fields in pipeline run #362 . Source order from the query is preserved on source detail view #364 . Setting HOME_DATA_DIR to specify a directory relative to the user's home directory to scan for FITS and text files to use in a Run initialised with the UI #361 . Adds a node graph to accompany the lightcurve that shows which measurement pairs exceed the default variability metric thresholds ( Vs >= 4.3 , |m| >= 0.26 ) #305 . Adds the MeasurementPair model to store two variability metrics for each flux type: Vs, the t-statistic; and m, the modulation index. The maximum of these metrics are also added to the Source model for joinless queries. These metrics are calculated during the pipeline run #305 . Adds radio buttons to change the lightcurve data points between peak and integrated fluxes #305 . Fills out information on all webinterface detail pages #345 . Adds frequency information the measurements and images webinterface tables. #345 . Adds celestial plot and tables to webinterface pipeline detail page #345 . Adds useful links to webinterface navbar #345 . Adds tool tips to webinterface source query #345 . Adds hash reading to webinterface source query to allow filling from URL hash parameters #345 . Add links to number cards on webinterface #345 . Added home icon on hover on webinterface #345 . Added copy-to-clipboard functionality on coordinates on webinterface #345 .","title":"Added"},{"location":"changelog/#changed_2","text":"Renamed 'alert-wrapper' container to 'toast-wrapper' #419 . Changed alerts to use the Bootstrap toasts system #419 . Bumped some npm package versions to address dependabot security alerts #411 . Images table on pipeline run detail page changed to order by datetime by default #417 . Changed config argument CREATE_MEASUREMENTS_ARROW_FILE -> CREATE_MEASUREMENTS_ARROW_FILES #393 . Naming of average flux query fields to account for other min max flux fields #406 . Expanded README.md to include DjangoQ and UI job scheduling information #404 . Shifted alerts location to the top right #404 . Log file card now expanded by default on pipeline run detail page #404 . Changed user comments on source detail pages to incorporate tagging feature #396 . Updated RACS HiPS URL in Aladin #399 . Changed home page changelog space to welcome/help messages #387 . The comment field in the Run model has been renamed to description . A comment many-to-many relationship was added to permit user comments on Run instances #368 . Moved sb-admin-2 Bootstrap theme static assets to NPM package dependency #370 . Refactored bulk uploading to use iterable generator objects #382 . Updated validation of config file to check that all options are present and valid #373 . Rewritten relation functions to improve speed #307 . Minor changes to association to increase speed #307 . Changes to decrease memory usage during the calculation of the ideal coverage dataframe #307 . Updated the get_src_skyregion_merged_df logic to account for epochs #277 . Updated the job creation modal layout #277 . Bumped datatables-buttons to 1.6.5 and enabled excel export buttton #380 . Bumped datatables to 1.10.22 #363 . Changed dom layout on datatables #363 . Changed external results table pagination buttons on source detail webinterface page pagination to include less numbers to avoid overlap #363 . Changes measurement counts view on website to use new model parameters #362 . Lightcurve plot now generated using Bokeh #305 . Multiple changes to webinterface page layouts #345 . Changes source names to the format ASKAP_hhmmss.ss(+/-)ddmmss.ss #345 . Simplified webinterface navbar #345 . Excludes sources and pipeline runs from being listed in the source query page that are not complete on the webinterface #345 . Clarifies number of measurements on webinterface detail pages #345 . Changed N.A. labels to N/A on the webinterface #345 .","title":"Changed"},{"location":"changelog/#fixed_2","text":"Fixed pipeline run DB loading in command line runpipeline command #401 . Fixed nodejs version #412 Fixed npm start failure #412 All queries using the 2-epoch metric Vs now operate on abs(Vs) . The original Vs stored in MeasurementPair objects is still signed #407 . Changed aggregate 2-epoch metric calculation for Source objects to ensure they come from the same pair #407 . Fixed new sources rms measurement returns when no measurements are valid #417 . Fixed measuring rms values from selavy created NAXIS=3 FITS images #417 . Fixed rms value calculation in non-cluster forced extractions #402 . Increase request limit for gunicorn #398 . Fixed max source Vs metric to being an absolute value #391 . Fixed misalignment of lightcurve card header text and the flux type radio buttons #386 . Fixes incorrently named GitHub social-auth settings variable that prevented users from logging in with GitHub #372 . Fixes webinterface navbar overspill at small sizes #345 . Fixes webinterface favourite source table #345 .","title":"Fixed"},{"location":"changelog/#removed_2","text":"Removed/Disabled obsolete test cases #412 Removed vast_pipeline/pipeline/forced_phot.py #408 . Removed 'selavy' from homepage measurements count label #391 . Removed leftover pipeline/plots.py file #391 . Removed static/css/pipeline.css , this file is now produced by compiling the Sass ( scss/**/*.scss ) files with Gulp #370 . Removed any storage of meas_dj_obj or src_dj_obj in the pipeline #382 . Removed static/vendor/chart-js package #305 . Removed static/css/collapse-box.css , content moved to pipeline.css #345 .","title":"Removed"},{"location":"changelog/#list-of-prs_2","text":"#421 feat: Delete output files on re-run & UI run check. #401 feat: Added source selection by name or id to query page. #412 feat: added some unit tests. #419 feat: Update alerts to use toasts. #408 feat: use forced_phot dependency instead of copied code. #407 fix, model: modified 2-epoch metric calculation. #411 fix: updated npm deps to fix security vulnerabilities. #415 feat: Added custom 404 and 500 templates. #393 feat: Added measurement_pairs arrow export. #406 feat, model: Added island flux ratio columns. #402 fix: Fixed rms value calculation in non-cluster forced extractions. #404 feat, dep, model: Completed schedule pipe run. #396 feat: added source tagging. #398 fix: gunicorn request limit #399 fix: Updated RACS HiPS path. #391 fix: Vs metric fix and removed pipeline/plots.py. #387 feat: Minor website updates. #386 fix: fix lightcurve header floats. #368 feat: vast-candidates merger: Add user commenting #370 feat: moved sb-admin-2 assets to dependencies. #382 feat: Refactored bulk uploading of objects. #374 feat, fix: Bring new source checks inline with forced extraction. #373 fix: Check all options are valid and present in validate_cfg. #307 feat: Improve relation functions and general association speed ups. #277 feat,model: Parallel and epoch based association. #380 feat, dep: Enable Excel export button. #379 feat: Add links to source detail template. #377 fix: Update image bkg path when not originally provided. #363 feat, dep: Add export and column visibility buttons to tables. #362 feat, model: Added number of measurements to Run DB model. #364 feat: preserve source query order on detail view. #361 feat, fix: restrict home dir scan to specified directory. #372 fix: fix social auth scope setting name. #305 feat: 2 epoch metrics #345 feat, fix: Website improvements.","title":"List of PRs"},{"location":"changelog/#010-2020-09-27","text":"First release of the Vast Pipeline. This was able to process 707 images (EPOCH01 to EPOCH11x) on a machine with 64 GB of RAM.","title":"0.1.0 (2020-09-27)"},{"location":"changelog/#list-of-prs_3","text":"#347 feat: Towards first release #354 fix, model: Updated Band model fields to floats #346 fix: fix JS9 overflow in measurement detail view #349 dep: Bump lodash from 4.17.15 to 4.17.20 #348 dep: Bump django from 3.0.5 to 3.0.7 in /requirements #344 fix: fixed aladin init for all pages #340 break: rename pipeline folder to vast_pipeline #342 fix: Hotfix - fixed parquet path on job detail view #336 feat: Simbad/NED async cone search #284 fix: Update Aladin surveys with RACS and VAST #333 feat: auth to GitHub org, add logging and docstring #325 fix, feat: fix forced extraction using Dask bags backend #334 doc: better migration management explanation #332 fix: added clean to build task, removed commented lines #322 fix, model: add unique to image name, remove timestamp from image folder #321 feat: added css and js sourcemaps #314 feat: query form redesign, sesame resolver, coord validator #318 feat: Suppress astropy warnings #317 fix: Forced photometry fixes for #298 and #312 #316 fix: fix migration file 0001_initial.py #310 fix: Fix run detail number of measurements display #309 fix: Added JS9 overlay filters and changed JS9 overlay behaviour on sources and measurements #303 fix: Fix write config feedback and validation #306 feat: Add config validation checks #302 fix: Fix RA correction for d3 celestial #300 fix: increase line limit for gunicorn server #299 fix: fix admin \"view site\" redirect #294 fix: Make lightcurves start at zero #268 feat: Production set up with static files and command #291 fix: Bug fix for forced_photom cluster allow_nan #289 fix: Fix broken UI run creation #287 fix: Fix forced measurement parquet files write #286 fix: compile JS9 without helper option #285 fix: Fix removing forced parquet and clear images from piperun","title":"List of PRs"},{"location":"code_of_conduct/","text":"Code Of Conduct \u00b6 By joining the VAST collaboration you agree to adhere to the Code of Conduct below. We are committed to making this collaboration productive and enjoyable for everyone, regardless of gender, sexual orientation, disability, physical appearance, body size, race, nationality or religion. We will not tolerate harassment of colleagues and students in any form. To achieve this, VAST members must endeavour to work together in a cooperative way on scientific projects that fall within the scope of VAST. In particular all members must: Exercise their best professional and ethical judgement and carry out their duties and functions with integrity and objectivity; Act fairly and reasonably, and treat colleagues and students with respect, impartiality, courtesy and sensitivity; Avoid conflicts of interest; Adhere to the VAST membership and publication policies. Please follow these guidelines in all your interactions (including online) within VAST: Behave professionally . Harassment and sexist, racist, or exclusionary comments or jokes are not appropriate. Harassment includes sustained disruption of talks or other events, inappropriate physical contact, sexual attention or innuendo, deliberate intimidation, stalking, and photography or recording of an individual without consent. It also includes offensive comments related to gender, sexual orientation, disability, physical appearance, body size, race or religion. All communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery is not appropriate. Be kind to others . Do not insult or put down other collaboration members. Participants asked to stop any inappropriate behaviour are expected to comply immediately. Members violating these rules may be asked to leave the collaboration at the discretion of the PIs. Any participant who wishes to report a violation of this policy is asked to speak, in confidence, to Tara Murphy (tara.murphy@sydney.edu.au) or David Kaplan (kaplan@uwm.edu). Extra Guidelines \u00b6 We are a community based on openness, as well as friendly and didactic discussions. We aspire to treat everybody equally, and value their contributions. Decisions are made based on technical merit and consensus. Code is not the only way to help the project. Reviewing pull requests, answering questions to help others on mailing lists or issues, organizing and teaching tutorials, working on the website, improving the documentation, are all priceless contributions. We abide by the principles of openness, respect, and consideration of others of the Python Software Foundation: https://www.python.org/psf/codeofconduct/ Acknowledgements \u00b6 We ask that all VAST publications (papers, ATELs, etc) include the line: This work was done as part of the ASKAP Variables and Slow Transients (VAST) collaboration (Murphy et al. 2013, PASA, 30, 6). Separately, all refereed publications should carry the standard CSIRO acknowledgement : The Australian SKA Pathfinder is part of the Australia Telescope National Facility which is managed by CSIRO. Operation of ASKAP is funded by the Australian Government with support from the National Collaborative Research Infrastructure Strategy. ASKAP uses the resources of the Pawsey Supercomputing Centre. Establishment of ASKAP, the Murchison Radio-astronomy Observatory and the Pawsey Supercomputing Centre are initiatives of the Australian Government, with support from the Government of Western Australia and the Science and Industry Endowment Fund. We acknowledge the Wajarri Yamatji people as the traditional owners of the Observatory site. This project is supported by the University of Sydney, the Australian Research Council, and CSIRO.","title":"Code of conduct"},{"location":"code_of_conduct/#code-of-conduct","text":"By joining the VAST collaboration you agree to adhere to the Code of Conduct below. We are committed to making this collaboration productive and enjoyable for everyone, regardless of gender, sexual orientation, disability, physical appearance, body size, race, nationality or religion. We will not tolerate harassment of colleagues and students in any form. To achieve this, VAST members must endeavour to work together in a cooperative way on scientific projects that fall within the scope of VAST. In particular all members must: Exercise their best professional and ethical judgement and carry out their duties and functions with integrity and objectivity; Act fairly and reasonably, and treat colleagues and students with respect, impartiality, courtesy and sensitivity; Avoid conflicts of interest; Adhere to the VAST membership and publication policies. Please follow these guidelines in all your interactions (including online) within VAST: Behave professionally . Harassment and sexist, racist, or exclusionary comments or jokes are not appropriate. Harassment includes sustained disruption of talks or other events, inappropriate physical contact, sexual attention or innuendo, deliberate intimidation, stalking, and photography or recording of an individual without consent. It also includes offensive comments related to gender, sexual orientation, disability, physical appearance, body size, race or religion. All communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery is not appropriate. Be kind to others . Do not insult or put down other collaboration members. Participants asked to stop any inappropriate behaviour are expected to comply immediately. Members violating these rules may be asked to leave the collaboration at the discretion of the PIs. Any participant who wishes to report a violation of this policy is asked to speak, in confidence, to Tara Murphy (tara.murphy@sydney.edu.au) or David Kaplan (kaplan@uwm.edu).","title":"Code Of Conduct"},{"location":"code_of_conduct/#extra-guidelines","text":"We are a community based on openness, as well as friendly and didactic discussions. We aspire to treat everybody equally, and value their contributions. Decisions are made based on technical merit and consensus. Code is not the only way to help the project. Reviewing pull requests, answering questions to help others on mailing lists or issues, organizing and teaching tutorials, working on the website, improving the documentation, are all priceless contributions. We abide by the principles of openness, respect, and consideration of others of the Python Software Foundation: https://www.python.org/psf/codeofconduct/","title":"Extra Guidelines"},{"location":"code_of_conduct/#acknowledgements","text":"We ask that all VAST publications (papers, ATELs, etc) include the line: This work was done as part of the ASKAP Variables and Slow Transients (VAST) collaboration (Murphy et al. 2013, PASA, 30, 6). Separately, all refereed publications should carry the standard CSIRO acknowledgement : The Australian SKA Pathfinder is part of the Australia Telescope National Facility which is managed by CSIRO. Operation of ASKAP is funded by the Australian Government with support from the National Collaborative Research Infrastructure Strategy. ASKAP uses the resources of the Pawsey Supercomputing Centre. Establishment of ASKAP, the Murchison Radio-astronomy Observatory and the Pawsey Supercomputing Centre are initiatives of the Australian Government, with support from the Government of Western Australia and the Science and Industry Endowment Fund. We acknowledge the Wajarri Yamatji people as the traditional owners of the Observatory site. This project is supported by the University of Sydney, the Australian Research Council, and CSIRO.","title":"Acknowledgements"},{"location":"faq/","text":"Frequently Asked Questions \u00b6 Can the VAST Pipeline be used with images from other telescopes? \u00b6 The base answer to this question is that the pipeline has been designed specifically for ASKAPsoft and ASKAPpipeline products, so compatibility with data from other telescopes is not supported. However, it's important to remember that the pipeline performs no source extraction itself, instead it reads in source catalogues that is expected to be in the format of the output of the Selavy source extractor. As seen from the Image Ingest page , the pipeline does not use any special or out of the ordinary FITS headers when reading the images, and the only inputs required are the images, catalogues, noise images and background images - which are standard products. Hence, the real answer to this question is yes, if one of the following is performed: Run the Selavy source extractor on the images to process. Convert the component output from a different source extractor to match that of the Selavy component file . The pipeline was also designed in a way such that other source extractor 'translators' could be plugged into the pipeline. So a further option is to develop new translators such that the pipeline can read in output from other source extractors. The translators can be found in vast_pipeline/surveys/translators.py . Please open a discussion or issue on GitHub if you intend to give this a go! Bug In reading the code recently I have a suspicion the FITS reading code is reliant on the TELESCOP FITS header being equal to ASKAP . This is unintentional as there is nothing special about the FITS headers being read. Worth to check if anyone goes down this path. - Adam, March 2021. Does the pipeline support any other Stokes products such as Stokes V? \u00b6 Currently the pipeline only supports Stokes I data. Users can view Stokes V HIPS maps of the RACS and VAST surveys in the Aladin Lite tool on the source detail page . The support of Stokes V is planned in a future update . Can the pipeline handle multi-frequency datasets? \u00b6 Currently the pipeline does not support multi-frequency datasets. Any images that are put through in a run are assumed by the pipeline to be directly comparable to one another. For example, all variability metrics are calculated directly on the fluxes provided from the source catalogues. Multi-frequency support is planned in a future update .","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#can-the-vast-pipeline-be-used-with-images-from-other-telescopes","text":"The base answer to this question is that the pipeline has been designed specifically for ASKAPsoft and ASKAPpipeline products, so compatibility with data from other telescopes is not supported. However, it's important to remember that the pipeline performs no source extraction itself, instead it reads in source catalogues that is expected to be in the format of the output of the Selavy source extractor. As seen from the Image Ingest page , the pipeline does not use any special or out of the ordinary FITS headers when reading the images, and the only inputs required are the images, catalogues, noise images and background images - which are standard products. Hence, the real answer to this question is yes, if one of the following is performed: Run the Selavy source extractor on the images to process. Convert the component output from a different source extractor to match that of the Selavy component file . The pipeline was also designed in a way such that other source extractor 'translators' could be plugged into the pipeline. So a further option is to develop new translators such that the pipeline can read in output from other source extractors. The translators can be found in vast_pipeline/surveys/translators.py . Please open a discussion or issue on GitHub if you intend to give this a go! Bug In reading the code recently I have a suspicion the FITS reading code is reliant on the TELESCOP FITS header being equal to ASKAP . This is unintentional as there is nothing special about the FITS headers being read. Worth to check if anyone goes down this path. - Adam, March 2021.","title":"Can the VAST Pipeline be used with images from other telescopes?"},{"location":"faq/#does-the-pipeline-support-any-other-stokes-products-such-as-stokes-v","text":"Currently the pipeline only supports Stokes I data. Users can view Stokes V HIPS maps of the RACS and VAST surveys in the Aladin Lite tool on the source detail page . The support of Stokes V is planned in a future update .","title":"Does the pipeline support any other Stokes products such as Stokes V?"},{"location":"faq/#can-the-pipeline-handle-multi-frequency-datasets","text":"Currently the pipeline does not support multi-frequency datasets. Any images that are put through in a run are assumed by the pipeline to be directly comparable to one another. For example, all variability metrics are calculated directly on the fluxes provided from the source catalogues. Multi-frequency support is planned in a future update .","title":"Can the pipeline handle multi-frequency datasets?"},{"location":"help_and_acknowledgements/","text":"Help and Acknowledgements \u00b6 Getting Help \u00b6 The best way to get help with the VAST Pipeline software is to contact the development team on GitHub. If you have encountered a specific problem while using the pipeline or attempting to install any of the components, please open a new issue . If you have a more general question or an idea for a new feature, please create a new discussion thread . General enquiries may also be sent via email to the VAST project Principal Investigators: Tara Murphy David Kaplan Contributors \u00b6 Sergio Pintaldi \u2013 Sydney Informatics Hub Adam Stewart \u2013 Sydney Institute for Astronomy Andrew O'Brien \u2013 Department of Physics, University of Wisconsin-Milwaukee Tara Murphy \u2013 Sydney Institute for Astronomy David Kaplan \u2013 Department of Physics, University of Wisconsin-Milwaukee Shibli Saleheen \u2013 ADACS David Liptai \u2013 ADACS Ella Xi Wang \u2013 ADACS Acknowledgements \u00b6 The VAST Pipeline development was supported by: The Australian Research Council through grants FT150100099 and DP190100561. The Sydney Informatics Hub (SIH), a core research facility at the University of Sydney. Software support resources awarded under the Astronomy Data and Computing Services (ADACS) Merit Allocation Program. ADACS is funded from the Astronomy National Collaborative Research Infrastructure Strategy (NCRIS) allocation provided by the Australian Government and managed by Astronomy Australia Limited (AAL). NSF grant AST-1816492. We also acknowledge the LOFAR Transients Pipeline (TraP) ( Swinbank, et al. 2015 ) from which various concepts and design choices have been implemented in the VAST Pipeline. The developers thank the creators of SB Admin 2 to make the dashboard template freely available.","title":"Help and Acknowledgements"},{"location":"help_and_acknowledgements/#help-and-acknowledgements","text":"","title":"Help and Acknowledgements"},{"location":"help_and_acknowledgements/#getting-help","text":"The best way to get help with the VAST Pipeline software is to contact the development team on GitHub. If you have encountered a specific problem while using the pipeline or attempting to install any of the components, please open a new issue . If you have a more general question or an idea for a new feature, please create a new discussion thread . General enquiries may also be sent via email to the VAST project Principal Investigators: Tara Murphy David Kaplan","title":"Getting Help"},{"location":"help_and_acknowledgements/#contributors","text":"Sergio Pintaldi \u2013 Sydney Informatics Hub Adam Stewart \u2013 Sydney Institute for Astronomy Andrew O'Brien \u2013 Department of Physics, University of Wisconsin-Milwaukee Tara Murphy \u2013 Sydney Institute for Astronomy David Kaplan \u2013 Department of Physics, University of Wisconsin-Milwaukee Shibli Saleheen \u2013 ADACS David Liptai \u2013 ADACS Ella Xi Wang \u2013 ADACS","title":"Contributors"},{"location":"help_and_acknowledgements/#acknowledgements","text":"The VAST Pipeline development was supported by: The Australian Research Council through grants FT150100099 and DP190100561. The Sydney Informatics Hub (SIH), a core research facility at the University of Sydney. Software support resources awarded under the Astronomy Data and Computing Services (ADACS) Merit Allocation Program. ADACS is funded from the Astronomy National Collaborative Research Infrastructure Strategy (NCRIS) allocation provided by the Australian Government and managed by Astronomy Australia Limited (AAL). NSF grant AST-1816492. We also acknowledge the LOFAR Transients Pipeline (TraP) ( Swinbank, et al. 2015 ) from which various concepts and design choices have been implemented in the VAST Pipeline. The developers thank the creators of SB Admin 2 to make the dashboard template freely available.","title":"Acknowledgements"},{"location":"license/","text":"MIT License Copyright (c) 2020-2025 ASKAP VAST Organisation, The University of Sydney (Sydney Informatics Hub), Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"adminusage/app/","text":"Web App \u00b6 This section describes how to run the pipeline Django web app/server. Starting the Pipeline Web App \u00b6 Make sure you installed and compiled correctly the frontend assets see guide Start the Django development web server: ( pipeline_env ) $ ./manage.py runserver Test the webserver by pointing your browser at http://127.0.0.1:8000 or http://localhost:8000 . The webserver is independent of runpipeline and you can use the website while the pipeline commands are running. Running a pipeline run via the web server \u00b6 It is possible to launch the processing of a pipeline run by using the relevant option on the pipeline run detail page. This uses DjangoQ to schedule and process the runs and a cluster needs to be set up in order for the runs to process: Check the Q_CLUSTER options in ./webinterface/settings.py . Refer to the DjangoQ docs if you are unsure on the meaning of any parameters. Launch the cluster using the following command, making sure you are in the pipeline environment: ( pipeline_env ) $ ./manage.py qcluster If the pipeline is updated then the qcluster also needs to be be restarted. A warning that if you submit jobs before the cluster is set up, or is taken down, then these jobs will begin immediately once the cluster is back online.","title":"Web App"},{"location":"adminusage/app/#web-app","text":"This section describes how to run the pipeline Django web app/server.","title":"Web App"},{"location":"adminusage/app/#starting-the-pipeline-web-app","text":"Make sure you installed and compiled correctly the frontend assets see guide Start the Django development web server: ( pipeline_env ) $ ./manage.py runserver Test the webserver by pointing your browser at http://127.0.0.1:8000 or http://localhost:8000 . The webserver is independent of runpipeline and you can use the website while the pipeline commands are running.","title":"Starting the Pipeline Web App"},{"location":"adminusage/app/#running-a-pipeline-run-via-the-web-server","text":"It is possible to launch the processing of a pipeline run by using the relevant option on the pipeline run detail page. This uses DjangoQ to schedule and process the runs and a cluster needs to be set up in order for the runs to process: Check the Q_CLUSTER options in ./webinterface/settings.py . Refer to the DjangoQ docs if you are unsure on the meaning of any parameters. Launch the cluster using the following command, making sure you are in the pipeline environment: ( pipeline_env ) $ ./manage.py qcluster If the pipeline is updated then the qcluster also needs to be be restarted. A warning that if you submit jobs before the cluster is set up, or is taken down, then these jobs will begin immediately once the cluster is back online.","title":"Running a pipeline run via the web server"},{"location":"adminusage/cli/","text":"Command Line Interface (CLI) \u00b6 This section describes the commands available to the administrators of the pipelines. Pipeline Usage \u00b6 All the pipeline commands are run using the Django global ./manage.py <command> interface. Therefore you need to activate the Python environment. You can have a look at the available commands for the pipeline app: (pipeline_env)$ ./manage.py help Output: ... [vast_pipeline] clearpiperun createmeasarrow debugrun ingestimages initingest initpiperun restorepiperun runpipeline ... There are 8 commands, described in detail below. clearpiperun \u00b6 Resetting a pipeline run can be done using the clearpiperun command. This will delete all images and related objects such as sources associated with that pipeline run. Images that have been used in other pipeline runs will not be deleted. ./manage.py clearpiperun --help usage: manage.py clearpiperun [-h] [--keep-parquet] [--remove-all] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperuns [piperuns ...] Delete a pipeline run and all related images, sources, etc. Will not delete objects if they are also related to another pipeline run. positional arguments: piperuns Name or path of pipeline run(s) to delete. Pass \"clearall\" to delete all the runs. optional arguments: -h, --help show this help message and exit --keep-parquet Flag to keep the pipeline run(s) parquet files. Will also apply to arrow files if present. --remove-all Flag to remove all the content of the pipeline run(s) folder. --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. Example usage: (pipeline_env)$ ./manage.py clearpiperun path/to/my_pipe_run # or (pipeline_env)$ ./manage.py clearpiperun my_pipe_run Tip Further information on clearing a specific run, or resetting the database, can be found in the Contributing and Developing section. createmeasarrow \u00b6 This command allows for the creation of the measurements.arrow and measurement_pairs.arrow files after a run has been successfully completed. See Arrow Files for more information. ./manage.py createmeasarrow --help usage: manage.py createmeasarrow [-h] [--overwrite] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperun Create `measurements.arrow` and `measurement_pairs.arrow` files for a completed pipeline run. positional arguments: piperun Path or name of the pipeline run. optional arguments: -h, --help show this help message and exit --overwrite Overwrite previous 'measurements.arrow' file. --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the co Example usage: ./manage.py createmeasarrow docs_example_run 2021-03-30 10:48:40,952 createmeasarrow INFO Creating measurements arrow file for 'docs_example_run'. 2021-03-30 10:48:40,952 utils INFO Creating measurements.arrow for run docs_example_run. 2021-03-30 10:48:41,829 createmeasarrow INFO Creating measurement pairs arrow file for 'docs_example_run'. 2021-03-30 10:48:41,829 utils INFO Creating measurement_pairs.arrow for run docs_example_run. debugrun \u00b6 The debugrun command is used to print out a summary of the pipeline run to the terminal. A single pipeline run can be entered as an argument or all can be entered to print the statistics of all the pipeline runs in the database. ./manage.py debugrun --help usage: manage.py debugrun [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperuns [piperuns ...] Print out total metrics such as nr of measurements for runs positional arguments: piperuns Name or path of pipeline run(s) to debug.Pass \"all\" to print summary data of all the runs. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. Example usage: ./manage.py debugrun docs_example_run * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Printing summary data of pipeline run \"docs_example_run\" Nr of images: 14 Nr of measurements: 4312 Nr of forced measurements: 2156 Nr of sources: 557 Nr of association: 3276 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ingestimages \u00b6 This command runs the first part of the pipeline only. It ingests/adds a set of images, and their measurements, to the database. It requires an image ingestion configuration file as input. A template ingest configuration file can be generated with the initingest command (below). (pipeline_env)$ ./manage.py ingestimages --help Output: usage: manage.py ingestimages [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] image_ingest_config Ingest/add a set of images to the database positional arguments: image_ingest_config Image ingestion configuration filename/path. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. General usage: (pipeline_env)$ ./manage.py ingestimages ./ingest_config.yml Output: 2021-06-25 03:08:44,313 loading INFO Reading image epoch01.fits ... 2021-06-25 03:08:44,348 utils INFO Adding new frequency band: 888 2021-06-25 03:08:44,390 utils INFO Created sky region 150.001, -30.001 2021-06-25 03:08:44,441 loading INFO Processed measurements dataframe of shape: (4, 40) 2021-06-25 03:08:44,452 loading INFO Bulk created #4 Measurement 2021-06-25 03:08:44,504 loading INFO Reading image epoch02.fits ... ... 2021-06-25 03:08:44,731 loading INFO Reading image epoch04.fits ... 2021-06-25 03:08:44,771 utils INFO Created sky region 150.021, -30.017 2021-06-25 03:08:44,805 loading INFO Processed measurements dataframe of shape: (5, 40) 2021-06-25 03:08:44,810 loading INFO Bulk created #5 Measurement 2021-06-25 03:08:44,819 loading INFO Total images upload/loading time: 0.97 seconds initingest \u00b6 This command generates a template configuration file for use with the ingestimages command. (pipeline_env)$ ./manage.py initingest --help Output: usage: manage.py initingest [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] config_file_name Create a template image ingestion configuration file positional arguments: config_file_name Filename to write template ingest configuration to. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. General usage: (pipeline_env)$ ./manage.py initingest ingest_config.yml Output: Writing template to: ingest_config.yml Then modify ingest_config.yml to suit your needs. initpiperun \u00b6 In order to process the images in the pipeline, you must create/initialise a pipeline run first. The pipeline run creation is done using the initpiperun django command, which requires a pipeline run folder. The command creates a folder with the pipeline run name under the settings PROJECT_WORKING_DIR defined in settings . (pipeline_env)$ ./manage.py initpiperun --help Output: usage: manage.py initpiperun [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] runname Create the pipeline run folder structure to run a pipeline instance positional arguments: runname Name of the pipeline run. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. The command yields the following folder structure: (pipeline_env)$ ./manage.py initpiperun my_pipe_run Output: 2020-02-27 23:04:33,344 initpiperun INFO creating pipeline run folder 2020-02-27 23:04:33,344 initpiperun INFO copying default config in pipeline run folder 2020-02-27 23:04:33,344 initpiperun INFO pipeline run initialisation successful! Please modify the \"config.yaml\" restorepiperun \u00b6 Details on the add images feature can be found here . It allows for a pipeline run that has had an image added to the run to be restored to the state it was in before the image addition was made. By default the command will ask for confirmation that the run is to be restored (the option --no-confirm skips this). ./manage.py restorepiperun --help usage: manage.py restorepiperun [-h] [--no-confirm] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperuns [piperuns ...] Restore a pipeline run to the previous person after image add mode has been used. positional arguments: piperuns Name or path of pipeline run(s) to restore. optional arguments: -h, --help show this help message and exit --no-confirm Flag to skip the confirmation stage and proceed to restore the pipeline run. --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. (pipeline_env)$ ./manage.py restorepiperun path/to/my_pipe_run # or (pipeline_env)$ ./manage.py restorepiperun my_pipe_run Example usage: (pipeline_env) $ ./manage.py restorepiperun docs_example_run 2021-04-02 21:24:20,497 restorepiperun INFO Will restore the run to the following config: run: path: /Users/obrienan/sandbox/vast-pipeline-dirs/pipeline-runs/docs_example_run suppress_astropy_warnings: yes inputs: image: 1: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout.fits 2: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout.fits 3: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout.fits 4: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout.fits 5: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout.fits 6: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout.fits 7: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout.fits selavy: 1: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout.components.txt 2: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout.components.txt 3: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout.components.txt 4: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout.components.txt 5: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout.components.txt 6: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout.components.txt 7: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout.components.txt noise: 1: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout_rms.fits 2: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout_rms.fits 3: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout_rms.fits 4: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout_rms.fits 5: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout_rms.fits 6: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout_rms.fits 7: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout_rms.fits source_monitoring: monitor: no min_sigma: 3.0 edge_buffer_scale: 1.2 cluster_threshold: 3.0 allow_nan: no source_association: method: basic radius: 10.0 deruiter_radius: 5.68 deruiter_beamwidth_limit: 1.5 parallel: no epoch_duplicate_radius: 2.5 new_sources: min_sigma: 5.0 measurements: source_finder: selavy flux_fractional_error: 0.0 condon_errors: yes selavy_local_rms_fill_value: 0.2 write_arrow_files: no ra_uncertainty: 1.0 dec_uncertainty: 1.0 variability: source_aggregate_pair_metrics_min_abs_vs: 4.3 Would you like to restore the run ? (y/n): y 2021-04-02 21:24:28,685 restorepiperun INFO Restoring 'docs_example_run' from backup parquet files. 2021-04-02 21:24:29,602 restorepiperun INFO Deleting new sources and associated objects to restore run Total objects deleted: 433 2021-04-02 21:24:29,624 restorepiperun INFO Restoring metrics for 461 sources. 2021-04-02 21:24:29,663 restorepiperun INFO Removing 7 images from the run. 2021-04-02 21:24:29,754 restorepiperun INFO Deleting associations to restore run. Total objects deleted: 846 2021-04-02 21:24:29,979 restorepiperun INFO Deleting measurement pairs to restore run. Total objects deleted: 4212 2021-04-02 21:24:29,981 restorepiperun INFO Restoring run metrics. 2021-04-02 21:24:29,990 restorepiperun INFO Restoring parquet files and removing .bak files. 2021-04-02 21:24:29,995 restorepiperun INFO Restore complete. runpipeline \u00b6 The pipeline is run using runpipeline django command. (pipeline_env)$ ./manage.py runpipeline --help Output: usage: manage.py runpipeline [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperun Process the pipeline for a list of images and Selavy catalogs positional arguments: piperun Path or name of the pipeline run. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. General usage: (pipeline_env)$ ./manage.py runpipeline path/to/my_pipe_run","title":"CLI"},{"location":"adminusage/cli/#command-line-interface-cli","text":"This section describes the commands available to the administrators of the pipelines.","title":"Command Line Interface (CLI)"},{"location":"adminusage/cli/#pipeline-usage","text":"All the pipeline commands are run using the Django global ./manage.py <command> interface. Therefore you need to activate the Python environment. You can have a look at the available commands for the pipeline app: (pipeline_env)$ ./manage.py help Output: ... [vast_pipeline] clearpiperun createmeasarrow debugrun ingestimages initingest initpiperun restorepiperun runpipeline ... There are 8 commands, described in detail below.","title":"Pipeline Usage"},{"location":"adminusage/cli/#clearpiperun","text":"Resetting a pipeline run can be done using the clearpiperun command. This will delete all images and related objects such as sources associated with that pipeline run. Images that have been used in other pipeline runs will not be deleted. ./manage.py clearpiperun --help usage: manage.py clearpiperun [-h] [--keep-parquet] [--remove-all] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperuns [piperuns ...] Delete a pipeline run and all related images, sources, etc. Will not delete objects if they are also related to another pipeline run. positional arguments: piperuns Name or path of pipeline run(s) to delete. Pass \"clearall\" to delete all the runs. optional arguments: -h, --help show this help message and exit --keep-parquet Flag to keep the pipeline run(s) parquet files. Will also apply to arrow files if present. --remove-all Flag to remove all the content of the pipeline run(s) folder. --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. Example usage: (pipeline_env)$ ./manage.py clearpiperun path/to/my_pipe_run # or (pipeline_env)$ ./manage.py clearpiperun my_pipe_run Tip Further information on clearing a specific run, or resetting the database, can be found in the Contributing and Developing section.","title":"clearpiperun"},{"location":"adminusage/cli/#createmeasarrow","text":"This command allows for the creation of the measurements.arrow and measurement_pairs.arrow files after a run has been successfully completed. See Arrow Files for more information. ./manage.py createmeasarrow --help usage: manage.py createmeasarrow [-h] [--overwrite] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperun Create `measurements.arrow` and `measurement_pairs.arrow` files for a completed pipeline run. positional arguments: piperun Path or name of the pipeline run. optional arguments: -h, --help show this help message and exit --overwrite Overwrite previous 'measurements.arrow' file. --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the co Example usage: ./manage.py createmeasarrow docs_example_run 2021-03-30 10:48:40,952 createmeasarrow INFO Creating measurements arrow file for 'docs_example_run'. 2021-03-30 10:48:40,952 utils INFO Creating measurements.arrow for run docs_example_run. 2021-03-30 10:48:41,829 createmeasarrow INFO Creating measurement pairs arrow file for 'docs_example_run'. 2021-03-30 10:48:41,829 utils INFO Creating measurement_pairs.arrow for run docs_example_run.","title":"createmeasarrow"},{"location":"adminusage/cli/#debugrun","text":"The debugrun command is used to print out a summary of the pipeline run to the terminal. A single pipeline run can be entered as an argument or all can be entered to print the statistics of all the pipeline runs in the database. ./manage.py debugrun --help usage: manage.py debugrun [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperuns [piperuns ...] Print out total metrics such as nr of measurements for runs positional arguments: piperuns Name or path of pipeline run(s) to debug.Pass \"all\" to print summary data of all the runs. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. Example usage: ./manage.py debugrun docs_example_run * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Printing summary data of pipeline run \"docs_example_run\" Nr of images: 14 Nr of measurements: 4312 Nr of forced measurements: 2156 Nr of sources: 557 Nr of association: 3276 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *","title":"debugrun"},{"location":"adminusage/cli/#ingestimages","text":"This command runs the first part of the pipeline only. It ingests/adds a set of images, and their measurements, to the database. It requires an image ingestion configuration file as input. A template ingest configuration file can be generated with the initingest command (below). (pipeline_env)$ ./manage.py ingestimages --help Output: usage: manage.py ingestimages [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] image_ingest_config Ingest/add a set of images to the database positional arguments: image_ingest_config Image ingestion configuration filename/path. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. General usage: (pipeline_env)$ ./manage.py ingestimages ./ingest_config.yml Output: 2021-06-25 03:08:44,313 loading INFO Reading image epoch01.fits ... 2021-06-25 03:08:44,348 utils INFO Adding new frequency band: 888 2021-06-25 03:08:44,390 utils INFO Created sky region 150.001, -30.001 2021-06-25 03:08:44,441 loading INFO Processed measurements dataframe of shape: (4, 40) 2021-06-25 03:08:44,452 loading INFO Bulk created #4 Measurement 2021-06-25 03:08:44,504 loading INFO Reading image epoch02.fits ... ... 2021-06-25 03:08:44,731 loading INFO Reading image epoch04.fits ... 2021-06-25 03:08:44,771 utils INFO Created sky region 150.021, -30.017 2021-06-25 03:08:44,805 loading INFO Processed measurements dataframe of shape: (5, 40) 2021-06-25 03:08:44,810 loading INFO Bulk created #5 Measurement 2021-06-25 03:08:44,819 loading INFO Total images upload/loading time: 0.97 seconds","title":"ingestimages"},{"location":"adminusage/cli/#initingest","text":"This command generates a template configuration file for use with the ingestimages command. (pipeline_env)$ ./manage.py initingest --help Output: usage: manage.py initingest [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] config_file_name Create a template image ingestion configuration file positional arguments: config_file_name Filename to write template ingest configuration to. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. General usage: (pipeline_env)$ ./manage.py initingest ingest_config.yml Output: Writing template to: ingest_config.yml Then modify ingest_config.yml to suit your needs.","title":"initingest"},{"location":"adminusage/cli/#initpiperun","text":"In order to process the images in the pipeline, you must create/initialise a pipeline run first. The pipeline run creation is done using the initpiperun django command, which requires a pipeline run folder. The command creates a folder with the pipeline run name under the settings PROJECT_WORKING_DIR defined in settings . (pipeline_env)$ ./manage.py initpiperun --help Output: usage: manage.py initpiperun [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] runname Create the pipeline run folder structure to run a pipeline instance positional arguments: runname Name of the pipeline run. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. The command yields the following folder structure: (pipeline_env)$ ./manage.py initpiperun my_pipe_run Output: 2020-02-27 23:04:33,344 initpiperun INFO creating pipeline run folder 2020-02-27 23:04:33,344 initpiperun INFO copying default config in pipeline run folder 2020-02-27 23:04:33,344 initpiperun INFO pipeline run initialisation successful! Please modify the \"config.yaml\"","title":"initpiperun"},{"location":"adminusage/cli/#restorepiperun","text":"Details on the add images feature can be found here . It allows for a pipeline run that has had an image added to the run to be restored to the state it was in before the image addition was made. By default the command will ask for confirmation that the run is to be restored (the option --no-confirm skips this). ./manage.py restorepiperun --help usage: manage.py restorepiperun [-h] [--no-confirm] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperuns [piperuns ...] Restore a pipeline run to the previous person after image add mode has been used. positional arguments: piperuns Name or path of pipeline run(s) to restore. optional arguments: -h, --help show this help message and exit --no-confirm Flag to skip the confirmation stage and proceed to restore the pipeline run. --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. (pipeline_env)$ ./manage.py restorepiperun path/to/my_pipe_run # or (pipeline_env)$ ./manage.py restorepiperun my_pipe_run Example usage: (pipeline_env) $ ./manage.py restorepiperun docs_example_run 2021-04-02 21:24:20,497 restorepiperun INFO Will restore the run to the following config: run: path: /Users/obrienan/sandbox/vast-pipeline-dirs/pipeline-runs/docs_example_run suppress_astropy_warnings: yes inputs: image: 1: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout.fits 2: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout.fits 3: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout.fits 4: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout.fits 5: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout.fits 6: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout.fits 7: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout.fits selavy: 1: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout.components.txt 2: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout.components.txt 3: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout.components.txt 4: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout.components.txt 5: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout.components.txt 6: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout.components.txt 7: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout.components.txt noise: 1: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_0127-73A.EPOCH01.I.cutout_rms.fits 2: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118+00A.EPOCH01.I.cutout_rms.fits 3: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH01/VAST_2118-06A.EPOCH01.I.cutout_rms.fits 4: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118+00A.EPOCH02.I.cutout_rms.fits 5: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH02/VAST_2118-06A.EPOCH02.I.cutout_rms.fits 6: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118+00A.EPOCH03x.I.cutout_rms.fits 7: - /Users/obrienan/sandbox/vast-pipeline-dirs/raw-images/regression-data/EPOCH03x/VAST_2118-06A.EPOCH03x.I.cutout_rms.fits source_monitoring: monitor: no min_sigma: 3.0 edge_buffer_scale: 1.2 cluster_threshold: 3.0 allow_nan: no source_association: method: basic radius: 10.0 deruiter_radius: 5.68 deruiter_beamwidth_limit: 1.5 parallel: no epoch_duplicate_radius: 2.5 new_sources: min_sigma: 5.0 measurements: source_finder: selavy flux_fractional_error: 0.0 condon_errors: yes selavy_local_rms_fill_value: 0.2 write_arrow_files: no ra_uncertainty: 1.0 dec_uncertainty: 1.0 variability: source_aggregate_pair_metrics_min_abs_vs: 4.3 Would you like to restore the run ? (y/n): y 2021-04-02 21:24:28,685 restorepiperun INFO Restoring 'docs_example_run' from backup parquet files. 2021-04-02 21:24:29,602 restorepiperun INFO Deleting new sources and associated objects to restore run Total objects deleted: 433 2021-04-02 21:24:29,624 restorepiperun INFO Restoring metrics for 461 sources. 2021-04-02 21:24:29,663 restorepiperun INFO Removing 7 images from the run. 2021-04-02 21:24:29,754 restorepiperun INFO Deleting associations to restore run. Total objects deleted: 846 2021-04-02 21:24:29,979 restorepiperun INFO Deleting measurement pairs to restore run. Total objects deleted: 4212 2021-04-02 21:24:29,981 restorepiperun INFO Restoring run metrics. 2021-04-02 21:24:29,990 restorepiperun INFO Restoring parquet files and removing .bak files. 2021-04-02 21:24:29,995 restorepiperun INFO Restore complete.","title":"restorepiperun"},{"location":"adminusage/cli/#runpipeline","text":"The pipeline is run using runpipeline django command. (pipeline_env)$ ./manage.py runpipeline --help Output: usage: manage.py runpipeline [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks] piperun Process the pipeline for a list of images and Selavy catalogs positional arguments: piperun Path or name of the pipeline run. optional arguments: -h, --help show this help message and exit --version show program's version number and exit -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output --settings SETTINGS The Python path to a settings module, e.g. \"myproject.settings.main\". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used. --pythonpath PYTHONPATH A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\". --traceback Raise on CommandError exceptions --no-color Don't colorize the command output. --force-color Force colorization of the command output. --skip-checks Skip system checks. General usage: (pipeline_env)$ ./manage.py runpipeline path/to/my_pipe_run","title":"runpipeline"},{"location":"adminusage/ingestimages/","text":"Standalone Image Ingestion \u00b6 Administrators have the added option of ingesting new images via the command line, without running the full pipeline. Note Currently, when non-admin users run the pipeline, any images listed in the configuration that do not exist in the database will be processed and uploaded to the database. Hence, it is not a requirement to ingest images using this process before a run is processed that uses new images. The relevant command is ingestimages , which requires and ingest configuration file as input. The configuration file must contain the images to be listed, as well as a subset of options from the main run configuration file. In particular, along with the images to be ingested, the configuration requires the following options: measurements.condon_errors Boolean. Calculate the Condon errors of the extractions when read in from the source extraction file. If False then the errors directly from the source finder output are used. Recommended to set to True for selavy extractions. Defaults to True . measurements.selavy_local_rms_fill_value Float. Value to substitute for the local_rms parameter in selavy extractions if a 0.0 value is found. Unit is mJy. Defaults to 0.2 . measurements.ra_uncertainty Float. Defines an uncertainty error to the RA that will be added in quadrature to the existing source extraction error. Used to represent a systematic positional error. Unit is arcseconds. Defaults to 1.0. measurements.dec_uncertainty Float. Defines an uncertainty error to the Dec that will be added in quadrature to the existing source extraction error. Used to represent systematic positional error. Unit is arcseconds. Defaults to 1.0. ingest_config.yaml # This file is used for ingest only mode # It specifies the images to be processed and the relevant # settings to be used. inputs : # NOTE: all the inputs must match with each other, i.e. the catalogue for the first # input image (inputs.image[0]) must be the first input catalogue (inputs.selavy[0]) # and so on. image : # list input images here, e.g. (note the leading hyphens) glob : ./data/epoch0?.fits selavy : # list input selavy catalogues here, as above with the images glob : ./data/epoch0?.selavy.components.txt noise : # list input noise (rms) images here, as above with the images glob : ./data/epoch0?.noiseMap.fits # Required only if source_monitoring.monitor is true, otherwise optional. If not providing # background images, remove the entire background section below. # background: # list input background images here, as above with the images measurements : # Replace the selavy errors with Condon (1997) errors. condon_errors : True # Sometimes the local rms for a source is reported as 0 by selavy. # Choose a value to use for the local rms in these cases in mJy/beam. selavy_local_rms_fill_value : 0.2 # The positional uncertainty of a measurement is in reality the fitting errors and the # astrometric uncertainty of the image/survey/instrument combined in quadrature. # These two parameters are the astrometric uncertainty in RA/Dec and they may be different. ra_uncertainty : 1 # arcsec dec_uncertainty : 1 # arcsec","title":"Standalone Image Ingestion"},{"location":"adminusage/ingestimages/#standalone-image-ingestion","text":"Administrators have the added option of ingesting new images via the command line, without running the full pipeline. Note Currently, when non-admin users run the pipeline, any images listed in the configuration that do not exist in the database will be processed and uploaded to the database. Hence, it is not a requirement to ingest images using this process before a run is processed that uses new images. The relevant command is ingestimages , which requires and ingest configuration file as input. The configuration file must contain the images to be listed, as well as a subset of options from the main run configuration file. In particular, along with the images to be ingested, the configuration requires the following options: measurements.condon_errors Boolean. Calculate the Condon errors of the extractions when read in from the source extraction file. If False then the errors directly from the source finder output are used. Recommended to set to True for selavy extractions. Defaults to True . measurements.selavy_local_rms_fill_value Float. Value to substitute for the local_rms parameter in selavy extractions if a 0.0 value is found. Unit is mJy. Defaults to 0.2 . measurements.ra_uncertainty Float. Defines an uncertainty error to the RA that will be added in quadrature to the existing source extraction error. Used to represent a systematic positional error. Unit is arcseconds. Defaults to 1.0. measurements.dec_uncertainty Float. Defines an uncertainty error to the Dec that will be added in quadrature to the existing source extraction error. Used to represent systematic positional error. Unit is arcseconds. Defaults to 1.0. ingest_config.yaml # This file is used for ingest only mode # It specifies the images to be processed and the relevant # settings to be used. inputs : # NOTE: all the inputs must match with each other, i.e. the catalogue for the first # input image (inputs.image[0]) must be the first input catalogue (inputs.selavy[0]) # and so on. image : # list input images here, e.g. (note the leading hyphens) glob : ./data/epoch0?.fits selavy : # list input selavy catalogues here, as above with the images glob : ./data/epoch0?.selavy.components.txt noise : # list input noise (rms) images here, as above with the images glob : ./data/epoch0?.noiseMap.fits # Required only if source_monitoring.monitor is true, otherwise optional. If not providing # background images, remove the entire background section below. # background: # list input background images here, as above with the images measurements : # Replace the selavy errors with Condon (1997) errors. condon_errors : True # Sometimes the local rms for a source is reported as 0 by selavy. # Choose a value to use for the local rms in these cases in mJy/beam. selavy_local_rms_fill_value : 0.2 # The positional uncertainty of a measurement is in reality the fitting errors and the # astrometric uncertainty of the image/survey/instrument combined in quadrature. # These two parameters are the astrometric uncertainty in RA/Dec and they may be different. ra_uncertainty : 1 # arcsec dec_uncertainty : 1 # arcsec","title":"Standalone Image Ingestion"},{"location":"architecture/database/","text":"Database Schema \u00b6 This section describes the relationships between the objects/tables stored in the database. Django Web App Schema \u00b6 The following figure shows a detailed schematics of the schema and relationships as well as tables parameters of the Django App. Pipeline Detailed Schema \u00b6 A focussed view of the pipeline schema is shown below: Important points \u00b6 Some of the key points of the above relationship diagram are: each image object is indipendent from the others and can belong to multiple pipeline runs to avoid duplication. An image can belong to multiple pipeline run objects and a run object can have multiple images. If a user want to upload an image object with different characteristic (i.e. using a custom source extraction tool), is free to do so but the image name need to be unique . So we suggest to assign a custom name to your image files. Each image is linked to a set of source measurement objects by means of a foreign key. Therefore those objects can belong to multiple source objects. A source object can have multiple measurements and a measurements can belong to multiple source objects. The pipeline schema has been mainly designed to allow for completely disjoint run objects so that each users can run their own processing with their specific settings, defined in the configuration file.","title":"Database Schema"},{"location":"architecture/database/#database-schema","text":"This section describes the relationships between the objects/tables stored in the database.","title":"Database Schema"},{"location":"architecture/database/#django-web-app-schema","text":"The following figure shows a detailed schematics of the schema and relationships as well as tables parameters of the Django App.","title":"Django Web App Schema"},{"location":"architecture/database/#pipeline-detailed-schema","text":"A focussed view of the pipeline schema is shown below:","title":"Pipeline Detailed Schema"},{"location":"architecture/database/#important-points","text":"Some of the key points of the above relationship diagram are: each image object is indipendent from the others and can belong to multiple pipeline runs to avoid duplication. An image can belong to multiple pipeline run objects and a run object can have multiple images. If a user want to upload an image object with different characteristic (i.e. using a custom source extraction tool), is free to do so but the image name need to be unique . So we suggest to assign a custom name to your image files. Each image is linked to a set of source measurement objects by means of a foreign key. Therefore those objects can belong to multiple source objects. A source object can have multiple measurements and a measurements can belong to multiple source objects. The pipeline schema has been mainly designed to allow for completely disjoint run objects so that each users can run their own processing with their specific settings, defined in the configuration file.","title":"Important points"},{"location":"architecture/intro/","text":"VAST Pipeline Architecture \u00b6 The pipeline is essentially a Django app in which the pipeline is run as a Django admin command. The main structure of the pipeline is described in this schematics: Design Philosophy \u00b6 We design the pipeline in order to make it easy to use but at the same time powerful and fast. We decided to use the familiar Pandas Dataframe structure to wrap all the data manipulations, including the association operations, in the back-end. The Python community, as well as the research and scientific communities (including the astro-physicists) are very familiar with Pandas, and they should be able to understand, mantain and develop the code base. Usually in the \"Big Data\" world the commond tools adopted by the industry and research are Apache Hadoop and Spark . We decided to use Dask which is similar to Spark in same ways, but it integrates well with Pandas Dataframe and its syntax is quite similar to Pandas. Further it provides scalability by means of clustering and integrating with HPC (High Performance Comptuing) stacks. The pipeline code itself and the web app are integrated into one code base, for the sake of simplicity, easy to develop using one central repository. The user can still run the pipeline via CLI (Command Line Interface), using Django Admin Commands , as well as thorugh the web app itself. The integration avoid duplication in code, especially on regards the declaration of the schema in the ORM (Object Relational Mapping), and add user and permission management on the underlyng data, through the in-built functionality of Django framework. The front-end is built in simple HTML, CSS and Javascript using a freely available Bootstrap 4 template. The developers know best practices in the web development are focusing mostly on single page applications using framework such as ReactJS and AngularJS . The choice of using just the basic web stack (HTML + CSS + JS) was driven by the fact that future developers do not need to learn modern web frameworks such as React and Angular, but the fundamental web programming which is still the core of those tools. Technology Stack \u00b6 Back-End \u00b6 Astropy 4+ Astroquery 0.4+ Bokeh 2+ Dask 2+ Django 3+ Django Rest Framework Rest Framework Datatables Django Q Python Social Auth - Django Django Crispy Forms Django Tagulous Pandas 1+ Python 3.7+ Pyarrow 0.17+ Postgres 10+ Q3C Front-End \u00b6 Aladin Lite Bokeh Bootstrap 4 DataTables D3 Celestial Jquery JS9 ParticleJS PrismJS SB Admin 2 template Additional \u00b6 Docker node 12+ npm 6+ gulp 4+ GitHub Actions","title":"Architecture Overview"},{"location":"architecture/intro/#vast-pipeline-architecture","text":"The pipeline is essentially a Django app in which the pipeline is run as a Django admin command. The main structure of the pipeline is described in this schematics:","title":"VAST Pipeline Architecture"},{"location":"architecture/intro/#design-philosophy","text":"We design the pipeline in order to make it easy to use but at the same time powerful and fast. We decided to use the familiar Pandas Dataframe structure to wrap all the data manipulations, including the association operations, in the back-end. The Python community, as well as the research and scientific communities (including the astro-physicists) are very familiar with Pandas, and they should be able to understand, mantain and develop the code base. Usually in the \"Big Data\" world the commond tools adopted by the industry and research are Apache Hadoop and Spark . We decided to use Dask which is similar to Spark in same ways, but it integrates well with Pandas Dataframe and its syntax is quite similar to Pandas. Further it provides scalability by means of clustering and integrating with HPC (High Performance Comptuing) stacks. The pipeline code itself and the web app are integrated into one code base, for the sake of simplicity, easy to develop using one central repository. The user can still run the pipeline via CLI (Command Line Interface), using Django Admin Commands , as well as thorugh the web app itself. The integration avoid duplication in code, especially on regards the declaration of the schema in the ORM (Object Relational Mapping), and add user and permission management on the underlyng data, through the in-built functionality of Django framework. The front-end is built in simple HTML, CSS and Javascript using a freely available Bootstrap 4 template. The developers know best practices in the web development are focusing mostly on single page applications using framework such as ReactJS and AngularJS . The choice of using just the basic web stack (HTML + CSS + JS) was driven by the fact that future developers do not need to learn modern web frameworks such as React and Angular, but the fundamental web programming which is still the core of those tools.","title":"Design Philosophy"},{"location":"architecture/intro/#technology-stack","text":"","title":"Technology Stack"},{"location":"architecture/intro/#back-end","text":"Astropy 4+ Astroquery 0.4+ Bokeh 2+ Dask 2+ Django 3+ Django Rest Framework Rest Framework Datatables Django Q Python Social Auth - Django Django Crispy Forms Django Tagulous Pandas 1+ Python 3.7+ Pyarrow 0.17+ Postgres 10+ Q3C","title":"Back-End"},{"location":"architecture/intro/#front-end","text":"Aladin Lite Bokeh Bootstrap 4 DataTables D3 Celestial Jquery JS9 ParticleJS PrismJS SB Admin 2 template","title":"Front-End"},{"location":"architecture/intro/#additional","text":"Docker node 12+ npm 6+ gulp 4+ GitHub Actions","title":"Additional"},{"location":"design/association/","text":"Source Association \u00b6 This page details the association stage of a pipeline run. There are three association methods available which are summarised in the table below, and detailed in the following sections. Tip For complex fields and large surveys the De Ruiter method is recommended. Method Fixed Assoc. Radius Astropy function Possible Relation Types Basic Yes match_coordinates_sky one-to-many Advanced Yes search_around_sky many-to-many, many-to-one, one-to-many de Ruiter (TraP) No search_around_sky many-to-many, many-to-one, one-to-many General Association Notes \u00b6 Terminology \u00b6 During association, measurements are associated into unique sources . Association Process \u00b6 By default, association is performed on an image-by-image basis, ordered by the observational date. The only time this isn't the case is when Epoch Based Association is used. Note Epoch Based Association is not an association method, rather it changes how the measurements are handled when passed to one of the three methods for association. Weighted Average Coordinates \u00b6 After every iteration of each association method, the average RA and Dec, weighted by the positional uncertainty, are calculated for each source. These weighted averages are then used as the base catalogue for the next association iteration. In other words, as the measurements are associated, new measurements are associated against the weighted average of the sources identified to that point in the process. Sources positions are reported using the weighted averages. Association Methods \u00b6 Tip For a better understanding on the underlying process, see this page in the astropy documentation for examples on matching catalogues. Basic \u00b6 The most basic association method uses the astropy match_coordinates_sky function which: Associates measurements using only the nearest neighbour for each source when comparing catalogues. Uses a fixed association radius as a threshold for a 'match'. Only one-to-many relations are possible. Advanced \u00b6 This method uses the same process as Basic , however the astropy function search_around_sky is used instead. This means: All possible matches between the two catalogues are found, rather than only the nearest neighbour. A fixed association radius is still applied as the threshold. All types of relations are possible. de Ruiter \u00b6 The de Ruiter method is a translation of the association method used by the LOFAR Transients Pipeline (TraP) , which uses the de Ruiter radius in order to define associations. The search_around_sky astropy method is still used, but the threshold for a potential match is first limited by a beamwidth limit value which is defined in the pipeline run configuration file ( source_association.deruiter_beamwidth_limit ), such that the initial threshold separation distance is set to \\[ \\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,img}}}{2}, \\] where \\(\\theta_{\\text{bmaj,img}}\\) is the major axis of the restoring beam of the image being associated. Then, the de Ruiter radius is calculated for all potential matches which is defined as \\[ r_{i,j} = \\sqrt{ \\frac{ (\\alpha_{i} - \\alpha_{j})^{2}((\\delta_{i} + \\delta_{j})/2)}{\\sigma^{2}_{\\alpha_{i}} + \\sigma^{2}_{\\alpha_{j}}} \\\\+ \\frac{(\\delta_{i} + \\delta_{j})^{2}}{\\sigma^{2}_{\\delta_{i}} + \\sigma^{2}_{\\delta_{j}}} } \\] where \\(\\alpha_{n}\\) is the right ascension of source n, \\(\\delta_{n}\\) is its declination, and \\(\\sigma_{y}\\) represents the error on the quantity y. Matches are then identified by applying a threshold maximum value to the de Ruiter radius which is defined by the user in the pipeline run configuration file ( source_association.deruiter_radius ). All relation types are possible using this method. Epoch Based Association Note When the de Ruiter association method is used with epoch based assocation, the beamwidth limit is applied to the maximum bmaj value out of all the images included in the epoch. See the de Ruiter and Epoch Based Association section below for further details. Relations \u00b6 Situations can arise where a source is associated with more than one source in the catalogue being cross-matched (or vice versa). Internally these types of associations are called: many-to-many one-to-many many-to-one a good explanation of these situations is presented in the TraP documentation here . The VAST Pipeline follows the TraP methods in handling these types of associations, which is also detailed in the linked documentation. In short: many-to-many associations are reduced to one-to-one or one-to-many associations. one-to-many and many-to-one associations create \"forked\" unique sources. I.e. an individual datapoint can belong to two different sources. The VAST Pipeline reports the one-to-many and many-to-one associations by relating sources. A source may have one or more relations which signifies the the source could be associated with more than one other source. This often happens for complex sources with many closely packed components. A read-through of the TraP documentation is highly encouraged on this point as it contains an excellent description. Relations False Variability \u00b6 The VAST Pipeline builds associations only using the component information. What this means is that, while the island information from the selavy source finder is stored, it is not considered during the association stage. Because of this, the relation process detailed above has the potential to cause sources to appear variable, when in reality it is not the case. For an example consider the source below: In the 3rd, 7th, and 8th measurement (EPOCH02, EPOCH09, and EPOCH12), the source is detected as an island with two Gaussian components, as opposed to the one component in all other epochs. The source lightcurve shows how the flux has reduced by approximately 50% in these three epochs, which makes the source appear variable. The pipeline provides information for each source that allows for these kind of situations to be swiftly identified: the number number of measurements that contain siblings, and the number of relations. These are the columns n_sibl and n_rel , respectively, in the pipeline sources output file (refer to the Column Descriptions section). If these values are not 0 for a source then care must be taken when analysing variability. For the example source above, the values are n_sibl = 3 and n_rel = 1 . The missing flux can be seen in the lightcurve of the related source: Epoch Based Association \u00b6 The pipeline is able to associate inputs on an epoch basis. What this means is that, for example, all VAST Pilot Epoch 1 measurements are grouped together and are associated with grouped together Epoch 2 measurements, and so on. In doing this, duplicate measurements from within the same epoch are cut with the measurement kept being that which is closest to the centre of its respective image. The separation distance that defines a duplicate is defined in the pipeline run configuration file ( source_association.epoch_duplicate_radius ). The mode is activated by entering the images to be processed under an extra heading in the .yaml configuration file as demonstrated below. The heading acts as the epoch 'key', hence be sure to use a string that can be ordered as the heading to maintain the correct epoch order. config.yaml inputs : image : epoch01 : - /full/path/to/image1.fits - /full/path/to/image2.fits epoch02 : - /full/path/to/image3.fits The lightcurves below show the difference between 'regular' association (top) and 'epoch based' association (lower) for a source. For large surveys where transient and variablity searches on the epoch timescale is required, using this mode can greatly speed up the association stage. Warning Epoch based association does eliminate the full time resolution of your data! The base time resolution will be between the defined epochs. de Ruiter and Epoch Based Association \u00b6 During the standard de Ruiter assoication , an initial on sky separation cut is made of \\(\\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,img}}}{2}\\) , where beamwidth limit is a value entered by the user and \\(\\theta_{\\text{bmaj,img}}\\) is the major component size of the restoring beam of the image being associated. When using epoch based association, an epoch that contains more than one image will have multiple values of \\(\\theta_{\\text{bmaj,img}}\\) to apply to the combined measurements. In this case, the maximum major axis value of all the images, \\(\\theta_{\\text{bmaj,max}}\\) , is used. Hence, the initial de Ruiter association step threshold becomes \\[ \\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,max}}}{2}. \\] Parallel Association \u00b6 When parallel association is used, the images to process are analysed and grouped into distinct patches of the sky that do not overlap. These distinct regions are then processed through the source association in parallel. It is recommended to use parallel association when your dataset covers three or more distinct patches of sky.","title":"Source Association"},{"location":"design/association/#source-association","text":"This page details the association stage of a pipeline run. There are three association methods available which are summarised in the table below, and detailed in the following sections. Tip For complex fields and large surveys the De Ruiter method is recommended. Method Fixed Assoc. Radius Astropy function Possible Relation Types Basic Yes match_coordinates_sky one-to-many Advanced Yes search_around_sky many-to-many, many-to-one, one-to-many de Ruiter (TraP) No search_around_sky many-to-many, many-to-one, one-to-many","title":"Source Association"},{"location":"design/association/#general-association-notes","text":"","title":"General Association Notes"},{"location":"design/association/#terminology","text":"During association, measurements are associated into unique sources .","title":"Terminology"},{"location":"design/association/#association-process","text":"By default, association is performed on an image-by-image basis, ordered by the observational date. The only time this isn't the case is when Epoch Based Association is used. Note Epoch Based Association is not an association method, rather it changes how the measurements are handled when passed to one of the three methods for association.","title":"Association Process"},{"location":"design/association/#weighted-average-coordinates","text":"After every iteration of each association method, the average RA and Dec, weighted by the positional uncertainty, are calculated for each source. These weighted averages are then used as the base catalogue for the next association iteration. In other words, as the measurements are associated, new measurements are associated against the weighted average of the sources identified to that point in the process. Sources positions are reported using the weighted averages.","title":"Weighted Average Coordinates"},{"location":"design/association/#association-methods","text":"Tip For a better understanding on the underlying process, see this page in the astropy documentation for examples on matching catalogues.","title":"Association Methods"},{"location":"design/association/#basic","text":"The most basic association method uses the astropy match_coordinates_sky function which: Associates measurements using only the nearest neighbour for each source when comparing catalogues. Uses a fixed association radius as a threshold for a 'match'. Only one-to-many relations are possible.","title":"Basic"},{"location":"design/association/#advanced","text":"This method uses the same process as Basic , however the astropy function search_around_sky is used instead. This means: All possible matches between the two catalogues are found, rather than only the nearest neighbour. A fixed association radius is still applied as the threshold. All types of relations are possible.","title":"Advanced"},{"location":"design/association/#de-ruiter","text":"The de Ruiter method is a translation of the association method used by the LOFAR Transients Pipeline (TraP) , which uses the de Ruiter radius in order to define associations. The search_around_sky astropy method is still used, but the threshold for a potential match is first limited by a beamwidth limit value which is defined in the pipeline run configuration file ( source_association.deruiter_beamwidth_limit ), such that the initial threshold separation distance is set to \\[ \\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,img}}}{2}, \\] where \\(\\theta_{\\text{bmaj,img}}\\) is the major axis of the restoring beam of the image being associated. Then, the de Ruiter radius is calculated for all potential matches which is defined as \\[ r_{i,j} = \\sqrt{ \\frac{ (\\alpha_{i} - \\alpha_{j})^{2}((\\delta_{i} + \\delta_{j})/2)}{\\sigma^{2}_{\\alpha_{i}} + \\sigma^{2}_{\\alpha_{j}}} \\\\+ \\frac{(\\delta_{i} + \\delta_{j})^{2}}{\\sigma^{2}_{\\delta_{i}} + \\sigma^{2}_{\\delta_{j}}} } \\] where \\(\\alpha_{n}\\) is the right ascension of source n, \\(\\delta_{n}\\) is its declination, and \\(\\sigma_{y}\\) represents the error on the quantity y. Matches are then identified by applying a threshold maximum value to the de Ruiter radius which is defined by the user in the pipeline run configuration file ( source_association.deruiter_radius ). All relation types are possible using this method. Epoch Based Association Note When the de Ruiter association method is used with epoch based assocation, the beamwidth limit is applied to the maximum bmaj value out of all the images included in the epoch. See the de Ruiter and Epoch Based Association section below for further details.","title":"de Ruiter"},{"location":"design/association/#relations","text":"Situations can arise where a source is associated with more than one source in the catalogue being cross-matched (or vice versa). Internally these types of associations are called: many-to-many one-to-many many-to-one a good explanation of these situations is presented in the TraP documentation here . The VAST Pipeline follows the TraP methods in handling these types of associations, which is also detailed in the linked documentation. In short: many-to-many associations are reduced to one-to-one or one-to-many associations. one-to-many and many-to-one associations create \"forked\" unique sources. I.e. an individual datapoint can belong to two different sources. The VAST Pipeline reports the one-to-many and many-to-one associations by relating sources. A source may have one or more relations which signifies the the source could be associated with more than one other source. This often happens for complex sources with many closely packed components. A read-through of the TraP documentation is highly encouraged on this point as it contains an excellent description.","title":"Relations"},{"location":"design/association/#relations-false-variability","text":"The VAST Pipeline builds associations only using the component information. What this means is that, while the island information from the selavy source finder is stored, it is not considered during the association stage. Because of this, the relation process detailed above has the potential to cause sources to appear variable, when in reality it is not the case. For an example consider the source below: In the 3rd, 7th, and 8th measurement (EPOCH02, EPOCH09, and EPOCH12), the source is detected as an island with two Gaussian components, as opposed to the one component in all other epochs. The source lightcurve shows how the flux has reduced by approximately 50% in these three epochs, which makes the source appear variable. The pipeline provides information for each source that allows for these kind of situations to be swiftly identified: the number number of measurements that contain siblings, and the number of relations. These are the columns n_sibl and n_rel , respectively, in the pipeline sources output file (refer to the Column Descriptions section). If these values are not 0 for a source then care must be taken when analysing variability. For the example source above, the values are n_sibl = 3 and n_rel = 1 . The missing flux can be seen in the lightcurve of the related source:","title":"Relations False Variability"},{"location":"design/association/#epoch-based-association","text":"The pipeline is able to associate inputs on an epoch basis. What this means is that, for example, all VAST Pilot Epoch 1 measurements are grouped together and are associated with grouped together Epoch 2 measurements, and so on. In doing this, duplicate measurements from within the same epoch are cut with the measurement kept being that which is closest to the centre of its respective image. The separation distance that defines a duplicate is defined in the pipeline run configuration file ( source_association.epoch_duplicate_radius ). The mode is activated by entering the images to be processed under an extra heading in the .yaml configuration file as demonstrated below. The heading acts as the epoch 'key', hence be sure to use a string that can be ordered as the heading to maintain the correct epoch order. config.yaml inputs : image : epoch01 : - /full/path/to/image1.fits - /full/path/to/image2.fits epoch02 : - /full/path/to/image3.fits The lightcurves below show the difference between 'regular' association (top) and 'epoch based' association (lower) for a source. For large surveys where transient and variablity searches on the epoch timescale is required, using this mode can greatly speed up the association stage. Warning Epoch based association does eliminate the full time resolution of your data! The base time resolution will be between the defined epochs.","title":"Epoch Based Association"},{"location":"design/association/#de-ruiter-and-epoch-based-association","text":"During the standard de Ruiter assoication , an initial on sky separation cut is made of \\(\\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,img}}}{2}\\) , where beamwidth limit is a value entered by the user and \\(\\theta_{\\text{bmaj,img}}\\) is the major component size of the restoring beam of the image being associated. When using epoch based association, an epoch that contains more than one image will have multiple values of \\(\\theta_{\\text{bmaj,img}}\\) to apply to the combined measurements. In this case, the maximum major axis value of all the images, \\(\\theta_{\\text{bmaj,max}}\\) , is used. Hence, the initial de Ruiter association step threshold becomes \\[ \\text{beamwidth limit} \\times \\frac{\\theta_{\\text{bmaj,max}}}{2}. \\]","title":"de Ruiter and Epoch Based Association"},{"location":"design/association/#parallel-association","text":"When parallel association is used, the images to process are analysed and grouped into distinct patches of the sky that do not overlap. These distinct regions are then processed through the source association in parallel. It is recommended to use parallel association when your dataset covers three or more distinct patches of sky.","title":"Parallel Association"},{"location":"design/imageingest/","text":"Image & Selavy Catalogue Ingest \u00b6 This page details the stage of the pipeline that ingests the images to be processed. When the pipeline encounters an image for the first time (in any pipeline run), the image and accompanying selavy catalogue are uploaded to the pipeline database. The portion of a pipeline log file below shows the messages for the ingestion of three images. Note Once an image is uploaded then that image is available for all other runs to use without having to re-upload. 2021-03-11-12-48-21_log.txt 2021-03-11 12:59:49,751 loading INFO Reading image VAST_0127-73A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:49,756 utils INFO Adding new frequency band: 887 2021-03-11 12:59:49,771 utils INFO Created sky region 21.838, -73.121 2021-03-11 12:59:49,775 utils INFO Adding new-test-data to sky region 21.838, -73.121 2021-03-11 12:59:50,100 loading INFO Processed measurements dataframe of shape: (203, 40) 2021-03-11 12:59:50,273 loading INFO Bulk created #203 Measurement 2021-03-11 12:59:50,334 loading INFO Reading image VAST_2118+00A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:50,345 utils INFO Created sky region 322.439, -3.987 2021-03-11 12:59:50,347 utils INFO Adding new-test-data to sky region 322.439, -3.987 2021-03-11 12:59:50,577 loading INFO Processed measurements dataframe of shape: (148, 40) 2021-03-11 12:59:50,708 loading INFO Bulk created #148 Measurement 2021-03-11 12:59:50,736 loading INFO Reading image VAST_2118-06A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:50,749 utils INFO Created sky region 322.439, -4.487 2021-03-11 12:59:50,752 utils INFO Adding new-test-data to sky region 322.439, -4.487 2021-03-11 12:59:50,977 loading INFO Processed measurements dataframe of shape: (159, 40) 2021-03-11 12:59:51,111 loading INFO Bulk created #159 Measurement Ingest Steps Summary \u00b6 The FITS file is opened and read (the header is used to obtain metadata) along with the selavy component catalogue text file. The selavy component file is cleaned for erroneous components along with the calculation of extra measurements metrics such as signal-to-noise ratio , compactness and positional uncertainties. Also, optionally, flux errors are recalculated using the Condon (1997) method. See the Selavy Measurements Processing section below for further details. Median, minimum and maximum root-mean-square (RMS) values are read from the accompanying RMS image provided by the user and these values are attached to the image. The image is also attached to a sky region and a frequency band based on its properties (see Sky Region and Frequency Band ). The cleaned measurements (selavy components) are saved to a parquet file for repeated easy access. The overall image, band and sky region information for the pipeline run are written to a parquet file. See Ingest Steps Details for further details on the steps. Uniqueness \u00b6 The image uniqueness is defined by the filename. If you wish to upload a different version of the same image, e.g. a version where different Selavy settings were used in the source extraction, then you would have to make sure the image filename was different to the previously ingested image. Ingest Steps Details \u00b6 Selavy Measurements Processing \u00b6 Cleaning \u00b6 The selavy measurements are checked for erroneous values that could cause issues with the source association. Any sources that are found to have the following properties are removed: Sources that have a peak or integrated flux value of 0. Sources that have a bmaj or bmin value of 0. Sources that have a bmaj or bmin value less than half of the respective values of the image restoring beam. In addition, components are also checked for zero values that can be corrected, where the correction values to apply are defined in either the user or overall pipeline configuration files. The field names of these zero checks are defined in the table below. Field Name Correct with Location flux_int_err FLUX_DEFAULT_MIN_ERROR settings.py flux_peak_err FLUX_DEFAULT_MIN_ERROR settings.py ra_err POS_DEFAULT_MIN_ERROR settings.py dec_err POS_DEFAULT_MIN_ERROR settings.py local_rms measurements.selavy_local_rms_fill_value config.yaml Note settings.py refers to the pipeline configuration file webinterface/settings.py which is configured by the system administrator and cannot be modified by regular users. config.yaml refers to a pipeline run configuration file which is set by the user. Condon (1997) Flux & Positional Errors \u00b6 If selected in the pipeline run configuration file, the flux and positional errors are recalculated using the Condon (1997) method. The following errors are replaced with those that are recalculated: flux_peak_err flux_int_err err_bmaj err_bmin err_pa ra_err dec_err Positional Errors (de Ruiter method) \u00b6 Firstly, the systematic astrometry error from the user pipeline run configuration file ( measurements.ra_uncertainty and measurements.dec_uncertainty ) are applied to the measurement. These values are saved as ew_sys_err and ns_sys_err . Warning Currently the systematic errors applied at the pipeline run stage are then permanently fixed to the measurements, meaning that all subsequent runs using these measurements will use the fixed astrometic error. It is recommended to leave the values to the default value of 1.0. In order to apply the TraP de Ruiter association method, some extra positional error values are calculated. Firstly the ra_err and dec_err are used to estimate the largest angular uncertainty of the measurement which is recorded as the error_radius . It is estimated by finding the largest angular separation between the measurement coordinate and every coordinate combination of \\(ra \\pm \\delta ra\\) and \\(dec \\pm \\delta dec\\) . The final uncertainties are then defined as the hypotenuse values of ew_sys_err / ns_sys_err and the error_radius . These are defined as the uncertainty_ew and uncertainty_ns , respectively. The weights of the errors are defined as \\(\\frac{1}{\\text{uncertainty_x}^{2}}\\) where x is either ew or ns . Other Metrics \u00b6 The table below defines extra metrics that are added to the measurements. Field Name Description time The image datetime applied to the measurement. snr \\(\\frac{\\text{flux_peak}}{\\text{local_rms}}\\) . compactness \\(\\frac{\\text{flux_int}}{\\text{flux_peak}}\\) . flux_int_isl_ratio \\(\\frac{\\text{flux_int}}{\\text{total_island_int_flux}}\\) . flux_peak_isl_ratio \\(\\frac{\\text{flux_peak}}{\\text{total_island_peak_flux}}\\) . Sky Region \u00b6 The pipeline defines sky regions that are used to easily find images that cover the same region of the sky. A sky region is defined by: The central coodinate. The width in both ra and dec (the physical_bmaj and physical_bmin values are used here, see Uploaded Image Information ). An extraction radius ( xtr_radius ; fov_bmin is used here, again see Uploaded Image Information ). Hence, images that cover the exact same patch of sky will be assigned to the same sky region. Frequency Band \u00b6 The image is associated to a frequency object in the pipeline that represents the observational frequency information of the image. The frequency and the bandwidth are recorded. Image RMS Values \u00b6 The median, minimum and maximum values are calculated directly from the RMS map supplied by the user as a required input. This is achieved by loading the data from the FITS file and using the respective numpy operations on the data array to obtain the values. FITS Headers Used \u00b6 The table below defines which header fields are used to read the image information. Header Field Used For DATE-OBS The date and time of the observation. TIMESYS The timezone of the date and time. DURATION Duration of the observation in seconds. STOKES Stokes parameter of the image. TELESCOP Telescope name. BMAJ Major axis size of the restoring beam. BMIN Minor axis size of the restoring beam. BPA Position angle of the restoring beam. NAXIS1 Size of the image RA axis in pixels. NAXIS2 Size of the image Dec axis in pixels. CTYPE3(or 4) Check if equal to FREQ to use for frequency information. CRVAL3(or 4) Central frequency. CDELT3(or 4) Bandwidth. RESTFREQ and RESTBW can also be used as fallback options for frequency detection. The pixel scales are obtained with astropy.wcs.utils.proj_plane_pixel_scales . Uploaded Image Information \u00b6 The table below defines what is defined and uploaded using the meta data (FITS header) and other inputs. Field Name Default Description measurements_path n/a The system path to the corresponding selavy components file (saved as a parquet file by the pipeline) polarisation I The polarisation of the image (currently only Stokes I is supported). name n/a The name of the image which is taken from the filename. path n/a The system path to the image FITS file. noise_path '' The system path to the related noise image FITS file. background_path '' The system path to the related background image FITS file. datetime n/a Observational datetime of the image. jd n/a Observational datetime of the image in Julian days format. duration 0 Duration of the observation (if found in header). Seconds. ra n/a The Right Ascension of the image pointing centre. Degrees. dec n/a The Declination of the image pointing centre. Degrees. fov_bmaj n/a The estimated major axis field-of-view value - the radius_pixels multiplied by the major axis pixel size. Degrees. fov_bmin n/a The estimated minor axis field-of-view value - the radius_pixels multiplied by the minor axis pixel size. Degrees. physical_bmaj n/a The actual major axis on-sky size - the number of pixels on the major axis multiplied by the major axis pixel size. Degrees. physical_bmin n/a The actual minor axis on-sky size - the number of pixels on the minor axis multiplied by the minor axis pixel size. Degrees. radius_pixels n/a Estimated 'diameter' of the useable image area. Pixels. beam_bmaj n/a The size of the major axis of the image restoring beam. Degrees. beam_bmin n/a The size of the minor axis of the image restoring beam. Degrees. beam_bpa n/a The position angle of the image restoring beam. Degrees East of North. rms_median n/a The median RMS value from the RMS map. mJy/beam. rms_min n/a The minimum RMS value from the RMS map (pixel value). mJy/beam. rms_max n/a The maximum RMS value from the RMS map (pixel value). mJy/beam.","title":"Image & Selavy Catalogue Ingest"},{"location":"design/imageingest/#image-selavy-catalogue-ingest","text":"This page details the stage of the pipeline that ingests the images to be processed. When the pipeline encounters an image for the first time (in any pipeline run), the image and accompanying selavy catalogue are uploaded to the pipeline database. The portion of a pipeline log file below shows the messages for the ingestion of three images. Note Once an image is uploaded then that image is available for all other runs to use without having to re-upload. 2021-03-11-12-48-21_log.txt 2021-03-11 12:59:49,751 loading INFO Reading image VAST_0127-73A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:49,756 utils INFO Adding new frequency band: 887 2021-03-11 12:59:49,771 utils INFO Created sky region 21.838, -73.121 2021-03-11 12:59:49,775 utils INFO Adding new-test-data to sky region 21.838, -73.121 2021-03-11 12:59:50,100 loading INFO Processed measurements dataframe of shape: (203, 40) 2021-03-11 12:59:50,273 loading INFO Bulk created #203 Measurement 2021-03-11 12:59:50,334 loading INFO Reading image VAST_2118+00A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:50,345 utils INFO Created sky region 322.439, -3.987 2021-03-11 12:59:50,347 utils INFO Adding new-test-data to sky region 322.439, -3.987 2021-03-11 12:59:50,577 loading INFO Processed measurements dataframe of shape: (148, 40) 2021-03-11 12:59:50,708 loading INFO Bulk created #148 Measurement 2021-03-11 12:59:50,736 loading INFO Reading image VAST_2118-06A.EPOCH01.I.cutout.fits ... 2021-03-11 12:59:50,749 utils INFO Created sky region 322.439, -4.487 2021-03-11 12:59:50,752 utils INFO Adding new-test-data to sky region 322.439, -4.487 2021-03-11 12:59:50,977 loading INFO Processed measurements dataframe of shape: (159, 40) 2021-03-11 12:59:51,111 loading INFO Bulk created #159 Measurement","title":"Image &amp; Selavy Catalogue Ingest"},{"location":"design/imageingest/#ingest-steps-summary","text":"The FITS file is opened and read (the header is used to obtain metadata) along with the selavy component catalogue text file. The selavy component file is cleaned for erroneous components along with the calculation of extra measurements metrics such as signal-to-noise ratio , compactness and positional uncertainties. Also, optionally, flux errors are recalculated using the Condon (1997) method. See the Selavy Measurements Processing section below for further details. Median, minimum and maximum root-mean-square (RMS) values are read from the accompanying RMS image provided by the user and these values are attached to the image. The image is also attached to a sky region and a frequency band based on its properties (see Sky Region and Frequency Band ). The cleaned measurements (selavy components) are saved to a parquet file for repeated easy access. The overall image, band and sky region information for the pipeline run are written to a parquet file. See Ingest Steps Details for further details on the steps.","title":"Ingest Steps Summary"},{"location":"design/imageingest/#uniqueness","text":"The image uniqueness is defined by the filename. If you wish to upload a different version of the same image, e.g. a version where different Selavy settings were used in the source extraction, then you would have to make sure the image filename was different to the previously ingested image.","title":"Uniqueness"},{"location":"design/imageingest/#ingest-steps-details","text":"","title":"Ingest Steps Details"},{"location":"design/imageingest/#selavy-measurements-processing","text":"","title":"Selavy Measurements Processing"},{"location":"design/imageingest/#cleaning","text":"The selavy measurements are checked for erroneous values that could cause issues with the source association. Any sources that are found to have the following properties are removed: Sources that have a peak or integrated flux value of 0. Sources that have a bmaj or bmin value of 0. Sources that have a bmaj or bmin value less than half of the respective values of the image restoring beam. In addition, components are also checked for zero values that can be corrected, where the correction values to apply are defined in either the user or overall pipeline configuration files. The field names of these zero checks are defined in the table below. Field Name Correct with Location flux_int_err FLUX_DEFAULT_MIN_ERROR settings.py flux_peak_err FLUX_DEFAULT_MIN_ERROR settings.py ra_err POS_DEFAULT_MIN_ERROR settings.py dec_err POS_DEFAULT_MIN_ERROR settings.py local_rms measurements.selavy_local_rms_fill_value config.yaml Note settings.py refers to the pipeline configuration file webinterface/settings.py which is configured by the system administrator and cannot be modified by regular users. config.yaml refers to a pipeline run configuration file which is set by the user.","title":"Cleaning"},{"location":"design/imageingest/#condon-1997-flux-positional-errors","text":"If selected in the pipeline run configuration file, the flux and positional errors are recalculated using the Condon (1997) method. The following errors are replaced with those that are recalculated: flux_peak_err flux_int_err err_bmaj err_bmin err_pa ra_err dec_err","title":"Condon (1997) Flux &amp; Positional Errors"},{"location":"design/imageingest/#positional-errors-de-ruiter-method","text":"Firstly, the systematic astrometry error from the user pipeline run configuration file ( measurements.ra_uncertainty and measurements.dec_uncertainty ) are applied to the measurement. These values are saved as ew_sys_err and ns_sys_err . Warning Currently the systematic errors applied at the pipeline run stage are then permanently fixed to the measurements, meaning that all subsequent runs using these measurements will use the fixed astrometic error. It is recommended to leave the values to the default value of 1.0. In order to apply the TraP de Ruiter association method, some extra positional error values are calculated. Firstly the ra_err and dec_err are used to estimate the largest angular uncertainty of the measurement which is recorded as the error_radius . It is estimated by finding the largest angular separation between the measurement coordinate and every coordinate combination of \\(ra \\pm \\delta ra\\) and \\(dec \\pm \\delta dec\\) . The final uncertainties are then defined as the hypotenuse values of ew_sys_err / ns_sys_err and the error_radius . These are defined as the uncertainty_ew and uncertainty_ns , respectively. The weights of the errors are defined as \\(\\frac{1}{\\text{uncertainty_x}^{2}}\\) where x is either ew or ns .","title":"Positional Errors (de Ruiter method)"},{"location":"design/imageingest/#other-metrics","text":"The table below defines extra metrics that are added to the measurements. Field Name Description time The image datetime applied to the measurement. snr \\(\\frac{\\text{flux_peak}}{\\text{local_rms}}\\) . compactness \\(\\frac{\\text{flux_int}}{\\text{flux_peak}}\\) . flux_int_isl_ratio \\(\\frac{\\text{flux_int}}{\\text{total_island_int_flux}}\\) . flux_peak_isl_ratio \\(\\frac{\\text{flux_peak}}{\\text{total_island_peak_flux}}\\) .","title":"Other Metrics"},{"location":"design/imageingest/#sky-region","text":"The pipeline defines sky regions that are used to easily find images that cover the same region of the sky. A sky region is defined by: The central coodinate. The width in both ra and dec (the physical_bmaj and physical_bmin values are used here, see Uploaded Image Information ). An extraction radius ( xtr_radius ; fov_bmin is used here, again see Uploaded Image Information ). Hence, images that cover the exact same patch of sky will be assigned to the same sky region.","title":"Sky Region"},{"location":"design/imageingest/#frequency-band","text":"The image is associated to a frequency object in the pipeline that represents the observational frequency information of the image. The frequency and the bandwidth are recorded.","title":"Frequency Band"},{"location":"design/imageingest/#image-rms-values","text":"The median, minimum and maximum values are calculated directly from the RMS map supplied by the user as a required input. This is achieved by loading the data from the FITS file and using the respective numpy operations on the data array to obtain the values.","title":"Image RMS Values"},{"location":"design/imageingest/#fits-headers-used","text":"The table below defines which header fields are used to read the image information. Header Field Used For DATE-OBS The date and time of the observation. TIMESYS The timezone of the date and time. DURATION Duration of the observation in seconds. STOKES Stokes parameter of the image. TELESCOP Telescope name. BMAJ Major axis size of the restoring beam. BMIN Minor axis size of the restoring beam. BPA Position angle of the restoring beam. NAXIS1 Size of the image RA axis in pixels. NAXIS2 Size of the image Dec axis in pixels. CTYPE3(or 4) Check if equal to FREQ to use for frequency information. CRVAL3(or 4) Central frequency. CDELT3(or 4) Bandwidth. RESTFREQ and RESTBW can also be used as fallback options for frequency detection. The pixel scales are obtained with astropy.wcs.utils.proj_plane_pixel_scales .","title":"FITS Headers Used"},{"location":"design/imageingest/#uploaded-image-information","text":"The table below defines what is defined and uploaded using the meta data (FITS header) and other inputs. Field Name Default Description measurements_path n/a The system path to the corresponding selavy components file (saved as a parquet file by the pipeline) polarisation I The polarisation of the image (currently only Stokes I is supported). name n/a The name of the image which is taken from the filename. path n/a The system path to the image FITS file. noise_path '' The system path to the related noise image FITS file. background_path '' The system path to the related background image FITS file. datetime n/a Observational datetime of the image. jd n/a Observational datetime of the image in Julian days format. duration 0 Duration of the observation (if found in header). Seconds. ra n/a The Right Ascension of the image pointing centre. Degrees. dec n/a The Declination of the image pointing centre. Degrees. fov_bmaj n/a The estimated major axis field-of-view value - the radius_pixels multiplied by the major axis pixel size. Degrees. fov_bmin n/a The estimated minor axis field-of-view value - the radius_pixels multiplied by the minor axis pixel size. Degrees. physical_bmaj n/a The actual major axis on-sky size - the number of pixels on the major axis multiplied by the major axis pixel size. Degrees. physical_bmin n/a The actual minor axis on-sky size - the number of pixels on the minor axis multiplied by the minor axis pixel size. Degrees. radius_pixels n/a Estimated 'diameter' of the useable image area. Pixels. beam_bmaj n/a The size of the major axis of the image restoring beam. Degrees. beam_bmin n/a The size of the minor axis of the image restoring beam. Degrees. beam_bpa n/a The position angle of the image restoring beam. Degrees East of North. rms_median n/a The median RMS value from the RMS map. mJy/beam. rms_min n/a The minimum RMS value from the RMS map (pixel value). mJy/beam. rms_max n/a The maximum RMS value from the RMS map (pixel value). mJy/beam.","title":"Uploaded Image Information"},{"location":"design/monitor/","text":"Forced Measurements \u00b6 This page details the forced measurements obtained by the pipeline. Definition \u00b6 When source_monitoring.monitor: true is set in the pipeline run configuration file, any sources that have non-detections in their lightcurve will have these measurements 'filled in' by performing forced measurements . This means that the flux at the source's current average position in the non-detection image is forcefully measured by fitting a Gaussian with the same shape as the respective image restoring beam. Forced measurements are labelled in the measurements table and parquet files by the column forced . Note Forced measurements are local to a pipeline run - they will not appear in any other pipeline run. Warning Forced measurements are not performed within 3 beamwidths of the image edge. Minimum Sigma Filter \u00b6 Before forced measurements are processed, a minimum sigma check is made to make sure that the forced measurements would provide useful information. For example, a dataset may contain an image that has significantly less sensitivity than the other images. In this case a faint source in the more sensitive images will not be expected to be seen in the less sensitive image. To avoid unnecessary computation, this source is not forcefully measured. The check is performed like that which is made in the New Sources process where the signal-to-noise ratio is calculated using the rms \\(_{min}\\) of the image it is to be extracted from. Hence, for a forced measurement to take place the following condition must be met: \\[ \\frac{f_{peak,det}}{\\text{rms}_{min,i}} > \\text{source_monitoring.min_sigma}, \\] where \\(i\\) is the image for which the measurement is to be forcefully measured. source_monitoring.min_sigma is able to be controlled by the user in the pipeline run configuration file. By default the value is set to 3.0. Tip Setting source_monitoring.min_sigma: 0.0 will ensure that all forced measurements are performed regardless of signal-to-noise. Configuration File Options \u00b6 The following options are present in the pipeline run configuration file to users along with their defaults: config.yaml source_monitoring : monitor : false min_sigma : 3.0 edge_buffer_scale : 1.2 cluster_threshold : 3.0 allow_nan : false source_monitoring.monitor turns forced measurements on ( true ) or off ( false ). source_monitoring.min_sigma controls the the minimum sigma check threshold as explained in Minimum Sigma Filter . source_monitoring.edge_buffer_scale controls the size of the buffer from the image edge where forced measurements are not performed ( source_monitoring.edge_buffer_scale \\(\\times 3\\theta_{beam}\\) ). An error can sometimes occur that increasing this value can solve. source_monitoring.cluster_threshold is directly passed to the forced photometry package used by the pipeline. It defines the multiple of major_axes to use for identifying clusters. source_monitoring.allow_nan is directly passed to the forced photometry package used by the pipeline. It defines whether NaN values are allowed to be present in the extraction area in the rms or background maps, i.e. true would mean that NaN values are allowed. Software - forced_phot \u00b6 The software used to perform the forced measurements, forced_phot , was written by David Kaplan and can be found on the VAST GitHub here .","title":"Forced Measurements"},{"location":"design/monitor/#forced-measurements","text":"This page details the forced measurements obtained by the pipeline.","title":"Forced Measurements"},{"location":"design/monitor/#definition","text":"When source_monitoring.monitor: true is set in the pipeline run configuration file, any sources that have non-detections in their lightcurve will have these measurements 'filled in' by performing forced measurements . This means that the flux at the source's current average position in the non-detection image is forcefully measured by fitting a Gaussian with the same shape as the respective image restoring beam. Forced measurements are labelled in the measurements table and parquet files by the column forced . Note Forced measurements are local to a pipeline run - they will not appear in any other pipeline run. Warning Forced measurements are not performed within 3 beamwidths of the image edge.","title":"Definition"},{"location":"design/monitor/#minimum-sigma-filter","text":"Before forced measurements are processed, a minimum sigma check is made to make sure that the forced measurements would provide useful information. For example, a dataset may contain an image that has significantly less sensitivity than the other images. In this case a faint source in the more sensitive images will not be expected to be seen in the less sensitive image. To avoid unnecessary computation, this source is not forcefully measured. The check is performed like that which is made in the New Sources process where the signal-to-noise ratio is calculated using the rms \\(_{min}\\) of the image it is to be extracted from. Hence, for a forced measurement to take place the following condition must be met: \\[ \\frac{f_{peak,det}}{\\text{rms}_{min,i}} > \\text{source_monitoring.min_sigma}, \\] where \\(i\\) is the image for which the measurement is to be forcefully measured. source_monitoring.min_sigma is able to be controlled by the user in the pipeline run configuration file. By default the value is set to 3.0. Tip Setting source_monitoring.min_sigma: 0.0 will ensure that all forced measurements are performed regardless of signal-to-noise.","title":"Minimum Sigma Filter"},{"location":"design/monitor/#configuration-file-options","text":"The following options are present in the pipeline run configuration file to users along with their defaults: config.yaml source_monitoring : monitor : false min_sigma : 3.0 edge_buffer_scale : 1.2 cluster_threshold : 3.0 allow_nan : false source_monitoring.monitor turns forced measurements on ( true ) or off ( false ). source_monitoring.min_sigma controls the the minimum sigma check threshold as explained in Minimum Sigma Filter . source_monitoring.edge_buffer_scale controls the size of the buffer from the image edge where forced measurements are not performed ( source_monitoring.edge_buffer_scale \\(\\times 3\\theta_{beam}\\) ). An error can sometimes occur that increasing this value can solve. source_monitoring.cluster_threshold is directly passed to the forced photometry package used by the pipeline. It defines the multiple of major_axes to use for identifying clusters. source_monitoring.allow_nan is directly passed to the forced photometry package used by the pipeline. It defines whether NaN values are allowed to be present in the extraction area in the rms or background maps, i.e. true would mean that NaN values are allowed.","title":"Configuration File Options"},{"location":"design/monitor/#software-forced_phot","text":"The software used to perform the forced measurements, forced_phot , was written by David Kaplan and can be found on the VAST GitHub here .","title":"Software - forced_phot"},{"location":"design/newsources/","text":"New Sources \u00b6 This page details the new source analysis performed by the pipeline. Definition \u00b6 A new source is defined as a source that is detected during the pipeline run that was not detected in any previous epoch of the location of the source, and has a peak flux such that it should have been detected. Note Remember that pipeline runs are self-contained - i.e. a run does not have any knowledge of another run, hence new sources are local to their pipeline run. New Source Process \u00b6 The pipeline identifies new sources by using the following steps: Sources are found that have 'incomplete' light curves, i.e. there are epochs in the pipeline run of the source location where the source is not detected. The would-be 'ideal' coverage is then calculated to determine which images contain the source location but have a non-detection. Remove sources where the epoch of the first detection is also the earliest possible detection epoch. For the remaining sources a general rms check is made to answer the question of should this source be expected to be detected at all in the previous epochs. This is done by taking the minimum \\(\\text{rms}_{min}\\) of all the non-detection images and making sure that \\[ \\frac{f_{peak,det}}{\\text{minimum rms}_{min}} > \\text{new_sources.min_sigma}, \\] where \\(f_{peak,det}\\) is the peak flux density of the first detection of the source. The default value of new_sources.min_sigma is 5.0, but the parameter can be controlled by the user in the pipeline run configuration file. The sources that meet the above criteria are marked as a new source . The new source high sigma value is calculated for all new sources. New Source High Sigma \u00b6 In the process detailed above, the rms check is made against the minimum rms of the previous non-detection images. This might not be an accurate representation of the rms at the source's actual location in the image, for example, the rms might be high at the source location such that a detection of the source wouldn't be expected at the \\(5\\sigma\\) level. To account for this the new source high sigma value is calculated for all new sources. For each non-detection image for a source, the true signal-to-noise ratio the source would have in the non-detection image is calculated, i.e. \\[ \\text{new source true sigma}_i = \\frac{f_{peak,det}}{\\text{rms}_{location,i}} \\] where \\(\\text{rms}_{location,i}\\) is the rms at the source location for each non-detection image, \\(i\\) . The new source high sigma is then equal to the maximum \\(\\text{rms}_{location,i}\\) . The value can be used to filter those new sources which would be borderline detections, or not expected to be detected at all, at the actual location in the previous images. This allows users to concentrate on the significant new sources. Viewing New Sources \u00b6 New sources are marked as new on the website interface (see Source Pages ) and in the source parquet file output there is a boolean column named new .","title":"New Sources"},{"location":"design/newsources/#new-sources","text":"This page details the new source analysis performed by the pipeline.","title":"New Sources"},{"location":"design/newsources/#definition","text":"A new source is defined as a source that is detected during the pipeline run that was not detected in any previous epoch of the location of the source, and has a peak flux such that it should have been detected. Note Remember that pipeline runs are self-contained - i.e. a run does not have any knowledge of another run, hence new sources are local to their pipeline run.","title":"Definition"},{"location":"design/newsources/#new-source-process","text":"The pipeline identifies new sources by using the following steps: Sources are found that have 'incomplete' light curves, i.e. there are epochs in the pipeline run of the source location where the source is not detected. The would-be 'ideal' coverage is then calculated to determine which images contain the source location but have a non-detection. Remove sources where the epoch of the first detection is also the earliest possible detection epoch. For the remaining sources a general rms check is made to answer the question of should this source be expected to be detected at all in the previous epochs. This is done by taking the minimum \\(\\text{rms}_{min}\\) of all the non-detection images and making sure that \\[ \\frac{f_{peak,det}}{\\text{minimum rms}_{min}} > \\text{new_sources.min_sigma}, \\] where \\(f_{peak,det}\\) is the peak flux density of the first detection of the source. The default value of new_sources.min_sigma is 5.0, but the parameter can be controlled by the user in the pipeline run configuration file. The sources that meet the above criteria are marked as a new source . The new source high sigma value is calculated for all new sources.","title":"New Source Process"},{"location":"design/newsources/#new-source-high-sigma","text":"In the process detailed above, the rms check is made against the minimum rms of the previous non-detection images. This might not be an accurate representation of the rms at the source's actual location in the image, for example, the rms might be high at the source location such that a detection of the source wouldn't be expected at the \\(5\\sigma\\) level. To account for this the new source high sigma value is calculated for all new sources. For each non-detection image for a source, the true signal-to-noise ratio the source would have in the non-detection image is calculated, i.e. \\[ \\text{new source true sigma}_i = \\frac{f_{peak,det}}{\\text{rms}_{location,i}} \\] where \\(\\text{rms}_{location,i}\\) is the rms at the source location for each non-detection image, \\(i\\) . The new source high sigma is then equal to the maximum \\(\\text{rms}_{location,i}\\) . The value can be used to filter those new sources which would be borderline detections, or not expected to be detected at all, at the actual location in the previous images. This allows users to concentrate on the significant new sources.","title":"New Source High Sigma"},{"location":"design/newsources/#viewing-new-sources","text":"New sources are marked as new on the website interface (see Source Pages ) and in the source parquet file output there is a boolean column named new .","title":"Viewing New Sources"},{"location":"design/overview/","text":"Pipeline Steps Overview \u00b6 This page gives an overview of the processing steps of a pipeline run. Each section contains a link to a feature page that contains more details. Terminology \u00b6 Run A single pipeline dataset defined by a configuration file. Image A FITS image that is being processed as part of a pipeline run. It also has related inputs of the selavy source catalogue, and the noise and background images also produced by selavy. Measurement An extracted measurement read from the selavy source catalogue from an associated image. The only measurements produced by the pipeline are forced measurements which are performed when monitoring is used. Source A group of measurements that have been identified as the same astrophysical source by a pipeline association method. Pipeline Processing Steps \u00b6 Note Each pipeline run is self-aware only, which means that each run does not draw on the results of other runs. However, since images and their measurements don't change, subsequent runs that use any image that was ingested as part of a previous run will not be ingested again. 1. Image & Selavy Catalogue Ingest \u00b6 Full details: Image & Selavy Catalogue Ingest . The first stage of the pipeline is to read and ingest to the database the input data that has been provided in the configuration file. This includes determing statistics about the image footprint and properties, and also importing and cleaning the associated measurements from the selavy file. The errors on the measurements can also be recalculated at this stage based upon the Condon (1997) method. Image uniqueness is determined by the filename, and once the image is ingested, it is available for other pipeline runs to use without having to re-ingest. 2. Source Association \u00b6 Full details: Source Association . Once all images and measurments have been ingested the source association step is performed, where measurements over time are associated to a unique astrophysical source. Images are arranged chronologically and association is performed on an image by image basis, or as a grouped \"epoch\" if epoch based association is used. The association is performed as per the settings entered in the run configuration file. 3. Ideal Coverage & New Source Analysis \u00b6 Full details: New Sources . With the measurements associated the sources are analysed to check for non-detections over time and whether the source should have been seen in any non-detection images. The ideal coverage calculation is also used to determine any sources that should be marked as new , i.e. a source that has appeared over time that was not detected in the first image of its location on the sky. The non-detections are then passed to the forced monitoring step. 4. Monitoring Forced Measurements \u00b6 Full details: Forced Measurements . This step is optional. The non-detections which form gaps in the lightcurves of each source are filled in by forcefully extracting a flux measurement at the location of the source. 5. Source Statistics Calculation \u00b6 Full details: Source Statistics . Statistics are calculated for each source such as the weighted average sky position, average flux values, variability metrics (including two-epoch pair metrics) and various counts. 6. Database Upload \u00b6 All the results from the pipeline run are uploaded to the database. Specifically at the end of the run the following is written to the database: Sources and their statistics. Relations between sources. Associations. For large runs this can be a substantial component of the pipeline run time. Bulk upload statements will be seen in the pipeline run log file such as these shown below: 2021-03-11-12-48-21_log.txt 2021-03-11 13:00:04,893 loading INFO Bulk created #557 Source 2021-03-11 13:00:04,910 loading INFO Populate \"related\" field of sources... 2021-03-11 13:00:04,919 loading INFO Bulk created #29 RelatedSource 2021-03-11 13:00:04,943 loading INFO Upload associations... 2021-03-11 13:00:05,650 loading INFO Bulk created #3276 Association","title":"Pipeline Steps Overview"},{"location":"design/overview/#pipeline-steps-overview","text":"This page gives an overview of the processing steps of a pipeline run. Each section contains a link to a feature page that contains more details.","title":"Pipeline Steps Overview"},{"location":"design/overview/#terminology","text":"Run A single pipeline dataset defined by a configuration file. Image A FITS image that is being processed as part of a pipeline run. It also has related inputs of the selavy source catalogue, and the noise and background images also produced by selavy. Measurement An extracted measurement read from the selavy source catalogue from an associated image. The only measurements produced by the pipeline are forced measurements which are performed when monitoring is used. Source A group of measurements that have been identified as the same astrophysical source by a pipeline association method.","title":"Terminology"},{"location":"design/overview/#pipeline-processing-steps","text":"Note Each pipeline run is self-aware only, which means that each run does not draw on the results of other runs. However, since images and their measurements don't change, subsequent runs that use any image that was ingested as part of a previous run will not be ingested again.","title":"Pipeline Processing Steps"},{"location":"design/overview/#1-image-selavy-catalogue-ingest","text":"Full details: Image & Selavy Catalogue Ingest . The first stage of the pipeline is to read and ingest to the database the input data that has been provided in the configuration file. This includes determing statistics about the image footprint and properties, and also importing and cleaning the associated measurements from the selavy file. The errors on the measurements can also be recalculated at this stage based upon the Condon (1997) method. Image uniqueness is determined by the filename, and once the image is ingested, it is available for other pipeline runs to use without having to re-ingest.","title":"1. Image &amp; Selavy Catalogue Ingest"},{"location":"design/overview/#2-source-association","text":"Full details: Source Association . Once all images and measurments have been ingested the source association step is performed, where measurements over time are associated to a unique astrophysical source. Images are arranged chronologically and association is performed on an image by image basis, or as a grouped \"epoch\" if epoch based association is used. The association is performed as per the settings entered in the run configuration file.","title":"2. Source Association"},{"location":"design/overview/#3-ideal-coverage-new-source-analysis","text":"Full details: New Sources . With the measurements associated the sources are analysed to check for non-detections over time and whether the source should have been seen in any non-detection images. The ideal coverage calculation is also used to determine any sources that should be marked as new , i.e. a source that has appeared over time that was not detected in the first image of its location on the sky. The non-detections are then passed to the forced monitoring step.","title":"3. Ideal Coverage &amp; New Source Analysis"},{"location":"design/overview/#4-monitoring-forced-measurements","text":"Full details: Forced Measurements . This step is optional. The non-detections which form gaps in the lightcurves of each source are filled in by forcefully extracting a flux measurement at the location of the source.","title":"4. Monitoring Forced Measurements"},{"location":"design/overview/#5-source-statistics-calculation","text":"Full details: Source Statistics . Statistics are calculated for each source such as the weighted average sky position, average flux values, variability metrics (including two-epoch pair metrics) and various counts.","title":"5. Source Statistics Calculation"},{"location":"design/overview/#6-database-upload","text":"All the results from the pipeline run are uploaded to the database. Specifically at the end of the run the following is written to the database: Sources and their statistics. Relations between sources. Associations. For large runs this can be a substantial component of the pipeline run time. Bulk upload statements will be seen in the pipeline run log file such as these shown below: 2021-03-11-12-48-21_log.txt 2021-03-11 13:00:04,893 loading INFO Bulk created #557 Source 2021-03-11 13:00:04,910 loading INFO Populate \"related\" field of sources... 2021-03-11 13:00:04,919 loading INFO Bulk created #29 RelatedSource 2021-03-11 13:00:04,943 loading INFO Upload associations... 2021-03-11 13:00:05,650 loading INFO Bulk created #3276 Association","title":"6. Database Upload"},{"location":"design/sourcestats/","text":"Source Statistics \u00b6 This page details the source statistics that are calculated by the pipeline. Overview \u00b6 The table below provides a summary of all the statistic and counts provided by the pipeline. See the Variability Statistics section for the table containing the variability metrics. Note Remember that all source statistics and counts are calculated from the individual measurements that are associated with the source. Parameter Includes Forced Meas. Description wavg_ra No The weighted average of the Right Ascension, degrees. wavg_dec No The weighted average of the Declination, degrees. wavg_uncertainty_ew No The weighted average uncertainty in the east-west (RA) direction, degrees. wavg_uncertainty_ns No The weighted average uncertainty in the north-south (Dec) direction, degrees. avg_flux_int Yes The average integrated flux, mJy. max_flux_int Yes The maximum integrated flux value, mJy. min_flux_int Yes The minimum integrated flux value, mJy. avg_flux_peak Yes The average peak flux, mJy/beam. max_flux_peak Yes The maximum peak flux value, mJy/beam. min_flux_peak Yes The minimum peak flux value, mJy/beam. min_flux_int_isl_ratio Yes The minimum integrated flux value island ratio (int_flux / total_isl_int_flux). min_flux_peak_isl_ratio Yes The minimum peak flux value island ratio (peak_flux / total_isl_peak_flux). avg_compactness No The average compactness of the source (compactness is defined by int_flux / peak_flux). min_snr No The minimum signal-to-noise ratio of the source. max_snr No The maximum signal-to-noise ratio of the source. n_neighbour_dist n/a On sky separation distance to the nearest neighbour within the same run, degrees (arcmin on webserver). new_high_sigma n/a The largest sigma value a new source would have if it was placed at its location in the previous images it was not detected in. See New Sources for more information. New sources only. n_meas Yes The total number of measurements associated to the source. Named Total Datapoints on the webserver. n_meas_sel No The total number of selavy measurements associated to the source. Named Selavy Datapoints on the webserver. n_meas_forced Yes The total number of forced measurements associated to the source. Named Forced Datapoints on the webserver. n_rel n/a The total number of relations the source has. See Source Association . Named Relations on the webserver. n_sibl n/a The total number measurements that has a sibling. On the webserver tables this is firstly presented as a boolean column of if the source contains measurements that have a sibling. Variability Statistics \u00b6 Below is a table describing the variability metrics of the source. See the following sections for further explanation of these metrics. Parameter Includes Forced Meas. Description v_int Yes The \\(V\\) metric for the integrated flux. v_peak Yes The \\(V\\) metric for the peak flux. eta_int Yes The \\(\\eta\\) metric for the integrated flux. eta_peak Yes The \\(\\eta\\) metric for the peak flux. vs_abs_significant_max_int Yes The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. m_abs_significant_max_int Yes The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. vs_abs_significant_max_peak Yes The \\(\\mid V_s \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. m_abs_significant_max_peak Yes The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. V and \u03b7 Metrics \u00b6 The \\(V\\) and \\(\\eta\\) metrics are the same as those used by the LOFAR Transients Pipeline (TraP) , for a complete description please refer to Swinbank et al. (2015) . In the VAST Pipeline, the metrics are calculated twice, for both the integrated and peak fluxes. \\(V\\) is the proportional flux variability of the source and is given by the ratio of the sample standard deviation ( \\(s\\) ) and mean of the flux, \\(I\\) : \\[ V = \\frac{s}{\\overline{I}} = \\frac{1}{\\overline{I}} \\sqrt{\\frac{N}{N - 1}\\left(\\overline{I^{2}}-\\overline{I}^{2}\\right)}. \\] The \\(\\eta\\) value is the significance of the variability, based on \\(\\chi^{2}\\) statistics, and is given by: \\[ \\eta = \\frac{N}{N - 1}\\left(\\overline{wI^{2}} - \\frac{\\overline{wI}^{2}}{\\overline{w}}\\right) \\] where \\(w\\) is the uncertainty ( \\(e\\) ) in \\(I\\) of a measurement, and is given by \\(w=\\frac{1}{e}\\) . Two-Epoch Metrics \u00b6 Alternative variability metrics, \\(V_s\\) and \\(m\\) , are also calculated which we refer to as the 'two-epoch metrics'. They are calculated for each unique pair of measurements assoicated with the source, with the most significant pair of values attached to the source (see section below). Please refer to Mooley et al. (2016) for further details. Note All the two-epoch pair \\(V_s\\) and \\(m\\) values for a run are saved in the output file measurement_pairs.parquet for offline analysis. \\(V_s\\) is a statistic to compare the flux densities of a source between two-epochs and is given by: \\[ V_s = \\frac{\\Delta S}{\\sigma} = \\frac{S_1 - S_2}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{2}^{2}}} \\] where \\(S\\) is the flux and \\(\\sigma\\) is the associated error. This metric is known to follow a Student-t distribution. Typically, in the literature, a source is defined as variable if this parameter is beyond the 95% confidence interval, i.e.: \\[ \\mid V_s \\mid \\geq 4.3. \\] \\(m\\) is a moduluation index variable given by: \\[ m = \\frac{\\Delta S}{\\overline{S}} \\] where \\(\\overline{S}\\) is the mean of the flux densities \\(S_1\\) and \\(S_2\\) . Typically, in the literature, the threshold for this value for a source to be considered variable is: \\[ \\mid m \\mid \\gt 0.26, \\] which equates to a variability of 30%. However the user is free to set their own level to define variablity. Significant Source Values \u00b6 The \\(V_s\\) and \\(m\\) metrics of the 'maximum signficant pair' is attached to the source. The maximum significant pair is determind by selecting the most significant \\(\\mid m \\mid\\) value given a minimum \\(V_s\\) threshold which is defined in the pipeline configuration file variability.source_aggregate_pair_metrics_min_abs_vs : config.yaml variability : # Only measurement pairs where the Vs metric exceeds this value are selected for the # aggregate pair metrics that are stored in Source objects. source_aggregate_pair_metrics_min_abs_vs : 4.3 By default this value is set to 4.3. For example, if a source with three associatied measurements gave the following pair metrics: Pair \\(\\mid V_s \\mid\\) \\(\\mid m \\mid\\) A-B 4.5 0.1 B-C 2.5 0.05 A-C 4.3 0.4 then the A-C pair metrics are attached to the source as the most significant. This can be used to quickly determine significant two-epoch variability for a source. If there are no pair values above the minimum \\(V_s\\) threshold then these values attached to the source will be 0. The measurement_pairs.parquet file can be used to manually explore the measurement pairs if one wishes to lower the threshold.","title":"Source Statistics"},{"location":"design/sourcestats/#source-statistics","text":"This page details the source statistics that are calculated by the pipeline.","title":"Source Statistics"},{"location":"design/sourcestats/#overview","text":"The table below provides a summary of all the statistic and counts provided by the pipeline. See the Variability Statistics section for the table containing the variability metrics. Note Remember that all source statistics and counts are calculated from the individual measurements that are associated with the source. Parameter Includes Forced Meas. Description wavg_ra No The weighted average of the Right Ascension, degrees. wavg_dec No The weighted average of the Declination, degrees. wavg_uncertainty_ew No The weighted average uncertainty in the east-west (RA) direction, degrees. wavg_uncertainty_ns No The weighted average uncertainty in the north-south (Dec) direction, degrees. avg_flux_int Yes The average integrated flux, mJy. max_flux_int Yes The maximum integrated flux value, mJy. min_flux_int Yes The minimum integrated flux value, mJy. avg_flux_peak Yes The average peak flux, mJy/beam. max_flux_peak Yes The maximum peak flux value, mJy/beam. min_flux_peak Yes The minimum peak flux value, mJy/beam. min_flux_int_isl_ratio Yes The minimum integrated flux value island ratio (int_flux / total_isl_int_flux). min_flux_peak_isl_ratio Yes The minimum peak flux value island ratio (peak_flux / total_isl_peak_flux). avg_compactness No The average compactness of the source (compactness is defined by int_flux / peak_flux). min_snr No The minimum signal-to-noise ratio of the source. max_snr No The maximum signal-to-noise ratio of the source. n_neighbour_dist n/a On sky separation distance to the nearest neighbour within the same run, degrees (arcmin on webserver). new_high_sigma n/a The largest sigma value a new source would have if it was placed at its location in the previous images it was not detected in. See New Sources for more information. New sources only. n_meas Yes The total number of measurements associated to the source. Named Total Datapoints on the webserver. n_meas_sel No The total number of selavy measurements associated to the source. Named Selavy Datapoints on the webserver. n_meas_forced Yes The total number of forced measurements associated to the source. Named Forced Datapoints on the webserver. n_rel n/a The total number of relations the source has. See Source Association . Named Relations on the webserver. n_sibl n/a The total number measurements that has a sibling. On the webserver tables this is firstly presented as a boolean column of if the source contains measurements that have a sibling.","title":"Overview"},{"location":"design/sourcestats/#variability-statistics","text":"Below is a table describing the variability metrics of the source. See the following sections for further explanation of these metrics. Parameter Includes Forced Meas. Description v_int Yes The \\(V\\) metric for the integrated flux. v_peak Yes The \\(V\\) metric for the peak flux. eta_int Yes The \\(\\eta\\) metric for the integrated flux. eta_peak Yes The \\(\\eta\\) metric for the peak flux. vs_abs_significant_max_int Yes The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. m_abs_significant_max_int Yes The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. vs_abs_significant_max_peak Yes The \\(\\mid V_s \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. m_abs_significant_max_peak Yes The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair.","title":"Variability Statistics"},{"location":"design/sourcestats/#v-and-metrics","text":"The \\(V\\) and \\(\\eta\\) metrics are the same as those used by the LOFAR Transients Pipeline (TraP) , for a complete description please refer to Swinbank et al. (2015) . In the VAST Pipeline, the metrics are calculated twice, for both the integrated and peak fluxes. \\(V\\) is the proportional flux variability of the source and is given by the ratio of the sample standard deviation ( \\(s\\) ) and mean of the flux, \\(I\\) : \\[ V = \\frac{s}{\\overline{I}} = \\frac{1}{\\overline{I}} \\sqrt{\\frac{N}{N - 1}\\left(\\overline{I^{2}}-\\overline{I}^{2}\\right)}. \\] The \\(\\eta\\) value is the significance of the variability, based on \\(\\chi^{2}\\) statistics, and is given by: \\[ \\eta = \\frac{N}{N - 1}\\left(\\overline{wI^{2}} - \\frac{\\overline{wI}^{2}}{\\overline{w}}\\right) \\] where \\(w\\) is the uncertainty ( \\(e\\) ) in \\(I\\) of a measurement, and is given by \\(w=\\frac{1}{e}\\) .","title":"V and \u03b7 Metrics"},{"location":"design/sourcestats/#two-epoch-metrics","text":"Alternative variability metrics, \\(V_s\\) and \\(m\\) , are also calculated which we refer to as the 'two-epoch metrics'. They are calculated for each unique pair of measurements assoicated with the source, with the most significant pair of values attached to the source (see section below). Please refer to Mooley et al. (2016) for further details. Note All the two-epoch pair \\(V_s\\) and \\(m\\) values for a run are saved in the output file measurement_pairs.parquet for offline analysis. \\(V_s\\) is a statistic to compare the flux densities of a source between two-epochs and is given by: \\[ V_s = \\frac{\\Delta S}{\\sigma} = \\frac{S_1 - S_2}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{2}^{2}}} \\] where \\(S\\) is the flux and \\(\\sigma\\) is the associated error. This metric is known to follow a Student-t distribution. Typically, in the literature, a source is defined as variable if this parameter is beyond the 95% confidence interval, i.e.: \\[ \\mid V_s \\mid \\geq 4.3. \\] \\(m\\) is a moduluation index variable given by: \\[ m = \\frac{\\Delta S}{\\overline{S}} \\] where \\(\\overline{S}\\) is the mean of the flux densities \\(S_1\\) and \\(S_2\\) . Typically, in the literature, the threshold for this value for a source to be considered variable is: \\[ \\mid m \\mid \\gt 0.26, \\] which equates to a variability of 30%. However the user is free to set their own level to define variablity.","title":"Two-Epoch Metrics"},{"location":"design/sourcestats/#significant-source-values","text":"The \\(V_s\\) and \\(m\\) metrics of the 'maximum signficant pair' is attached to the source. The maximum significant pair is determind by selecting the most significant \\(\\mid m \\mid\\) value given a minimum \\(V_s\\) threshold which is defined in the pipeline configuration file variability.source_aggregate_pair_metrics_min_abs_vs : config.yaml variability : # Only measurement pairs where the Vs metric exceeds this value are selected for the # aggregate pair metrics that are stored in Source objects. source_aggregate_pair_metrics_min_abs_vs : 4.3 By default this value is set to 4.3. For example, if a source with three associatied measurements gave the following pair metrics: Pair \\(\\mid V_s \\mid\\) \\(\\mid m \\mid\\) A-B 4.5 0.1 B-C 2.5 0.05 A-C 4.3 0.4 then the A-C pair metrics are attached to the source as the most significant. This can be used to quickly determine significant two-epoch variability for a source. If there are no pair values above the minimum \\(V_s\\) threshold then these values attached to the source will be 0. The measurement_pairs.parquet file can be used to manually explore the measurement pairs if one wishes to lower the threshold.","title":"Significant Source Values"},{"location":"developing/docsdev/","text":"Development Guidelines for Documentation \u00b6 The pipeline documentation has been developed using the python package mkdocs and the material theme . It is published as a static website using GitHub pages. Documentation development server \u00b6 Note Remember to install the development dependencies which include the modules required for the documentation. This section describes how to set up a development server to live reload your changes to the pipeline documentation. The main code of the documentation is located in the docs directory. In order to keep the repository, CHANGELOG.md , LICENSE.txt and CODE_OF_CONDUCT.md on the root path, relative soft links have been created under the docs folder: adminusage architecture changelog.md -> ../CHANGELOG.md code_of_conduct.md -> ../CODE_OF_CONDUCT.md design developing exploringwebsite faqs.md gen_doc_stubs.py gettingstarted img index.md license.md -> ../LICENSE.txt outputs reference theme using Start the development server: (pipeline_env) $ mkdocs serve Files in the docs directory are then 'watched' such that any changes will cause the mkdocs server to reload the new content. The structure of the site (see nav section) and the settings are in the mkdocs.yml in the root of the repository. Plugins and Customisations \u00b6 mkdocs and the material theme have a lot of customisations and plugins available. The more involved plugins or customisations that are in use for this documentation are detailed in the following sections. Custom theme directory \u00b6 The theme directory contains various overrides to the standard material template. This is enabled by the following line in the mkdocs.yml file: mkdocs.yml theme : custom_dir : docs/theme Custom javascript and css files are also stored in this directory and are enabled in the following mkdocs.yml file: mkdocs.yml extra_css : - theme/css/extra.css extra_javascript : - theme/js/extra.js - https://polyfill.io/v3/polyfill.min.js?features=es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js The contents of the theme directory are shown below: docs/theme \u251c\u2500\u2500 assets \u2502 \u2514\u2500\u2500 stylesheets \u2502 \u2514\u2500\u2500 overrides.css \u251c\u2500\u2500 css \u2502 \u251c\u2500\u2500 extra.css \u2502 \u2514\u2500\u2500 lightgallery.min.css \u251c\u2500\u2500 fonts \u2502 \u251c\u2500\u2500 lg.svg \u2502 \u251c\u2500\u2500 lg.ttf \u2502 \u2514\u2500\u2500 lg.woff \u251c\u2500\u2500 home.html \u251c\u2500\u2500 img \u2502 \u2514\u2500\u2500 loading.gif \u251c\u2500\u2500 js \u2502 \u251c\u2500\u2500 extra.js \u2502 \u251c\u2500\u2500 lg-zoom.js \u2502 \u2514\u2500\u2500 lightgallery.min.js \u2514\u2500\u2500 main.html Full documentation on customising the mkdocs-material theme can be found in the documentation . main.html contains edits to the main.html as described in the documentation linked to above. The other files shown are used for the custom homepage and the Lightgallery. Custom Homepage \u00b6 The homepage is overwritten by a custom stylised HTML page. This is achieved using the following files: docs \u251c\u2500\u2500 index.md \u251c\u2500\u2500 theme \u2502 \u251c\u2500\u2500 assets \u2502 \u2502 \u2514\u2500\u2500 stylesheets \u2502 \u2502 \u2514\u2500\u2500 overrides.css \u2502 \u251c\u2500\u2500 home.html \u2502 \u2514\u2500\u2500 main.html The index.md file in the main docs directory should only contain the following. index.md --- title: VAST Pipeline template: home.html --- Creation and Last Updated Dates \u00b6 Each page displays a date of creation and also a \"last updated\" date. This is done by using the plugin mkdocs-git-revision-date-localized-plugin . The following options are used: mkdocs.yml plugins : - git-revision-date-localized : fallback_to_build_date : true enable_creation_date : true Dates and Code Reference Pages The option fallback_to_build_date: true is required for the code reference pages that are auto generated by the mkdocs-gen-files plugin (see the Python Docstrings and Source Code section below). These pages will show the build date rather than the \"last updated\" date. During the build process the following warning will be seen, which is expected and ok to ignore: WARNING - [git-revision-date-localized-plugin] Unable to find a git directory and/or git is not installed. Option 'fallback_to_build_date' set to 'true': Falling back to build date Lightgallery \u00b6 Images are displayed in the documentation using the lightgallery-markdown plugin. As described in the repository linked to above, certain assets from lightgallery.js are copied over to make the extension work, the files that apply here are: docs/theme \u251c\u2500\u2500 css \u2502 \u2514\u2500\u2500 lightgallery.min.css \u251c\u2500\u2500 fonts \u2502 \u251c\u2500\u2500 lg.svg \u2502 \u251c\u2500\u2500 lg.ttf \u2502 \u2514\u2500\u2500 lg.woff \u251c\u2500\u2500 img \u2502 \u2514\u2500\u2500 loading.gif \u251c\u2500\u2500 js \u2502 \u251c\u2500\u2500 extra.js \u2502 \u251c\u2500\u2500 lg-zoom.js \u2502 \u2514\u2500\u2500 lightgallery.min.js \u2514\u2500\u2500 main.html lg-zoom.js is also copied over to activate the zoom feature on the gallery images. Note The javascript to select the lightgallery class objects and run them through the lightgallery function is placed in theme/js/extra.js such that the plugin works with instant navigation . MathJax \u00b6 MathJax is enabled as described in the mkdocs-material documentation . Python Docstrings and Source Code \u00b6 Python docstrings and source code are rendered using the mkdocstrings plugin. Docstrings Format All docstrings must be done using the Google format . The markdown files for the docstrings are automatically generated using mkdocs-gen-files in the file docs/gen_doc_stubs.py . It is activated by the following lines in the mkdocs.yml file: mkdocs.yml plugins : - search - gen-files : scripts : - docs/gen_doc_stubs.py The markdown files are programmatically generated, this means that the markdown files don't actually appear, but are virtually part of the site build. The files must still be referenced in the nav: section of mkdocs.yml , for example: mkdocs.yml - nav : - Code Reference : - vast_pipeline : - image : - main.py : reference/image/main.md - utils.py : reference/image/utils.md The files are virtually created relative to the doc path that is stated in gen_doc_stubs.py . For example, using the vast_pipeline as the base for the python files and reference as the doc path, the markdown file for the docstrings in vast_pipeline/image/main.py is created at reference/image/main.md . Please refer to the mkdocstrings documentation for full details of the options available to declare in mkdocs.yml . Versioning \u00b6 Versioning is enabled on the documentation by using the mike package. The documentation is published for each release version, along with a development version that is updated with every commit to the default dev branch. Setup of this has been completed on the initial deployment, and the GitHub workflows will automatically take care of the continual deployment. Vist the mike package page linked above for details on general management commands such as how to delete a version. Deployment to GitHub pages \u00b6 Automatic deployment to GitHub pages is set up using GitHub actions and workflows. See the workflows ci-docs-dev.yml and ci-docs-release.yml .","title":"Documentation"},{"location":"developing/docsdev/#development-guidelines-for-documentation","text":"The pipeline documentation has been developed using the python package mkdocs and the material theme . It is published as a static website using GitHub pages.","title":"Development Guidelines for Documentation"},{"location":"developing/docsdev/#documentation-development-server","text":"Note Remember to install the development dependencies which include the modules required for the documentation. This section describes how to set up a development server to live reload your changes to the pipeline documentation. The main code of the documentation is located in the docs directory. In order to keep the repository, CHANGELOG.md , LICENSE.txt and CODE_OF_CONDUCT.md on the root path, relative soft links have been created under the docs folder: adminusage architecture changelog.md -> ../CHANGELOG.md code_of_conduct.md -> ../CODE_OF_CONDUCT.md design developing exploringwebsite faqs.md gen_doc_stubs.py gettingstarted img index.md license.md -> ../LICENSE.txt outputs reference theme using Start the development server: (pipeline_env) $ mkdocs serve Files in the docs directory are then 'watched' such that any changes will cause the mkdocs server to reload the new content. The structure of the site (see nav section) and the settings are in the mkdocs.yml in the root of the repository.","title":"Documentation development server"},{"location":"developing/docsdev/#plugins-and-customisations","text":"mkdocs and the material theme have a lot of customisations and plugins available. The more involved plugins or customisations that are in use for this documentation are detailed in the following sections.","title":"Plugins and Customisations"},{"location":"developing/docsdev/#custom-theme-directory","text":"The theme directory contains various overrides to the standard material template. This is enabled by the following line in the mkdocs.yml file: mkdocs.yml theme : custom_dir : docs/theme Custom javascript and css files are also stored in this directory and are enabled in the following mkdocs.yml file: mkdocs.yml extra_css : - theme/css/extra.css extra_javascript : - theme/js/extra.js - https://polyfill.io/v3/polyfill.min.js?features=es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js The contents of the theme directory are shown below: docs/theme \u251c\u2500\u2500 assets \u2502 \u2514\u2500\u2500 stylesheets \u2502 \u2514\u2500\u2500 overrides.css \u251c\u2500\u2500 css \u2502 \u251c\u2500\u2500 extra.css \u2502 \u2514\u2500\u2500 lightgallery.min.css \u251c\u2500\u2500 fonts \u2502 \u251c\u2500\u2500 lg.svg \u2502 \u251c\u2500\u2500 lg.ttf \u2502 \u2514\u2500\u2500 lg.woff \u251c\u2500\u2500 home.html \u251c\u2500\u2500 img \u2502 \u2514\u2500\u2500 loading.gif \u251c\u2500\u2500 js \u2502 \u251c\u2500\u2500 extra.js \u2502 \u251c\u2500\u2500 lg-zoom.js \u2502 \u2514\u2500\u2500 lightgallery.min.js \u2514\u2500\u2500 main.html Full documentation on customising the mkdocs-material theme can be found in the documentation . main.html contains edits to the main.html as described in the documentation linked to above. The other files shown are used for the custom homepage and the Lightgallery.","title":"Custom theme directory"},{"location":"developing/docsdev/#custom-homepage","text":"The homepage is overwritten by a custom stylised HTML page. This is achieved using the following files: docs \u251c\u2500\u2500 index.md \u251c\u2500\u2500 theme \u2502 \u251c\u2500\u2500 assets \u2502 \u2502 \u2514\u2500\u2500 stylesheets \u2502 \u2502 \u2514\u2500\u2500 overrides.css \u2502 \u251c\u2500\u2500 home.html \u2502 \u2514\u2500\u2500 main.html The index.md file in the main docs directory should only contain the following. index.md --- title: VAST Pipeline template: home.html ---","title":"Custom Homepage"},{"location":"developing/docsdev/#creation-and-last-updated-dates","text":"Each page displays a date of creation and also a \"last updated\" date. This is done by using the plugin mkdocs-git-revision-date-localized-plugin . The following options are used: mkdocs.yml plugins : - git-revision-date-localized : fallback_to_build_date : true enable_creation_date : true Dates and Code Reference Pages The option fallback_to_build_date: true is required for the code reference pages that are auto generated by the mkdocs-gen-files plugin (see the Python Docstrings and Source Code section below). These pages will show the build date rather than the \"last updated\" date. During the build process the following warning will be seen, which is expected and ok to ignore: WARNING - [git-revision-date-localized-plugin] Unable to find a git directory and/or git is not installed. Option 'fallback_to_build_date' set to 'true': Falling back to build date","title":"Creation and Last Updated Dates"},{"location":"developing/docsdev/#lightgallery","text":"Images are displayed in the documentation using the lightgallery-markdown plugin. As described in the repository linked to above, certain assets from lightgallery.js are copied over to make the extension work, the files that apply here are: docs/theme \u251c\u2500\u2500 css \u2502 \u2514\u2500\u2500 lightgallery.min.css \u251c\u2500\u2500 fonts \u2502 \u251c\u2500\u2500 lg.svg \u2502 \u251c\u2500\u2500 lg.ttf \u2502 \u2514\u2500\u2500 lg.woff \u251c\u2500\u2500 img \u2502 \u2514\u2500\u2500 loading.gif \u251c\u2500\u2500 js \u2502 \u251c\u2500\u2500 extra.js \u2502 \u251c\u2500\u2500 lg-zoom.js \u2502 \u2514\u2500\u2500 lightgallery.min.js \u2514\u2500\u2500 main.html lg-zoom.js is also copied over to activate the zoom feature on the gallery images. Note The javascript to select the lightgallery class objects and run them through the lightgallery function is placed in theme/js/extra.js such that the plugin works with instant navigation .","title":"Lightgallery"},{"location":"developing/docsdev/#mathjax","text":"MathJax is enabled as described in the mkdocs-material documentation .","title":"MathJax"},{"location":"developing/docsdev/#python-docstrings-and-source-code","text":"Python docstrings and source code are rendered using the mkdocstrings plugin. Docstrings Format All docstrings must be done using the Google format . The markdown files for the docstrings are automatically generated using mkdocs-gen-files in the file docs/gen_doc_stubs.py . It is activated by the following lines in the mkdocs.yml file: mkdocs.yml plugins : - search - gen-files : scripts : - docs/gen_doc_stubs.py The markdown files are programmatically generated, this means that the markdown files don't actually appear, but are virtually part of the site build. The files must still be referenced in the nav: section of mkdocs.yml , for example: mkdocs.yml - nav : - Code Reference : - vast_pipeline : - image : - main.py : reference/image/main.md - utils.py : reference/image/utils.md The files are virtually created relative to the doc path that is stated in gen_doc_stubs.py . For example, using the vast_pipeline as the base for the python files and reference as the doc path, the markdown file for the docstrings in vast_pipeline/image/main.py is created at reference/image/main.md . Please refer to the mkdocstrings documentation for full details of the options available to declare in mkdocs.yml .","title":"Python Docstrings and Source Code"},{"location":"developing/docsdev/#versioning","text":"Versioning is enabled on the documentation by using the mike package. The documentation is published for each release version, along with a development version that is updated with every commit to the default dev branch. Setup of this has been completed on the initial deployment, and the GitHub workflows will automatically take care of the continual deployment. Vist the mike package page linked above for details on general management commands such as how to delete a version.","title":"Versioning"},{"location":"developing/docsdev/#deployment-to-github-pages","text":"Automatic deployment to GitHub pages is set up using GitHub actions and workflows. See the workflows ci-docs-dev.yml and ci-docs-release.yml .","title":"Deployment to GitHub pages"},{"location":"developing/github/","text":"GitHub Platform Guidelines \u00b6 This section explains how to interact with GitHub platform for opening issues, starting discussions, creating pull requests (PR), and some notes how to make a release of the pipeline if you are a maintainer of the code base. The VAST team uses the \"git flow\" branching model which we briefly summarise here. More detail can be found here . There are two main branches, both with infinite lifetimes (they are never deleted): master for stable, production-ready code that has been released, and dev for the latest reviewed updates for the next release. Other branches for bug fixes and new features are created as needed, branching off and merging back into dev . An exception to this is for critical patches for a released version called a \"hotfix\". These are branched off master and merged back into both master and dev . Branches are also created for each new release candidate, which are branched off dev and merged into master and dev when completed. See the Releases section below for more information. Issues \u00b6 An issue can be created by anyone with access to the repository. Users are encouraged to create issues for problems they encounter while using the pipeline or to request a new feature be added to the software. Issues are created by clicking the \"New issue\" button near the top-right of the issues page . When creating a new issue, please consider the following: Search for a similar issue before opening a new one by using the search box near the top of the issues page. When opening a new issue, please specify the issue type (e.g. bug, feature, etc.) and provide a detailed description with use cases when appropriate. Discussions \u00b6 GitHub repositories also have a discussions page which serves as a collaborative forum to discuss ideas and ask questions. Users are encouraged to ask general questions, or start a conversation about potential new features to the software by creating a new discussion thread on the discussions page . Note that new software features may also be requested by creating an issue, but a discussion thread is more appropriate if the details of the new feature are still yet to be determined or require wider discussion \u2013 issues can be created from discussions once a consensus is reached. Pull Requests \u00b6 Pull requests are created when a developer wishes to merge changes they have made in a branch into another branch. They enable others to review the changes and make comments. While issues typically describe in detail a specific problem or proposed feature, pull requests contain a detailed description and the required code changes for a solution to a problem or implementation of a new feature. Opening a PR \u00b6 First consider ... Search existing issues for similar problems or feature proposals. Opening an issue to describe the problem or feature before creating a PR. This will help separate problems from solutions. Steps to issue a pull request: Create a new issue on GitHub, giving it a succinct title and describe the problem. GitHub will assign an ID e.g. #123 . Create a new branch off the dev branch and name the branch based on the issue title, e.g. fix-123-some-problem (keep it short please). Make your changes. Run the test suite locally with python manage.py test vast_pipeline . See the complete guide on the test for more details. Run the webserver and check the functionality. This is important as the test suite does not currently check the web UI. Commit the changes to your branch, push it to GitHub, and open a PR for the branch. Update the CHANGELOG.md file by adding the link to your PR and briefly describing the changes. An example of the change log format can be found here Assign the review to one or more reviewers if none are assigned by default. Warning PRs not branched off dev will be rejected !. Reviewing a PR \u00b6 The guidelines to dealing with reviews and conversations on GitHub are essentially: Be nice with the review and do not offend the author of the PR: Nobody is a perfect developer or born so! The reviewers will in general mark the conversation as \"resolved\" (e.g. he/she is satisfied with the answer from the PR author). The PR author will re-request the review by clicking on the on the top right corner and might ping the reviewer on a comment if necessary with @github_name . When the PR is approved by at least one reviewer you might want to merge it to dev (you should have that privileges), unless you want to make sure that such PR is reviewed by another reviewer (e.g. you are doing big changes or important changes or you want to make sure that other person is aware/updated about the changes in that PR). Releases \u00b6 In to order to make a release, please follow these steps: Make sure that all new feature and bug fix PRs that should be part of the new release have been merged to dev . Checkout the dev branch and update it with git pull . Ensure that there are no uncommitted changes. Create a new branch off dev , naming it release-vX.Y.Z where X.Y.Z is the new version. Typically, patch version increments for bug fixes, minor version increments for new features that do not break backward compatibility with previous versions (i.e. no database schema changes), and major version increments for large changes or for changes that would break backward compatibility. Bump the version number of the Python package using Poetry, i.e. poetry version X.Y.Z . This will update the version number in pyproject.toml . Update the version in package.json and vast_pipeline/_version.py to match the new version number, then run npm install to update the package-lock.json file. Update the \"announcement bar\" in the documentation to refer to the new release. This can be found in docs/theme/main.html at line 37. Update the CHANGELOG.md by making a copy of the \"Unreleased\" heading at the top, and renaming the second one to the new version. Include a link to the release - it won't exist yet, so just follow the format of the others. After this there should be an \"Unreleased\" heading at the top, immediately followed by another heading with the new version number, which is followed by all the existing changes. Commit all the changes made above to the new branch and push it to GitHub. Open a PR to merge the new branch into master . Note that the default target branch is dev so you will need to change this to master when creating the PR. Once the PR has been reviewed and approved, merge the branch into master . This can only be done by administrators of the repository. Tag the merge commit on master with the version, i.e. git tag vX.Y.Z , then push the tag to GitHub. Warning If you merged the release branch into master with the GitHub web UI, you will need to sync that merge to your local copy and checkout master before creating the tag. You cannot create tags with the GitHub web UI. Push the tag to GitHub, i.e. git push origin vX.Y.Z . Merge the release branch into dev , resolving any conflicts. Append \"dev\" to the version numbers in pyproject.toml , package.json and vast_pipeline/_version.py , then run npm install to update package-lock.json , and commit the changes to dev . This can either be done as a new commit, or while resolving merge conflicts in the previous step, if appropriate. Create a new release on GitHub that points to the tagged commit on master.","title":"GitHub Platform"},{"location":"developing/github/#github-platform-guidelines","text":"This section explains how to interact with GitHub platform for opening issues, starting discussions, creating pull requests (PR), and some notes how to make a release of the pipeline if you are a maintainer of the code base. The VAST team uses the \"git flow\" branching model which we briefly summarise here. More detail can be found here . There are two main branches, both with infinite lifetimes (they are never deleted): master for stable, production-ready code that has been released, and dev for the latest reviewed updates for the next release. Other branches for bug fixes and new features are created as needed, branching off and merging back into dev . An exception to this is for critical patches for a released version called a \"hotfix\". These are branched off master and merged back into both master and dev . Branches are also created for each new release candidate, which are branched off dev and merged into master and dev when completed. See the Releases section below for more information.","title":"GitHub Platform Guidelines"},{"location":"developing/github/#issues","text":"An issue can be created by anyone with access to the repository. Users are encouraged to create issues for problems they encounter while using the pipeline or to request a new feature be added to the software. Issues are created by clicking the \"New issue\" button near the top-right of the issues page . When creating a new issue, please consider the following: Search for a similar issue before opening a new one by using the search box near the top of the issues page. When opening a new issue, please specify the issue type (e.g. bug, feature, etc.) and provide a detailed description with use cases when appropriate.","title":"Issues"},{"location":"developing/github/#discussions","text":"GitHub repositories also have a discussions page which serves as a collaborative forum to discuss ideas and ask questions. Users are encouraged to ask general questions, or start a conversation about potential new features to the software by creating a new discussion thread on the discussions page . Note that new software features may also be requested by creating an issue, but a discussion thread is more appropriate if the details of the new feature are still yet to be determined or require wider discussion \u2013 issues can be created from discussions once a consensus is reached.","title":"Discussions"},{"location":"developing/github/#pull-requests","text":"Pull requests are created when a developer wishes to merge changes they have made in a branch into another branch. They enable others to review the changes and make comments. While issues typically describe in detail a specific problem or proposed feature, pull requests contain a detailed description and the required code changes for a solution to a problem or implementation of a new feature.","title":"Pull Requests"},{"location":"developing/github/#opening-a-pr","text":"First consider ... Search existing issues for similar problems or feature proposals. Opening an issue to describe the problem or feature before creating a PR. This will help separate problems from solutions. Steps to issue a pull request: Create a new issue on GitHub, giving it a succinct title and describe the problem. GitHub will assign an ID e.g. #123 . Create a new branch off the dev branch and name the branch based on the issue title, e.g. fix-123-some-problem (keep it short please). Make your changes. Run the test suite locally with python manage.py test vast_pipeline . See the complete guide on the test for more details. Run the webserver and check the functionality. This is important as the test suite does not currently check the web UI. Commit the changes to your branch, push it to GitHub, and open a PR for the branch. Update the CHANGELOG.md file by adding the link to your PR and briefly describing the changes. An example of the change log format can be found here Assign the review to one or more reviewers if none are assigned by default. Warning PRs not branched off dev will be rejected !.","title":"Opening a PR"},{"location":"developing/github/#reviewing-a-pr","text":"The guidelines to dealing with reviews and conversations on GitHub are essentially: Be nice with the review and do not offend the author of the PR: Nobody is a perfect developer or born so! The reviewers will in general mark the conversation as \"resolved\" (e.g. he/she is satisfied with the answer from the PR author). The PR author will re-request the review by clicking on the on the top right corner and might ping the reviewer on a comment if necessary with @github_name . When the PR is approved by at least one reviewer you might want to merge it to dev (you should have that privileges), unless you want to make sure that such PR is reviewed by another reviewer (e.g. you are doing big changes or important changes or you want to make sure that other person is aware/updated about the changes in that PR).","title":"Reviewing a PR"},{"location":"developing/github/#releases","text":"In to order to make a release, please follow these steps: Make sure that all new feature and bug fix PRs that should be part of the new release have been merged to dev . Checkout the dev branch and update it with git pull . Ensure that there are no uncommitted changes. Create a new branch off dev , naming it release-vX.Y.Z where X.Y.Z is the new version. Typically, patch version increments for bug fixes, minor version increments for new features that do not break backward compatibility with previous versions (i.e. no database schema changes), and major version increments for large changes or for changes that would break backward compatibility. Bump the version number of the Python package using Poetry, i.e. poetry version X.Y.Z . This will update the version number in pyproject.toml . Update the version in package.json and vast_pipeline/_version.py to match the new version number, then run npm install to update the package-lock.json file. Update the \"announcement bar\" in the documentation to refer to the new release. This can be found in docs/theme/main.html at line 37. Update the CHANGELOG.md by making a copy of the \"Unreleased\" heading at the top, and renaming the second one to the new version. Include a link to the release - it won't exist yet, so just follow the format of the others. After this there should be an \"Unreleased\" heading at the top, immediately followed by another heading with the new version number, which is followed by all the existing changes. Commit all the changes made above to the new branch and push it to GitHub. Open a PR to merge the new branch into master . Note that the default target branch is dev so you will need to change this to master when creating the PR. Once the PR has been reviewed and approved, merge the branch into master . This can only be done by administrators of the repository. Tag the merge commit on master with the version, i.e. git tag vX.Y.Z , then push the tag to GitHub. Warning If you merged the release branch into master with the GitHub web UI, you will need to sync that merge to your local copy and checkout master before creating the tag. You cannot create tags with the GitHub web UI. Push the tag to GitHub, i.e. git push origin vX.Y.Z . Merge the release branch into dev , resolving any conflicts. Append \"dev\" to the version numbers in pyproject.toml , package.json and vast_pipeline/_version.py , then run npm install to update package-lock.json , and commit the changes to dev . This can either be done as a new commit, or while resolving merge conflicts in the previous step, if appropriate. Create a new release on GitHub that points to the tagged commit on master.","title":"Releases"},{"location":"developing/intro/","text":"Contributing and Developing Guidelines \u00b6 This section explains how to contribute to the project code base and collaborate on GitHub platform. Please use the section navigator to left to visit pages for specific details on areas of development. Terminology \u00b6 These below is the terminology used to identify pipeline objects. Pipeline run (or \"Run\") -> Pipeline run instance, also referred as run, p_run, piperun, pipe_run, ... in the code Measurement -> the extracted measurement from the source finder of a single astrophysical source from an image, referred in the code as measurement(s), meas, ... Source -> A collection of single measurements for the same astrophysical source, referred as src, source, ... in the code Docstrings \u00b6 All docstrings must be done using the Google format to ensure compatibility with the documentation.","title":"Development Guidlines"},{"location":"developing/intro/#contributing-and-developing-guidelines","text":"This section explains how to contribute to the project code base and collaborate on GitHub platform. Please use the section navigator to left to visit pages for specific details on areas of development.","title":"Contributing and Developing Guidelines"},{"location":"developing/intro/#terminology","text":"These below is the terminology used to identify pipeline objects. Pipeline run (or \"Run\") -> Pipeline run instance, also referred as run, p_run, piperun, pipe_run, ... in the code Measurement -> the extracted measurement from the source finder of a single astrophysical source from an image, referred in the code as measurement(s), meas, ... Source -> A collection of single measurements for the same astrophysical source, referred as src, source, ... in the code","title":"Terminology"},{"location":"developing/intro/#docstrings","text":"All docstrings must be done using the Google format to ensure compatibility with the documentation.","title":"Docstrings"},{"location":"developing/localdevenv/","text":"Pipeline Local Development Environment Guidelines \u00b6 This section describes how to set up a local development environment more in details. Back End \u00b6 Installation \u00b6 The installation instructions are the same as the ones describes in the Getting Started section with one key difference. Rather than installing the Python dependencies with pip, you will need to install and use Poetry . After installing Poetry, running the command below will install the pipeline dependencies defined in poetry.lock into a virtual environment. The main difference between using Poetry and pip is that pip will only install the dependencies necessary for using the pipeline, whereas Poetry will also install development dependencies required for contributing (e.g. tools to build the documentation). poetry install Note Poetry will automatically create a virtual environment if it detects that your shell isn't currently using one. This should be fine for most users. If you prefer to use an alternative virtual environment manager (e.g. Miniconda), you can prevent Poetry from creating virtual environments . However, even if you are using something like Miniconda, allowing Poetry to manage the virtualenv (the default behaviour) is fine. The development team only uses this option during our automated testing since our test runner machines only contain a single Python environment so using virtualenvs is redundant. Changes to the data models \u00b6 When changes are made to the data models defined in vast_pipeline/models.py , the database schema needs to be updated to reflect those changes. Django can handle this for you by generating migration files that contain the necessary code to update the schema. Migration files are generated with the Django management command python manage.py makemigrations and applied to the database with python manage.py migrate . Depending on the nature of the changes, this may break backward compatibility, i.e. runs created with previous versions of the pipeline may not be compatible with your changes. Database migrations must be committed to source control so that others can pull in your model changes. They can sometimes be complex and require additional attention. If you have any difficulty with migrations, please contact the VAST development team for help. More information can be found in the Django documentation on migrations . Removing/Clearing Data \u00b6 The following sub-sections show how to completely drop every data in the database and how to remove only the data related to one or more pipeline runs. Reset the database \u00b6 Make sure you installed the requirements dev.txt . And django_extensions is in EXTRA_APPS in your setting configuration file .env (e.g. EXTRA_APPS=django_extensions,another_app,... ). ( pipeline_env ) $ ./manage.py reset_db && ./manage.py migrate # use the following for no confirmation prompt ( pipeline_env ) $ ./manage.py reset_db --noinput && ./manage.py migrate Clearing Run Data \u00b6 It is sometimes convenient to remove the data belonging to one or more pipeline runs while developing the code base. This is particularly useful to save time by not having to re-upload the image data along with the measurements. The data related to the pipeline are the Sources, Associations, Forced extractions entries in database and the parquet files in the respective folder. By default the command will keep the run folder with the config and the log files. ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run To clear more than one run: ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run1 my-pipe-run2 path/to/my-pipe-run3 The command accept both a path or a name of the pipeline run(s). To remove all the runs, issue: ( pipeline_env ) $ ./manage.py clearpiperun clearall The command to keep the parquet files is: ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run --keep-parquet The remove completely the pipeline folder ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run --remove-all FrontEnd Assets Management and Guidelines \u00b6 This guide explain the installation, compilation and development of the front end assets (HTML, CSS, JS and relative modules). We make use of a node installation with npm and gulp tasks to build the front end assets. Installation of node packages \u00b6 After installing a node version and npm , install the node modules using from the base folder ( vast-pipeline ): npm ci Npm will install the node packages in a node_modules folder under the main root. ... \u251c\u2500\u2500 node_modules ... For installing future additional dependencies you can run npm install --save my-package or npm install --save-dev my-dev-package (to save a development module), and after that commit both package.json and package-lock.json files. For details about the installed packages and npm scripts see package.json . FrontEnd Tasks with gulp \u00b6 Using gulp and npm scripts you can: Install dependencies under the ./static/vendor folder. Building (e.g. minify/uglify) CSS and/or Javascript files. Run a development server that \"hot-reload\" your web page when any HTML, CSS or Javascript file is modified. The command to list all the gulp \"tasks\" and sub-tasks is (you might need gulp-cli installed globally, i.e. npm i --global gulp-cli , more info here ): gulp --tasks Output: [11:55:30] Tasks for ~/PATH/TO/REPO/vast-pipeline/gulpfile.js [11:55:30] \u251c\u2500\u2500 clean [11:55:30] \u251c\u2500\u252c js9 [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u251c\u2500\u2500 css [11:55:30] \u251c\u2500\u2500 js [11:55:30] \u251c\u2500\u252c vendor [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u251c\u2500\u252c build [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u251c\u2500\u252c watch [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 watchFiles [11:55:30] \u2502 \u2514\u2500\u2500 browserSync [11:55:30] \u251c\u2500\u252c default [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u2514\u2500\u2500 debug Alternatively you can run gulp from the installed version in the node_modules folder with: ./node_modules/.bin/gulp --tasks For further details about tasks, see gulpfile . 1. Install Dependencies under vendor Folder \u00b6 Install the dependencies under the ./static/vendor folder, with: npm run vendor Or, using global gulp-cli : gulp vendor As seen in the tasks diagram above, the vendor task run the module task in parallel with the js9 tasks. JS9 has many task as these run with manual command that involve make/make install and then writing configuration to js9prefs.js file. You can run manually the installation of JS9 with gulp js9 . 2. Building CSS and Javascript files \u00b6 npm run build # or npm start # or gulp build # or gulp default # or gulp will run the vendor task and minify both CSS and Javascript files. By default, when no other tasks is specified, gulp runs the build task. You can run single tasks with: gulp css to run just the minification of the CSS files. 3. Run Development Server \u00b6 Start your normal Django server with ( NOTE : do not change the default port!): ( pipeline_env ) $: ./manage.py runserver In another terminal run: npm run watch # or gulp watch The latter will open your dev server, that will auto reload and apply your latest changes in any CSS, Javascript and/or HTML files. As pointed out in the gulp task tree above the watch task run both the vendor and build tasks. 4. Debug Task \u00b6 This task is for debugging the paths used in the others task, but also serve as a place holder to debug commands. npm run debug # or gulp debug 5. Clean Task \u00b6 This task delete the vendor folder ( /static/vendor ) along with all the files. npm run clean # or gulp clean FrontEnd assets for Production \u00b6 In order to compile the frontend assets for production, activate the Python virtual environment, then run: ( pipeline_env ) $ npm run js9staticprod && ./manage.py collectstatic -c This command will collect all static assets (Javascript and CSS files) and copy them to STATIC_ROOT path in setting.py, so make sure you have permission to write to that. STATIC_ROOT is assigned to ./staticfiles by default, otherwise assigned to the path you defined in your .env file. The js9staticprod gulp task is necessary if you specify a STATIC_URL and a BASE_URL different than the default, for example if you need to prefix the site / with a base url because you are running another webserver (e.g. another web server is running on https://my-astro-platform.com/ so you want to run the pipeline on the same server/domain https://my-astro-platform.com/pipeline , so you need to set BASE_URL='/pipeline/' and STATIC_URL=/pipeline-static/ in settings.py ). We recommend to run this in any case! Then you can move that folder to where it can be served by the production static files server ( Ningx or Apache are usually good choices, in case refer to the Django documentation ).","title":"Local Development Environment"},{"location":"developing/localdevenv/#pipeline-local-development-environment-guidelines","text":"This section describes how to set up a local development environment more in details.","title":"Pipeline Local Development Environment Guidelines"},{"location":"developing/localdevenv/#back-end","text":"","title":"Back End"},{"location":"developing/localdevenv/#installation","text":"The installation instructions are the same as the ones describes in the Getting Started section with one key difference. Rather than installing the Python dependencies with pip, you will need to install and use Poetry . After installing Poetry, running the command below will install the pipeline dependencies defined in poetry.lock into a virtual environment. The main difference between using Poetry and pip is that pip will only install the dependencies necessary for using the pipeline, whereas Poetry will also install development dependencies required for contributing (e.g. tools to build the documentation). poetry install Note Poetry will automatically create a virtual environment if it detects that your shell isn't currently using one. This should be fine for most users. If you prefer to use an alternative virtual environment manager (e.g. Miniconda), you can prevent Poetry from creating virtual environments . However, even if you are using something like Miniconda, allowing Poetry to manage the virtualenv (the default behaviour) is fine. The development team only uses this option during our automated testing since our test runner machines only contain a single Python environment so using virtualenvs is redundant.","title":"Installation"},{"location":"developing/localdevenv/#changes-to-the-data-models","text":"When changes are made to the data models defined in vast_pipeline/models.py , the database schema needs to be updated to reflect those changes. Django can handle this for you by generating migration files that contain the necessary code to update the schema. Migration files are generated with the Django management command python manage.py makemigrations and applied to the database with python manage.py migrate . Depending on the nature of the changes, this may break backward compatibility, i.e. runs created with previous versions of the pipeline may not be compatible with your changes. Database migrations must be committed to source control so that others can pull in your model changes. They can sometimes be complex and require additional attention. If you have any difficulty with migrations, please contact the VAST development team for help. More information can be found in the Django documentation on migrations .","title":"Changes to the data models"},{"location":"developing/localdevenv/#removingclearing-data","text":"The following sub-sections show how to completely drop every data in the database and how to remove only the data related to one or more pipeline runs.","title":"Removing/Clearing Data"},{"location":"developing/localdevenv/#reset-the-database","text":"Make sure you installed the requirements dev.txt . And django_extensions is in EXTRA_APPS in your setting configuration file .env (e.g. EXTRA_APPS=django_extensions,another_app,... ). ( pipeline_env ) $ ./manage.py reset_db && ./manage.py migrate # use the following for no confirmation prompt ( pipeline_env ) $ ./manage.py reset_db --noinput && ./manage.py migrate","title":"Reset the database"},{"location":"developing/localdevenv/#clearing-run-data","text":"It is sometimes convenient to remove the data belonging to one or more pipeline runs while developing the code base. This is particularly useful to save time by not having to re-upload the image data along with the measurements. The data related to the pipeline are the Sources, Associations, Forced extractions entries in database and the parquet files in the respective folder. By default the command will keep the run folder with the config and the log files. ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run To clear more than one run: ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run1 my-pipe-run2 path/to/my-pipe-run3 The command accept both a path or a name of the pipeline run(s). To remove all the runs, issue: ( pipeline_env ) $ ./manage.py clearpiperun clearall The command to keep the parquet files is: ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run --keep-parquet The remove completely the pipeline folder ( pipeline_env ) $ ./manage.py clearpiperun path/to/my-pipe-run --remove-all","title":"Clearing Run Data"},{"location":"developing/localdevenv/#frontend-assets-management-and-guidelines","text":"This guide explain the installation, compilation and development of the front end assets (HTML, CSS, JS and relative modules). We make use of a node installation with npm and gulp tasks to build the front end assets.","title":"FrontEnd Assets Management and Guidelines"},{"location":"developing/localdevenv/#installation-of-node-packages","text":"After installing a node version and npm , install the node modules using from the base folder ( vast-pipeline ): npm ci Npm will install the node packages in a node_modules folder under the main root. ... \u251c\u2500\u2500 node_modules ... For installing future additional dependencies you can run npm install --save my-package or npm install --save-dev my-dev-package (to save a development module), and after that commit both package.json and package-lock.json files. For details about the installed packages and npm scripts see package.json .","title":"Installation of node packages"},{"location":"developing/localdevenv/#frontend-tasks-with-gulp","text":"Using gulp and npm scripts you can: Install dependencies under the ./static/vendor folder. Building (e.g. minify/uglify) CSS and/or Javascript files. Run a development server that \"hot-reload\" your web page when any HTML, CSS or Javascript file is modified. The command to list all the gulp \"tasks\" and sub-tasks is (you might need gulp-cli installed globally, i.e. npm i --global gulp-cli , more info here ): gulp --tasks Output: [11:55:30] Tasks for ~/PATH/TO/REPO/vast-pipeline/gulpfile.js [11:55:30] \u251c\u2500\u2500 clean [11:55:30] \u251c\u2500\u252c js9 [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u251c\u2500\u2500 css [11:55:30] \u251c\u2500\u2500 js [11:55:30] \u251c\u2500\u252c vendor [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u251c\u2500\u252c build [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u251c\u2500\u252c watch [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 watchFiles [11:55:30] \u2502 \u2514\u2500\u2500 browserSync [11:55:30] \u251c\u2500\u252c default [11:55:30] \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u251c\u2500\u252c <parallel> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 modules [11:55:30] \u2502 \u2502 \u2514\u2500\u252c <series> [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Dir [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeConfig [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9Make [11:55:30] \u2502 \u2502 \u251c\u2500\u2500 js9MakeInst [11:55:30] \u2502 \u2502 \u2514\u2500\u2500 js9Config [11:55:30] \u2502 \u2514\u2500\u252c <parallel> [11:55:30] \u2502 \u251c\u2500\u2500 cssTask [11:55:30] \u2502 \u2514\u2500\u2500 jsTask [11:55:30] \u2514\u2500\u2500 debug Alternatively you can run gulp from the installed version in the node_modules folder with: ./node_modules/.bin/gulp --tasks For further details about tasks, see gulpfile .","title":"FrontEnd Tasks with gulp"},{"location":"developing/localdevenv/#1-install-dependencies-under-vendor-folder","text":"Install the dependencies under the ./static/vendor folder, with: npm run vendor Or, using global gulp-cli : gulp vendor As seen in the tasks diagram above, the vendor task run the module task in parallel with the js9 tasks. JS9 has many task as these run with manual command that involve make/make install and then writing configuration to js9prefs.js file. You can run manually the installation of JS9 with gulp js9 .","title":"1. Install Dependencies under vendor Folder"},{"location":"developing/localdevenv/#2-building-css-and-javascript-files","text":"npm run build # or npm start # or gulp build # or gulp default # or gulp will run the vendor task and minify both CSS and Javascript files. By default, when no other tasks is specified, gulp runs the build task. You can run single tasks with: gulp css to run just the minification of the CSS files.","title":"2. Building CSS and Javascript files"},{"location":"developing/localdevenv/#3-run-development-server","text":"Start your normal Django server with ( NOTE : do not change the default port!): ( pipeline_env ) $: ./manage.py runserver In another terminal run: npm run watch # or gulp watch The latter will open your dev server, that will auto reload and apply your latest changes in any CSS, Javascript and/or HTML files. As pointed out in the gulp task tree above the watch task run both the vendor and build tasks.","title":"3. Run Development Server"},{"location":"developing/localdevenv/#4-debug-task","text":"This task is for debugging the paths used in the others task, but also serve as a place holder to debug commands. npm run debug # or gulp debug","title":"4. Debug Task"},{"location":"developing/localdevenv/#5-clean-task","text":"This task delete the vendor folder ( /static/vendor ) along with all the files. npm run clean # or gulp clean","title":"5. Clean Task"},{"location":"developing/localdevenv/#frontend-assets-for-production","text":"In order to compile the frontend assets for production, activate the Python virtual environment, then run: ( pipeline_env ) $ npm run js9staticprod && ./manage.py collectstatic -c This command will collect all static assets (Javascript and CSS files) and copy them to STATIC_ROOT path in setting.py, so make sure you have permission to write to that. STATIC_ROOT is assigned to ./staticfiles by default, otherwise assigned to the path you defined in your .env file. The js9staticprod gulp task is necessary if you specify a STATIC_URL and a BASE_URL different than the default, for example if you need to prefix the site / with a base url because you are running another webserver (e.g. another web server is running on https://my-astro-platform.com/ so you want to run the pipeline on the same server/domain https://my-astro-platform.com/pipeline , so you need to set BASE_URL='/pipeline/' and STATIC_URL=/pipeline-static/ in settings.py ). We recommend to run this in any case! Then you can move that folder to where it can be served by the production static files server ( Ningx or Apache are usually good choices, in case refer to the Django documentation ).","title":"FrontEnd assets for Production"},{"location":"developing/profiling/","text":"Benchmarks \u00b6 Initial Profiling \u00b6 These profiling tests were run with the pipeline codebase correspondent to commit 373c2ce (some commits after the first release). Running on 12GB of data with 464MB peak memory usage takes 4 mins: performance: ~80% final_operations , ~10% get_src_skyregion_merged_df . final_operations calls other functions, out of these the largest is 50% in make_upload_sources which spends about 20% of time on utils <method 'execute' of 'psycopg2.extensions.cursor' objects> . The get_src_skyregion_merged_df time sink is in threading 15% of time is on wait ( final_operations spends some time on threading as well, hence 15% > 10%). memory: 40% pyarrow parquet write_table , rest is mostly fragmented, some more pyarrow and some pandas Running on 3MB of data with peak memory usage 176MB, takes 1.5s: performance: ~30% goes to pipeline (about 9% of this is pickle ), ~11% goes to read , rest goes to django I think memory: 30% memory is spent on django , 20% is spent on astropy/coordinates/matrix_utilities , 10% on importing other modules, rest is fragmented quite small Note that I didn't include the generation of the images/*/measurements.parquet or other files in these profiles. Database Update Operations \u00b6 Delete ( Model.objects.all().delete() ) and reupload ( bulk_upload ) (in seconds) columns\\rows 10 3 10 4 10 5 4 0.15 1.24 12.95 8 0.26 1.64 19.11 12 0.31 2.18 21.49 Per cell, 10 3 rows is slower than 10 4 and 10 5 rows, possibly due to overhead. Best to avoid uploading 10 3 rows each bulk_create call. Django bulk_update columns\\rows 10 3 10 4 10 5 4 3.39 na na 8 4.38 na na 12 5.50 na na I don't think there's any point testing 10 4 or 10 5 rows, it's obviously the worst performing function, and I've already had to force quit the terminal twice because keyboard interrupt didn't work. SQL join as ( SQL_update in vast_pipeline.pipeline.loading ) columns\\rows 10 3 10 4 10 5 4 0.016 0.11 3.08 8 0.019 0.32 4.31 12 0.027 0.38 5.39 10 5 is slower per cell than 10 4 and 10 3 , not sure why. Recommend updating 10 4 rows each time. This timing info does vary a bit on randomness. Sometimes the SQL join as takes as long as 1 second to complete 10 3 rows, I'm not sure what's causing this.","title":"Benchmarks"},{"location":"developing/profiling/#benchmarks","text":"","title":"Benchmarks"},{"location":"developing/profiling/#initial-profiling","text":"These profiling tests were run with the pipeline codebase correspondent to commit 373c2ce (some commits after the first release). Running on 12GB of data with 464MB peak memory usage takes 4 mins: performance: ~80% final_operations , ~10% get_src_skyregion_merged_df . final_operations calls other functions, out of these the largest is 50% in make_upload_sources which spends about 20% of time on utils <method 'execute' of 'psycopg2.extensions.cursor' objects> . The get_src_skyregion_merged_df time sink is in threading 15% of time is on wait ( final_operations spends some time on threading as well, hence 15% > 10%). memory: 40% pyarrow parquet write_table , rest is mostly fragmented, some more pyarrow and some pandas Running on 3MB of data with peak memory usage 176MB, takes 1.5s: performance: ~30% goes to pipeline (about 9% of this is pickle ), ~11% goes to read , rest goes to django I think memory: 30% memory is spent on django , 20% is spent on astropy/coordinates/matrix_utilities , 10% on importing other modules, rest is fragmented quite small Note that I didn't include the generation of the images/*/measurements.parquet or other files in these profiles.","title":"Initial Profiling"},{"location":"developing/profiling/#database-update-operations","text":"Delete ( Model.objects.all().delete() ) and reupload ( bulk_upload ) (in seconds) columns\\rows 10 3 10 4 10 5 4 0.15 1.24 12.95 8 0.26 1.64 19.11 12 0.31 2.18 21.49 Per cell, 10 3 rows is slower than 10 4 and 10 5 rows, possibly due to overhead. Best to avoid uploading 10 3 rows each bulk_create call. Django bulk_update columns\\rows 10 3 10 4 10 5 4 3.39 na na 8 4.38 na na 12 5.50 na na I don't think there's any point testing 10 4 or 10 5 rows, it's obviously the worst performing function, and I've already had to force quit the terminal twice because keyboard interrupt didn't work. SQL join as ( SQL_update in vast_pipeline.pipeline.loading ) columns\\rows 10 3 10 4 10 5 4 0.016 0.11 3.08 8 0.019 0.32 4.31 12 0.027 0.38 5.39 10 5 is slower per cell than 10 4 and 10 3 , not sure why. Recommend updating 10 4 rows each time. This timing info does vary a bit on randomness. Sometimes the SQL join as takes as long as 1 second to complete 10 3 rows, I'm not sure what's causing this.","title":"Database Update Operations"},{"location":"developing/tests/","text":"Tests Guidelines \u00b6 This section describes how to run the test suite of the VAST Pipeline. General Tests \u00b6 Test are found under the folder tests . Have a look and feel free to include new tests. Run the tests with the following: To run all tests: ( pipeline_env ) $ ./manage.py test To run one test file or class, use: ( pipeline_env ) $ ./manage.py test <path.to.test> for example, to run the test class CheckRunConfigValidationTest located in test_pipelineconfig.py , use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_pipelineconfig.CheckRunConfigValidationTest to run the tests located in test_webserver.py , use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_webserver Regression Tests \u00b6 Regression tests located in test_regression require the use of the VAST_2118-06A field test dataset which is not a part of the repository. This data is downloadable from cloudstor . You can use the script located in tests/regression-data/ : cd vast_pipeline/tests/regression-data/ && ./download.sh to download the VAST_2118-06A field test dataset into the regression-data folder. Or manually by clicking the button below: Download data for test and place the VAST_2118-06A field test dataset into the regression-data folder. These regression tests are skipped if the dataset is not present. All tests should be run before pushing to master. Running all the tests takes a few minutes, so it is not recommended to run them for every change. If you have made a minor change and would like to only run unit tests, skipping regression tests, use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_pipeline Note If changes are made to the default config keys, these changes need to be propagated to the test config files.","title":"Tests"},{"location":"developing/tests/#tests-guidelines","text":"This section describes how to run the test suite of the VAST Pipeline.","title":"Tests Guidelines"},{"location":"developing/tests/#general-tests","text":"Test are found under the folder tests . Have a look and feel free to include new tests. Run the tests with the following: To run all tests: ( pipeline_env ) $ ./manage.py test To run one test file or class, use: ( pipeline_env ) $ ./manage.py test <path.to.test> for example, to run the test class CheckRunConfigValidationTest located in test_pipelineconfig.py , use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_pipelineconfig.CheckRunConfigValidationTest to run the tests located in test_webserver.py , use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_webserver","title":"General Tests"},{"location":"developing/tests/#regression-tests","text":"Regression tests located in test_regression require the use of the VAST_2118-06A field test dataset which is not a part of the repository. This data is downloadable from cloudstor . You can use the script located in tests/regression-data/ : cd vast_pipeline/tests/regression-data/ && ./download.sh to download the VAST_2118-06A field test dataset into the regression-data folder. Or manually by clicking the button below: Download data for test and place the VAST_2118-06A field test dataset into the regression-data folder. These regression tests are skipped if the dataset is not present. All tests should be run before pushing to master. Running all the tests takes a few minutes, so it is not recommended to run them for every change. If you have made a minor change and would like to only run unit tests, skipping regression tests, use: ( pipeline_env ) $ ./manage.py test vast_pipeline.tests.test_pipeline Note If changes are made to the default config keys, these changes need to be propagated to the test config files.","title":"Regression Tests"},{"location":"exploringwebsite/admintools/","text":"Website Admin Tools \u00b6 Accessing the Admin Tools \u00b6 Users designated as administrators of the pipeline instance being explored (controlled by GitHub team membership) will be able to see the admin button at the top of the navbar as shown below. Clicking this button and then selecting the Django open will take the user to the Django admin backend interface shown below. Authentification and Authorization \u00b6 This section allows for the management of the user accounts and groups. Here users can be made admins and details such as email address updated. Django Q Tasks \u00b6 This area allows for the management of the Django Q processing queue. Admins are able to cancel scheduled tasks, view failed tasks or schedule new tasks. Python Social Auth \u00b6 The area for managing aspects of the authentification system that allows users to log in via GitHub. VAST_PIPELINE \u00b6 Admins are able to interact with the pipeline results data that has been uploaded from pipeline runs. This includes editing and removing objects or fields in the data as well as tags and comments.","title":"Web App Admin Tools"},{"location":"exploringwebsite/admintools/#website-admin-tools","text":"","title":"Website Admin Tools"},{"location":"exploringwebsite/admintools/#accessing-the-admin-tools","text":"Users designated as administrators of the pipeline instance being explored (controlled by GitHub team membership) will be able to see the admin button at the top of the navbar as shown below. Clicking this button and then selecting the Django open will take the user to the Django admin backend interface shown below.","title":"Accessing the Admin Tools"},{"location":"exploringwebsite/admintools/#authentification-and-authorization","text":"This section allows for the management of the user accounts and groups. Here users can be made admins and details such as email address updated.","title":"Authentification and Authorization"},{"location":"exploringwebsite/admintools/#django-q-tasks","text":"This area allows for the management of the Django Q processing queue. Admins are able to cancel scheduled tasks, view failed tasks or schedule new tasks.","title":"Django Q Tasks"},{"location":"exploringwebsite/admintools/#python-social-auth","text":"The area for managing aspects of the authentification system that allows users to log in via GitHub.","title":"Python Social Auth"},{"location":"exploringwebsite/admintools/#vast_pipeline","text":"Admins are able to interact with the pipeline results data that has been uploaded from pipeline runs. This includes editing and removing objects or fields in the data as well as tags and comments.","title":"VAST_PIPELINE"},{"location":"exploringwebsite/datatables/","text":"DataTables \u00b6 Much of the data is presented using tables that share consistent functionality across the website. An example of a table is shown below, note the interactive features across the top of the table, these are explained after the screenshot. Show 10 entries : A selectable limiter of how many rows to display at once (maximum 100). Column visibility : Enables the user to hide and show columns columns. In the screenshot below the compactness column is hidden by deselecting it in the presented list. CSV : Will download a CSV file of the data currently shown on screen. Excel : Will download an Excel file of the data currently shown on screen. Warning Note the statement currently shown on screen - only this data will be downloaded to the CSV and Excel files. All the records are not able to be downloaded in this manner - for this it is recommened to interact with the output parquet files . Search : A search bar for the user to filter the table to the row they require. The search will take into account all appropriate columns when searching.","title":"DataTables"},{"location":"exploringwebsite/datatables/#datatables","text":"Much of the data is presented using tables that share consistent functionality across the website. An example of a table is shown below, note the interactive features across the top of the table, these are explained after the screenshot. Show 10 entries : A selectable limiter of how many rows to display at once (maximum 100). Column visibility : Enables the user to hide and show columns columns. In the screenshot below the compactness column is hidden by deselecting it in the presented list. CSV : Will download a CSV file of the data currently shown on screen. Excel : Will download an Excel file of the data currently shown on screen. Warning Note the statement currently shown on screen - only this data will be downloaded to the CSV and Excel files. All the records are not able to be downloaded in this manner - for this it is recommened to interact with the output parquet files . Search : A search bar for the user to filter the table to the row they require. The search will take into account all appropriate columns when searching.","title":"DataTables"},{"location":"exploringwebsite/imagepages/","text":"Image Pages \u00b6 This page details the website pages for information on the images. List of Images \u00b6 Shown on this page is a list of images that have been ingested into the pipeline database from all pipeline runs, along with their statistics. From this page the full detail page of a specific image can be accessed by clicking on the image name. Explanation of the table options can be found in the DataTables section . Image Detail Page \u00b6 This page presents all the information about the selected image. Previous & Next Buttons \u00b6 These buttons do the following: Previous : Navigates to the previous image by id value. Next : Navigates to the next image by id value. Details \u00b6 A text representation of details of the image. Aladin Lite Viewer \u00b6 Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the image central coordinates of the image. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS. The red square shows the footprint of the image sky region on the sky. User Comments \u00b6 Users are able to read and post comments on an image using this form. Measurements Table \u00b6 This table displays all the Selavy measurements that were ingested with the image (no forced measurements appear here as they are run specific). The measurement detail page can be reached by clicking the measurement name. Pipeline Runs Table \u00b6 This table displays all the pipeline runs that use the current image. The pipeline detail page can be reached by clicking the run name.","title":"Image Pages"},{"location":"exploringwebsite/imagepages/#image-pages","text":"This page details the website pages for information on the images.","title":"Image Pages"},{"location":"exploringwebsite/imagepages/#list-of-images","text":"Shown on this page is a list of images that have been ingested into the pipeline database from all pipeline runs, along with their statistics. From this page the full detail page of a specific image can be accessed by clicking on the image name. Explanation of the table options can be found in the DataTables section .","title":"List of Images"},{"location":"exploringwebsite/imagepages/#image-detail-page","text":"This page presents all the information about the selected image.","title":"Image Detail Page"},{"location":"exploringwebsite/imagepages/#previous-next-buttons","text":"These buttons do the following: Previous : Navigates to the previous image by id value. Next : Navigates to the next image by id value.","title":"Previous &amp; Next Buttons"},{"location":"exploringwebsite/imagepages/#details","text":"A text representation of details of the image.","title":"Details"},{"location":"exploringwebsite/imagepages/#aladin-lite-viewer","text":"Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the image central coordinates of the image. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS. The red square shows the footprint of the image sky region on the sky.","title":"Aladin Lite Viewer"},{"location":"exploringwebsite/imagepages/#user-comments","text":"Users are able to read and post comments on an image using this form.","title":"User Comments"},{"location":"exploringwebsite/imagepages/#measurements-table","text":"This table displays all the Selavy measurements that were ingested with the image (no forced measurements appear here as they are run specific). The measurement detail page can be reached by clicking the measurement name.","title":"Measurements Table"},{"location":"exploringwebsite/imagepages/#pipeline-runs-table","text":"This table displays all the pipeline runs that use the current image. The pipeline detail page can be reached by clicking the run name.","title":"Pipeline Runs Table"},{"location":"exploringwebsite/measurementpages/","text":"Measurement Pages \u00b6 This page details the website pages for information on the measurements. List of Measurements \u00b6 A list of measurements that have been ingested into the pipeline database from all pipeline runs, along with their statistics, is shown on this page. From this page the full detail page of a specific measurement can be accessed by clicking on the name of the measurement. Explanation of the table options can be found in the DataTables section . Measurement Detail Page \u00b6 This page presents all the information about the selected measurement, including a postage stamp cutout of the component. SIMBAD, NED, Previous & Next Buttons \u00b6 These buttons do the following: SIMBAD : Performs a cone search on SIMBAD with a radius of 10 arcmin centered on the measurement location. NED : Performs a cone search on NED with a radius of 10 arcmin centered on the measurement location. Previous : Navigates to the previous measurement by id value. Next : Navigates to the next measurement by id value. Details \u00b6 A text representation of details of the measurement. Aladin Lite Viewer \u00b6 Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the location of the measurement. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS. JS9 Viewer \u00b6 JS9 website . The right panel contains a JS9 viewer showing the postage stamp FITS image of the measurement loaded from its respective image FITS file. Note If the image data is removed from its location when the pipeline run was processed the JS9 viewer will no longer work. User Comments \u00b6 Users are able to read and post comments on a measurement using this form. Sources Table \u00b6 The sources table shows all the sources, from all pipeline runs, that the measurement is associated to. Explanation of the table options can be found on the overview page here . Siblings Table \u00b6 The siblings table displays all other measurements that are a sibling of the current measurement, i.e., the measurements belong to the same island (as determined by Selavy ).","title":"Measurement Pages"},{"location":"exploringwebsite/measurementpages/#measurement-pages","text":"This page details the website pages for information on the measurements.","title":"Measurement Pages"},{"location":"exploringwebsite/measurementpages/#list-of-measurements","text":"A list of measurements that have been ingested into the pipeline database from all pipeline runs, along with their statistics, is shown on this page. From this page the full detail page of a specific measurement can be accessed by clicking on the name of the measurement. Explanation of the table options can be found in the DataTables section .","title":"List of Measurements"},{"location":"exploringwebsite/measurementpages/#measurement-detail-page","text":"This page presents all the information about the selected measurement, including a postage stamp cutout of the component.","title":"Measurement Detail Page"},{"location":"exploringwebsite/measurementpages/#simbad-ned-previous-next-buttons","text":"These buttons do the following: SIMBAD : Performs a cone search on SIMBAD with a radius of 10 arcmin centered on the measurement location. NED : Performs a cone search on NED with a radius of 10 arcmin centered on the measurement location. Previous : Navigates to the previous measurement by id value. Next : Navigates to the next measurement by id value.","title":"SIMBAD, NED, Previous &amp; Next Buttons"},{"location":"exploringwebsite/measurementpages/#details","text":"A text representation of details of the measurement.","title":"Details"},{"location":"exploringwebsite/measurementpages/#aladin-lite-viewer","text":"Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the location of the measurement. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS.","title":"Aladin Lite Viewer"},{"location":"exploringwebsite/measurementpages/#js9-viewer","text":"JS9 website . The right panel contains a JS9 viewer showing the postage stamp FITS image of the measurement loaded from its respective image FITS file. Note If the image data is removed from its location when the pipeline run was processed the JS9 viewer will no longer work.","title":"JS9 Viewer"},{"location":"exploringwebsite/measurementpages/#user-comments","text":"Users are able to read and post comments on a measurement using this form.","title":"User Comments"},{"location":"exploringwebsite/measurementpages/#sources-table","text":"The sources table shows all the sources, from all pipeline runs, that the measurement is associated to. Explanation of the table options can be found on the overview page here .","title":"Sources Table"},{"location":"exploringwebsite/measurementpages/#siblings-table","text":"The siblings table displays all other measurements that are a sibling of the current measurement, i.e., the measurements belong to the same island (as determined by Selavy ).","title":"Siblings Table"},{"location":"exploringwebsite/runpages/","text":"Pipeline Run Pages \u00b6 This page details the website pages for information on the pipeline runs. List of Pipeline Runs \u00b6 A list of the pipeline runs that have been processed or initialised are presented on this page along with basic statistics, including the run status. From this page the full detail page of a specific pipeline run can be accessed by clicking on the name of the pipeline run. Explanation of the table options can be found in the DataTables section . Run Status Description Completed The run has successfully finished processing. Deleting The pipeline run is currently being deleted. Error The run has encountered an error during processing and has stopped. Initialised The run has been created but not yet run. Queued The run has been sent to the scheduler for running but has not started yet. Restoring The pipeline run is currently being restored. Running The run is currently processing. Pipeline Run Detail Page \u00b6 This page presents all the information about the pipeline run, including options to edit the configuration file and to schedule the run for processing, restore the run, delete the run and generate the arrow measurement files. Action Buttons \u00b6 For admins and creators of runs there are four action buttons available: Generate Arrow Files A process to generate the arrow measurement files. See Generating Arrow Files . Delete Run Delete the pipeline run. See Deleting a Run . Restore Run A process to restore the run to the previous successful state. See Restoring a Run . Add Images or Re-Process Run/Process Run Process the pipeline run. See Processing a Run . Summary Cards \u00b6 The cards at the top of the page give a summary of the total numbers of: Images in the pipeline run. Measurements in the pipeline run. Sources in the pipeline run. New sources in the pipeline run. Clicking on the total number of images or measurements will navigate the user to the Image and Measurements tables on this page, where as the source cards will take the user to the Sources Query page. Warning When sent to the source query page, the user should make sure to click submit on the search. Details \u00b6 A text representation of details of the pipeline run. Run Sky Regions \u00b6 A sky map showing the area of sky covered by the images associated with the pipeline run. Configuration File \u00b6 Here the pipeline run configuration file can be viewed, edited and validated. Editing the Configuration File \u00b6 To edit the configuration file first select the Toggle on/off Config Edit option, that is shown in the screenshot to the right. This will enter edit mode on the configuration file as denoted by the --Edit Mode-- message shown in the screenshot below. Warning Do not toggle off edit mode without first selecting Wrtie Current Config otherwise changes will be lost. When all changes are applied, select the Write Current Config to save the changes. Validating the Configuration File \u00b6 From the configuration file menu select the Validate Config option. A feedback modal will then appear with feedback stating whether the configuration validation was successful or failed. The feedback may take a moment to appear as the check is performed. User Comments \u00b6 Users are able to read and post comments on a pipeline run using this form. Log Files \u00b6 There are three log files available, which are present depending on the actions performed. All logs are timestamped with the run time, and by default the most recent log is shown. A dropdown menu of available log files to view is available at the right hand side of the header as shown in the examples below. If there are no logs to show for the respective task then the log window will display No logs to show and the dropdown menu will appear empty. Run Log File \u00b6 The full log file of the pipeline run process. Restore Log File \u00b6 The log file of the restore run action. Generate Arrow Files Log File \u00b6 The log file of the generate arrow files action. Image and Measurements Tables \u00b6 Two tables are on the pipeline run detail page displaying the images and measurements (including forced measurements) that are part of the pipeline run.","title":"Pipeline Run Pages"},{"location":"exploringwebsite/runpages/#pipeline-run-pages","text":"This page details the website pages for information on the pipeline runs.","title":"Pipeline Run Pages"},{"location":"exploringwebsite/runpages/#list-of-pipeline-runs","text":"A list of the pipeline runs that have been processed or initialised are presented on this page along with basic statistics, including the run status. From this page the full detail page of a specific pipeline run can be accessed by clicking on the name of the pipeline run. Explanation of the table options can be found in the DataTables section . Run Status Description Completed The run has successfully finished processing. Deleting The pipeline run is currently being deleted. Error The run has encountered an error during processing and has stopped. Initialised The run has been created but not yet run. Queued The run has been sent to the scheduler for running but has not started yet. Restoring The pipeline run is currently being restored. Running The run is currently processing.","title":"List of Pipeline Runs"},{"location":"exploringwebsite/runpages/#pipeline-run-detail-page","text":"This page presents all the information about the pipeline run, including options to edit the configuration file and to schedule the run for processing, restore the run, delete the run and generate the arrow measurement files.","title":"Pipeline Run Detail Page"},{"location":"exploringwebsite/runpages/#action-buttons","text":"For admins and creators of runs there are four action buttons available: Generate Arrow Files A process to generate the arrow measurement files. See Generating Arrow Files . Delete Run Delete the pipeline run. See Deleting a Run . Restore Run A process to restore the run to the previous successful state. See Restoring a Run . Add Images or Re-Process Run/Process Run Process the pipeline run. See Processing a Run .","title":"Action Buttons"},{"location":"exploringwebsite/runpages/#summary-cards","text":"The cards at the top of the page give a summary of the total numbers of: Images in the pipeline run. Measurements in the pipeline run. Sources in the pipeline run. New sources in the pipeline run. Clicking on the total number of images or measurements will navigate the user to the Image and Measurements tables on this page, where as the source cards will take the user to the Sources Query page. Warning When sent to the source query page, the user should make sure to click submit on the search.","title":"Summary Cards"},{"location":"exploringwebsite/runpages/#details","text":"A text representation of details of the pipeline run.","title":"Details"},{"location":"exploringwebsite/runpages/#run-sky-regions","text":"A sky map showing the area of sky covered by the images associated with the pipeline run.","title":"Run Sky Regions"},{"location":"exploringwebsite/runpages/#configuration-file","text":"Here the pipeline run configuration file can be viewed, edited and validated.","title":"Configuration File"},{"location":"exploringwebsite/runpages/#editing-the-configuration-file","text":"To edit the configuration file first select the Toggle on/off Config Edit option, that is shown in the screenshot to the right. This will enter edit mode on the configuration file as denoted by the --Edit Mode-- message shown in the screenshot below. Warning Do not toggle off edit mode without first selecting Wrtie Current Config otherwise changes will be lost. When all changes are applied, select the Write Current Config to save the changes.","title":"Editing the Configuration File"},{"location":"exploringwebsite/runpages/#validating-the-configuration-file","text":"From the configuration file menu select the Validate Config option. A feedback modal will then appear with feedback stating whether the configuration validation was successful or failed. The feedback may take a moment to appear as the check is performed.","title":"Validating the Configuration File"},{"location":"exploringwebsite/runpages/#user-comments","text":"Users are able to read and post comments on a pipeline run using this form.","title":"User Comments"},{"location":"exploringwebsite/runpages/#log-files","text":"There are three log files available, which are present depending on the actions performed. All logs are timestamped with the run time, and by default the most recent log is shown. A dropdown menu of available log files to view is available at the right hand side of the header as shown in the examples below. If there are no logs to show for the respective task then the log window will display No logs to show and the dropdown menu will appear empty.","title":"Log Files"},{"location":"exploringwebsite/runpages/#run-log-file","text":"The full log file of the pipeline run process.","title":"Run Log File"},{"location":"exploringwebsite/runpages/#restore-log-file","text":"The log file of the restore run action.","title":"Restore Log File"},{"location":"exploringwebsite/runpages/#generate-arrow-files-log-file","text":"The log file of the generate arrow files action.","title":"Generate Arrow Files Log File"},{"location":"exploringwebsite/runpages/#image-and-measurements-tables","text":"Two tables are on the pipeline run detail page displaying the images and measurements (including forced measurements) that are part of the pipeline run.","title":"Image and Measurements Tables"},{"location":"exploringwebsite/sourceanalysis/","text":"Source \u03b7-V Analysis \u00b6 This page details the interactive analysis tool of the \u03b7 and V metrics for a selection of sources. Further Reading Descriptions of the \u03b7 and V metrics can be found on the source statistics page . For a detailed overview of the method, please refer to Rowlinson et al., 2019 . The analysis can be performed on the results of a source query . Tip: Sensible Querying! To get the most out of the tool is advised to design a query that will eliminate as many erroneous results as possible. For example, making sure sources are isolated and have no siblings or relations. The VAST Tools package should be used for more advanced queries. Accessing the Analysis Page \u00b6 Once a query has been performed on the source query page the Go to \u03b7-V analysis button will be active as highlighted in the image below. Click this button and the analysis page will open. Warning: Bad Sources Bad sources for analysis will be automatically removed. These include sources that only have one datapoint or where the \u03b7 and/or V are equal to 0 or have failed. \u03b7-V Plot Description \u00b6 Shown on the left side of the page is the the log-log plot of the peak flux \u03b7 and V metrics of the sources from the query. The distributions of the metrics are fitted with a Gaussian, and a sigma cut is displayed on the plot that by default is set to a value of \\(3\\sigma\\) . The highlighted region represents the area of plot that is beyond both thresholds, and this is where transient sources will be found. The colour of the points represent how many detection datapoints the source contains. Plot Options \u00b6 At the bottom of the plot are options to change both the flux type and the multiplication factor of the sigma cut. Once the new settings are entered click the Apply button and the plot will reload with the new options. Viewing Source Light Curves & Information \u00b6 Hovering over a source on the plot will show an information window that displays the source name, id and the \u03b7 and V values. Clicking on the source will load the light curve, source information and external crossmatching search results into the panels on the right side of the page. Selecting another source will dynamically update these panels without having to leave the page. The source information panel contains a link to go to the full source detail page and the ability to favourite a source directly from this page. Displaying High Source Counts \u00b6 When querying large pipeline runs it is possible that a query will return tens of thousands of results. Plotting such a high number of sources is very intensive and would take a significant amount of time to render. To solve this, when a high number of sources are requested to be plotted, all the sources outside of the threshold transient area are plotted as a static image that represents the distribution of the sources. These sources are not interactive. By default the threshold is set to 20,000 datapoints and is configurable by the administrator . Any sources that fall within the transient threshold region are plotted as normal and are interactive as with the standard plot. Warning: Setting Low Thresholds Setting low thresholds with high source counts could cause a significant amount of candidates to be plotted with the normal method. This may result in the plot taking up to a few minutes to load within the browser. Note: Colour Bar The colour bar only applies to the interactive datapoints.","title":"Source \u03b7-V Analysis"},{"location":"exploringwebsite/sourceanalysis/#source-v-analysis","text":"This page details the interactive analysis tool of the \u03b7 and V metrics for a selection of sources. Further Reading Descriptions of the \u03b7 and V metrics can be found on the source statistics page . For a detailed overview of the method, please refer to Rowlinson et al., 2019 . The analysis can be performed on the results of a source query . Tip: Sensible Querying! To get the most out of the tool is advised to design a query that will eliminate as many erroneous results as possible. For example, making sure sources are isolated and have no siblings or relations. The VAST Tools package should be used for more advanced queries.","title":"Source \u03b7-V Analysis"},{"location":"exploringwebsite/sourceanalysis/#accessing-the-analysis-page","text":"Once a query has been performed on the source query page the Go to \u03b7-V analysis button will be active as highlighted in the image below. Click this button and the analysis page will open. Warning: Bad Sources Bad sources for analysis will be automatically removed. These include sources that only have one datapoint or where the \u03b7 and/or V are equal to 0 or have failed.","title":"Accessing the Analysis Page"},{"location":"exploringwebsite/sourceanalysis/#-v-plot-description","text":"Shown on the left side of the page is the the log-log plot of the peak flux \u03b7 and V metrics of the sources from the query. The distributions of the metrics are fitted with a Gaussian, and a sigma cut is displayed on the plot that by default is set to a value of \\(3\\sigma\\) . The highlighted region represents the area of plot that is beyond both thresholds, and this is where transient sources will be found. The colour of the points represent how many detection datapoints the source contains.","title":"\u03b7-V Plot Description"},{"location":"exploringwebsite/sourceanalysis/#plot-options","text":"At the bottom of the plot are options to change both the flux type and the multiplication factor of the sigma cut. Once the new settings are entered click the Apply button and the plot will reload with the new options.","title":"Plot Options"},{"location":"exploringwebsite/sourceanalysis/#viewing-source-light-curves-information","text":"Hovering over a source on the plot will show an information window that displays the source name, id and the \u03b7 and V values. Clicking on the source will load the light curve, source information and external crossmatching search results into the panels on the right side of the page. Selecting another source will dynamically update these panels without having to leave the page. The source information panel contains a link to go to the full source detail page and the ability to favourite a source directly from this page.","title":"Viewing Source Light Curves &amp; Information"},{"location":"exploringwebsite/sourceanalysis/#displaying-high-source-counts","text":"When querying large pipeline runs it is possible that a query will return tens of thousands of results. Plotting such a high number of sources is very intensive and would take a significant amount of time to render. To solve this, when a high number of sources are requested to be plotted, all the sources outside of the threshold transient area are plotted as a static image that represents the distribution of the sources. These sources are not interactive. By default the threshold is set to 20,000 datapoints and is configurable by the administrator . Any sources that fall within the transient threshold region are plotted as normal and are interactive as with the standard plot. Warning: Setting Low Thresholds Setting low thresholds with high source counts could cause a significant amount of candidates to be plotted with the normal method. This may result in the plot taking up to a few minutes to load within the browser. Note: Colour Bar The colour bar only applies to the interactive datapoints.","title":"Displaying High Source Counts"},{"location":"exploringwebsite/sourcedetail/","text":"Source Detail \u00b6 This page presents all the information about the selected source, including a light curve and cutouts of all the measurements that are associated to the source. Star, SIMBAD, NED, ZTF, Previous & Next Buttons \u00b6 These buttons do the following: Star : Adds the source to the user's favourites, see Source Tags and Favourites . SIMBAD : Performs a cone search on SIMBAD with a radius of 10 arcmin centered on the source location. NED : Performs a cone search on NED with a radius of 10 arcmin centered on the source location. ZTF : Performs a cone search for optical transient alerts from the Zwicky Transient Facility (ZTF) with a radius of 10 arcsec centered on the source location. Previous : Navigates to the previous source that was returned in the source query. Next : Navigates to the next source that was returned in the source query Details \u00b6 A text representation of details of the source. First Detection Postage Stamp \u00b6 The JS9 viewer is used to show the postage stamp FITS image (cutout) of the first source-finder detection for this source. Cutouts for each measurement are shown further down the page. Aladin Lite Viewer \u00b6 Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the location of the source. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS. Flux & Variability Details \u00b6 Aggregate flux density statistics and variability metrics for this source, separated by flux type (peak or integrated). Light Curve \u00b6 The light curve of the source is shown. The peak or integrated flux can be selected by using the radio selection buttons. Hovering over the data points on the light curve will show an information panel that contains the date of the measurement, the flux and the measurement number. It also contains a thumbnail image preview of the respective measurement. Two-epoch Node Graph \u00b6 The node graph is a visual representation of what two-epoch pairings have significant variability metric values. If an epoch pairing is significant then they are joined by a line on the graph. Hovering over the line will display the pair metrics for the selected flux type (peak or integrated) and highlight the epoch pairing on the light curve plot. External Search Results Table \u00b6 This table shows the result of a query to the SIMBAD, NED, and TNS services for astronomical sources within 1 arcmin of the source location. Along with the name and coordinate of the matches, the on-sky separation between the source is shown along with the source type. User Comments & Tags \u00b6 Users are able to read and post comments on a source using this form, in addition to adding and removing tags, see Source Tags and Favourites . JS9 Viewer Postage Stamps \u00b6 JS9 website . More JS9 viewers are used to show the postage stamp FITS images of the measurements that are associated with the source, loaded from their respective image FITS files. Note If the image data is removed from its location when the pipeline run was processed the JS9 viewer will no longer work. Source Measurements Table \u00b6 This table displays the measurements that are associated with the source. The detail page for the measurement can be reached by clicking the name of the respective measurement. Related Sources Table \u00b6 This table displays the sources that are a relation of the source in question. For further information refer to the Relations section in the association documentation.","title":"Source Detail"},{"location":"exploringwebsite/sourcedetail/#source-detail","text":"This page presents all the information about the selected source, including a light curve and cutouts of all the measurements that are associated to the source.","title":"Source Detail"},{"location":"exploringwebsite/sourcedetail/#star-simbad-ned-ztf-previous-next-buttons","text":"These buttons do the following: Star : Adds the source to the user's favourites, see Source Tags and Favourites . SIMBAD : Performs a cone search on SIMBAD with a radius of 10 arcmin centered on the source location. NED : Performs a cone search on NED with a radius of 10 arcmin centered on the source location. ZTF : Performs a cone search for optical transient alerts from the Zwicky Transient Facility (ZTF) with a radius of 10 arcsec centered on the source location. Previous : Navigates to the previous source that was returned in the source query. Next : Navigates to the next source that was returned in the source query","title":"Star, SIMBAD, NED, ZTF, Previous &amp; Next Buttons"},{"location":"exploringwebsite/sourcedetail/#details","text":"A text representation of details of the source.","title":"Details"},{"location":"exploringwebsite/sourcedetail/#first-detection-postage-stamp","text":"The JS9 viewer is used to show the postage stamp FITS image (cutout) of the first source-finder detection for this source. Cutouts for each measurement are shown further down the page.","title":"First Detection Postage Stamp"},{"location":"exploringwebsite/sourcedetail/#aladin-lite-viewer","text":"Aladin Lite Documentation . The central panel contains an Aladin Lite viewer, which by default displays the HIPS image from the Rapid ASKAP Continuum Survey , centred on the location of the source. Other surveys are available such as all epochs of the VAST Pilot Survey (including Stokes V) and other wavelength surveys such as 2MASS.","title":"Aladin Lite Viewer"},{"location":"exploringwebsite/sourcedetail/#flux-variability-details","text":"Aggregate flux density statistics and variability metrics for this source, separated by flux type (peak or integrated).","title":"Flux &amp; Variability Details"},{"location":"exploringwebsite/sourcedetail/#light-curve","text":"The light curve of the source is shown. The peak or integrated flux can be selected by using the radio selection buttons. Hovering over the data points on the light curve will show an information panel that contains the date of the measurement, the flux and the measurement number. It also contains a thumbnail image preview of the respective measurement.","title":"Light Curve"},{"location":"exploringwebsite/sourcedetail/#two-epoch-node-graph","text":"The node graph is a visual representation of what two-epoch pairings have significant variability metric values. If an epoch pairing is significant then they are joined by a line on the graph. Hovering over the line will display the pair metrics for the selected flux type (peak or integrated) and highlight the epoch pairing on the light curve plot.","title":"Two-epoch Node Graph"},{"location":"exploringwebsite/sourcedetail/#external-search-results-table","text":"This table shows the result of a query to the SIMBAD, NED, and TNS services for astronomical sources within 1 arcmin of the source location. Along with the name and coordinate of the matches, the on-sky separation between the source is shown along with the source type.","title":"External Search Results Table"},{"location":"exploringwebsite/sourcedetail/#user-comments-tags","text":"Users are able to read and post comments on a source using this form, in addition to adding and removing tags, see Source Tags and Favourites .","title":"User Comments &amp; Tags"},{"location":"exploringwebsite/sourcedetail/#js9-viewer-postage-stamps","text":"JS9 website . More JS9 viewers are used to show the postage stamp FITS images of the measurements that are associated with the source, loaded from their respective image FITS files. Note If the image data is removed from its location when the pipeline run was processed the JS9 viewer will no longer work.","title":"JS9 Viewer Postage Stamps"},{"location":"exploringwebsite/sourcedetail/#source-measurements-table","text":"This table displays the measurements that are associated with the source. The detail page for the measurement can be reached by clicking the name of the respective measurement.","title":"Source Measurements Table"},{"location":"exploringwebsite/sourcedetail/#related-sources-table","text":"This table displays the sources that are a relation of the source in question. For further information refer to the Relations section in the association documentation.","title":"Related Sources Table"},{"location":"exploringwebsite/sourcequery/","text":"Source Query \u00b6 This page details the Source Query interface. Query Options \u00b6 Users can filter and query the sources currently in the database by using the form located on this page. The form is submitted by clicking the blue :fontawesome-solid-search: button, the red button will reset the form by removing all entered values. Once the form is submitted the results are dynamically updated in the results table below the form (i.e. on the same page). The following sections provide further details on the form. Data Source \u00b6 Here a specific pipeline run can be selected from a dropdown list to filter sources to only those from the chosen run. By default sources from all runs are shown. Note Only successfully completed pipeline runs are available to select. This rule also applies when all is selected. Cone Search \u00b6 Users can choose whether to input their coordinates directly or use the object name resolver to attempt to fetch the coordinates. Manual Input \u00b6 The format of the coordinates should be in a standard format that is recognised by astropy, for example: 21 29 45.29 -04 29 11.9 21:29:45.29 -04:29:11.9 322.4387083 -4.4866389 Galactic coordinates can also be entered by selecting Galactic from the dropdown menu that is set to ICRS by default. Feedback will be given immediately whether the coordinates are valid, as shown in the screenshots below: Once the coordinates have been entered the radius value must also be specified as shown in the screenshot above. Use the dropdown menu to change the radius unit to be arcsec , arcmin or deg . Name Resolver \u00b6 To use the name resolver, the name of the source should be entered into the Object Name field (e.g. PSR J2129-04 ), select the name resolver service and then click the Resolve button. The coordinates will then be automatically filled on a successful match. if no match is found then this will be communicated by the form as below: Table Filters \u00b6 This section of the form allows the user to place thresholds and selections on specific metrics of the sources. Please refer to the Source Statistics page for details on the different metrics. There are also tooltips located on the form to offer explanations. The following options are not standard source metrics: Include and Exclude Tags \u00b6 Users can attach tags to sources (see Source Tags and Favourites ) and here tags can be selected to include or exclude in the source search. Source Selection \u00b6 Here specific sources can be searched for by entering the source names, or source database id values, in a comma-separated list. For example: J011816.05-730747.77,J011816.05-730747.77,J213221.21-040900.42 1031,1280,52 are valid entries to this search field. Use the dropdown menu to declare whether name (default) or id values are being searched. Results Table \u00b6 Located directly below the form is the results table which is dynamically updated once the form is submitted. The full detail page of a specific source can be accessed by clicking on the source name in the table. Explanation of the table options can be found in the DataTables section . The Go to \u03b7-V analysis button will launch the \u03b7-V analysis page for the sources contained in the query results. Please refer to the Source \u03b7-V Analysis Page section for the full details of this feature. Note The Go to \u03b7-V analysis button is only active after a query has been performed.","title":"Source Query"},{"location":"exploringwebsite/sourcequery/#source-query","text":"This page details the Source Query interface.","title":"Source Query"},{"location":"exploringwebsite/sourcequery/#query-options","text":"Users can filter and query the sources currently in the database by using the form located on this page. The form is submitted by clicking the blue :fontawesome-solid-search: button, the red button will reset the form by removing all entered values. Once the form is submitted the results are dynamically updated in the results table below the form (i.e. on the same page). The following sections provide further details on the form.","title":"Query Options"},{"location":"exploringwebsite/sourcequery/#data-source","text":"Here a specific pipeline run can be selected from a dropdown list to filter sources to only those from the chosen run. By default sources from all runs are shown. Note Only successfully completed pipeline runs are available to select. This rule also applies when all is selected.","title":"Data Source"},{"location":"exploringwebsite/sourcequery/#cone-search","text":"Users can choose whether to input their coordinates directly or use the object name resolver to attempt to fetch the coordinates.","title":"Cone Search"},{"location":"exploringwebsite/sourcequery/#manual-input","text":"The format of the coordinates should be in a standard format that is recognised by astropy, for example: 21 29 45.29 -04 29 11.9 21:29:45.29 -04:29:11.9 322.4387083 -4.4866389 Galactic coordinates can also be entered by selecting Galactic from the dropdown menu that is set to ICRS by default. Feedback will be given immediately whether the coordinates are valid, as shown in the screenshots below: Once the coordinates have been entered the radius value must also be specified as shown in the screenshot above. Use the dropdown menu to change the radius unit to be arcsec , arcmin or deg .","title":"Manual Input"},{"location":"exploringwebsite/sourcequery/#name-resolver","text":"To use the name resolver, the name of the source should be entered into the Object Name field (e.g. PSR J2129-04 ), select the name resolver service and then click the Resolve button. The coordinates will then be automatically filled on a successful match. if no match is found then this will be communicated by the form as below:","title":"Name Resolver"},{"location":"exploringwebsite/sourcequery/#table-filters","text":"This section of the form allows the user to place thresholds and selections on specific metrics of the sources. Please refer to the Source Statistics page for details on the different metrics. There are also tooltips located on the form to offer explanations. The following options are not standard source metrics:","title":"Table Filters"},{"location":"exploringwebsite/sourcequery/#include-and-exclude-tags","text":"Users can attach tags to sources (see Source Tags and Favourites ) and here tags can be selected to include or exclude in the source search.","title":"Include and Exclude Tags"},{"location":"exploringwebsite/sourcequery/#source-selection","text":"Here specific sources can be searched for by entering the source names, or source database id values, in a comma-separated list. For example: J011816.05-730747.77,J011816.05-730747.77,J213221.21-040900.42 1031,1280,52 are valid entries to this search field. Use the dropdown menu to declare whether name (default) or id values are being searched.","title":"Source Selection"},{"location":"exploringwebsite/sourcequery/#results-table","text":"Located directly below the form is the results table which is dynamically updated once the form is submitted. The full detail page of a specific source can be accessed by clicking on the source name in the table. Explanation of the table options can be found in the DataTables section . The Go to \u03b7-V analysis button will launch the \u03b7-V analysis page for the sources contained in the query results. Please refer to the Source \u03b7-V Analysis Page section for the full details of this feature. Note The Go to \u03b7-V analysis button is only active after a query has been performed.","title":"Results Table"},{"location":"exploringwebsite/sourcetagsfavs/","text":"Source Tags and Favourites \u00b6 Users are able to save a source as a favourite for later reference, in addition to adding tags to sources that can be used in source queries. Adding a Source to Favourites \u00b6 A source can be added to a user's favourites by: Selecting the 'star' button at the top of the source detail page as shown below. A modal window will open to confirm the saving of the source as a favourite. An optional comment can be entered. Select Add to Favourites and a confirmation alert will be shown to signify the source has been added successfully. Viewing Favourite Sources \u00b6 A user can access their favourite sources by selecting the Favourite Sources option from the menu when clicking on their username at the top right-hand corner of the page. The user will then be navigated to their favourite sources table as shown below. Adding a Tag \u00b6 Follow these steps to add a tag to a source: Type the tag to be added into the field tag field. If the tag has already been used it will appear in the dropdown text options and can be selected by clicking the text. To add a new tag, enter the complete text of the new tag and again click the text in the dropdown text. After clicking the text the tag will then show as a bordered tag in the input field. Finally, click the submit button (a comment is optional) and the tag will be saved as shown below. A comment will appear stating the addition of the tag. Removing a Tag \u00b6 Click the x on the tag to remove it and then click the Submit button to save the removal.","title":"Source Tags & Favourites"},{"location":"exploringwebsite/sourcetagsfavs/#source-tags-and-favourites","text":"Users are able to save a source as a favourite for later reference, in addition to adding tags to sources that can be used in source queries.","title":"Source Tags and Favourites"},{"location":"exploringwebsite/sourcetagsfavs/#adding-a-source-to-favourites","text":"A source can be added to a user's favourites by: Selecting the 'star' button at the top of the source detail page as shown below. A modal window will open to confirm the saving of the source as a favourite. An optional comment can be entered. Select Add to Favourites and a confirmation alert will be shown to signify the source has been added successfully.","title":"Adding a Source to Favourites"},{"location":"exploringwebsite/sourcetagsfavs/#viewing-favourite-sources","text":"A user can access their favourite sources by selecting the Favourite Sources option from the menu when clicking on their username at the top right-hand corner of the page. The user will then be navigated to their favourite sources table as shown below.","title":"Viewing Favourite Sources"},{"location":"exploringwebsite/sourcetagsfavs/#adding-a-tag","text":"Follow these steps to add a tag to a source: Type the tag to be added into the field tag field. If the tag has already been used it will appear in the dropdown text options and can be selected by clicking the text. To add a new tag, enter the complete text of the new tag and again click the text in the dropdown text. After clicking the text the tag will then show as a bordered tag in the input field. Finally, click the submit button (a comment is optional) and the tag will be saved as shown below. A comment will appear stating the addition of the tag.","title":"Adding a Tag"},{"location":"exploringwebsite/sourcetagsfavs/#removing-a-tag","text":"Click the x on the tag to remove it and then click the Submit button to save the removal.","title":"Removing a Tag"},{"location":"exploringwebsite/websiteoverview/","text":"Website Overview \u00b6 This page gives an overview of the pipeline website, with links to main pages on features where appropriate. Refer to Accessing the Pipeline for details on how to access the pipeline instance that is hosted by the VAST collaboration. For admins, refer to the following pages for details of the configuration and set up of the web server: Configuration , Deployment and Web App Admin Usage . Homepage \u00b6 The homepage provides a summary of the data currently held in the pipeline instance that has been processed. The four cards at the top of the homepage provide total values for the amount of pipeline runs, images, measurements and sources that are stored in the database. Clicking any of them will take you to the respective overview page for the data type. Note The totals presented on the homepage are totals for all pipeline runs combined! Also displayed is a sky region map that shows all the areas of the sky that have had successful and completed pipeline runs performed. Navbar \u00b6 The navbar, shown to the right, acts as the main method in which to navigate around the website. The following sections link to the respective documentation pages explainging the features of each link. Note The admin button on the navbar is only seen when the user is designated as an administrator. Tip The navbar can be collapsed by pressing the menu (or hamburger) button next to it at the top of the page. Admin \u00b6 See the website admin tools page. Allows for admins to manage users, Django Q schedules and the data itself. Pipeline Runs \u00b6 See the Pipeline Run Pages doc. Navigates the user to the list of pipeline runs available, which in turn link to the detail page for each respective run. Sources Query \u00b6 See the Source Query section. Takes the user to the source query page, where users can search for sources by defining a set of thresholds and feature requirements. From the results users can also access the detail page for individual sources. Measurements \u00b6 See the Measurement Pages section. Navigates the user to the measurements page that features a table containing all the measurements currently held in the database. From here users can also access the detail page for individual measurements. Images \u00b6 See the Image Pages section. Takes the user to the images page that features a table containing all the images currently held in the database. From here users can also access the detail page for individual images. External Links \u00b6 Documentation : Links to this documentation website. Pipeline Repository : A link to the GitHub pipeline repository. Raise an Issue : A link to open a new issue on the GitHub repository. Start a Discussion : A link to open a new discussion on the GitHub repository. VAST Links GitHub : A link to the VAST organisation GitHub page. JupyterHub : Links to the VAST hosted JupyterHub instance which includes access to the pipeline results and vast-tools . Website : Links to the VAST collaboration website. Wiki : Links to the VAST Wiki which is hosted on GitHub.","title":"Website Overview"},{"location":"exploringwebsite/websiteoverview/#website-overview","text":"This page gives an overview of the pipeline website, with links to main pages on features where appropriate. Refer to Accessing the Pipeline for details on how to access the pipeline instance that is hosted by the VAST collaboration. For admins, refer to the following pages for details of the configuration and set up of the web server: Configuration , Deployment and Web App Admin Usage .","title":"Website Overview"},{"location":"exploringwebsite/websiteoverview/#homepage","text":"The homepage provides a summary of the data currently held in the pipeline instance that has been processed. The four cards at the top of the homepage provide total values for the amount of pipeline runs, images, measurements and sources that are stored in the database. Clicking any of them will take you to the respective overview page for the data type. Note The totals presented on the homepage are totals for all pipeline runs combined! Also displayed is a sky region map that shows all the areas of the sky that have had successful and completed pipeline runs performed.","title":"Homepage"},{"location":"exploringwebsite/websiteoverview/#navbar","text":"The navbar, shown to the right, acts as the main method in which to navigate around the website. The following sections link to the respective documentation pages explainging the features of each link. Note The admin button on the navbar is only seen when the user is designated as an administrator. Tip The navbar can be collapsed by pressing the menu (or hamburger) button next to it at the top of the page.","title":"Navbar"},{"location":"exploringwebsite/websiteoverview/#admin","text":"See the website admin tools page. Allows for admins to manage users, Django Q schedules and the data itself.","title":"Admin"},{"location":"exploringwebsite/websiteoverview/#pipeline-runs","text":"See the Pipeline Run Pages doc. Navigates the user to the list of pipeline runs available, which in turn link to the detail page for each respective run.","title":"Pipeline Runs"},{"location":"exploringwebsite/websiteoverview/#sources-query","text":"See the Source Query section. Takes the user to the source query page, where users can search for sources by defining a set of thresholds and feature requirements. From the results users can also access the detail page for individual sources.","title":"Sources Query"},{"location":"exploringwebsite/websiteoverview/#measurements","text":"See the Measurement Pages section. Navigates the user to the measurements page that features a table containing all the measurements currently held in the database. From here users can also access the detail page for individual measurements.","title":"Measurements"},{"location":"exploringwebsite/websiteoverview/#images","text":"See the Image Pages section. Takes the user to the images page that features a table containing all the images currently held in the database. From here users can also access the detail page for individual images.","title":"Images"},{"location":"exploringwebsite/websiteoverview/#external-links","text":"Documentation : Links to this documentation website. Pipeline Repository : A link to the GitHub pipeline repository. Raise an Issue : A link to open a new issue on the GitHub repository. Start a Discussion : A link to open a new discussion on the GitHub repository. VAST Links GitHub : A link to the VAST organisation GitHub page. JupyterHub : Links to the VAST hosted JupyterHub instance which includes access to the pipeline results and vast-tools . Website : Links to the VAST collaboration website. Wiki : Links to the VAST Wiki which is hosted on GitHub.","title":"External Links"},{"location":"gettingstarted/configuration/","text":"Configuration \u00b6 This section describe how to configure your VAST Pipeline installation. Pipeline Configuration \u00b6 The following instructions, will get you started in setting up the database and pipeline configuration. Note The commands given in this section, unless otherwise stated, assume that the current directory is the pipeline root and that your pipeline Python environment has been activated. Create a database for the pipeline. If you followed the installation process, you will have a PostgreSQL Docker container running on your system. Use the provided script init-tools/init-db.py script to create a new database for the pipeline. As a security precaution, this script will also create a new database user and set the pipeline database owner to this new user. The initialization script requires several input parameters. For usage information, run with the --help option: python init-tools/init-db.py --help usage: init-db.py [-h] host port admin-username admin-password username password database-name Initialize a PostgreSQL database for VAST Pipeline use. Creates a new superuser and creates a new database owned by the new superuser. positional arguments: host database host port database port admin-username database administrator username admin-password database administrator password username username for the new user/role to create for the VAST Pipeline password password for the new user/role to create for the VAST Pipeline database-name name of the new database to create for the VAST Pipeline optional arguments: -h, --help show this help message and exit Fill in the parameters as appropriate for your configuration. If you followed the installation instructions, these would be the details for your PostgreSQL Docker container. Following from the same example in the installation section: python init-tools/init-db.py localhost 55002 postgres <password> vast <vast-user-password> vastdb Info Where <password> is the superuser password that was passed to docker run , and <vast-user-password> is a new password of your choice for the new vast database user. You may change the values for the username and database-name, the above is just an example. If everything went well the output should be: Creating new user/role vast ... Creating new database vastdb ... Done! Copy the setting configuration file template and modify it with your desired settings. Please refer to the .env File section on this page for further details about the settings that are set in this file along with their defaults. cp webinterface/.env.template webinterface/.env Set the database connection settings in the webinterface/.env file by modifying DATABASE_URL (for URL syntax see this link ). For example: .env DATABASE_URL=psql://vast:<vast-user-password>@localhost:55002/vastdb Note The connection details are the same that you setup during the installation . The database/user names must not contain any spaces or dashes, so use the underscore if you want, e.g. this_is_my_db_name . Create the pipeline database tables. The createcachetable command creates the cache tables required by DjangoQ. python manage.py migrate python manage.py createcachetable Create the pipeline data directories. The pipeline has several directories that can be configured in webinterface/.env : PIPELINE_WORKING_DIR : location to store various pipeline output files. RAW_IMAGE_DIR : default location that the pipeline will search for input images and catalogues to ingest during a pipeline run. Data inputs can also be defined as absolute paths in a pipeline run configuration file, so this setting only affects relative paths in the pipeline run configuration. HOME_DATA_DIR : a path relative to a user's home directory to search for additional input images and catalogues. Intended for multi-user server deployments and unlikely to be useful for local installations. See below for some examples. HOME_DATA_ROOT : path to the location of user home directories. Used together with HOME_DATA_DIR . If not supplied, the pipeline will search for the user's home directory using the default OS location. See below for some examples. .env \u2013 User data configuration examples In the following examples, assume that the user's name is foo . # HOME_DATA_ROOT = Uncomment to set a custom path to user data dirs HOME_DATA_DIR=vast-pipeline-extra-data Using the above settings, the pipeline will search for additional input data in the user's home directory as resolved by the OS. e.g. on an Ubuntu system, this would be /home/foo/vast-pipeline-extra-data . HOME_DATA_ROOT=/data/home HOME_DATA_DIR=vast-pipeline-extra-data Using the above settings, the pipeline will search for additional input data in /data/home/foo/vast-pipeline-extra-data . While the default values for these settings are relative to the pipeline codebase root (i.e. within the repo), we recommend creating these directories outside of the repo and updating the webinterface/.env file appropriately with absolute paths. For example, assuming you wish to create these directories in /data/vast-pipeline : mkdir -p /data/vast-pipeline mkdir /data/vast-pipeline/pipeline-runs mkdir /data/vast-pipeline/raw-images and update the webinterface/.env file with: .env PIPELINE_WORKING_DIR=/data/vast-pipeline/pipeline-runs RAW_IMAGE_DIR=/data/vast-pipeline/raw-images .env File \u00b6 The .env file contains various top-level settings that apply to Django, authentication and the running of the pipeline itself. Shown below is the .env.template file which is provided to be able to copy in step 3 above. .env.template # Django DEBUG=True SECRET_KEY=FillMeUPWithSomeComplicatedString # see https://django-environ.readthedocs.io/en/latest/#tips DATABASE_URL=psql://FILLMYUSER:FILLMYPASSWORD@FILLMYHOST:FILLMYPORT/FILLMYDBNAME # BASE_URL = this for append a base url in a production deployment STATIC_ROOT=./staticfiles/ STATIC_URL=/static/ # STATICFILES_DIRS = uncomment and fill to use # EXTRA_APPS = uncomment and fill to use # EXTRA_MIDDLEWARE = uncomment and fill to use ALLOWED_HOSTS=localhost # Github Authentication GITHUB_AUTH_TYPE='org' SOCIAL_AUTH_GITHUB_KEY=fillMeUp SOCIAL_AUTH_GITHUB_SECRET=fillMeUp SOCIAL_AUTH_GITHUB_ORG_NAME=fillMeUp SOCIAL_AUTH_GITHUB_ADMIN_TEAM=fillMeUp # External APIs # TNS_API_KEY = uncomment and fill to use # TNS_USER_AGENT = uncomment and fill to use # Pipeline PIPELINE_WORKING_DIR=pipeline-runs FLUX_DEFAULT_MIN_ERROR=0.001 POS_DEFAULT_MIN_ERROR=0.01 RAW_IMAGE_DIR=raw-images HOME_DATA_DIR=vast-pipeline-extra-data # HOME_DATA_ROOT = Uncomment to set a custom path to user data dirs # PIPELINE_MAINTAINANCE_MESSAGE = Uncomment and fill to show MAX_PIPELINE_RUNS=3 MAX_PIPERUN_IMAGES=200 # Q_CLUSTER_TIMEOUT = 86400 # Q_CLUSTER_RETRY = 86402 # Q_CLUSTER_MAX_ATTEMPTS = 1 ETA_V_DATASHADER_THRESHOLD=20000 The available settings are grouped into 4 distinct categories: Django \u00b6 These settings are standard Django settings that are commonly set in the settings.py file of Django projects. Please see this page in the Django documentation for explanations on their meaning. Multiple entries for settings such as EXTRA_APPS or EXTRA_MIDDLEWARE can be entered as comma-separated strings like the following example: .env EXTRA_APPS=django_extensions,debug_toolbar GitHub Authentication \u00b6 The settings in this section control the GitHub organization authentication method. Please refer to the Python Social Auth documentation for descriptions of the required settings. Note By default the pipeline is set up for authentication using GitHub organizations. Note that switching to teams will require changes to settings.py . Please refer to the instructions in the Python Social Auth documentation . External APIs \u00b6 The pipeline website interface supports querying some external APIs, e.g. SIMBAD, NED, VizieR, TNS. Some of these APIs require authentication which are described below. Transient Name Server (TNS) \u00b6 If you wish to enable TNS cone search results on the source detail page , you must first obtain an API key for TNS. Create a TNS account at https://www.wis-tns.org/user/register if you do not already have one. Once logged in, create a bot by navigating to https://www.wis-tns.org/bots and clicking \"Add bot\" near the top of the table. Fill in the create bot form. Ensure that you select \"Create new API key\". Securely store the API key and paste it into your pipeline webinterface/.env file under TNS_API_KEY . Warning Do not lose the API key! It is not possible to retrieve it again past this point. Navigate to your account page on TNS and copy the User-Agent specification. Paste it into your pipeline webinterface/.env file under TNS_USER_AGENT . webinterface/.env TNS_USER_AGENT='tns_marker{\"tns_id\": 0000, \"type\": \"user\", \"name\": \"your_username\"}' Pipeline \u00b6 These settings apply to various aspects of the VAST pipeline itself. The table below provides descriptions of each setting. Setting Default Value Description PIPELINE_WORKING_DIR pipeline-runs The name of the working directory where pipeline run directories are created. The pipeline location acts as the root directory. FLUX_DEFAULT_MIN_ERROR 0.001 In the event a measurement is ingested with a flux error of 0 from Selavy, the error is replaced with this default value (mJy). POS_DEFAULT_MIN_ERROR 0.01 In the event a measurement is ingested with an positional error of 0 from Selavy, the error is replaced with this default value (arcsec). RAW_IMAGE_DIR raw-images Directory where the majority of raw ASKAP FITS images are expected to be stored. This directory is scanned to provide user with an image list when configuration a job using the website interface. HOME_DATA_DIR vast-pipeline-extra-data Directory relative to the user's home directory that contains additional input images and catalogues. Safe to ignore if you don't intend to use this functionality. HOME_DATA_ROOT Disabled Path to directory that contains user's home directories. Enable by uncommenting and setting the desired path. If left disabled (commented), the pipeline will assume the OS default home directory location. PIPELINE_MAINTAINANCE_MESSAGE Disabled The message to display at the top of the webserver. See image below this table for an example. Comment out the setting to disable. MAX_PIPELINE_RUNS 3 The allowed maximum number of concurrent pipeline runs. MAX_PIPERUN_IMAGES 200 The allowed maximum number of images in a single pipeline run (non-admins). Q_CLUSTER_TIMEOUT 86400 Number of seconds a Django-Q cluster worker may spend on a task before it is terminated. See the Django-Q documentation . Q_CLUSTER_RETRY 86402 Number of seconds a Django-Q broker will wait for a cluster to finish a task before it's presented again. See the Django-Q documentation . Q_CLUSTER_MAX_ATTEMPTS 1 Number of times a failed task is retried. See the Django-Q documentation . ETA_V_DATASHADER_THRESHOLD 20000 The number of datapoints above which the eta-V plot uses datashader to plot the non-threshold distribution. Maintenance Message Example \u00b6 .env PIPELINE_MAINTAINANCE_MESSAGE=This website is subject to rapid changes which may result in data loss and may go offline with minimal warning. Please be mindful of usage. Authentication \u00b6 The pipeline supports two authentication methods: GitHub Organizations, intended to multi-user server deployments; and local Django administrator. For a single-user local installation, we recommend creating a Django superuser account. GitHub Organizations \u00b6 Please refer to the Python Social Auth documentation for a complete description on this authentication method and how to set up the GitHub app used for authentication. All settings are entered into the .env file as detailed in the above section . Django superuser \u00b6 Create a Django superuser account with the following command and follow the interactive prompts. python manage.py createsuperuser This account can be used to log into the Django admin panel once the webserver is running (see Starting the Pipeline Web App ) by navigating to https://localhost:8000/pipe-admin/ . Once logged in, you will land on the Django admin page. Navigate back to the pipeline homepage http://localhost:8000/ and you should be authenticated. Data Exploration via Django Web Server \u00b6 You can start the web app/server via the instructions provided in Starting the Pipeline Web App .","title":"Configuration"},{"location":"gettingstarted/configuration/#configuration","text":"This section describe how to configure your VAST Pipeline installation.","title":"Configuration"},{"location":"gettingstarted/configuration/#pipeline-configuration","text":"The following instructions, will get you started in setting up the database and pipeline configuration. Note The commands given in this section, unless otherwise stated, assume that the current directory is the pipeline root and that your pipeline Python environment has been activated. Create a database for the pipeline. If you followed the installation process, you will have a PostgreSQL Docker container running on your system. Use the provided script init-tools/init-db.py script to create a new database for the pipeline. As a security precaution, this script will also create a new database user and set the pipeline database owner to this new user. The initialization script requires several input parameters. For usage information, run with the --help option: python init-tools/init-db.py --help usage: init-db.py [-h] host port admin-username admin-password username password database-name Initialize a PostgreSQL database for VAST Pipeline use. Creates a new superuser and creates a new database owned by the new superuser. positional arguments: host database host port database port admin-username database administrator username admin-password database administrator password username username for the new user/role to create for the VAST Pipeline password password for the new user/role to create for the VAST Pipeline database-name name of the new database to create for the VAST Pipeline optional arguments: -h, --help show this help message and exit Fill in the parameters as appropriate for your configuration. If you followed the installation instructions, these would be the details for your PostgreSQL Docker container. Following from the same example in the installation section: python init-tools/init-db.py localhost 55002 postgres <password> vast <vast-user-password> vastdb Info Where <password> is the superuser password that was passed to docker run , and <vast-user-password> is a new password of your choice for the new vast database user. You may change the values for the username and database-name, the above is just an example. If everything went well the output should be: Creating new user/role vast ... Creating new database vastdb ... Done! Copy the setting configuration file template and modify it with your desired settings. Please refer to the .env File section on this page for further details about the settings that are set in this file along with their defaults. cp webinterface/.env.template webinterface/.env Set the database connection settings in the webinterface/.env file by modifying DATABASE_URL (for URL syntax see this link ). For example: .env DATABASE_URL=psql://vast:<vast-user-password>@localhost:55002/vastdb Note The connection details are the same that you setup during the installation . The database/user names must not contain any spaces or dashes, so use the underscore if you want, e.g. this_is_my_db_name . Create the pipeline database tables. The createcachetable command creates the cache tables required by DjangoQ. python manage.py migrate python manage.py createcachetable Create the pipeline data directories. The pipeline has several directories that can be configured in webinterface/.env : PIPELINE_WORKING_DIR : location to store various pipeline output files. RAW_IMAGE_DIR : default location that the pipeline will search for input images and catalogues to ingest during a pipeline run. Data inputs can also be defined as absolute paths in a pipeline run configuration file, so this setting only affects relative paths in the pipeline run configuration. HOME_DATA_DIR : a path relative to a user's home directory to search for additional input images and catalogues. Intended for multi-user server deployments and unlikely to be useful for local installations. See below for some examples. HOME_DATA_ROOT : path to the location of user home directories. Used together with HOME_DATA_DIR . If not supplied, the pipeline will search for the user's home directory using the default OS location. See below for some examples. .env \u2013 User data configuration examples In the following examples, assume that the user's name is foo . # HOME_DATA_ROOT = Uncomment to set a custom path to user data dirs HOME_DATA_DIR=vast-pipeline-extra-data Using the above settings, the pipeline will search for additional input data in the user's home directory as resolved by the OS. e.g. on an Ubuntu system, this would be /home/foo/vast-pipeline-extra-data . HOME_DATA_ROOT=/data/home HOME_DATA_DIR=vast-pipeline-extra-data Using the above settings, the pipeline will search for additional input data in /data/home/foo/vast-pipeline-extra-data . While the default values for these settings are relative to the pipeline codebase root (i.e. within the repo), we recommend creating these directories outside of the repo and updating the webinterface/.env file appropriately with absolute paths. For example, assuming you wish to create these directories in /data/vast-pipeline : mkdir -p /data/vast-pipeline mkdir /data/vast-pipeline/pipeline-runs mkdir /data/vast-pipeline/raw-images and update the webinterface/.env file with: .env PIPELINE_WORKING_DIR=/data/vast-pipeline/pipeline-runs RAW_IMAGE_DIR=/data/vast-pipeline/raw-images","title":"Pipeline Configuration"},{"location":"gettingstarted/configuration/#env-file","text":"The .env file contains various top-level settings that apply to Django, authentication and the running of the pipeline itself. Shown below is the .env.template file which is provided to be able to copy in step 3 above. .env.template # Django DEBUG=True SECRET_KEY=FillMeUPWithSomeComplicatedString # see https://django-environ.readthedocs.io/en/latest/#tips DATABASE_URL=psql://FILLMYUSER:FILLMYPASSWORD@FILLMYHOST:FILLMYPORT/FILLMYDBNAME # BASE_URL = this for append a base url in a production deployment STATIC_ROOT=./staticfiles/ STATIC_URL=/static/ # STATICFILES_DIRS = uncomment and fill to use # EXTRA_APPS = uncomment and fill to use # EXTRA_MIDDLEWARE = uncomment and fill to use ALLOWED_HOSTS=localhost # Github Authentication GITHUB_AUTH_TYPE='org' SOCIAL_AUTH_GITHUB_KEY=fillMeUp SOCIAL_AUTH_GITHUB_SECRET=fillMeUp SOCIAL_AUTH_GITHUB_ORG_NAME=fillMeUp SOCIAL_AUTH_GITHUB_ADMIN_TEAM=fillMeUp # External APIs # TNS_API_KEY = uncomment and fill to use # TNS_USER_AGENT = uncomment and fill to use # Pipeline PIPELINE_WORKING_DIR=pipeline-runs FLUX_DEFAULT_MIN_ERROR=0.001 POS_DEFAULT_MIN_ERROR=0.01 RAW_IMAGE_DIR=raw-images HOME_DATA_DIR=vast-pipeline-extra-data # HOME_DATA_ROOT = Uncomment to set a custom path to user data dirs # PIPELINE_MAINTAINANCE_MESSAGE = Uncomment and fill to show MAX_PIPELINE_RUNS=3 MAX_PIPERUN_IMAGES=200 # Q_CLUSTER_TIMEOUT = 86400 # Q_CLUSTER_RETRY = 86402 # Q_CLUSTER_MAX_ATTEMPTS = 1 ETA_V_DATASHADER_THRESHOLD=20000 The available settings are grouped into 4 distinct categories:","title":".env File"},{"location":"gettingstarted/configuration/#django","text":"These settings are standard Django settings that are commonly set in the settings.py file of Django projects. Please see this page in the Django documentation for explanations on their meaning. Multiple entries for settings such as EXTRA_APPS or EXTRA_MIDDLEWARE can be entered as comma-separated strings like the following example: .env EXTRA_APPS=django_extensions,debug_toolbar","title":"Django"},{"location":"gettingstarted/configuration/#github-authentication","text":"The settings in this section control the GitHub organization authentication method. Please refer to the Python Social Auth documentation for descriptions of the required settings. Note By default the pipeline is set up for authentication using GitHub organizations. Note that switching to teams will require changes to settings.py . Please refer to the instructions in the Python Social Auth documentation .","title":"GitHub Authentication"},{"location":"gettingstarted/configuration/#external-apis","text":"The pipeline website interface supports querying some external APIs, e.g. SIMBAD, NED, VizieR, TNS. Some of these APIs require authentication which are described below.","title":"External APIs"},{"location":"gettingstarted/configuration/#transient-name-server-tns","text":"If you wish to enable TNS cone search results on the source detail page , you must first obtain an API key for TNS. Create a TNS account at https://www.wis-tns.org/user/register if you do not already have one. Once logged in, create a bot by navigating to https://www.wis-tns.org/bots and clicking \"Add bot\" near the top of the table. Fill in the create bot form. Ensure that you select \"Create new API key\". Securely store the API key and paste it into your pipeline webinterface/.env file under TNS_API_KEY . Warning Do not lose the API key! It is not possible to retrieve it again past this point. Navigate to your account page on TNS and copy the User-Agent specification. Paste it into your pipeline webinterface/.env file under TNS_USER_AGENT . webinterface/.env TNS_USER_AGENT='tns_marker{\"tns_id\": 0000, \"type\": \"user\", \"name\": \"your_username\"}'","title":"Transient Name Server (TNS)"},{"location":"gettingstarted/configuration/#pipeline","text":"These settings apply to various aspects of the VAST pipeline itself. The table below provides descriptions of each setting. Setting Default Value Description PIPELINE_WORKING_DIR pipeline-runs The name of the working directory where pipeline run directories are created. The pipeline location acts as the root directory. FLUX_DEFAULT_MIN_ERROR 0.001 In the event a measurement is ingested with a flux error of 0 from Selavy, the error is replaced with this default value (mJy). POS_DEFAULT_MIN_ERROR 0.01 In the event a measurement is ingested with an positional error of 0 from Selavy, the error is replaced with this default value (arcsec). RAW_IMAGE_DIR raw-images Directory where the majority of raw ASKAP FITS images are expected to be stored. This directory is scanned to provide user with an image list when configuration a job using the website interface. HOME_DATA_DIR vast-pipeline-extra-data Directory relative to the user's home directory that contains additional input images and catalogues. Safe to ignore if you don't intend to use this functionality. HOME_DATA_ROOT Disabled Path to directory that contains user's home directories. Enable by uncommenting and setting the desired path. If left disabled (commented), the pipeline will assume the OS default home directory location. PIPELINE_MAINTAINANCE_MESSAGE Disabled The message to display at the top of the webserver. See image below this table for an example. Comment out the setting to disable. MAX_PIPELINE_RUNS 3 The allowed maximum number of concurrent pipeline runs. MAX_PIPERUN_IMAGES 200 The allowed maximum number of images in a single pipeline run (non-admins). Q_CLUSTER_TIMEOUT 86400 Number of seconds a Django-Q cluster worker may spend on a task before it is terminated. See the Django-Q documentation . Q_CLUSTER_RETRY 86402 Number of seconds a Django-Q broker will wait for a cluster to finish a task before it's presented again. See the Django-Q documentation . Q_CLUSTER_MAX_ATTEMPTS 1 Number of times a failed task is retried. See the Django-Q documentation . ETA_V_DATASHADER_THRESHOLD 20000 The number of datapoints above which the eta-V plot uses datashader to plot the non-threshold distribution.","title":"Pipeline"},{"location":"gettingstarted/configuration/#maintenance-message-example","text":".env PIPELINE_MAINTAINANCE_MESSAGE=This website is subject to rapid changes which may result in data loss and may go offline with minimal warning. Please be mindful of usage.","title":"Maintenance Message Example"},{"location":"gettingstarted/configuration/#authentication","text":"The pipeline supports two authentication methods: GitHub Organizations, intended to multi-user server deployments; and local Django administrator. For a single-user local installation, we recommend creating a Django superuser account.","title":"Authentication"},{"location":"gettingstarted/configuration/#github-organizations","text":"Please refer to the Python Social Auth documentation for a complete description on this authentication method and how to set up the GitHub app used for authentication. All settings are entered into the .env file as detailed in the above section .","title":"GitHub Organizations"},{"location":"gettingstarted/configuration/#django-superuser","text":"Create a Django superuser account with the following command and follow the interactive prompts. python manage.py createsuperuser This account can be used to log into the Django admin panel once the webserver is running (see Starting the Pipeline Web App ) by navigating to https://localhost:8000/pipe-admin/ . Once logged in, you will land on the Django admin page. Navigate back to the pipeline homepage http://localhost:8000/ and you should be authenticated.","title":"Django superuser"},{"location":"gettingstarted/configuration/#data-exploration-via-django-web-server","text":"You can start the web app/server via the instructions provided in Starting the Pipeline Web App .","title":"Data Exploration via Django Web Server"},{"location":"gettingstarted/deployment/","text":"Deployment \u00b6 Production System \u00b6 This section describes a simple deployment without using Docker containers, assuming the use of WhiteNoise to serve the static files. It is possible to serve the static files using other methods (e.g. Nginx). And in the future it is possible to upgrade the deployment stack using Docker container and Docker compose (we foresee 3 main containers: Django, Dask and Traefik/Nginx). We recommend in any case reading Django deployment documentation for general knowledge. Note We assume deployment to a UNIX server . The following steps describes how to set up the Django side of the production deployment, and can be of reference for a future Dockerization. They assumed you have SSH access to your remote server and have sudo priviledges. Web App Deployment \u00b6 Clone the repo in a suitable path, e.g. /opt/ . $ cd /opt && sudo git clone https://github.com/askap-vast/vast-pipeline Follow the Installation Instructions . We recommend installing the Python virtual environment under the pipeline folder. $ cd /opt/vast-pipeline && virtualenv -p python3 pipeline_env Configure your .env files with all the right settings. Check that your server is running fine by changing DEBUG = True in the .env file. Run Django deployment checklist command to see what are you missing. It is possible that some options are turned off, as implemented in the reverse proxy or load balancer of your server (e.g. SECURE_SSL_REDIRECT = False or not set, assumes your reverse proxy redirect HTTP to HTTPS). ( pipeline_env ) $ ./manage.py check --deploy Build up the static and fix url in JS9: ( pipeline_env ) $ cd /opt/vast-pipeline && npm ci && npm start \\ && npm run js9staticprod && ./manage.py collectstatic -c --noinput Set up a unit/systemd file as recommended in Gunicorn docs (feel free to use the socket or an IP and port). An example of command to write in the file is (assuming a virtual environment is installed in venv under the main pipeline folder): ExecStart = /opt/vast-pipeline/venv/bin/gunicorn -w 3 -k gevent \\ --worker-connections = 1000 --timeout 120 --limit-request-line 6500 \\ -b 127 .0.0.1:8000 webinterface.wsgi NOTE : (for future development) the --limit-request-line parameter needs to be adjusted for the actual request length as that might change if more parameters are added to the query. Finalise the installation of the unit file. Some good instructions on where to put, link and install the unit file are described in the Jupyter Hub docs Extra Service(s) Deployment \u00b6 Django Q \u00b6 In order to run a pipeline run from the Web App, the Django Q process needs to be started and managed as a service by the OS. In order to do so we recommend building a unit/systemd file to manage the Django Q process, in a similar way of the gunicorn process (following the Jupyter Hub docs ): ... WorkingDirectory = /opt/vast-pipeline ExecStart = /opt/vast-pipeline/venv/bin/python manage.py qcluster ... Tip In the examples above, the Python virtual enviroment used by the pipeline is installed in the venv folder under the cloned repository. Security \u00b6 By default the settings file has some security parameters that are set when you run the web app in production ( DEBUG = False ), but you can read more in the Django documentation or in this blog post in which they explain how to get an A+ rating for your web site.","title":"Deployment"},{"location":"gettingstarted/deployment/#deployment","text":"","title":"Deployment"},{"location":"gettingstarted/deployment/#production-system","text":"This section describes a simple deployment without using Docker containers, assuming the use of WhiteNoise to serve the static files. It is possible to serve the static files using other methods (e.g. Nginx). And in the future it is possible to upgrade the deployment stack using Docker container and Docker compose (we foresee 3 main containers: Django, Dask and Traefik/Nginx). We recommend in any case reading Django deployment documentation for general knowledge. Note We assume deployment to a UNIX server . The following steps describes how to set up the Django side of the production deployment, and can be of reference for a future Dockerization. They assumed you have SSH access to your remote server and have sudo priviledges.","title":"Production System"},{"location":"gettingstarted/deployment/#web-app-deployment","text":"Clone the repo in a suitable path, e.g. /opt/ . $ cd /opt && sudo git clone https://github.com/askap-vast/vast-pipeline Follow the Installation Instructions . We recommend installing the Python virtual environment under the pipeline folder. $ cd /opt/vast-pipeline && virtualenv -p python3 pipeline_env Configure your .env files with all the right settings. Check that your server is running fine by changing DEBUG = True in the .env file. Run Django deployment checklist command to see what are you missing. It is possible that some options are turned off, as implemented in the reverse proxy or load balancer of your server (e.g. SECURE_SSL_REDIRECT = False or not set, assumes your reverse proxy redirect HTTP to HTTPS). ( pipeline_env ) $ ./manage.py check --deploy Build up the static and fix url in JS9: ( pipeline_env ) $ cd /opt/vast-pipeline && npm ci && npm start \\ && npm run js9staticprod && ./manage.py collectstatic -c --noinput Set up a unit/systemd file as recommended in Gunicorn docs (feel free to use the socket or an IP and port). An example of command to write in the file is (assuming a virtual environment is installed in venv under the main pipeline folder): ExecStart = /opt/vast-pipeline/venv/bin/gunicorn -w 3 -k gevent \\ --worker-connections = 1000 --timeout 120 --limit-request-line 6500 \\ -b 127 .0.0.1:8000 webinterface.wsgi NOTE : (for future development) the --limit-request-line parameter needs to be adjusted for the actual request length as that might change if more parameters are added to the query. Finalise the installation of the unit file. Some good instructions on where to put, link and install the unit file are described in the Jupyter Hub docs","title":"Web App Deployment"},{"location":"gettingstarted/deployment/#extra-services-deployment","text":"","title":"Extra Service(s) Deployment"},{"location":"gettingstarted/deployment/#django-q","text":"In order to run a pipeline run from the Web App, the Django Q process needs to be started and managed as a service by the OS. In order to do so we recommend building a unit/systemd file to manage the Django Q process, in a similar way of the gunicorn process (following the Jupyter Hub docs ): ... WorkingDirectory = /opt/vast-pipeline ExecStart = /opt/vast-pipeline/venv/bin/python manage.py qcluster ... Tip In the examples above, the Python virtual enviroment used by the pipeline is installed in the venv folder under the cloned repository.","title":"Django Q"},{"location":"gettingstarted/deployment/#security","text":"By default the settings file has some security parameters that are set when you run the web app in production ( DEBUG = False ), but you can read more in the Django documentation or in this blog post in which they explain how to get an A+ rating for your web site.","title":"Security"},{"location":"gettingstarted/installation/","text":"Installation \u00b6 This document provides instructions on installing the VAST Pipeline for local use. The VAST Pipeline consists of 3 main components that require installation: a PostgreSQL database, a Django application, a front-end website. The instructions have been tested on Debian/Ubuntu and macOS. PostgreSQL \u00b6 We recommend using a Docker container for the database rather than installing the database system-wide. Steps: Install Docker. Refer to the official documentation , and for Ubuntu users to this . Remember to add your user account to the docker group official docs , by running: sudo groupadd docker sudo usermod -aG docker $USER Create a PostgreSQL container. The VAST Pipeline requires a PostgreSQL database with the Q3C plugin to enable special indexing on coordinates and fast cone-search queries. We have prepared a Docker image based on the latest PostgreSQL image that includes Q3C . Start a container using this image by running the command below, replacing <container-name> with a name of your choice (e.g. vast-pipeline-db) and <password> with a password of your choice which will be set for the default postgres database superuser account. docker run --name <container-name> --env POSTGRES_PASSWORD=<password> --publish-all --detach ghcr.io/marxide/postgres-q3c:latest The --publish-all option will make the PostgreSQL server port 5432 in the container accessible on a random available port on your system (the host). The --detach option instructs Docker to start the container in the background rather than taking over your current shell. Verify that the container is running and note the host port that 5432/tcp is published on by running docker ps , e.g. in the example below, the host port is 55002 . docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8ff553add2ed ghcr.io/marxide/postgres-q3c:latest \"docker-entrypoint.s\u2026\" 4 seconds ago Up 3 seconds 0.0.0.0:55002->5432/tcp vast-pipeline-db The database server should now be running in a container on your machine. Tip To stop the database server, simply stop the container with the following command docker stop <container-name or container-id> You can start an existing stopped container with the following command docker start <container-name or container-id> Note that docker run and docker start are not the same. docker run will create and start a container from an image; docker start will start an existing stopped container. If you have previously created a VAST Pipeline database container and you wish to reuse it, you want to use docker start . You will likely need to restart the container after a system reboot. Python Environment \u00b6 We strongly recommend installing the VAST Pipeline in an isolated virtual environment (e.g. using Miniconda , Virtualenv , or venv ). This will keep the rather complex set of dependencies separated from the system-wide Python installation. Create a new Python environment using your chosen virtual environment manager and activate it. For example, Miniconda users should run the following command, replacing <environment-name> with an appropriate name (e.g. pipeline-env): conda create --name <environment-name> python=3.8 conda activate <environment-name> Note All further installation instructions will assume you have activated your new virtual environment. Your environment manager will usually prepend the virtual environment name to the shell prompt, e.g. (pipeline-env) $ ... Clone the pipeline repository https://github.com/askap-vast/vast-pipeline and change into the repo directory. git clone https://github.com/askap-vast/vast-pipeline.git cd vast-pipeline Warning Do not change the the repo folder name, e.g. git clone https://github.com/askap-vast/vast-pipeline.git my-pipeline-local-dev (Optional) Checkout the version you want to install. Currently, the repo will have cloned the latest code from the master branch. If you require a specific version, checkout the appropriate version tag into a new branch e.g. for version 0.2.0 git checkout -b <new-branch-name> 0.2.0 Install non-Python dependencies. Some of the Python dependencies required by the pipeline depend on some non-Python libraries. These can also be installed by Miniconda, otherwise they are best installed using an appropriate package manager for your operating system e.g. apt for Debian/Ubuntu, dnf for RHEL 8/CentOS 8, Homebrew for macOS. The dependencies are: Miniconda Debian/Ubuntu RHEL/CentOS Homebrew libpq graphviz Both are available on the conda-forge channel. They are also specified in the environment file requirements/environment.yml which can be used to install the required packages into an activated conda environment with the following command conda env update -f requirements/environment.yml libpq-dev libgraphviz-dev libpq-devel graphviz-devel CentOS users You may need to enable the PowerTools repository to install graphviz-devel . dnf install dnf-plugins-core dnf config-manager --set-enabled powertools libpq graphviz Install the pipeline and it's Python dependencies. pip install . Warning Don't forget the . at the end of the above command. It instructs pip that the root directory of the package to install is the current directory. Tip If you are intending to deploy an instance of the pipeline onto a server, you may also want to install the recommended production extras with pip install .[prod] . However, note that these are recommendations only and there are other alternative packages that may work just as well. Tip If you intend to contribute to development of the pipeline, you will need the Python dependency management tool Poetry . See the development guidelines . Front-End Assets Quickstart \u00b6 In order to install and compile the front-end website assets (modules like js9 and bootstrap, as well as minification of JS and CSS files) you need a recent version of NodeJS installed. Installation of NodeJS \u00b6 If you are using Miniconda and installed the requirements/environment.yml file as shown above, then NodeJS is already installed. Otherwise, we recommend following the instructions on the NodeJS downloads page for your OS (there are many installation options). Setting up the front-end assets \u00b6 In order to set up the front end assets, run: npm ci && npm start Note Ensure you are still in the root of the repo before running the command above. The npm ci command (\"clean install\") will remove all previous node modules and install all the dependencies from scratch. The npm start command will run the default gulp \"task\" which, among other things, compiles Sass into CSS, minifies CSS and JS files, and copies these files into the static/vendor folder. For more details of compilation of frontend assets (e.g. single tasks), and front-end developement set up read the Front End Developing Guidelines . Bug When npm start or npm run start was run in a Ubuntu 20.04 LTS (containerised environment), for some unknown reasons, both commands failed with the following error. [12:48:19] 'js9Make' errored after 7.67 ms [12:48:19] Error: spawn make ENOENT at Process.ChildProcess._handle.onexit (internal/child_process.js:267:19) at onErrorNT (internal/child_process.js:469:16) at processTicksAndRejections (internal/process/task_queues.js:84:21) [12:48:19] 'default' errored after 2.63 s npm ERR! code ELIFECYCLE npm ERR! errno 1 npm ERR! vast-pipeline@99.99.99-dev start: `gulp default` npm ERR! Exit status 1 npm ERR! npm ERR! Failed at the vast-pipeline@99.99.99-dev start script. npm ERR! This is probably not a problem with npm. There is likely additional logging output above. npm ERR! A complete log of this run can be found in: npm ERR! /home/vast/.npm/_logs/2020-10-06T01_48_19_215Z-debug.log The way around for this issue is unorthodox. The following steps were followed to overcome the issue: cd node_modules/js9/ ./configure make make install cd ~/vast-pipeline/ ## (to comeback to the root folder of the project) npm install That somehow solved the issue mentioned above. Done! Now go to Vast Pipeline Configuration file to see how to initialize and run the pipeline. Otherwise if you intend on developing the repo open the Contributing and Developing Guidelines file for instructions on how to contribute to the repo.","title":"Installation"},{"location":"gettingstarted/installation/#installation","text":"This document provides instructions on installing the VAST Pipeline for local use. The VAST Pipeline consists of 3 main components that require installation: a PostgreSQL database, a Django application, a front-end website. The instructions have been tested on Debian/Ubuntu and macOS.","title":"Installation"},{"location":"gettingstarted/installation/#postgresql","text":"We recommend using a Docker container for the database rather than installing the database system-wide. Steps: Install Docker. Refer to the official documentation , and for Ubuntu users to this . Remember to add your user account to the docker group official docs , by running: sudo groupadd docker sudo usermod -aG docker $USER Create a PostgreSQL container. The VAST Pipeline requires a PostgreSQL database with the Q3C plugin to enable special indexing on coordinates and fast cone-search queries. We have prepared a Docker image based on the latest PostgreSQL image that includes Q3C . Start a container using this image by running the command below, replacing <container-name> with a name of your choice (e.g. vast-pipeline-db) and <password> with a password of your choice which will be set for the default postgres database superuser account. docker run --name <container-name> --env POSTGRES_PASSWORD=<password> --publish-all --detach ghcr.io/marxide/postgres-q3c:latest The --publish-all option will make the PostgreSQL server port 5432 in the container accessible on a random available port on your system (the host). The --detach option instructs Docker to start the container in the background rather than taking over your current shell. Verify that the container is running and note the host port that 5432/tcp is published on by running docker ps , e.g. in the example below, the host port is 55002 . docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8ff553add2ed ghcr.io/marxide/postgres-q3c:latest \"docker-entrypoint.s\u2026\" 4 seconds ago Up 3 seconds 0.0.0.0:55002->5432/tcp vast-pipeline-db The database server should now be running in a container on your machine. Tip To stop the database server, simply stop the container with the following command docker stop <container-name or container-id> You can start an existing stopped container with the following command docker start <container-name or container-id> Note that docker run and docker start are not the same. docker run will create and start a container from an image; docker start will start an existing stopped container. If you have previously created a VAST Pipeline database container and you wish to reuse it, you want to use docker start . You will likely need to restart the container after a system reboot.","title":"PostgreSQL"},{"location":"gettingstarted/installation/#python-environment","text":"We strongly recommend installing the VAST Pipeline in an isolated virtual environment (e.g. using Miniconda , Virtualenv , or venv ). This will keep the rather complex set of dependencies separated from the system-wide Python installation. Create a new Python environment using your chosen virtual environment manager and activate it. For example, Miniconda users should run the following command, replacing <environment-name> with an appropriate name (e.g. pipeline-env): conda create --name <environment-name> python=3.8 conda activate <environment-name> Note All further installation instructions will assume you have activated your new virtual environment. Your environment manager will usually prepend the virtual environment name to the shell prompt, e.g. (pipeline-env) $ ... Clone the pipeline repository https://github.com/askap-vast/vast-pipeline and change into the repo directory. git clone https://github.com/askap-vast/vast-pipeline.git cd vast-pipeline Warning Do not change the the repo folder name, e.g. git clone https://github.com/askap-vast/vast-pipeline.git my-pipeline-local-dev (Optional) Checkout the version you want to install. Currently, the repo will have cloned the latest code from the master branch. If you require a specific version, checkout the appropriate version tag into a new branch e.g. for version 0.2.0 git checkout -b <new-branch-name> 0.2.0 Install non-Python dependencies. Some of the Python dependencies required by the pipeline depend on some non-Python libraries. These can also be installed by Miniconda, otherwise they are best installed using an appropriate package manager for your operating system e.g. apt for Debian/Ubuntu, dnf for RHEL 8/CentOS 8, Homebrew for macOS. The dependencies are: Miniconda Debian/Ubuntu RHEL/CentOS Homebrew libpq graphviz Both are available on the conda-forge channel. They are also specified in the environment file requirements/environment.yml which can be used to install the required packages into an activated conda environment with the following command conda env update -f requirements/environment.yml libpq-dev libgraphviz-dev libpq-devel graphviz-devel CentOS users You may need to enable the PowerTools repository to install graphviz-devel . dnf install dnf-plugins-core dnf config-manager --set-enabled powertools libpq graphviz Install the pipeline and it's Python dependencies. pip install . Warning Don't forget the . at the end of the above command. It instructs pip that the root directory of the package to install is the current directory. Tip If you are intending to deploy an instance of the pipeline onto a server, you may also want to install the recommended production extras with pip install .[prod] . However, note that these are recommendations only and there are other alternative packages that may work just as well. Tip If you intend to contribute to development of the pipeline, you will need the Python dependency management tool Poetry . See the development guidelines .","title":"Python Environment"},{"location":"gettingstarted/installation/#front-end-assets-quickstart","text":"In order to install and compile the front-end website assets (modules like js9 and bootstrap, as well as minification of JS and CSS files) you need a recent version of NodeJS installed.","title":"Front-End Assets Quickstart"},{"location":"gettingstarted/installation/#installation-of-nodejs","text":"If you are using Miniconda and installed the requirements/environment.yml file as shown above, then NodeJS is already installed. Otherwise, we recommend following the instructions on the NodeJS downloads page for your OS (there are many installation options).","title":"Installation of NodeJS"},{"location":"gettingstarted/installation/#setting-up-the-front-end-assets","text":"In order to set up the front end assets, run: npm ci && npm start Note Ensure you are still in the root of the repo before running the command above. The npm ci command (\"clean install\") will remove all previous node modules and install all the dependencies from scratch. The npm start command will run the default gulp \"task\" which, among other things, compiles Sass into CSS, minifies CSS and JS files, and copies these files into the static/vendor folder. For more details of compilation of frontend assets (e.g. single tasks), and front-end developement set up read the Front End Developing Guidelines . Bug When npm start or npm run start was run in a Ubuntu 20.04 LTS (containerised environment), for some unknown reasons, both commands failed with the following error. [12:48:19] 'js9Make' errored after 7.67 ms [12:48:19] Error: spawn make ENOENT at Process.ChildProcess._handle.onexit (internal/child_process.js:267:19) at onErrorNT (internal/child_process.js:469:16) at processTicksAndRejections (internal/process/task_queues.js:84:21) [12:48:19] 'default' errored after 2.63 s npm ERR! code ELIFECYCLE npm ERR! errno 1 npm ERR! vast-pipeline@99.99.99-dev start: `gulp default` npm ERR! Exit status 1 npm ERR! npm ERR! Failed at the vast-pipeline@99.99.99-dev start script. npm ERR! This is probably not a problem with npm. There is likely additional logging output above. npm ERR! A complete log of this run can be found in: npm ERR! /home/vast/.npm/_logs/2020-10-06T01_48_19_215Z-debug.log The way around for this issue is unorthodox. The following steps were followed to overcome the issue: cd node_modules/js9/ ./configure make make install cd ~/vast-pipeline/ ## (to comeback to the root folder of the project) npm install That somehow solved the issue mentioned above. Done! Now go to Vast Pipeline Configuration file to see how to initialize and run the pipeline. Otherwise if you intend on developing the repo open the Contributing and Developing Guidelines file for instructions on how to contribute to the repo.","title":"Setting up the front-end assets"},{"location":"outputs/coldesc/","text":"Column Descriptions \u00b6 This page details the columns contained in each output file. associations \u00b6 Column Unit Description source_id n/a The database id of the source for the association. meas_id n/a The database id of the measurement for the association. d2d arcsec The on-sky separation of the measurement to the source at the iteration stage the association was created. dr n/a The de Ruiter radius of the measurement to the source at the iteration stage the association was created. Will be 0 if de Ruiter assocation is not being used. bands \u00b6 Column Unit Description id n/a The database id of the band. name n/a The string name of the band, equal to the frequency value. frequency MHz The band central frequency. bandwidth MHz The bandwidth of the frequency band, will be 0 if not known. images \u00b6 Column Unit Description id n/a The database id of the image. band_id n/a The database id of the associated band. skyreg_id n/a The database id of the associated sky region. measurements_path n/a The system path to the measurements parquet file. polarisation n/a The polarisation of the image. name n/a The name of the image, taken from the filename. path n/a The system path to the image FITS file. noise_path n/a The system path to the associated noise image FITS file. background_path n/a The system path to the associated background image FITS file. datetime n/a The date and time of the observation, read from the FITS header. jd days The date and time of the observation in Julian Days. duration s The duration of the observation taken from the FITS header, if available. ra deg The central Right Ascension coordinate of the image. dec deg The central Declination coordinate of the image. fov_bmaj deg The estimated major axis field-of-view value - the radius_pixels multipled by the major axis pixel size. fov_bmin deg The estimated minor axis field-of-view value - the radius_pixels multipled by the minor axis pixel size. physical_bmaj deg The actual major axis on-sky size - the number of pixels on the major axis multiplied by the major axis pixel size. physical_bmin deg The actual minor axis on-sky size - the number of pixels on the minor axis multiplied by the minor axis pixel size. radius_pixels pixels Estimated 'diameter' of the useable image area. beam_bmaj deg The size of the major axis of the image restoring beam. beam_bmin deg The size of the minor axis of the image restoring beam. beam_bpa deg The position angle of the image restoring beam. rms_median mJy/beam The median RMS value derrived from the RMS map. rms_min mJy/beam The minimum RMS value derrived from the RMS map (pixel value). rms_max mJy/beam The maximum RMS value derrived from the RMS map (pixel value). measurements \u00b6 Tip Some columns are the same as that defined in the Selavy source finder output . Column Unit Description island_id n/a The Selavy assigned island_id. component_id n/a The Selavy assigned component_id. local_rms mJy The rms value at the location of the measurement. ra deg The right ascension coordinate of the measurement. ra_err deg The error of the right ascension coordinate of the measurement. dec deg The declination coordinate of the measurement. dec_err deg The error of the declination coordinate of the measurement. flux_peak mJy/beam The measured peak flux of the component. flux_peak_err mJy/beam The error of the measured peak flux of the component. flux_int mJy The measured integrated flux of the component. flux_int_err mJy The error of the measured integrated flux of the component. bmaj arcsec The major axis size of the fitted Gaussian (FWHM). err_bmaj deg The error of the major axis size of the fitted Gaussian (FWHM). bmin arcsec The minor axis size of the fitted Gaussian (FWHM). err_bmin deg The error of the minor axis size of the fitted Gaussian (FWHM). pa deg The position angle of the fitted Gaussian (FWHM). err_pa deg The error of the position angle of the fitted Gaussian (FWHM). psf_bmaj arcsec The Selavy deconvolved size of the major axis of the fitted Gaussian. psf_bmin arcsec The Selavy deconvolved size of the minor axis of the fitted Gaussian. psf_pa deg The Selavy deconvolved position angle of the fitted Gaussian. flag_c4 n/a Selavy flag denoting whether the component is considered formally bad (doesn't meet chi-squared criterion). chi_squared_fit n/a The Selavy quality of the fit. spectral_index n/a The fitted Selavy spectral index of the component. spectral_index_from_TT n/a Selavy flag to denote if the spectral index has been derived from the Taylor-term images ( True ). has_siblings n/a Selavy flag to denote whether the component is one of many fitted to the same island. image_id n/a The database id of the image the measurement is from. time n/a The date and time of observation the measurement is from (obtained from the image). name n/a The string name of the measurement. snr n/a The signal-to-noise ratio of the measurement. compactness n/a The compactness of the measurement ( flux_int / flux_peak ). ew_sys_err deg The systematic right ascension error assigned to the measurement. ns_sys_err deg The systematic declination error assigned to the measurement. error_radius deg The pipeline estimated error radius of the measurement. uncertainty_ew deg Total RA positional error of the measurement. uncertainty_ns deg Total Dec positional error of the measurement. weight_ew deg \\(^{-1}\\) The weight of the RA error (1/e). weight_ns deg \\(^{-1}\\) The weight of the Dec error (1/e). forced n/a Flag to denote whether the measurement is produced from the forced fitting procedure ( True ). flux_int_isl_ratio n/a The ratio of the measurements integrated flux to the total island integrated flux. flux_peak_isl_ratio n/a The ratio of the measurements peak flux to the total island peak flux. id n/a The database id of the measurement. measurement_pairs \u00b6 Column Unit Description meas_id_a n/a The database id of measurement a of the pair. meas_id_b n/a The database id of measurement b of the pair. flux_int_a mJy The integrated flux of measurement a of the pair. flux_int_err_a mJy The error of the integrated flux of measurement a of the pair. flux_peak_a mJy/beam The peak flux of measurement a of the pair. flux_peak_err_a mJy/beam The error of the peak flux of measurement a of the pair. image_name_a n/a The image name of measurement a of the pair. flux_int_b mJy The integrated flux of measurement b of the pair. flux_int_err_b mJy The error of the integrated flux of measurement b of the pair. flux_peak_b mJy/beam The peak flux of measurement b of the pair. flux_peak_err_b mJy/beam The error of the peak flux of measurement b of the pair. image_name_b n/a The image name of measurement b of the pair. vs_peak n/a The pair \\(V_s\\) value using the peak flux. vs_int n/a The pair \\(V_s\\) value using the integrated flux. m_peak n/a The pair \\(m\\) value using the peak flux. m_int n/a The pair \\(m\\) value using the integrated flux. source_id n/a The database id of the source the pair is associated to. relations \u00b6 Column Unit Description from_source_id n/a The database id of the first source in the relation pair. to_source_id n/a The database id of the second source in the relation pair. skyregions \u00b6 Column Unit Description id n/a The database id of the sky region. centre_ra deg The right ascension value of the sky region central coordinate. centre_dec deg The declination value of the sky region central coordinate. width_ra deg The width of the area covered by the sky region. width_dec deg The height of the area covered by the sky region. xtr_radius deg The hypotenuse radius of the sky region. x rad The central cartesian x coordinate of the sky region. y rad The central cartesian y coordinate of the sky region. z rad The central cartesian z coordinate of the sky region. sources \u00b6 Note The index column of the sources parquet is set to the database id of the source. Column Unit Description n_meas n/a The total number of measurements associated to the source (selavy and forced). n_meas_sel n/a The total number of selavy measurements associated to the source. n_meas_forced n/a The total number of forced measurements associated to the source. n_sibl n/a The total number of measurements that have a has_sibling value of True . n_rel n/a The total number of relations the source has. wavg_ra deg The weighted average of the Right Ascension of the measurements, that acts as the source position. wavg_dec deg The weighted average of the Declination of the measurements, that acts as the source position. wavg_uncertainty_ew deg The error of the weighted average right ascension value. wavg_uncertainty_ns deg The error of the weighted average declination value. new n/a Flag to signify the source is classed as a new source ( True ). new_high_sigma n/a The largest sigma value a new source would have if it was placed at its location in the previous images it was not detected in. See New Sources for more information. Set to 0 for non-new sources. n_neighbour_dist deg The on-sky separation to the nearest source with in the same pipeline run. avg_compactness n/a The average compactness value of the associated measurements. min_snr n/a The minimum signal-to-noise ratio of the associated measurements. max_snr n/a The maximum signal-to-noise ratio of the associated measurements. avg_flux_int mJy The average integrated flux value of the measurements associated to the source (inc. forced measurements). max_flux_int mJy The maximum integrated flux value of the measurements associated to the source (inc. forced measurements). min_flux_int mJy The minimum integrated flux value of the measurements associated to the source (inc. forced measurements). avg_flux_peak mJy/beam The average peak flux value of the measurements associated to the source (inc. forced measurements). max_flux_peak mJy/beam The maximum peak flux value of the measurements associated to the source (inc. forced measurements). min_flux_peak mJy/beam The minimum peak flux value of the measurements associated to the source (inc. forced measurements). min_flux_peak_isl_ratio n/a The minimum ratio of the peak flux to the total island peak flux of the measurements associated to the source. min_flux_int_isl_ratio n/a The minimum ratio of the integrated flux to the total island integrated flux of the measurements associated to the source. v_int n/a The calculated variability \\(V\\) metric using the integrated flux values. See variability statistics . v_peak n/a The calculated variability \\(V\\) metric using the peak flux values. See variability statistics . eta_int n/a The calculated variability \\(\\eta\\) metric using the integrated flux. See variability statistics . eta_peak n/a The calculated variability \\(\\eta\\) metric using the peak flux. See variability statistics . vs_abs_significant_max_peak n/a The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. See variability statistics . m_abs_significant_max_peak n/a The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. See variability statistics . vs_abs_significant_max_int n/a The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. See variability statistics . m_abs_significant_max_int n/a The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. See variability statistics .","title":"Column Descriptions"},{"location":"outputs/coldesc/#column-descriptions","text":"This page details the columns contained in each output file.","title":"Column Descriptions"},{"location":"outputs/coldesc/#associations","text":"Column Unit Description source_id n/a The database id of the source for the association. meas_id n/a The database id of the measurement for the association. d2d arcsec The on-sky separation of the measurement to the source at the iteration stage the association was created. dr n/a The de Ruiter radius of the measurement to the source at the iteration stage the association was created. Will be 0 if de Ruiter assocation is not being used.","title":"associations"},{"location":"outputs/coldesc/#bands","text":"Column Unit Description id n/a The database id of the band. name n/a The string name of the band, equal to the frequency value. frequency MHz The band central frequency. bandwidth MHz The bandwidth of the frequency band, will be 0 if not known.","title":"bands"},{"location":"outputs/coldesc/#images","text":"Column Unit Description id n/a The database id of the image. band_id n/a The database id of the associated band. skyreg_id n/a The database id of the associated sky region. measurements_path n/a The system path to the measurements parquet file. polarisation n/a The polarisation of the image. name n/a The name of the image, taken from the filename. path n/a The system path to the image FITS file. noise_path n/a The system path to the associated noise image FITS file. background_path n/a The system path to the associated background image FITS file. datetime n/a The date and time of the observation, read from the FITS header. jd days The date and time of the observation in Julian Days. duration s The duration of the observation taken from the FITS header, if available. ra deg The central Right Ascension coordinate of the image. dec deg The central Declination coordinate of the image. fov_bmaj deg The estimated major axis field-of-view value - the radius_pixels multipled by the major axis pixel size. fov_bmin deg The estimated minor axis field-of-view value - the radius_pixels multipled by the minor axis pixel size. physical_bmaj deg The actual major axis on-sky size - the number of pixels on the major axis multiplied by the major axis pixel size. physical_bmin deg The actual minor axis on-sky size - the number of pixels on the minor axis multiplied by the minor axis pixel size. radius_pixels pixels Estimated 'diameter' of the useable image area. beam_bmaj deg The size of the major axis of the image restoring beam. beam_bmin deg The size of the minor axis of the image restoring beam. beam_bpa deg The position angle of the image restoring beam. rms_median mJy/beam The median RMS value derrived from the RMS map. rms_min mJy/beam The minimum RMS value derrived from the RMS map (pixel value). rms_max mJy/beam The maximum RMS value derrived from the RMS map (pixel value).","title":"images"},{"location":"outputs/coldesc/#measurements","text":"Tip Some columns are the same as that defined in the Selavy source finder output . Column Unit Description island_id n/a The Selavy assigned island_id. component_id n/a The Selavy assigned component_id. local_rms mJy The rms value at the location of the measurement. ra deg The right ascension coordinate of the measurement. ra_err deg The error of the right ascension coordinate of the measurement. dec deg The declination coordinate of the measurement. dec_err deg The error of the declination coordinate of the measurement. flux_peak mJy/beam The measured peak flux of the component. flux_peak_err mJy/beam The error of the measured peak flux of the component. flux_int mJy The measured integrated flux of the component. flux_int_err mJy The error of the measured integrated flux of the component. bmaj arcsec The major axis size of the fitted Gaussian (FWHM). err_bmaj deg The error of the major axis size of the fitted Gaussian (FWHM). bmin arcsec The minor axis size of the fitted Gaussian (FWHM). err_bmin deg The error of the minor axis size of the fitted Gaussian (FWHM). pa deg The position angle of the fitted Gaussian (FWHM). err_pa deg The error of the position angle of the fitted Gaussian (FWHM). psf_bmaj arcsec The Selavy deconvolved size of the major axis of the fitted Gaussian. psf_bmin arcsec The Selavy deconvolved size of the minor axis of the fitted Gaussian. psf_pa deg The Selavy deconvolved position angle of the fitted Gaussian. flag_c4 n/a Selavy flag denoting whether the component is considered formally bad (doesn't meet chi-squared criterion). chi_squared_fit n/a The Selavy quality of the fit. spectral_index n/a The fitted Selavy spectral index of the component. spectral_index_from_TT n/a Selavy flag to denote if the spectral index has been derived from the Taylor-term images ( True ). has_siblings n/a Selavy flag to denote whether the component is one of many fitted to the same island. image_id n/a The database id of the image the measurement is from. time n/a The date and time of observation the measurement is from (obtained from the image). name n/a The string name of the measurement. snr n/a The signal-to-noise ratio of the measurement. compactness n/a The compactness of the measurement ( flux_int / flux_peak ). ew_sys_err deg The systematic right ascension error assigned to the measurement. ns_sys_err deg The systematic declination error assigned to the measurement. error_radius deg The pipeline estimated error radius of the measurement. uncertainty_ew deg Total RA positional error of the measurement. uncertainty_ns deg Total Dec positional error of the measurement. weight_ew deg \\(^{-1}\\) The weight of the RA error (1/e). weight_ns deg \\(^{-1}\\) The weight of the Dec error (1/e). forced n/a Flag to denote whether the measurement is produced from the forced fitting procedure ( True ). flux_int_isl_ratio n/a The ratio of the measurements integrated flux to the total island integrated flux. flux_peak_isl_ratio n/a The ratio of the measurements peak flux to the total island peak flux. id n/a The database id of the measurement.","title":"measurements"},{"location":"outputs/coldesc/#measurement_pairs","text":"Column Unit Description meas_id_a n/a The database id of measurement a of the pair. meas_id_b n/a The database id of measurement b of the pair. flux_int_a mJy The integrated flux of measurement a of the pair. flux_int_err_a mJy The error of the integrated flux of measurement a of the pair. flux_peak_a mJy/beam The peak flux of measurement a of the pair. flux_peak_err_a mJy/beam The error of the peak flux of measurement a of the pair. image_name_a n/a The image name of measurement a of the pair. flux_int_b mJy The integrated flux of measurement b of the pair. flux_int_err_b mJy The error of the integrated flux of measurement b of the pair. flux_peak_b mJy/beam The peak flux of measurement b of the pair. flux_peak_err_b mJy/beam The error of the peak flux of measurement b of the pair. image_name_b n/a The image name of measurement b of the pair. vs_peak n/a The pair \\(V_s\\) value using the peak flux. vs_int n/a The pair \\(V_s\\) value using the integrated flux. m_peak n/a The pair \\(m\\) value using the peak flux. m_int n/a The pair \\(m\\) value using the integrated flux. source_id n/a The database id of the source the pair is associated to.","title":"measurement_pairs"},{"location":"outputs/coldesc/#relations","text":"Column Unit Description from_source_id n/a The database id of the first source in the relation pair. to_source_id n/a The database id of the second source in the relation pair.","title":"relations"},{"location":"outputs/coldesc/#skyregions","text":"Column Unit Description id n/a The database id of the sky region. centre_ra deg The right ascension value of the sky region central coordinate. centre_dec deg The declination value of the sky region central coordinate. width_ra deg The width of the area covered by the sky region. width_dec deg The height of the area covered by the sky region. xtr_radius deg The hypotenuse radius of the sky region. x rad The central cartesian x coordinate of the sky region. y rad The central cartesian y coordinate of the sky region. z rad The central cartesian z coordinate of the sky region.","title":"skyregions"},{"location":"outputs/coldesc/#sources","text":"Note The index column of the sources parquet is set to the database id of the source. Column Unit Description n_meas n/a The total number of measurements associated to the source (selavy and forced). n_meas_sel n/a The total number of selavy measurements associated to the source. n_meas_forced n/a The total number of forced measurements associated to the source. n_sibl n/a The total number of measurements that have a has_sibling value of True . n_rel n/a The total number of relations the source has. wavg_ra deg The weighted average of the Right Ascension of the measurements, that acts as the source position. wavg_dec deg The weighted average of the Declination of the measurements, that acts as the source position. wavg_uncertainty_ew deg The error of the weighted average right ascension value. wavg_uncertainty_ns deg The error of the weighted average declination value. new n/a Flag to signify the source is classed as a new source ( True ). new_high_sigma n/a The largest sigma value a new source would have if it was placed at its location in the previous images it was not detected in. See New Sources for more information. Set to 0 for non-new sources. n_neighbour_dist deg The on-sky separation to the nearest source with in the same pipeline run. avg_compactness n/a The average compactness value of the associated measurements. min_snr n/a The minimum signal-to-noise ratio of the associated measurements. max_snr n/a The maximum signal-to-noise ratio of the associated measurements. avg_flux_int mJy The average integrated flux value of the measurements associated to the source (inc. forced measurements). max_flux_int mJy The maximum integrated flux value of the measurements associated to the source (inc. forced measurements). min_flux_int mJy The minimum integrated flux value of the measurements associated to the source (inc. forced measurements). avg_flux_peak mJy/beam The average peak flux value of the measurements associated to the source (inc. forced measurements). max_flux_peak mJy/beam The maximum peak flux value of the measurements associated to the source (inc. forced measurements). min_flux_peak mJy/beam The minimum peak flux value of the measurements associated to the source (inc. forced measurements). min_flux_peak_isl_ratio n/a The minimum ratio of the peak flux to the total island peak flux of the measurements associated to the source. min_flux_int_isl_ratio n/a The minimum ratio of the integrated flux to the total island integrated flux of the measurements associated to the source. v_int n/a The calculated variability \\(V\\) metric using the integrated flux values. See variability statistics . v_peak n/a The calculated variability \\(V\\) metric using the peak flux values. See variability statistics . eta_int n/a The calculated variability \\(\\eta\\) metric using the integrated flux. See variability statistics . eta_peak n/a The calculated variability \\(\\eta\\) metric using the peak flux. See variability statistics . vs_abs_significant_max_peak n/a The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. See variability statistics . m_abs_significant_max_peak n/a The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the peak fluxes. Will be 0 if no significant pair. See variability statistics . vs_abs_significant_max_int n/a The \\(\\mid V_{s}\\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. See variability statistics . m_abs_significant_max_int n/a The \\(\\mid m \\mid\\) value of the most significant two-epoch pair using the integrated fluxes. Will be 0 if no significant pair. See variability statistics .","title":"sources"},{"location":"outputs/outputs/","text":"Outputs Overview \u00b6 This page gives details on the output files that the pipeline writes to disk. Pipeline Run Output Overview \u00b6 The output for a pipeline run will be located in the pipeline working directory, which is defined at the pipeline configuration stage (see Pipeline Configuration ). A sub-directory will exist for each pipeline run that contains the output products for the run. Note If you do not administrate your system or do not have access to a vast-tools notebook interface, please contact your system admin to confirm the working directory and how to best access the files. The pipeline uses the Apache Parquet file format to write results to disk. Details on how to read these files can be found below in Reading the Outputs . Below is the output structure for a pipeline run named new-test-data when the pipeline run option measurements.write_arrow_files has been set to True and the working directory is named pipeline-runs (see File Details for descriptions): pipeline-runs \u251c\u2500\u2500 new-test-data \u2502 \u251c\u2500\u2500 associations.parquet \u2502 \u251c\u2500\u2500 bands.parquet \u2502 \u251c\u2500\u2500 config.yaml \u2502 \u251c\u2500\u2500 config_prev.yaml \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH02_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH03x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH02_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH03x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH12_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 images.parquet \u2502 \u251c\u2500\u2500 YYYY-MM-DD-HH-MM-SS_log.txt \u2502 \u251c\u2500\u2500 measurements.arrow \u2502 \u251c\u2500\u2500 measurement_pairs.arrow \u2502 \u251c\u2500\u2500 measurement_pairs.parquet \u2502 \u251c\u2500\u2500 relations.parquet \u2502 \u251c\u2500\u2500 skyregions.parquet \u2502 \u2514\u2500\u2500 sources.parquet Arrow Files \u00b6 Large pipeline runs (hundreds of images) mean that to read the measurements, hundreds of parquet files need to be read in, and can contain millions of rows. This can be slow using libraries such as pandas, and also consumes a lot of system memory. A solution to this is to save all the measurements associated with the pipeline run into one single file in the Apache Arrow format. The library vaex is able to open .arrow files in an out-of-core context so the memory footprint is hugely reduced along with the reading of the file being very fast. The two-epoch measurement pairs are also saved to arrow format due to the same reasons. See Reading with vaex for further details on using vaex . Note At the time of development vaex could not open parquets in an out-of-core context. This will be reviewed in the future if such functionality is added to vaex . To enable the arrow files to be produced, the option measurements.write_arrow_files is required to be set to True in the pipeline run config. Alternatively, the arrow files can be generated after the completion of the run, see the Generating Arrow Files page for full details. Image Data \u00b6 The data for the images ingested into the pipeline is also stored in the pipeline working directory under the subdirectory images : pipeline-runs \u251c\u2500\u2500 images \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH02_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH03x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH02_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH03x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u2514\u2500\u2500 VAST_2118-06A_EPOCH12_I_cutout_fits \u2502 \u2514\u2500\u2500 measurements.parquet Here, for each image, the selavy measurements that have been ingested are stored in the parquet format under a subdirectory of the respective image name. File Details \u00b6 File Description associations.parquet Contains the association information between sources and measurements. bands.parquet Contains the information of the bands associated with the pipeline run. config.yaml The pipeline run configuration file. config_prev.yaml The previous pipeline run configuration file used by the add image mode. forced_measurements*.parquet Multiple files that contain the forced measurements extracted from the respective image denoted in the filename. images.parquet Contains the information of the images processed in the pipeline run. YYYY-MM-DD-HH-MM-SS_log.txt The log file of the pipeline run. It is timestamped with the date and time of the run start. measurements.arrow An Apache Arrow format file containing all the measurements associated with the pipeline run (see Arrow Files ). measurement_pairs.arrow An Apache Arrow format file containing all the measurement pair metrics (see Arrow Files ). measurement_pairs.parquet Contains all the measurement pairs metrics. relations.parquet Contains the relation information between sources. skyregions.parquet Contains the sky region information of the pipeline run. sources.parquet Contains all the sources resulting from teh pipeline run.","title":"Outputs Overview"},{"location":"outputs/outputs/#outputs-overview","text":"This page gives details on the output files that the pipeline writes to disk.","title":"Outputs Overview"},{"location":"outputs/outputs/#pipeline-run-output-overview","text":"The output for a pipeline run will be located in the pipeline working directory, which is defined at the pipeline configuration stage (see Pipeline Configuration ). A sub-directory will exist for each pipeline run that contains the output products for the run. Note If you do not administrate your system or do not have access to a vast-tools notebook interface, please contact your system admin to confirm the working directory and how to best access the files. The pipeline uses the Apache Parquet file format to write results to disk. Details on how to read these files can be found below in Reading the Outputs . Below is the output structure for a pipeline run named new-test-data when the pipeline run option measurements.write_arrow_files has been set to True and the working directory is named pipeline-runs (see File Details for descriptions): pipeline-runs \u251c\u2500\u2500 new-test-data \u2502 \u251c\u2500\u2500 associations.parquet \u2502 \u251c\u2500\u2500 bands.parquet \u2502 \u251c\u2500\u2500 config.yaml \u2502 \u251c\u2500\u2500 config_prev.yaml \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_0127-73A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH02_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH03x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118+00A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH01_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH02_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH03x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH05x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH06x_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 forced_measurements_VAST_2118-06A_EPOCH12_I_cutout_fits.parquet \u2502 \u251c\u2500\u2500 images.parquet \u2502 \u251c\u2500\u2500 YYYY-MM-DD-HH-MM-SS_log.txt \u2502 \u251c\u2500\u2500 measurements.arrow \u2502 \u251c\u2500\u2500 measurement_pairs.arrow \u2502 \u251c\u2500\u2500 measurement_pairs.parquet \u2502 \u251c\u2500\u2500 relations.parquet \u2502 \u251c\u2500\u2500 skyregions.parquet \u2502 \u2514\u2500\u2500 sources.parquet","title":"Pipeline Run Output Overview"},{"location":"outputs/outputs/#arrow-files","text":"Large pipeline runs (hundreds of images) mean that to read the measurements, hundreds of parquet files need to be read in, and can contain millions of rows. This can be slow using libraries such as pandas, and also consumes a lot of system memory. A solution to this is to save all the measurements associated with the pipeline run into one single file in the Apache Arrow format. The library vaex is able to open .arrow files in an out-of-core context so the memory footprint is hugely reduced along with the reading of the file being very fast. The two-epoch measurement pairs are also saved to arrow format due to the same reasons. See Reading with vaex for further details on using vaex . Note At the time of development vaex could not open parquets in an out-of-core context. This will be reviewed in the future if such functionality is added to vaex . To enable the arrow files to be produced, the option measurements.write_arrow_files is required to be set to True in the pipeline run config. Alternatively, the arrow files can be generated after the completion of the run, see the Generating Arrow Files page for full details.","title":"Arrow Files"},{"location":"outputs/outputs/#image-data","text":"The data for the images ingested into the pipeline is also stored in the pipeline working directory under the subdirectory images : pipeline-runs \u251c\u2500\u2500 images \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_0127-73A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH02_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH03x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118+00A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH01_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH02_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH03x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH05x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u251c\u2500\u2500 VAST_2118-06A_EPOCH06x_I_cutout_fits \u2502 \u2502 \u2514\u2500\u2500 measurements.parquet \u2502 \u2514\u2500\u2500 VAST_2118-06A_EPOCH12_I_cutout_fits \u2502 \u2514\u2500\u2500 measurements.parquet Here, for each image, the selavy measurements that have been ingested are stored in the parquet format under a subdirectory of the respective image name.","title":"Image Data"},{"location":"outputs/outputs/#file-details","text":"File Description associations.parquet Contains the association information between sources and measurements. bands.parquet Contains the information of the bands associated with the pipeline run. config.yaml The pipeline run configuration file. config_prev.yaml The previous pipeline run configuration file used by the add image mode. forced_measurements*.parquet Multiple files that contain the forced measurements extracted from the respective image denoted in the filename. images.parquet Contains the information of the images processed in the pipeline run. YYYY-MM-DD-HH-MM-SS_log.txt The log file of the pipeline run. It is timestamped with the date and time of the run start. measurements.arrow An Apache Arrow format file containing all the measurements associated with the pipeline run (see Arrow Files ). measurement_pairs.arrow An Apache Arrow format file containing all the measurement pair metrics (see Arrow Files ). measurement_pairs.parquet Contains all the measurement pairs metrics. relations.parquet Contains the relation information between sources. skyregions.parquet Contains the sky region information of the pipeline run. sources.parquet Contains all the sources resulting from teh pipeline run.","title":"File Details"},{"location":"outputs/usingoutputs/","text":"Using the Outputs \u00b6 This page gives details on how to open and use the pipeline output files. It is recommended to use pandas or vaex to read the pipeline results from the parquet files. See the sections below for more information on using each library. Note It is also possible to use Dask to read the parquets in an out-of-core context but the general performance can sometimes be poor with many parquet files. vaex is the preferred out-of-core method. Tip Be sure to look at vast-tools , a ready-made library for exploring pipeline results! Reading with pandas \u00b6 pandas documentation . Warning pyarrow will be required to open parquets with pandas . We recommend using this instead of fastparquet . To open a parquet using pandas use the read_parquet method: import pandas as pd sources = pd . read_parquet ( 'pipeline-runs/new-test-data/sources.parquet' ) sources . head () n_meas_forced n_meas ... vs_abs_significant_max_int m_abs_significant_max_int id ... 1 0 3 ... 55.050146 0.191083 2 1 3 ... 29.367098 0.525999 3 0 3 ... 4.388447 0.199877 4 1 3 ... 20.000058 1.047998 5 0 3 ... 0.000000 0.000000 [ 5 rows x 31 columns ] To read multiple parquets at once using pandas a loop must be used: import glob import pandas as pd files = glob . glob ( \"pipeline-runs/images/*/measurements.parquet\" ) data = [ pd . read_parquet ( f ) for f in files ] measurements = pd . concat ( data , ignore_index = True ) Tip If you don't require all the columns you can specify which columes to read using the columns variable. sources = pd . read_parquet ( 'pipeline-runs/new-test-data/sources.parquet' , columns = [ 'id' , 'n_meas' ]) Reading with vaex \u00b6 vaex documentation . Warning vaex is a young project so bugs may be expected along with frequent updates. It has currently been tested with version 3.0.0 . Version 4.0.0 promises opening parquet files in an out-of-core context. Warning Some pipeline parquet format files do not open with vaex 3.0.0. arrow format files should open successfully. A parquet, or arrow file, can be opened using the open() method: import vaex measurements = vaex . open ( 'pipeline-runs/new-test-data/measurements.arrow' ) measurements . head () # source island_id component_id local_rms ra ra_err dec dec_err flux_peak flux_peak_err flux_int flux_int_err bmaj err_bmaj bmin err_bmin pa err_pa psf_bmaj psf_bmin psf_pa flag_c4 chi_squared_fit spectral_index spectral_index_from_TT has_siblings image_id time name snr compactness ew_sys_err ns_sys_err error_radius uncertainty_ew uncertainty_ns weight_ew weight_ns forced flux_int_isl_ratio flux_peak_isl_ratio id 0 730 SB00004_island_1 SB00004_component_1a 0.463596 321.902 4.42819e-06 - 4.20097 2.2637e-06 307.991 0.4742 425.175 1.04728 20.95 1.08838e-05 12.28 4.32359e-06 108.19 0.00160065 15.58 0 - 73.86 False 3516.59 - 99 True True 2 2019 - 08 - 27 13 : 38 : 38.810000000 VAST_2118 + 00 A_SB00004_component_1a 664.352 1.38048 0.000277778 0.000277778 5.05099e-06 0.000277824 0.000277824 1.29557e+07 1.29557e+07 False 0.651019 0.709182 204 1 730 SB00009_island_1 SB00009_component_1a 0.463422 321.902 3.52062e-06 - 4.20103 2.45968e-06 318.544 0.472982 349.471 0.87264 21.4 8.50698e-06 12.78 5.46908e-06 107.02 0.0020224 5.72 0 46.03 True 15427.1 - 99 True True 3 2019 - 08 - 27 18 : 52 : 00.556000000 VAST_2118 - 06 A_SB00009_component_1a 687.374 1.09709 0.000277778 0.000277778 4.26887e-06 0.000277811 0.000277811 1.29569e+07 1.29569e+07 False 0.721399 0.7483 352 2 730 SB00006_island_1 SB00006_component_1a 0.627357 321.901 4.58076e-06 - 4.20086 2.92861e-06 310.503 0.662503 421.137 1.4171 17.05 1.07873e-05 12.42 6.89559e-06 90.71 0.00438684 10.51 4.24 - 82.52 False 4483.74 - 99 True True 5 2019 - 10 - 29 10 : 28 : 07.911000000 VAST_2118 + 00 A_SB00006_component_1a 494.938 1.35631 0.000277778 0.000277778 5.46682e-06 0.000277832 0.000277832 1.2955e+07 1.2955e+07 False 0.677562 0.721427 670 3 730 SB00011_island_1 SB00011_component_1a 0.627496 321.901 3.92144e-06 - 4.20087 3.21715e-06 310.998 0.643049 350.901 1.21083 17.06 9.23494e-06 12.43 7.57501e-06 91.19 0.00481863 6.43 0 27.62 False 4405.81 - 99 True True 4 2019 - 10 - 29 13 : 39 : 33.996000000 VAST_2118 - 06 A_SB00011_component_1a 495.618 1.12831 0.000277778 0.000277778 5.05099e-06 0.000277824 0.000277824 1.29557e+07 1.29557e+07 False 0.677576 0.721816 511 4 730 SB00005_island_1 SB00005_component_1a 0.346147 321.901 1.83032e-06 - 4.20051 1.71797e-06 299.072 0.342783 288.462 0.579893 14.13 4.37963e-06 12.14 3.97013e-06 65.14 0.00546325 2.86 0 6.42 False 2661.49 - 99 True True 7 2019 - 10 - 30 09 : 10 : 04.340000000 VAST_2118 + 00 A_SB00005_component_1a 864.004 0.964524 0.000277778 0.000277778 2.69987e-06 0.000277791 0.000277791 1.29588e+07 1.29588e+07 False 0.659887 0.719556 976 5 730 SB00010_island_1 SB00010_component_1a 0.347692 321.901 1.97328e-06 - 4.20052 1.75466e-06 300.969 0.360198 353.643 0.695754 14.16 4.76862e-06 12.16 3.99059e-06 65.77 0.00546515 6.12 3.95 49.18 False 2412.18 - 99 True True 6 2019 - 10 - 30 10 : 11 : 56.913000000 VAST_2118 - 06 A_SB00010_component_1a 865.62 1.17502 0.000277778 0.000277778 2.56132e-06 0.00027779 0.00027779 1.29589e+07 1.29589e+07 False 0.664605 0.723765 816 6 730 SB00007_island_1 SB00007_component_1a 0.387605 321.901 2.03947e-06 - 4.20032 1.77854e-06 317.014 0.392106 332.662 0.701451 14.56 4.96382e-06 11.54 3.99573e-06 64.78 0.00375775 4.81 0 24.08 False 1486.99 - 99 True True 10 2020 - 01 - 11 05 : 27 : 24.605000000 VAST_2118 + 00 A_SB00007_component_1a 817.88 1.04936 0.000277778 0.000277778 2.83165e-06 0.000277792 0.000277792 1.29587e+07 1.29587e+07 False 0.666924 0.716339 1493 7 730 SB00012_island_1 SB00012_component_1a 0.391978 321.901 2.12442e-06 - 4.20032 1.81129e-06 318.042 0.404457 365.987 0.770202 14.57 5.18203e-06 11.53 4.0454e-06 65.33 0.00378202 5.75 3.63 46.53 False 1328.58 - 99 True True 9 2020 - 01 - 11 05 : 40 : 11.007000000 VAST_2118 - 06 A_SB00012_component_1a 811.376 1.15075 0.000277778 0.000277778 2.69987e-06 0.000277791 0.000277791 1.29588e+07 1.29588e+07 False 0.667317 0.717746 1330 8 730 SB00008_island_1 SB00008_component_1a 0.432726 321.901 2.98443e-06 - 4.20052 2.32201e-06 293.737 0.436863 309.072 0.784657 18.35 7.14713e-06 12.12 5.31097e-06 105.78 0.00261377 4.82 0 18.62 False 2448.82 - 99 True True 13 2020 - 01 - 12 05 : 23 : 07.478000000 VAST_2118 + 00 A_SB00008_component_1a 678.807 1.05221 0.000277778 0.000277778 3.81819e-06 0.000277804 0.000277804 1.29576e+07 1.29576e+07 False 0.63994 0.725001 1889 9 730 SB00013_island_1 SB00013_component_1a 0.437279 321.901 3.14407e-06 - 4.20052 2.36161e-06 294.141 0.451346 340.92 0.864347 18.38 7.55055e-06 12.12 5.36009e-06 106.18 0.00262701 6.01 4 51.55 False 2368.93 - 99 True True 12 2020 - 01 - 12 05 : 36 : 03.834000000 VAST_2118 - 06 A_SB00013_component_1a 672.663 1.15903 0.000277778 0.000277778 4.00455e-06 0.000277807 0.000277807 1.29573e+07 1.29573e+07 False 0.640807 0.72508 1740 Multiple parquet files can be opened at once using the open_many() method: import glob import vaex files = glob . glob ( \"pipeline-runs/images/*/measurements.parquet\" ) measurements = vaex . open_many ( files ) Tip You can convert a vaex dataframe to pandas by using the to_pandas_df() method: import vaex sources = vaex . open ( 'pipeline-runs/new-test-data/sources.parquet' ) sources = sources . to_pandas_df () Linking the Results \u00b6 The table below shows what parameters act as keys to link data from the different results tables. Tip If loading the measurements via the .arrow file, then the measurements already have the source column in-place. Tip The images.parquet file contains the column measurements_path which can be used to get the filepaths for all the selavy parquet files. Data Column Linked to Column associations.parquet meas_id measurements.parquet , forced_measurements*.parquet id associations.parquet source_id sources.parquet id (index column) measurements.parquet , forced_measurements*.parquet image_id images.parquet id images.parquet band_id bands.parquet id images.parquet skyreg_id skyregions.parquet id measurement_pairs.parquet meas_id_a , meas_id_b measurements.parquet , forced_measurements*.parquet id measurement_pairs.parquet source_id sources.parquet id (index column) relations.parquet from_source_id , to_source_id sources.parquet id (index column) vast-tools \u00b6 Link to the vast-tools documentation . VAST has developed a python library called vast-tools that makes the exploration of results from the pipeline simple and efficient, in addition to being designed to be used in a Jupyter Notebook environment. Full details can be found in the documentation linked above, which also includes example notebooks of how to interact with the data.","title":"Using the Outputs"},{"location":"outputs/usingoutputs/#using-the-outputs","text":"This page gives details on how to open and use the pipeline output files. It is recommended to use pandas or vaex to read the pipeline results from the parquet files. See the sections below for more information on using each library. Note It is also possible to use Dask to read the parquets in an out-of-core context but the general performance can sometimes be poor with many parquet files. vaex is the preferred out-of-core method. Tip Be sure to look at vast-tools , a ready-made library for exploring pipeline results!","title":"Using the Outputs"},{"location":"outputs/usingoutputs/#reading-with-pandas","text":"pandas documentation . Warning pyarrow will be required to open parquets with pandas . We recommend using this instead of fastparquet . To open a parquet using pandas use the read_parquet method: import pandas as pd sources = pd . read_parquet ( 'pipeline-runs/new-test-data/sources.parquet' ) sources . head () n_meas_forced n_meas ... vs_abs_significant_max_int m_abs_significant_max_int id ... 1 0 3 ... 55.050146 0.191083 2 1 3 ... 29.367098 0.525999 3 0 3 ... 4.388447 0.199877 4 1 3 ... 20.000058 1.047998 5 0 3 ... 0.000000 0.000000 [ 5 rows x 31 columns ] To read multiple parquets at once using pandas a loop must be used: import glob import pandas as pd files = glob . glob ( \"pipeline-runs/images/*/measurements.parquet\" ) data = [ pd . read_parquet ( f ) for f in files ] measurements = pd . concat ( data , ignore_index = True ) Tip If you don't require all the columns you can specify which columes to read using the columns variable. sources = pd . read_parquet ( 'pipeline-runs/new-test-data/sources.parquet' , columns = [ 'id' , 'n_meas' ])","title":"Reading with pandas"},{"location":"outputs/usingoutputs/#reading-with-vaex","text":"vaex documentation . Warning vaex is a young project so bugs may be expected along with frequent updates. It has currently been tested with version 3.0.0 . Version 4.0.0 promises opening parquet files in an out-of-core context. Warning Some pipeline parquet format files do not open with vaex 3.0.0. arrow format files should open successfully. A parquet, or arrow file, can be opened using the open() method: import vaex measurements = vaex . open ( 'pipeline-runs/new-test-data/measurements.arrow' ) measurements . head () # source island_id component_id local_rms ra ra_err dec dec_err flux_peak flux_peak_err flux_int flux_int_err bmaj err_bmaj bmin err_bmin pa err_pa psf_bmaj psf_bmin psf_pa flag_c4 chi_squared_fit spectral_index spectral_index_from_TT has_siblings image_id time name snr compactness ew_sys_err ns_sys_err error_radius uncertainty_ew uncertainty_ns weight_ew weight_ns forced flux_int_isl_ratio flux_peak_isl_ratio id 0 730 SB00004_island_1 SB00004_component_1a 0.463596 321.902 4.42819e-06 - 4.20097 2.2637e-06 307.991 0.4742 425.175 1.04728 20.95 1.08838e-05 12.28 4.32359e-06 108.19 0.00160065 15.58 0 - 73.86 False 3516.59 - 99 True True 2 2019 - 08 - 27 13 : 38 : 38.810000000 VAST_2118 + 00 A_SB00004_component_1a 664.352 1.38048 0.000277778 0.000277778 5.05099e-06 0.000277824 0.000277824 1.29557e+07 1.29557e+07 False 0.651019 0.709182 204 1 730 SB00009_island_1 SB00009_component_1a 0.463422 321.902 3.52062e-06 - 4.20103 2.45968e-06 318.544 0.472982 349.471 0.87264 21.4 8.50698e-06 12.78 5.46908e-06 107.02 0.0020224 5.72 0 46.03 True 15427.1 - 99 True True 3 2019 - 08 - 27 18 : 52 : 00.556000000 VAST_2118 - 06 A_SB00009_component_1a 687.374 1.09709 0.000277778 0.000277778 4.26887e-06 0.000277811 0.000277811 1.29569e+07 1.29569e+07 False 0.721399 0.7483 352 2 730 SB00006_island_1 SB00006_component_1a 0.627357 321.901 4.58076e-06 - 4.20086 2.92861e-06 310.503 0.662503 421.137 1.4171 17.05 1.07873e-05 12.42 6.89559e-06 90.71 0.00438684 10.51 4.24 - 82.52 False 4483.74 - 99 True True 5 2019 - 10 - 29 10 : 28 : 07.911000000 VAST_2118 + 00 A_SB00006_component_1a 494.938 1.35631 0.000277778 0.000277778 5.46682e-06 0.000277832 0.000277832 1.2955e+07 1.2955e+07 False 0.677562 0.721427 670 3 730 SB00011_island_1 SB00011_component_1a 0.627496 321.901 3.92144e-06 - 4.20087 3.21715e-06 310.998 0.643049 350.901 1.21083 17.06 9.23494e-06 12.43 7.57501e-06 91.19 0.00481863 6.43 0 27.62 False 4405.81 - 99 True True 4 2019 - 10 - 29 13 : 39 : 33.996000000 VAST_2118 - 06 A_SB00011_component_1a 495.618 1.12831 0.000277778 0.000277778 5.05099e-06 0.000277824 0.000277824 1.29557e+07 1.29557e+07 False 0.677576 0.721816 511 4 730 SB00005_island_1 SB00005_component_1a 0.346147 321.901 1.83032e-06 - 4.20051 1.71797e-06 299.072 0.342783 288.462 0.579893 14.13 4.37963e-06 12.14 3.97013e-06 65.14 0.00546325 2.86 0 6.42 False 2661.49 - 99 True True 7 2019 - 10 - 30 09 : 10 : 04.340000000 VAST_2118 + 00 A_SB00005_component_1a 864.004 0.964524 0.000277778 0.000277778 2.69987e-06 0.000277791 0.000277791 1.29588e+07 1.29588e+07 False 0.659887 0.719556 976 5 730 SB00010_island_1 SB00010_component_1a 0.347692 321.901 1.97328e-06 - 4.20052 1.75466e-06 300.969 0.360198 353.643 0.695754 14.16 4.76862e-06 12.16 3.99059e-06 65.77 0.00546515 6.12 3.95 49.18 False 2412.18 - 99 True True 6 2019 - 10 - 30 10 : 11 : 56.913000000 VAST_2118 - 06 A_SB00010_component_1a 865.62 1.17502 0.000277778 0.000277778 2.56132e-06 0.00027779 0.00027779 1.29589e+07 1.29589e+07 False 0.664605 0.723765 816 6 730 SB00007_island_1 SB00007_component_1a 0.387605 321.901 2.03947e-06 - 4.20032 1.77854e-06 317.014 0.392106 332.662 0.701451 14.56 4.96382e-06 11.54 3.99573e-06 64.78 0.00375775 4.81 0 24.08 False 1486.99 - 99 True True 10 2020 - 01 - 11 05 : 27 : 24.605000000 VAST_2118 + 00 A_SB00007_component_1a 817.88 1.04936 0.000277778 0.000277778 2.83165e-06 0.000277792 0.000277792 1.29587e+07 1.29587e+07 False 0.666924 0.716339 1493 7 730 SB00012_island_1 SB00012_component_1a 0.391978 321.901 2.12442e-06 - 4.20032 1.81129e-06 318.042 0.404457 365.987 0.770202 14.57 5.18203e-06 11.53 4.0454e-06 65.33 0.00378202 5.75 3.63 46.53 False 1328.58 - 99 True True 9 2020 - 01 - 11 05 : 40 : 11.007000000 VAST_2118 - 06 A_SB00012_component_1a 811.376 1.15075 0.000277778 0.000277778 2.69987e-06 0.000277791 0.000277791 1.29588e+07 1.29588e+07 False 0.667317 0.717746 1330 8 730 SB00008_island_1 SB00008_component_1a 0.432726 321.901 2.98443e-06 - 4.20052 2.32201e-06 293.737 0.436863 309.072 0.784657 18.35 7.14713e-06 12.12 5.31097e-06 105.78 0.00261377 4.82 0 18.62 False 2448.82 - 99 True True 13 2020 - 01 - 12 05 : 23 : 07.478000000 VAST_2118 + 00 A_SB00008_component_1a 678.807 1.05221 0.000277778 0.000277778 3.81819e-06 0.000277804 0.000277804 1.29576e+07 1.29576e+07 False 0.63994 0.725001 1889 9 730 SB00013_island_1 SB00013_component_1a 0.437279 321.901 3.14407e-06 - 4.20052 2.36161e-06 294.141 0.451346 340.92 0.864347 18.38 7.55055e-06 12.12 5.36009e-06 106.18 0.00262701 6.01 4 51.55 False 2368.93 - 99 True True 12 2020 - 01 - 12 05 : 36 : 03.834000000 VAST_2118 - 06 A_SB00013_component_1a 672.663 1.15903 0.000277778 0.000277778 4.00455e-06 0.000277807 0.000277807 1.29573e+07 1.29573e+07 False 0.640807 0.72508 1740 Multiple parquet files can be opened at once using the open_many() method: import glob import vaex files = glob . glob ( \"pipeline-runs/images/*/measurements.parquet\" ) measurements = vaex . open_many ( files ) Tip You can convert a vaex dataframe to pandas by using the to_pandas_df() method: import vaex sources = vaex . open ( 'pipeline-runs/new-test-data/sources.parquet' ) sources = sources . to_pandas_df ()","title":"Reading with vaex"},{"location":"outputs/usingoutputs/#linking-the-results","text":"The table below shows what parameters act as keys to link data from the different results tables. Tip If loading the measurements via the .arrow file, then the measurements already have the source column in-place. Tip The images.parquet file contains the column measurements_path which can be used to get the filepaths for all the selavy parquet files. Data Column Linked to Column associations.parquet meas_id measurements.parquet , forced_measurements*.parquet id associations.parquet source_id sources.parquet id (index column) measurements.parquet , forced_measurements*.parquet image_id images.parquet id images.parquet band_id bands.parquet id images.parquet skyreg_id skyregions.parquet id measurement_pairs.parquet meas_id_a , meas_id_b measurements.parquet , forced_measurements*.parquet id measurement_pairs.parquet source_id sources.parquet id (index column) relations.parquet from_source_id , to_source_id sources.parquet id (index column)","title":"Linking the Results"},{"location":"outputs/usingoutputs/#vast-tools","text":"Link to the vast-tools documentation . VAST has developed a python library called vast-tools that makes the exploration of results from the pipeline simple and efficient, in addition to being designed to be used in a Jupyter Notebook environment. Full details can be found in the documentation linked above, which also includes example notebooks of how to interact with the data.","title":"vast-tools"},{"location":"using/access/","text":"Accessing the Pipeline \u00b6 Access to the pipeline website is done using GitHub as the authentification method. In particular it checks organisation membership to confirm that the user is allowed access. For example for those wanting to access the VAST instance hosted on Nimbus must make sure they are a member of the askap-vast GitHub organisation. Note If you are attempting to access an instance of the VAST pipeline not hosted by the VAST group, confirm with the administrator what GitHub organisation membership is required. Adminstrators can refer to Pipeline Login for information on how to configure the login system. Logging In \u00b6 Navigate to the VAST Pipeline (or other hosted instance) and the following page will appear. Click the Login with GitHub button and you will be presented with the following page to enter you GitHub details. Note that if you are already logged into GitHub in your browser then you will likely not see this page. After a successful login you will then be redirected to the Pipeline homepage. Troubleshooting \u00b6 Failures will commonly be caused by the user not being a member of the correct GitHub organisation or an error in the configuration of the login system by the administrator. Contact the administrator of the pipeline instance if you encounter problems logging in.","title":"Accessing the Pipeline"},{"location":"using/access/#accessing-the-pipeline","text":"Access to the pipeline website is done using GitHub as the authentification method. In particular it checks organisation membership to confirm that the user is allowed access. For example for those wanting to access the VAST instance hosted on Nimbus must make sure they are a member of the askap-vast GitHub organisation. Note If you are attempting to access an instance of the VAST pipeline not hosted by the VAST group, confirm with the administrator what GitHub organisation membership is required. Adminstrators can refer to Pipeline Login for information on how to configure the login system.","title":"Accessing the Pipeline"},{"location":"using/access/#logging-in","text":"Navigate to the VAST Pipeline (or other hosted instance) and the following page will appear. Click the Login with GitHub button and you will be presented with the following page to enter you GitHub details. Note that if you are already logged into GitHub in your browser then you will likely not see this page. After a successful login you will then be redirected to the Pipeline homepage.","title":"Logging In"},{"location":"using/access/#troubleshooting","text":"Failures will commonly be caused by the user not being a member of the correct GitHub organisation or an error in the configuration of the login system by the administrator. Contact the administrator of the pipeline instance if you encounter problems logging in.","title":"Troubleshooting"},{"location":"using/addtorun/","text":"Adding Images to a Run \u00b6 This page describes how to add images to a completed run, including how to restore the run to the previous state if an addition goes wrong. Note Adding images to an existing run will update the sources already present from the respective run, such that the existing source IDs, comments, and tags are retained. If a full re-run was used instead then new IDs would be created and the comments and tags lost. There is no limit on how many times images may be added to a run. Step-by-step Guide \u00b6 Warning A run must have a Completed status before images can be added to it. No other settings other than the input data can be changed in the config. 1. Navigate to the Run Detail Page \u00b6 Navigate to the detail page of the run you wish to process, and confirm that the job is marked as Completed . 2. Add the New Images to the Configuration \u00b6 Scroll to the configuration editor, enter edit mode and add the new images to the existing data inputs. If using epoch mode notation, add a new epoch(s) to the file. Once all the images, selavy files, rms images and background images have been added, select the Write Current Config option to save the file. Warning Do not remove the previous images from the configuration inputs or change any other options! Remember to make sure the order of the new input data is consistent between types! 3. Perform a Config Validation \u00b6 Check that the configuration file is still valid by selecting Validate Config from the same menu as shown in the previous step 2 screenshot. 4. Process the Run \u00b6 Select the Add Images or Re-Process Run button at the top right of the run detail page to open the processing modal window. Select whether to turn debugging log output On or Off and when ready select the Schedule Run . Warning Do not toggle Full Re-Run to On ! You can refresh the page to check the status of the run. You can confirm that the images have been added correctly by consulting the log output found below the configuration file. New images should have been ingested and output similar to the following should be present: 2021-04-02-21-14-31_log.txt 2021-04-02 21:17:00,628 association INFO Starting association. 2021-04-02 21:17:00,628 association INFO Association mode selected: basic. 2021-04-02 21:17:00,708 association INFO Found 7 images to add to the run. Once the processing has Completed the run detail page will now show the updated statistics and information of the run. Restore Run to Pre-Add Version \u00b6 When images are added to a run, a backup is made of the run before proceeding which can be used to restore the run to the pre-addition version. For example, perhaps the wrong images were added or an error occurred mid-addition that could not be resolved. For full details see the documentation page for restoring a run here . Warning Do not add any further images if you wish to restore otherwise the backup version will be lost!","title":"Adding Images to a Run"},{"location":"using/addtorun/#adding-images-to-a-run","text":"This page describes how to add images to a completed run, including how to restore the run to the previous state if an addition goes wrong. Note Adding images to an existing run will update the sources already present from the respective run, such that the existing source IDs, comments, and tags are retained. If a full re-run was used instead then new IDs would be created and the comments and tags lost. There is no limit on how many times images may be added to a run.","title":"Adding Images to a Run"},{"location":"using/addtorun/#step-by-step-guide","text":"Warning A run must have a Completed status before images can be added to it. No other settings other than the input data can be changed in the config.","title":"Step-by-step Guide"},{"location":"using/addtorun/#1-navigate-to-the-run-detail-page","text":"Navigate to the detail page of the run you wish to process, and confirm that the job is marked as Completed .","title":"1. Navigate to the Run Detail Page"},{"location":"using/addtorun/#2-add-the-new-images-to-the-configuration","text":"Scroll to the configuration editor, enter edit mode and add the new images to the existing data inputs. If using epoch mode notation, add a new epoch(s) to the file. Once all the images, selavy files, rms images and background images have been added, select the Write Current Config option to save the file. Warning Do not remove the previous images from the configuration inputs or change any other options! Remember to make sure the order of the new input data is consistent between types!","title":"2. Add the New Images to the Configuration"},{"location":"using/addtorun/#3-perform-a-config-validation","text":"Check that the configuration file is still valid by selecting Validate Config from the same menu as shown in the previous step 2 screenshot.","title":"3. Perform a Config Validation"},{"location":"using/addtorun/#4-process-the-run","text":"Select the Add Images or Re-Process Run button at the top right of the run detail page to open the processing modal window. Select whether to turn debugging log output On or Off and when ready select the Schedule Run . Warning Do not toggle Full Re-Run to On ! You can refresh the page to check the status of the run. You can confirm that the images have been added correctly by consulting the log output found below the configuration file. New images should have been ingested and output similar to the following should be present: 2021-04-02-21-14-31_log.txt 2021-04-02 21:17:00,628 association INFO Starting association. 2021-04-02 21:17:00,628 association INFO Association mode selected: basic. 2021-04-02 21:17:00,708 association INFO Found 7 images to add to the run. Once the processing has Completed the run detail page will now show the updated statistics and information of the run.","title":"4. Process the Run"},{"location":"using/addtorun/#restore-run-to-pre-add-version","text":"When images are added to a run, a backup is made of the run before proceeding which can be used to restore the run to the pre-addition version. For example, perhaps the wrong images were added or an error occurred mid-addition that could not be resolved. For full details see the documentation page for restoring a run here . Warning Do not add any further images if you wish to restore otherwise the backup version will be lost!","title":"Restore Run to Pre-Add Version"},{"location":"using/deleterun/","text":"Deleting a Run \u00b6 This page describes how to delete a pipeline run through the website interface. Deleting a run means that all outputs such as sources and associations are deleted from the database and the run itself is also removed. Images and the accompanying measurements are only removed if they were used solely by the deleted run. The run directory is also deleted that contains the output files and configuration files. A pipeline run can only be deleted by the creator or an administrator. Warning As stated above, deleting a run through the website will also delete the full run directory, which includes the configuration files. Please manually back up the configuration file if you think you are likely to revisit that particular run configuration in the future. Admin Tip Administrators can refer to the clearpiperun command for details on how to reset a pipeline run via the command line. Step-by-step Guide \u00b6 1. Navigate to the Run Detail Page \u00b6 Navigate to the detail page of the run you wish to delete. 2. Click on the Delete Run Button \u00b6 Click on the Delete Run button at the top right of the page, to open the confirmation modal. 3. Confirm Deletion \u00b6 To confirm the deletion click on the Delete Run button in the modal. Once pressed the website will direct back to the Pipeline Runs page and a confirmation message will appear in the top right. Note Runs with lots of database entries may take a short time to delete. Hence, the run may still appear in the pipeline runs list for a short time following the request with a status of Deleting . Refreshing the page will show a deleting status if the process is still running:","title":"Deleting a Run"},{"location":"using/deleterun/#deleting-a-run","text":"This page describes how to delete a pipeline run through the website interface. Deleting a run means that all outputs such as sources and associations are deleted from the database and the run itself is also removed. Images and the accompanying measurements are only removed if they were used solely by the deleted run. The run directory is also deleted that contains the output files and configuration files. A pipeline run can only be deleted by the creator or an administrator. Warning As stated above, deleting a run through the website will also delete the full run directory, which includes the configuration files. Please manually back up the configuration file if you think you are likely to revisit that particular run configuration in the future. Admin Tip Administrators can refer to the clearpiperun command for details on how to reset a pipeline run via the command line.","title":"Deleting a Run"},{"location":"using/deleterun/#step-by-step-guide","text":"","title":"Step-by-step Guide"},{"location":"using/deleterun/#1-navigate-to-the-run-detail-page","text":"Navigate to the detail page of the run you wish to delete.","title":"1. Navigate to the Run Detail Page"},{"location":"using/deleterun/#2-click-on-the-delete-run-button","text":"Click on the Delete Run button at the top right of the page, to open the confirmation modal.","title":"2. Click on the Delete Run Button"},{"location":"using/deleterun/#3-confirm-deletion","text":"To confirm the deletion click on the Delete Run button in the modal. Once pressed the website will direct back to the Pipeline Runs page and a confirmation message will appear in the top right. Note Runs with lots of database entries may take a short time to delete. Hence, the run may still appear in the pipeline runs list for a short time following the request with a status of Deleting . Refreshing the page will show a deleting status if the process is still running:","title":"3. Confirm Deletion"},{"location":"using/genarrow/","text":"Generating Arrow Files \u00b6 This page describes how to generate the measurement arrow files for a pipeline run if the option in the configuration file to create them was turned off. Arrow files only be generated by the creator or an administrator. Two files are produced by the method: File Description measurements.arrow An Apache Arrow format file containing all the measurements associated with the pipeline run (see Arrow Files ). Extra processing is performed in the creation of this file such that source ids are already in place for the measurements. measurement_pairs.arrow An Apache Arrow format file containing all the measurement pair metrics (see Arrow Files ). Arrow Files Available Users can see if arrow files are present for the run of interest by checking the respective run detail page. Admin Tip The arrow files can be generated using the command line using the command createmaeasarrow ). Why Create Arrow Files? \u00b6 Large pipeline runs (hundreds of images) mean that to read the measurements, hundreds of parquet files need to be read in, and can contain millions of rows. This can be slow using libraries such as pandas, and also consumes a lot of system memory. Instead, if the measurements are saved in the Apache Arrow format, libraries such as vaex are able to open .arrow files in an out-of-core context so the memory footprint is hugely reduced along with the reading of the file being very fast. The two-epoch measurement pairs are also saved to arrow format due to the same reasons. See Reading with vaex for further details on using vaex . Step-by-step Guide \u00b6 1. Navigate to the Run Detail Page \u00b6 Navigate to the detail page of the run you wish to generate arrow files for. 2. Select the Generate Arrow Files Option \u00b6 Click the Generate Arrow Files option at the top-right of the page. This will open the generate arrow files modal. 3. Submit Generate Arrow Files Request \u00b6 It is possible to overwrite existing arrow files by toggling the Overwrite Current Files option. When ready, click the Generate Arrow Files button on the modal to submit the generate request. A notification will show to indicate whether the submission was successful. 4. Refresh and Check the Generate Arrow Files Log File \u00b6 It is possible to check the progress by looking at the Generate Arrow Files Log File which can be found on the run detail page. The log will not be refreshed automatically and instead the page needs to be manually refreshed. Once completed the arrow files will be available for use.","title":"Generating Arrow Files"},{"location":"using/genarrow/#generating-arrow-files","text":"This page describes how to generate the measurement arrow files for a pipeline run if the option in the configuration file to create them was turned off. Arrow files only be generated by the creator or an administrator. Two files are produced by the method: File Description measurements.arrow An Apache Arrow format file containing all the measurements associated with the pipeline run (see Arrow Files ). Extra processing is performed in the creation of this file such that source ids are already in place for the measurements. measurement_pairs.arrow An Apache Arrow format file containing all the measurement pair metrics (see Arrow Files ). Arrow Files Available Users can see if arrow files are present for the run of interest by checking the respective run detail page. Admin Tip The arrow files can be generated using the command line using the command createmaeasarrow ).","title":"Generating Arrow Files"},{"location":"using/genarrow/#why-create-arrow-files","text":"Large pipeline runs (hundreds of images) mean that to read the measurements, hundreds of parquet files need to be read in, and can contain millions of rows. This can be slow using libraries such as pandas, and also consumes a lot of system memory. Instead, if the measurements are saved in the Apache Arrow format, libraries such as vaex are able to open .arrow files in an out-of-core context so the memory footprint is hugely reduced along with the reading of the file being very fast. The two-epoch measurement pairs are also saved to arrow format due to the same reasons. See Reading with vaex for further details on using vaex .","title":"Why Create Arrow Files?"},{"location":"using/genarrow/#step-by-step-guide","text":"","title":"Step-by-step Guide"},{"location":"using/genarrow/#1-navigate-to-the-run-detail-page","text":"Navigate to the detail page of the run you wish to generate arrow files for.","title":"1. Navigate to the Run Detail Page"},{"location":"using/genarrow/#2-select-the-generate-arrow-files-option","text":"Click the Generate Arrow Files option at the top-right of the page. This will open the generate arrow files modal.","title":"2. Select the Generate Arrow Files Option"},{"location":"using/genarrow/#3-submit-generate-arrow-files-request","text":"It is possible to overwrite existing arrow files by toggling the Overwrite Current Files option. When ready, click the Generate Arrow Files button on the modal to submit the generate request. A notification will show to indicate whether the submission was successful.","title":"3. Submit Generate Arrow Files Request"},{"location":"using/genarrow/#4-refresh-and-check-the-generate-arrow-files-log-file","text":"It is possible to check the progress by looking at the Generate Arrow Files Log File which can be found on the run detail page. The log will not be refreshed automatically and instead the page needs to be manually refreshed. Once completed the arrow files will be available for use.","title":"4. Refresh and Check the Generate Arrow Files Log File"},{"location":"using/initrun/","text":"Initialising a Pipeline Run \u00b6 This page outlines the steps required to create a pipeline run through the web interface. A description of the run configuration options can be found in the next section . Note Administrators please refer to this section in the admin documentation for instructions on how to initialise a pipeline run via the command line interface. Warning No data quality control is performed by the pipeline. Make sure your input data is clean and error free before processing using your preferred method. Step-by-step Guide \u00b6 1. Navigate to the Pipeline Runs Overview Page \u00b6 Navigate to the Pipeline Runs overview page by clicking on the Pipeline Runs option in the left hand side navigation bar, as highlighted below. 2. Select the New Pipeline Run Option \u00b6 From the Pipeline Runs overview page, select the New Pipeline Run button as highlighted in the screenshot below. This will open up a modal window to begin the run initialisation process. 3. Fill in the Run Details \u00b6 Fill in the name and description of the run and then press next to navigate to the next form to enter and select the configuration options. For full details on how to configure a pipeline run see the Run Configuration page, but a few notes here: Any settings entered here are not final, they can still be changed once the run is created. The order of the input files must match between the data types - i.e. the first selavy file, rms image and background image must all be the products of the first image, and so on. If you have a high number of images, selavy files, rms images and background images, it may be easier to leave these empty and instead use the text editor on the run detail page to directly enter the list to the configuration file. By default, non-admin users have a 200 image limit. Once you have finished filling in the configuration options, press the create button. 4. The Run Detail Page \u00b6 After pressing create, the run will be initialised in the pipeline, which means the required configuration files have been created and the run is ready to be processed. You will be navigated to the detail page of the created run (shown below). On this page you can: View details of the run. View and edit the configuration file. Leave a comment about the run. View tables of associated images and measurements (once processed). Submit the run to be processed. For full details on: how to use the text editor to edit the configuration file see Run Configuration , how to submit the job to be processed see Processing a Run , and how to add images to a run that has already been processed Adding Images to a Run .","title":"Initialising a Run"},{"location":"using/initrun/#initialising-a-pipeline-run","text":"This page outlines the steps required to create a pipeline run through the web interface. A description of the run configuration options can be found in the next section . Note Administrators please refer to this section in the admin documentation for instructions on how to initialise a pipeline run via the command line interface. Warning No data quality control is performed by the pipeline. Make sure your input data is clean and error free before processing using your preferred method.","title":"Initialising a Pipeline Run"},{"location":"using/initrun/#step-by-step-guide","text":"","title":"Step-by-step Guide"},{"location":"using/initrun/#1-navigate-to-the-pipeline-runs-overview-page","text":"Navigate to the Pipeline Runs overview page by clicking on the Pipeline Runs option in the left hand side navigation bar, as highlighted below.","title":"1. Navigate to the Pipeline Runs Overview Page"},{"location":"using/initrun/#2-select-the-new-pipeline-run-option","text":"From the Pipeline Runs overview page, select the New Pipeline Run button as highlighted in the screenshot below. This will open up a modal window to begin the run initialisation process.","title":"2. Select the New Pipeline Run Option"},{"location":"using/initrun/#3-fill-in-the-run-details","text":"Fill in the name and description of the run and then press next to navigate to the next form to enter and select the configuration options. For full details on how to configure a pipeline run see the Run Configuration page, but a few notes here: Any settings entered here are not final, they can still be changed once the run is created. The order of the input files must match between the data types - i.e. the first selavy file, rms image and background image must all be the products of the first image, and so on. If you have a high number of images, selavy files, rms images and background images, it may be easier to leave these empty and instead use the text editor on the run detail page to directly enter the list to the configuration file. By default, non-admin users have a 200 image limit. Once you have finished filling in the configuration options, press the create button.","title":"3. Fill in the Run Details"},{"location":"using/initrun/#4-the-run-detail-page","text":"After pressing create, the run will be initialised in the pipeline, which means the required configuration files have been created and the run is ready to be processed. You will be navigated to the detail page of the created run (shown below). On this page you can: View details of the run. View and edit the configuration file. Leave a comment about the run. View tables of associated images and measurements (once processed). Submit the run to be processed. For full details on: how to use the text editor to edit the configuration file see Run Configuration , how to submit the job to be processed see Processing a Run , and how to add images to a run that has already been processed Adding Images to a Run .","title":"4. The Run Detail Page"},{"location":"using/processrun/","text":"Processing a Run \u00b6 This page describes how to submit a pipeline run for processing. Admin Tip Administrators please refer to this section in the admin documentation for instructions on how to process a pipeline run via the command line interface. Admin Warning The Django Q service must be running in order for pipeline runs to be processed. See the Deployment page for further details. Tip Use the editor window on the run detail page to make adjustments to the run configuration file before processing. Step-by-step Guide \u00b6 1. Navigate to the Run Detail Page \u00b6 Navigate to the detail page of the run you wish to process. 2. Run a config validation \u00b6 Before processing it is recommended to check that the configuration file is valid. This is done by scrolling down to the config file card on the page and selecting the Validate Config option accessed by clicking the three dots menu button. Doing this will check if the configuration contains any errors prior to processing. Feedback will be provided on whether the configuration file is valid. In the event of an error, this can be corrected by using the edit option found in the same menu. 3. Confirm Processing \u00b6 With a successful configuration validation, scroll back up to the top of the page and click the Process Run button. This will open a modal window for you to confirm processing. For a newly initialised run, the only option that requires attention is whether to toggle the Debug Log Output on. This can be helpful if processing a new set of images which the pipeline hasn't seen before. If you are processing a run that has errored in the initial processing then the Full Re-Run option should be toggled to On . Once ready, press the Schedule Run button which will send the run to the queue for processing. Warning For non-admin users, by default there is a run image limit of 200. Monitoring the Run \u00b6 You can check the status of the run by refreshing the run detail page and seeing if the Run Status field has been updated. You can also check the log output by scrolling down to the log file card found below the configuration file. There is currently no automated notification on completion or errors. Full Re-Run \u00b6 A full re-run will be required if the run configuration needs to be changed, or in the event that an initial run has errored. Warning The Full Re-Run option will remove all associated existing data for that run. Note If images have been added to a run and the processing errors, there is a one time undo option that may avoid having to use the Full Re-Run command. Adding Images to a Run \u00b6 See the dedicated documentation page here .","title":"Processing a Run"},{"location":"using/processrun/#processing-a-run","text":"This page describes how to submit a pipeline run for processing. Admin Tip Administrators please refer to this section in the admin documentation for instructions on how to process a pipeline run via the command line interface. Admin Warning The Django Q service must be running in order for pipeline runs to be processed. See the Deployment page for further details. Tip Use the editor window on the run detail page to make adjustments to the run configuration file before processing.","title":"Processing a Run"},{"location":"using/processrun/#step-by-step-guide","text":"","title":"Step-by-step Guide"},{"location":"using/processrun/#1-navigate-to-the-run-detail-page","text":"Navigate to the detail page of the run you wish to process.","title":"1. Navigate to the Run Detail Page"},{"location":"using/processrun/#2-run-a-config-validation","text":"Before processing it is recommended to check that the configuration file is valid. This is done by scrolling down to the config file card on the page and selecting the Validate Config option accessed by clicking the three dots menu button. Doing this will check if the configuration contains any errors prior to processing. Feedback will be provided on whether the configuration file is valid. In the event of an error, this can be corrected by using the edit option found in the same menu.","title":"2. Run a config validation"},{"location":"using/processrun/#3-confirm-processing","text":"With a successful configuration validation, scroll back up to the top of the page and click the Process Run button. This will open a modal window for you to confirm processing. For a newly initialised run, the only option that requires attention is whether to toggle the Debug Log Output on. This can be helpful if processing a new set of images which the pipeline hasn't seen before. If you are processing a run that has errored in the initial processing then the Full Re-Run option should be toggled to On . Once ready, press the Schedule Run button which will send the run to the queue for processing. Warning For non-admin users, by default there is a run image limit of 200.","title":"3. Confirm Processing"},{"location":"using/processrun/#monitoring-the-run","text":"You can check the status of the run by refreshing the run detail page and seeing if the Run Status field has been updated. You can also check the log output by scrolling down to the log file card found below the configuration file. There is currently no automated notification on completion or errors.","title":"Monitoring the Run"},{"location":"using/processrun/#full-re-run","text":"A full re-run will be required if the run configuration needs to be changed, or in the event that an initial run has errored. Warning The Full Re-Run option will remove all associated existing data for that run. Note If images have been added to a run and the processing errors, there is a one time undo option that may avoid having to use the Full Re-Run command.","title":"Full Re-Run"},{"location":"using/processrun/#adding-images-to-a-run","text":"See the dedicated documentation page here .","title":"Adding Images to a Run"},{"location":"using/requireddata/","text":"Required Data \u00b6 This page gives an overview of what data is required to run the pipeline along with how to obtain the data. Acquiring ASKAP Data \u00b6 Note: VAST Data Releases If you are a member of the VAST collaboration you will have access to the VAST data release which contains the data for the VAST Pilot Survey. Refer to the VAST wiki for more details. Data produced by the ASKAP telescope can be accessed by using The CSIRO ASKAP Science Data Archive (CASDA) . CASDA provides a web form for users to search for the data they are interested in and request for the data to be staged for download. Note that a form of account or registration is required to download image cube products. All data products from CASDA should be compatible without any modifications. Please report an issue if you find this to not be the case. Tip: CASDA Data Products Descriptions of the data products available on CASDA can be found on this page . Pipeline Required Data \u00b6 Warning: Correcting Data Currently, the pipeline does not contain any processes to correct the data as it is ingested by the pipeline. For example, if corrections to the flux or positions need to be applied, these should be done to the data directly before they are processed by the pipeline. If an image has been previously ingested before corrections were applied, the filename of the corrected image must be changed. Doing so will make the pipeline see the corrected image as a new image and reingest the corrected data. A pipeline run requires a minimum of two ASKAP observational images to process. For each image, the following table provides a summary of the files that are required. Data File Type Description Primary image .fits The primary, Stokes I, taylor 0, image of the observation. Component Catalogue .xml, .csv, .txt The component source catalogue produced by the source finder selavy from the primary image. Noise map of primary image .fits The noise (or rms) map produced by the source finder selavy from the primary image. Background map of primary image (optional) .fits The background (or mean) map produced by the source finder selavy from the primary image. The background image is only required when source monitoring is enabled. Refer to the respective sections on this page for more details on these inputs. Image File \u00b6 This is the primary image file produced by the ASKAP pipeline. The pipeline requires the Stokes I, total intensity (taylor 0) image file. It is also recommended to use the convolved final image, where each of the 36 individual beams have been convolved to a common resolution prior to the creation of the combined mosaic of the field. These files are denoted by a .conv in the file name. Must be in the FITS file format. Example CASDA Filename image.i.SB25597.cont.taylor.0.restored.conv.fits Component Catalogues \u00b6 Currently, the pipeline does not contain any source finding capabilities so the source catalogue must be provided. In particular, the pipeline requires the continuum component catalogue produced by the selavy source finder. The files can be one of three formats: XML The VOTable XML format of the selavy components catalogue. Provided by CASDA. Must have a file extension of .xml . CSV The csv format of the selavy components catalogue. Provided by CASDA. Must have a file extension of .csv . TXT 'Fixed-width' formatted output produced by the selavy source finder directly. These are not available through CASDA. Must have a file extension of .txt . Example CASDA Filenames xml csv selavy-image.i.SB25597.cont.taylor.0.restored.conv.components.xml AS107_Continuum_Component_Catalogue_25597_3792.csv Note that CASDA does not provide the fixed width text format selavy output. File format examples xml csv txt <?xml version=\"1.0\"?> <VOTABLE version= \"1.3\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xmlns= \"http://www.ivoa.net/xml/VOTable/v1.3\" xmlns:stc= \"http://www.ivoa.net/xml/STC/v1.30\" > <COOSYS ID= \"J2000\" equinox= \"J2000\" system= \"eq_FK5\" /> <RESOURCE name= \"Component catalogue from Selavy source finding\" > <TABLE name= \"Component catalogue\" > <DESCRIPTION></DESCRIPTION> <PARAM name= \"table_version\" ucd= \"meta.version\" datatype= \"char\" arraysize= \"43\" value= \"casda.continuum_component_description_v1.9\" /> <PARAM name= \"imageFile\" ucd= \"meta.file;meta.fits\" datatype= \"char\" arraysize= \"48\" value= \"image.i.SB25600.cont.taylor.0.restored.conv.fits\" /> <PARAM name= \"flagSubsection\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"subsection\" ucd= \"\" datatype= \"char\" arraysize= \"25\" value= \"[1:16449,1:14759,1:1,1:1]\" /> <PARAM name= \"flagStatSec\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"StatSec\" ucd= \"\" datatype= \"char\" arraysize= \"7\" value= \"[*,*,*]\" /> <PARAM name= \"searchType\" ucd= \"meta.note\" datatype= \"char\" arraysize= \"7\" value= \"spatial\" /> <PARAM name= \"flagNegative\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagBaseline\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagRobustStats\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"flagFDR\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"threshold\" ucd= \"phot.flux;stat.min\" datatype= \"float\" value= \"5\" /> <PARAM name= \"flagGrowth\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"growthThreshold\" ucd= \"phot.flux;stat.min\" datatype= \"float\" value= \"3\" /> <PARAM name= \"minPix\" ucd= \"\" datatype= \"int\" value= \"3\" /> <PARAM name= \"minChannels\" ucd= \"\" datatype= \"int\" value= \"0\" /> <PARAM name= \"minVoxels\" ucd= \"\" datatype= \"int\" value= \"3\" /> <PARAM name= \"flagAdjacent\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"threshVelocity\" ucd= \"\" datatype= \"float\" value= \"7\" /> <PARAM name= \"flagRejectBeforeMerge\" ucd= \"\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagTwoStageMerging\" ucd= \"\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"pixelCentre\" ucd= \"\" datatype= \"char\" arraysize= \"8\" value= \"centroid\" /> <PARAM name= \"flagSmooth\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagATrous\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"Reference frequency\" ucd= \"em.freq;meta.main\" datatype= \"float\" unit= \"Hz\" value= \"1.36749e+09\" /> <PARAM name= \"thresholdActual\" ucd= \"\" datatype= \"float\" unit= \"Jy/beam\" value= \"5\" /> <FIELD name= \"island_id\" ID= \"col_island_id\" ucd= \"meta.id.parent\" datatype= \"char\" unit= \"--\" arraysize= \"20\" /> <FIELD name= \"component_id\" ID= \"col_component_id\" ucd= \"meta.id;meta.main\" datatype= \"char\" unit= \"--\" arraysize= \"24\" /> <FIELD name= \"component_name\" ID= \"col_component_name\" ucd= \"meta.id\" datatype= \"char\" unit= \"\" arraysize= \"26\" /> <FIELD name= \"ra_hms_cont\" ID= \"col_ra_hms_cont\" ucd= \"pos.eq.ra\" ref= \"J2000\" datatype= \"char\" unit= \"\" arraysize= \"12\" /> <FIELD name= \"dec_dms_cont\" ID= \"col_dec_dms_cont\" ucd= \"pos.eq.dec\" ref= \"J2000\" datatype= \"char\" unit= \"\" arraysize= \"13\" /> <FIELD name= \"ra_deg_cont\" ID= \"col_ra_deg_cont\" ucd= \"pos.eq.ra;meta.main\" ref= \"J2000\" datatype= \"double\" unit= \"deg\" width= \"12\" precision= \"6\" /> <FIELD name= \"dec_deg_cont\" ID= \"col_dec_deg_cont\" ucd= \"pos.eq.dec;meta.main\" ref= \"J2000\" datatype= \"double\" unit= \"deg\" width= \"13\" precision= \"6\" /> <FIELD name= \"ra_err\" ID= \"col_ra_err\" ucd= \"stat.error;pos.eq.ra\" ref= \"J2000\" datatype= \"float\" unit= \"arcsec\" width= \"11\" precision= \"2\" /> <FIELD name= \"dec_err\" ID= \"col_dec_err\" ucd= \"stat.error;pos.eq.dec\" ref= \"J2000\" datatype= \"float\" unit= \"arcsec\" width= \"11\" precision= \"2\" /> <FIELD name= \"freq\" ID= \"col_freq\" ucd= \"em.freq\" datatype= \"float\" unit= \"MHz\" width= \"11\" precision= \"1\" /> <FIELD name= \"flux_peak\" ID= \"col_flux_peak\" ucd= \"phot.flux.density;stat.max;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy/beam\" width= \"11\" precision= \"3\" /> <FIELD name= \"flux_peak_err\" ID= \"col_flux_peak_err\" ucd= \"stat.error;phot.flux.density;stat.max;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy/beam\" width= \"14\" precision= \"3\" /> <FIELD name= \"flux_int\" ID= \"col_flux_int\" ucd= \"phot.flux.density;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy\" width= \"10\" precision= \"3\" /> <FIELD name= \"flux_int_err\" ID= \"col_flux_int_err\" ucd= \"stat.error;phot.flux.density;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy\" width= \"13\" precision= \"3\" /> <FIELD name= \"maj_axis\" ID= \"col_maj_axis\" ucd= \"phys.angSize.smajAxis;em.radio;stat.fit\" datatype= \"float\" unit= \"arcsec\" width= \"9\" precision= \"2\" /> <FIELD name= \"min_axis\" ID= \"col_min_axis\" ucd= \"phys.angSize.sminAxis;em.radio;stat.fit\" datatype= \"float\" unit= \"arcsec\" width= \"9\" precision= \"2\" /> <FIELD name= \"pos_ang\" ID= \"col_pos_ang\" ucd= \"phys.angSize;pos.posAng;em.radio;stat.fit\" datatype= \"float\" unit= \"deg\" width= \"8\" precision= \"2\" /> <FIELD name= \"maj_axis_err\" ID= \"col_maj_axis_err\" ucd= \"stat.error;phys.angSize.smajAxis;em.radio\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"min_axis_err\" ID= \"col_min_axis_err\" ucd= \"stat.error;phys.angSize.sminAxis;em.radio\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"pos_ang_err\" ID= \"col_pos_ang_err\" ucd= \"stat.error;phys.angSize;pos.posAng;em.radio\" datatype= \"float\" unit= \"deg\" width= \"12\" precision= \"2\" /> <FIELD name= \"maj_axis_deconv\" ID= \"col_maj_axis_deconv\" ucd= \"phys.angSize.smajAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"18\" precision= \"2\" /> <FIELD name= \"min_axis_deconv\" ID= \"col_min_axis_deconv\" ucd= \"phys.angSize.sminAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"16\" precision= \"2\" /> <FIELD name= \"pos_ang_deconv\" ID= \"col_pos_ang_deconv\" ucd= \"phys.angSize;pos.posAng;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"deg\" width= \"15\" precision= \"2\" /> <FIELD name= \"maj_axis_deconv_err\" ID= \"col_maj_axis_deconv_err\" ucd= \"stat.error;phys.angSize.smajAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"min_axis_deconv_err\" ID= \"col_min_axis_deconv_err\" ucd= \"stat.error;phys.angSize.sminAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"pos_ang_deconv_err\" ID= \"col_pos_ang_deconv_err\" ucd= \"stat.error;phys.angSize;pos.posAng;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"deg\" width= \"12\" precision= \"2\" /> <FIELD name= \"chi_squared_fit\" ID= \"col_chi_squared_fit\" ucd= \"stat.fit.chi2\" datatype= \"float\" unit= \"--\" width= \"17\" precision= \"3\" /> <FIELD name= \"rms_fit_gauss\" ID= \"col_rms_fit_gauss\" ucd= \"stat.stdev;stat.fit\" datatype= \"float\" unit= \"mJy/beam\" width= \"15\" precision= \"3\" /> <FIELD name= \"spectral_index\" ID= \"col_spectral_index\" ucd= \"spect.index;em.radio\" datatype= \"float\" unit= \"--\" width= \"15\" precision= \"2\" /> <FIELD name= \"spectral_curvature\" ID= \"col_spectral_curvature\" ucd= \"askap:spect.curvature;em.radio\" datatype= \"float\" unit= \"--\" width= \"19\" precision= \"2\" /> <FIELD name= \"spectral_index_err\" ID= \"col_spectral_index_err\" ucd= \"stat.error;spect.index;em.radio\" datatype= \"float\" unit= \"--\" width= \"15\" precision= \"2\" /> <FIELD name= \"spectral_curvature_err\" ID= \"col_spectral_curvature_err\" ucd= \"stat.error;askap:spect.curvature;em.radio\" datatype= \"float\" unit= \"--\" width= \"19\" precision= \"2\" /> <FIELD name= \"rms_image\" ID= \"col_rms_image\" ucd= \"stat.stdev;phot.flux.density\" datatype= \"float\" unit= \"mJy/beam\" width= \"12\" precision= \"3\" /> <FIELD name= \"has_siblings\" ID= \"col_has_siblings\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"fit_is_estimate\" ID= \"col_fit_is_estimate\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"spectral_index_from_TT\" ID= \"col_spectral_index_from_TT\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"flag_c4\" ID= \"col_flag_c4\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"comment\" ID= \"col_comment\" ucd= \"meta.note\" datatype= \"char\" unit= \"\" arraysize= \"100\" /> <DATA> <TABLEDATA> <TR> <TD> SB25600_island_1 </TD><TD> SB25600_component_1a </TD><TD> J053527-691611 </TD><TD> 05:35:27.9 </TD><TD> -69:16:11 </TD><TD> 83.866441 </TD><TD> -69.269927 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 1367.5 </TD><TD> 780.068 </TD><TD> 0.721 </TD><TD> 843.563 </TD><TD> 1.692 </TD><TD> 11.80 </TD><TD> 8.40 </TD><TD> 176.22 </TD><TD> 0.01 </TD><TD> 0.01 </TD><TD> 0.11 </TD><TD> 3.09 </TD><TD> 1.82 </TD><TD> 56.44 </TD><TD> 0.78 </TD><TD> 2.35 </TD><TD> 1.49 </TD><TD> 596.911 </TD><TD> 1811.003 </TD><TD> -0.93 </TD><TD> -99.00 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 1.038 </TD><TD> 0 </TD><TD> 0 </TD><TD> 1 </TD><TD> 0 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_2 </TD><TD> SB25600_component_2a </TD><TD> J060004-703833 </TD><TD> 06:00:04.9 </TD><TD> -70:38:33 </TD><TD> 90.020608 </TD><TD> -70.642664 </TD><TD> 0.01 </TD><TD> 0.01 </TD><TD> 1367.5 </TD><TD> 438.885 </TD><TD> 1.202 </TD><TD> 494.222 </TD><TD> 2.918 </TD><TD> 11.65 </TD><TD> 8.86 </TD><TD> 9.51 </TD><TD> 0.03 </TD><TD> 0.04 </TD><TD> 0.40 </TD><TD> 5.22 </TD><TD> 0.00 </TD><TD> 58.20 </TD><TD> 0.09 </TD><TD> 0.00 </TD><TD> 0.82 </TD><TD> 5351.118 </TD><TD> 4419.234 </TD><TD> -1.06 </TD><TD> -99.00 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 0.715 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 1 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_2 </TD><TD> SB25600_component_2b </TD><TD> J060006-703854 </TD><TD> 06:00:06.1 </TD><TD> -70:38:54 </TD><TD> 90.025359 </TD><TD> -70.648364 </TD><TD> 0.19 </TD><TD> 0.24 </TD><TD> 1367.5 </TD><TD> 24.719 </TD><TD> 1.326 </TD><TD> 24.048 </TD><TD> 2.986 </TD><TD> 10.66 </TD><TD> 8.37 </TD><TD> 167.45 </TD><TD> 0.59 </TD><TD> 0.80 </TD><TD> 9.41 </TD><TD> 3.00 </TD><TD> 0.00 </TD><TD> -86.64 </TD><TD> 5.43 </TD><TD> 0.00 </TD><TD> 14.18 </TD><TD> 5351.118 </TD><TD> 4419.234 </TD><TD> -99.00 </TD><TD> -99.00 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 0.715 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 1 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_3 </TD><TD> SB25600_component_3a </TD><TD> J061931-681533 </TD><TD> 06:19:31.4 </TD><TD> -68:15:33 </TD><TD> 94.880875 </TD><TD> -68.259404 </TD><TD> 0.49 </TD><TD> 0.65 </TD><TD> 1367.5 </TD><TD> 258.844 </TD><TD> 72.505 </TD><TD> 302.138 </TD><TD> 84.967 </TD><TD> 11.83 </TD><TD> 9.04 </TD><TD> 0.90 </TD><TD> 0.21 </TD><TD> 0.18 </TD><TD> 3.84 </TD><TD> 4.77 </TD><TD> 1.40 </TD><TD> 63.44 </TD><TD> 1.39 </TD><TD> 19.02 </TD><TD> 9.15 </TD><TD> 1284.683 </TD><TD> 2467.498 </TD><TD> -0.74 </TD><TD> -99.00 </TD><TD> 0.01 </TD><TD> 0.00 </TD><TD> 0.277 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 0 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_3 </TD><TD> SB25600_component_3b </TD><TD> J061931-681531 </TD><TD> 06:19:31.0 </TD><TD> -68:15:31 </TD><TD> 94.879037 </TD><TD> -68.258688 </TD><TD> 0.21 </TD><TD> 0.57 </TD><TD> 1367.5 </TD><TD> 153.160 </TD><TD> 84.478 </TD><TD> 157.853 </TD><TD> 87.200 </TD><TD> 11.40 </TD><TD> 8.28 </TD><TD> 6.90 </TD><TD> 0.20 </TD><TD> 0.21 </TD><TD> 1.40 </TD><TD> 4.07 </TD><TD> 0.00 </TD><TD> 55.21 </TD><TD> 1.06 </TD><TD> 0.00 </TD><TD> 5.16 </TD><TD> 1284.683 </TD><TD> 2467.498 </TD><TD> -1.17 </TD><TD> -99.00 </TD><TD> 0.02 </TD><TD> 0.00 </TD><TD> 0.277 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 0 </TD><TD> </TD> </TR> </TABLEDATA> </DATA> </TABLE> </RESOURCE> </VOTABLE> id,catalogue_id,first_sbid,other_sbids,project_id,island_id,component_id,component_name,ra_hms_cont,dec_dms_cont,ra_deg_cont,dec_deg_cont,ra_err,dec_err,freq,flux_peak,flux_peak_err,flux_int,flux_int_err,maj_axis,min_axis,pos_ang,maj_axis_err,min_axis_err,pos_ang_err,maj_axis_deconv,min_axis_deconv,maj_axis_deconv_err,pos_ang_deconv,min_axis_deconv_err,pos_ang_deconv_err,chi_squared_fit,rms_fit_gauss,spectral_index,spectral_index_err,spectral_curvature,spectral_curvature_err,rms_image,has_siblings,fit_is_estimate,spectral_index_from_tt,flag_c4,comment,quality_level,released_date 20793260,3790,25600,,18,SB25600_island_2768,SB25600_component_2768a,J055644-690942,05:56:44.5,-69:09:42,89.185558,-69.161889,0.04,0.05,1367.5,0.834,0.009,0.68,0.037,9.63,7.76,165.12,0.23,0.33,3.9,0.0,0.0,0.0,-89.09,0.0,3.48,0.034,52.957,-99.0,0.0,-99.0,0.0,0.166,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z 20793259,3790,25600,,18,SB25600_island_2767,SB25600_component_2767a,J060047-692500,06:00:47.7,-69:25:00,90.198913,-69.416437,0.09,0.37,1367.5,0.889,0.016,2.642,0.2,29.12,9.35,2.08,2.0,0.99,1.35,26.73,4.84,0.03,2.91,6.03,1.5,1.869,234.431,-99.0,0.0,-99.0,0.0,0.175,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z 20793258,3790,25600,,18,SB25600_island_2766,SB25600_component_2766a,J055849-690051,05:58:49.4,-69:00:51,89.705808,-69.014253,0.14,0.24,1367.5,0.885,0.025,1.181,0.191,13.04,9.38,6.04,1.3,1.45,8.83,6.87,3.72,3.2,36.6,14.03,34.31,0.561,193.467,-99.0,0.0,-99.0,0.0,0.177,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z 20793257,3790,25600,,18,SB25600_island_2765,SB25600_component_2765a,J055339-683120,05:53:39.5,-68:31:20,88.414758,-68.522436,0.08,0.1,1367.5,0.906,0.014,0.895,0.063,12.78,7.08,10.0,0.43,0.43,2.33,6.37,0.0,0.54,28.11,0.0,4.65,0.138,99.302,-99.0,0.0,-99.0,0.0,0.173,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z # island_id component_id component_name ra_hms_cont dec_dms_cont ra_deg_cont dec_deg_cont ra_err dec_err freq flux_peak flux_peak_err flux_int flux_int_err maj_axis min_axis pos_ang maj_axis_err min_axis_err pos_ang_err maj_axis_deconv min_axis_deconv pos_ang_deconv maj_axis_deconv_err min_axis_deconv_err pos_ang_deconv_err chi_squared_fit rms_fit_gauss spectral_index spectral_curvature spectral_index_err spectral_curvature_err rms_image has_siblings fit_is_estimate spectral_index_from_TT flag_c4 comment # -- -- [deg] [deg] [arcsec] [arcsec] [MHz] [mJy/beam] [mJy/beam] [mJy] [mJy] [arcsec] [arcsec] [deg] [arcsec] [arcsec] [deg] [arcsec] [arcsec] [deg] [arcsec] [arcsec] [deg] -- [mJy/beam] -- -- -- -- [mJy/beam] SB10342_island_1000 SB10342_component_1000a B2337-0423 23:37:09.9 -04:23:13 354.291208 -4.387204 0.03 0.02 -0.0 15.907 0.067 19.273 0.112 17.29 13.32 86.15 0.08 0.01 0.66 7.68 2.86 -35.37 0.08 0.72 0.71 57.119 678.701 -0.79 -99.00 0.00 0.00 0.304 0 0 1 0 SB10342_island_1001 SB10342_component_1001a B0001-0346 00:01:57.6 -03:46:12 0.490036 -3.770075 0.19 0.15 -0.0 12.139 0.425 24.396 0.914 24.14 15.82 61.05 0.45 0.04 2.12 17.96 10.07 50.17 0.06 0.24 4.91 690.545 1639.191 -0.58 -99.00 0.00 0.00 0.339 1 0 1 0 SB10342_island_1001 SB10342_component_1001b B0001-0346 00:01:57.8 -03:46:04 0.491002 -3.767781 1.13 1.35 -0.0 4.044 0.387 10.072 1.053 50.40 9.39 42.19 3.33 0.04 0.73 48.09 0.00 40.49 0.08 0.00 31.48 690.545 1639.191 -0.36 -99.00 0.00 0.00 0.339 1 0 1 0 SB10342_island_1002 SB10342_component_1002a B2333-0141 23:33:29.4 -01:41:04 353.372617 -1.684465 0.13 0.17 -0.0 14.516 0.128 47.232 0.561 35.56 17.39 140.82 0.51 0.02 0.52 33.30 7.00 -35.89 0.02 0.39 3.49 926.099 1383.267 -0.53 -99.00 0.00 0.00 0.459 1 0 1 0 SB10342_island_1002 SB10342_component_1002b B2333-0141 23:33:30.8 -01:41:38 353.378227 -1.693919 0.20 0.26 -0.0 8.051 0.134 25.654 0.550 28.02 21.61 133.92 0.67 0.06 3.37 24.95 14.85 -35.94 0.05 0.13 4.34 926.099 1383.267 -0.21 -99.00 0.00 0.00 0.459 1 0 1 0 Noise Image File \u00b6 This is the noise map that is created from the primary image file during source finding by selavy . Must be in the FITS file format. Example CASDA Filename noiseMap.image.i.SB25600.cont.taylor.0.restored.conv.fits Background Image File \u00b6 This is the mean map that is created from the primary image file during source finding by selavy . The background image files are only required when source monitoring is enabled in the pipeline run. Must be in the FITS file format. Example CASDA Filename meanMap.image.i.SB25600.cont.taylor.0.restored.conv.fits Data Location \u00b6 Data should be placed in the directories denoted by RAW_IMAGE_DIR and HOME_DATA_DIR in the pipeline configuration. The HOME_DATA_DIR is designed to allow users to upload their own data to their home directory on the system where the pipeline is installed. By default, the HOME_DATA_DIR is set to scan the directory called vast-pipeline-extra-data in the users home area. Refer to the Pipeline Configuration section for more information.","title":"Required Data"},{"location":"using/requireddata/#required-data","text":"This page gives an overview of what data is required to run the pipeline along with how to obtain the data.","title":"Required Data"},{"location":"using/requireddata/#acquiring-askap-data","text":"Note: VAST Data Releases If you are a member of the VAST collaboration you will have access to the VAST data release which contains the data for the VAST Pilot Survey. Refer to the VAST wiki for more details. Data produced by the ASKAP telescope can be accessed by using The CSIRO ASKAP Science Data Archive (CASDA) . CASDA provides a web form for users to search for the data they are interested in and request for the data to be staged for download. Note that a form of account or registration is required to download image cube products. All data products from CASDA should be compatible without any modifications. Please report an issue if you find this to not be the case. Tip: CASDA Data Products Descriptions of the data products available on CASDA can be found on this page .","title":"Acquiring ASKAP Data"},{"location":"using/requireddata/#pipeline-required-data","text":"Warning: Correcting Data Currently, the pipeline does not contain any processes to correct the data as it is ingested by the pipeline. For example, if corrections to the flux or positions need to be applied, these should be done to the data directly before they are processed by the pipeline. If an image has been previously ingested before corrections were applied, the filename of the corrected image must be changed. Doing so will make the pipeline see the corrected image as a new image and reingest the corrected data. A pipeline run requires a minimum of two ASKAP observational images to process. For each image, the following table provides a summary of the files that are required. Data File Type Description Primary image .fits The primary, Stokes I, taylor 0, image of the observation. Component Catalogue .xml, .csv, .txt The component source catalogue produced by the source finder selavy from the primary image. Noise map of primary image .fits The noise (or rms) map produced by the source finder selavy from the primary image. Background map of primary image (optional) .fits The background (or mean) map produced by the source finder selavy from the primary image. The background image is only required when source monitoring is enabled. Refer to the respective sections on this page for more details on these inputs.","title":"Pipeline Required Data"},{"location":"using/requireddata/#image-file","text":"This is the primary image file produced by the ASKAP pipeline. The pipeline requires the Stokes I, total intensity (taylor 0) image file. It is also recommended to use the convolved final image, where each of the 36 individual beams have been convolved to a common resolution prior to the creation of the combined mosaic of the field. These files are denoted by a .conv in the file name. Must be in the FITS file format. Example CASDA Filename image.i.SB25597.cont.taylor.0.restored.conv.fits","title":"Image File"},{"location":"using/requireddata/#component-catalogues","text":"Currently, the pipeline does not contain any source finding capabilities so the source catalogue must be provided. In particular, the pipeline requires the continuum component catalogue produced by the selavy source finder. The files can be one of three formats: XML The VOTable XML format of the selavy components catalogue. Provided by CASDA. Must have a file extension of .xml . CSV The csv format of the selavy components catalogue. Provided by CASDA. Must have a file extension of .csv . TXT 'Fixed-width' formatted output produced by the selavy source finder directly. These are not available through CASDA. Must have a file extension of .txt . Example CASDA Filenames xml csv selavy-image.i.SB25597.cont.taylor.0.restored.conv.components.xml AS107_Continuum_Component_Catalogue_25597_3792.csv Note that CASDA does not provide the fixed width text format selavy output. File format examples xml csv txt <?xml version=\"1.0\"?> <VOTABLE version= \"1.3\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xmlns= \"http://www.ivoa.net/xml/VOTable/v1.3\" xmlns:stc= \"http://www.ivoa.net/xml/STC/v1.30\" > <COOSYS ID= \"J2000\" equinox= \"J2000\" system= \"eq_FK5\" /> <RESOURCE name= \"Component catalogue from Selavy source finding\" > <TABLE name= \"Component catalogue\" > <DESCRIPTION></DESCRIPTION> <PARAM name= \"table_version\" ucd= \"meta.version\" datatype= \"char\" arraysize= \"43\" value= \"casda.continuum_component_description_v1.9\" /> <PARAM name= \"imageFile\" ucd= \"meta.file;meta.fits\" datatype= \"char\" arraysize= \"48\" value= \"image.i.SB25600.cont.taylor.0.restored.conv.fits\" /> <PARAM name= \"flagSubsection\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"subsection\" ucd= \"\" datatype= \"char\" arraysize= \"25\" value= \"[1:16449,1:14759,1:1,1:1]\" /> <PARAM name= \"flagStatSec\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"StatSec\" ucd= \"\" datatype= \"char\" arraysize= \"7\" value= \"[*,*,*]\" /> <PARAM name= \"searchType\" ucd= \"meta.note\" datatype= \"char\" arraysize= \"7\" value= \"spatial\" /> <PARAM name= \"flagNegative\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagBaseline\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagRobustStats\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"flagFDR\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"threshold\" ucd= \"phot.flux;stat.min\" datatype= \"float\" value= \"5\" /> <PARAM name= \"flagGrowth\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"growthThreshold\" ucd= \"phot.flux;stat.min\" datatype= \"float\" value= \"3\" /> <PARAM name= \"minPix\" ucd= \"\" datatype= \"int\" value= \"3\" /> <PARAM name= \"minChannels\" ucd= \"\" datatype= \"int\" value= \"0\" /> <PARAM name= \"minVoxels\" ucd= \"\" datatype= \"int\" value= \"3\" /> <PARAM name= \"flagAdjacent\" ucd= \"meta.code\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"threshVelocity\" ucd= \"\" datatype= \"float\" value= \"7\" /> <PARAM name= \"flagRejectBeforeMerge\" ucd= \"\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagTwoStageMerging\" ucd= \"\" datatype= \"boolean\" value= \"1\" /> <PARAM name= \"pixelCentre\" ucd= \"\" datatype= \"char\" arraysize= \"8\" value= \"centroid\" /> <PARAM name= \"flagSmooth\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"flagATrous\" ucd= \"meta.code\" datatype= \"boolean\" value= \"0\" /> <PARAM name= \"Reference frequency\" ucd= \"em.freq;meta.main\" datatype= \"float\" unit= \"Hz\" value= \"1.36749e+09\" /> <PARAM name= \"thresholdActual\" ucd= \"\" datatype= \"float\" unit= \"Jy/beam\" value= \"5\" /> <FIELD name= \"island_id\" ID= \"col_island_id\" ucd= \"meta.id.parent\" datatype= \"char\" unit= \"--\" arraysize= \"20\" /> <FIELD name= \"component_id\" ID= \"col_component_id\" ucd= \"meta.id;meta.main\" datatype= \"char\" unit= \"--\" arraysize= \"24\" /> <FIELD name= \"component_name\" ID= \"col_component_name\" ucd= \"meta.id\" datatype= \"char\" unit= \"\" arraysize= \"26\" /> <FIELD name= \"ra_hms_cont\" ID= \"col_ra_hms_cont\" ucd= \"pos.eq.ra\" ref= \"J2000\" datatype= \"char\" unit= \"\" arraysize= \"12\" /> <FIELD name= \"dec_dms_cont\" ID= \"col_dec_dms_cont\" ucd= \"pos.eq.dec\" ref= \"J2000\" datatype= \"char\" unit= \"\" arraysize= \"13\" /> <FIELD name= \"ra_deg_cont\" ID= \"col_ra_deg_cont\" ucd= \"pos.eq.ra;meta.main\" ref= \"J2000\" datatype= \"double\" unit= \"deg\" width= \"12\" precision= \"6\" /> <FIELD name= \"dec_deg_cont\" ID= \"col_dec_deg_cont\" ucd= \"pos.eq.dec;meta.main\" ref= \"J2000\" datatype= \"double\" unit= \"deg\" width= \"13\" precision= \"6\" /> <FIELD name= \"ra_err\" ID= \"col_ra_err\" ucd= \"stat.error;pos.eq.ra\" ref= \"J2000\" datatype= \"float\" unit= \"arcsec\" width= \"11\" precision= \"2\" /> <FIELD name= \"dec_err\" ID= \"col_dec_err\" ucd= \"stat.error;pos.eq.dec\" ref= \"J2000\" datatype= \"float\" unit= \"arcsec\" width= \"11\" precision= \"2\" /> <FIELD name= \"freq\" ID= \"col_freq\" ucd= \"em.freq\" datatype= \"float\" unit= \"MHz\" width= \"11\" precision= \"1\" /> <FIELD name= \"flux_peak\" ID= \"col_flux_peak\" ucd= \"phot.flux.density;stat.max;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy/beam\" width= \"11\" precision= \"3\" /> <FIELD name= \"flux_peak_err\" ID= \"col_flux_peak_err\" ucd= \"stat.error;phot.flux.density;stat.max;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy/beam\" width= \"14\" precision= \"3\" /> <FIELD name= \"flux_int\" ID= \"col_flux_int\" ucd= \"phot.flux.density;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy\" width= \"10\" precision= \"3\" /> <FIELD name= \"flux_int_err\" ID= \"col_flux_int_err\" ucd= \"stat.error;phot.flux.density;em.radio;stat.fit\" datatype= \"float\" unit= \"mJy\" width= \"13\" precision= \"3\" /> <FIELD name= \"maj_axis\" ID= \"col_maj_axis\" ucd= \"phys.angSize.smajAxis;em.radio;stat.fit\" datatype= \"float\" unit= \"arcsec\" width= \"9\" precision= \"2\" /> <FIELD name= \"min_axis\" ID= \"col_min_axis\" ucd= \"phys.angSize.sminAxis;em.radio;stat.fit\" datatype= \"float\" unit= \"arcsec\" width= \"9\" precision= \"2\" /> <FIELD name= \"pos_ang\" ID= \"col_pos_ang\" ucd= \"phys.angSize;pos.posAng;em.radio;stat.fit\" datatype= \"float\" unit= \"deg\" width= \"8\" precision= \"2\" /> <FIELD name= \"maj_axis_err\" ID= \"col_maj_axis_err\" ucd= \"stat.error;phys.angSize.smajAxis;em.radio\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"min_axis_err\" ID= \"col_min_axis_err\" ucd= \"stat.error;phys.angSize.sminAxis;em.radio\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"pos_ang_err\" ID= \"col_pos_ang_err\" ucd= \"stat.error;phys.angSize;pos.posAng;em.radio\" datatype= \"float\" unit= \"deg\" width= \"12\" precision= \"2\" /> <FIELD name= \"maj_axis_deconv\" ID= \"col_maj_axis_deconv\" ucd= \"phys.angSize.smajAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"18\" precision= \"2\" /> <FIELD name= \"min_axis_deconv\" ID= \"col_min_axis_deconv\" ucd= \"phys.angSize.sminAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"16\" precision= \"2\" /> <FIELD name= \"pos_ang_deconv\" ID= \"col_pos_ang_deconv\" ucd= \"phys.angSize;pos.posAng;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"deg\" width= \"15\" precision= \"2\" /> <FIELD name= \"maj_axis_deconv_err\" ID= \"col_maj_axis_deconv_err\" ucd= \"stat.error;phys.angSize.smajAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"min_axis_deconv_err\" ID= \"col_min_axis_deconv_err\" ucd= \"stat.error;phys.angSize.sminAxis;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"arcsec\" width= \"13\" precision= \"2\" /> <FIELD name= \"pos_ang_deconv_err\" ID= \"col_pos_ang_deconv_err\" ucd= \"stat.error;phys.angSize;pos.posAng;em.radio;askap:meta.deconvolved\" datatype= \"float\" unit= \"deg\" width= \"12\" precision= \"2\" /> <FIELD name= \"chi_squared_fit\" ID= \"col_chi_squared_fit\" ucd= \"stat.fit.chi2\" datatype= \"float\" unit= \"--\" width= \"17\" precision= \"3\" /> <FIELD name= \"rms_fit_gauss\" ID= \"col_rms_fit_gauss\" ucd= \"stat.stdev;stat.fit\" datatype= \"float\" unit= \"mJy/beam\" width= \"15\" precision= \"3\" /> <FIELD name= \"spectral_index\" ID= \"col_spectral_index\" ucd= \"spect.index;em.radio\" datatype= \"float\" unit= \"--\" width= \"15\" precision= \"2\" /> <FIELD name= \"spectral_curvature\" ID= \"col_spectral_curvature\" ucd= \"askap:spect.curvature;em.radio\" datatype= \"float\" unit= \"--\" width= \"19\" precision= \"2\" /> <FIELD name= \"spectral_index_err\" ID= \"col_spectral_index_err\" ucd= \"stat.error;spect.index;em.radio\" datatype= \"float\" unit= \"--\" width= \"15\" precision= \"2\" /> <FIELD name= \"spectral_curvature_err\" ID= \"col_spectral_curvature_err\" ucd= \"stat.error;askap:spect.curvature;em.radio\" datatype= \"float\" unit= \"--\" width= \"19\" precision= \"2\" /> <FIELD name= \"rms_image\" ID= \"col_rms_image\" ucd= \"stat.stdev;phot.flux.density\" datatype= \"float\" unit= \"mJy/beam\" width= \"12\" precision= \"3\" /> <FIELD name= \"has_siblings\" ID= \"col_has_siblings\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"fit_is_estimate\" ID= \"col_fit_is_estimate\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"spectral_index_from_TT\" ID= \"col_spectral_index_from_TT\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"flag_c4\" ID= \"col_flag_c4\" ucd= \"meta.code\" datatype= \"int\" unit= \"\" width= \"8\" /> <FIELD name= \"comment\" ID= \"col_comment\" ucd= \"meta.note\" datatype= \"char\" unit= \"\" arraysize= \"100\" /> <DATA> <TABLEDATA> <TR> <TD> SB25600_island_1 </TD><TD> SB25600_component_1a </TD><TD> J053527-691611 </TD><TD> 05:35:27.9 </TD><TD> -69:16:11 </TD><TD> 83.866441 </TD><TD> -69.269927 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 1367.5 </TD><TD> 780.068 </TD><TD> 0.721 </TD><TD> 843.563 </TD><TD> 1.692 </TD><TD> 11.80 </TD><TD> 8.40 </TD><TD> 176.22 </TD><TD> 0.01 </TD><TD> 0.01 </TD><TD> 0.11 </TD><TD> 3.09 </TD><TD> 1.82 </TD><TD> 56.44 </TD><TD> 0.78 </TD><TD> 2.35 </TD><TD> 1.49 </TD><TD> 596.911 </TD><TD> 1811.003 </TD><TD> -0.93 </TD><TD> -99.00 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 1.038 </TD><TD> 0 </TD><TD> 0 </TD><TD> 1 </TD><TD> 0 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_2 </TD><TD> SB25600_component_2a </TD><TD> J060004-703833 </TD><TD> 06:00:04.9 </TD><TD> -70:38:33 </TD><TD> 90.020608 </TD><TD> -70.642664 </TD><TD> 0.01 </TD><TD> 0.01 </TD><TD> 1367.5 </TD><TD> 438.885 </TD><TD> 1.202 </TD><TD> 494.222 </TD><TD> 2.918 </TD><TD> 11.65 </TD><TD> 8.86 </TD><TD> 9.51 </TD><TD> 0.03 </TD><TD> 0.04 </TD><TD> 0.40 </TD><TD> 5.22 </TD><TD> 0.00 </TD><TD> 58.20 </TD><TD> 0.09 </TD><TD> 0.00 </TD><TD> 0.82 </TD><TD> 5351.118 </TD><TD> 4419.234 </TD><TD> -1.06 </TD><TD> -99.00 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 0.715 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 1 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_2 </TD><TD> SB25600_component_2b </TD><TD> J060006-703854 </TD><TD> 06:00:06.1 </TD><TD> -70:38:54 </TD><TD> 90.025359 </TD><TD> -70.648364 </TD><TD> 0.19 </TD><TD> 0.24 </TD><TD> 1367.5 </TD><TD> 24.719 </TD><TD> 1.326 </TD><TD> 24.048 </TD><TD> 2.986 </TD><TD> 10.66 </TD><TD> 8.37 </TD><TD> 167.45 </TD><TD> 0.59 </TD><TD> 0.80 </TD><TD> 9.41 </TD><TD> 3.00 </TD><TD> 0.00 </TD><TD> -86.64 </TD><TD> 5.43 </TD><TD> 0.00 </TD><TD> 14.18 </TD><TD> 5351.118 </TD><TD> 4419.234 </TD><TD> -99.00 </TD><TD> -99.00 </TD><TD> 0.00 </TD><TD> 0.00 </TD><TD> 0.715 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 1 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_3 </TD><TD> SB25600_component_3a </TD><TD> J061931-681533 </TD><TD> 06:19:31.4 </TD><TD> -68:15:33 </TD><TD> 94.880875 </TD><TD> -68.259404 </TD><TD> 0.49 </TD><TD> 0.65 </TD><TD> 1367.5 </TD><TD> 258.844 </TD><TD> 72.505 </TD><TD> 302.138 </TD><TD> 84.967 </TD><TD> 11.83 </TD><TD> 9.04 </TD><TD> 0.90 </TD><TD> 0.21 </TD><TD> 0.18 </TD><TD> 3.84 </TD><TD> 4.77 </TD><TD> 1.40 </TD><TD> 63.44 </TD><TD> 1.39 </TD><TD> 19.02 </TD><TD> 9.15 </TD><TD> 1284.683 </TD><TD> 2467.498 </TD><TD> -0.74 </TD><TD> -99.00 </TD><TD> 0.01 </TD><TD> 0.00 </TD><TD> 0.277 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 0 </TD><TD> </TD> </TR> <TR> <TD> SB25600_island_3 </TD><TD> SB25600_component_3b </TD><TD> J061931-681531 </TD><TD> 06:19:31.0 </TD><TD> -68:15:31 </TD><TD> 94.879037 </TD><TD> -68.258688 </TD><TD> 0.21 </TD><TD> 0.57 </TD><TD> 1367.5 </TD><TD> 153.160 </TD><TD> 84.478 </TD><TD> 157.853 </TD><TD> 87.200 </TD><TD> 11.40 </TD><TD> 8.28 </TD><TD> 6.90 </TD><TD> 0.20 </TD><TD> 0.21 </TD><TD> 1.40 </TD><TD> 4.07 </TD><TD> 0.00 </TD><TD> 55.21 </TD><TD> 1.06 </TD><TD> 0.00 </TD><TD> 5.16 </TD><TD> 1284.683 </TD><TD> 2467.498 </TD><TD> -1.17 </TD><TD> -99.00 </TD><TD> 0.02 </TD><TD> 0.00 </TD><TD> 0.277 </TD><TD> 1 </TD><TD> 0 </TD><TD> 1 </TD><TD> 0 </TD><TD> </TD> </TR> </TABLEDATA> </DATA> </TABLE> </RESOURCE> </VOTABLE> id,catalogue_id,first_sbid,other_sbids,project_id,island_id,component_id,component_name,ra_hms_cont,dec_dms_cont,ra_deg_cont,dec_deg_cont,ra_err,dec_err,freq,flux_peak,flux_peak_err,flux_int,flux_int_err,maj_axis,min_axis,pos_ang,maj_axis_err,min_axis_err,pos_ang_err,maj_axis_deconv,min_axis_deconv,maj_axis_deconv_err,pos_ang_deconv,min_axis_deconv_err,pos_ang_deconv_err,chi_squared_fit,rms_fit_gauss,spectral_index,spectral_index_err,spectral_curvature,spectral_curvature_err,rms_image,has_siblings,fit_is_estimate,spectral_index_from_tt,flag_c4,comment,quality_level,released_date 20793260,3790,25600,,18,SB25600_island_2768,SB25600_component_2768a,J055644-690942,05:56:44.5,-69:09:42,89.185558,-69.161889,0.04,0.05,1367.5,0.834,0.009,0.68,0.037,9.63,7.76,165.12,0.23,0.33,3.9,0.0,0.0,0.0,-89.09,0.0,3.48,0.034,52.957,-99.0,0.0,-99.0,0.0,0.166,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z 20793259,3790,25600,,18,SB25600_island_2767,SB25600_component_2767a,J060047-692500,06:00:47.7,-69:25:00,90.198913,-69.416437,0.09,0.37,1367.5,0.889,0.016,2.642,0.2,29.12,9.35,2.08,2.0,0.99,1.35,26.73,4.84,0.03,2.91,6.03,1.5,1.869,234.431,-99.0,0.0,-99.0,0.0,0.175,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z 20793258,3790,25600,,18,SB25600_island_2766,SB25600_component_2766a,J055849-690051,05:58:49.4,-69:00:51,89.705808,-69.014253,0.14,0.24,1367.5,0.885,0.025,1.181,0.191,13.04,9.38,6.04,1.3,1.45,8.83,6.87,3.72,3.2,36.6,14.03,34.31,0.561,193.467,-99.0,0.0,-99.0,0.0,0.177,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z 20793257,3790,25600,,18,SB25600_island_2765,SB25600_component_2765a,J055339-683120,05:53:39.5,-68:31:20,88.414758,-68.522436,0.08,0.1,1367.5,0.906,0.014,0.895,0.063,12.78,7.08,10.0,0.43,0.43,2.33,6.37,0.0,0.54,28.11,0.0,4.65,0.138,99.302,-99.0,0.0,-99.0,0.0,0.173,0,0,,0,,NOT_VALIDATED,2021-04-22T03:48:37.856Z # island_id component_id component_name ra_hms_cont dec_dms_cont ra_deg_cont dec_deg_cont ra_err dec_err freq flux_peak flux_peak_err flux_int flux_int_err maj_axis min_axis pos_ang maj_axis_err min_axis_err pos_ang_err maj_axis_deconv min_axis_deconv pos_ang_deconv maj_axis_deconv_err min_axis_deconv_err pos_ang_deconv_err chi_squared_fit rms_fit_gauss spectral_index spectral_curvature spectral_index_err spectral_curvature_err rms_image has_siblings fit_is_estimate spectral_index_from_TT flag_c4 comment # -- -- [deg] [deg] [arcsec] [arcsec] [MHz] [mJy/beam] [mJy/beam] [mJy] [mJy] [arcsec] [arcsec] [deg] [arcsec] [arcsec] [deg] [arcsec] [arcsec] [deg] [arcsec] [arcsec] [deg] -- [mJy/beam] -- -- -- -- [mJy/beam] SB10342_island_1000 SB10342_component_1000a B2337-0423 23:37:09.9 -04:23:13 354.291208 -4.387204 0.03 0.02 -0.0 15.907 0.067 19.273 0.112 17.29 13.32 86.15 0.08 0.01 0.66 7.68 2.86 -35.37 0.08 0.72 0.71 57.119 678.701 -0.79 -99.00 0.00 0.00 0.304 0 0 1 0 SB10342_island_1001 SB10342_component_1001a B0001-0346 00:01:57.6 -03:46:12 0.490036 -3.770075 0.19 0.15 -0.0 12.139 0.425 24.396 0.914 24.14 15.82 61.05 0.45 0.04 2.12 17.96 10.07 50.17 0.06 0.24 4.91 690.545 1639.191 -0.58 -99.00 0.00 0.00 0.339 1 0 1 0 SB10342_island_1001 SB10342_component_1001b B0001-0346 00:01:57.8 -03:46:04 0.491002 -3.767781 1.13 1.35 -0.0 4.044 0.387 10.072 1.053 50.40 9.39 42.19 3.33 0.04 0.73 48.09 0.00 40.49 0.08 0.00 31.48 690.545 1639.191 -0.36 -99.00 0.00 0.00 0.339 1 0 1 0 SB10342_island_1002 SB10342_component_1002a B2333-0141 23:33:29.4 -01:41:04 353.372617 -1.684465 0.13 0.17 -0.0 14.516 0.128 47.232 0.561 35.56 17.39 140.82 0.51 0.02 0.52 33.30 7.00 -35.89 0.02 0.39 3.49 926.099 1383.267 -0.53 -99.00 0.00 0.00 0.459 1 0 1 0 SB10342_island_1002 SB10342_component_1002b B2333-0141 23:33:30.8 -01:41:38 353.378227 -1.693919 0.20 0.26 -0.0 8.051 0.134 25.654 0.550 28.02 21.61 133.92 0.67 0.06 3.37 24.95 14.85 -35.94 0.05 0.13 4.34 926.099 1383.267 -0.21 -99.00 0.00 0.00 0.459 1 0 1 0","title":"Component Catalogues"},{"location":"using/requireddata/#noise-image-file","text":"This is the noise map that is created from the primary image file during source finding by selavy . Must be in the FITS file format. Example CASDA Filename noiseMap.image.i.SB25600.cont.taylor.0.restored.conv.fits","title":"Noise Image File"},{"location":"using/requireddata/#background-image-file","text":"This is the mean map that is created from the primary image file during source finding by selavy . The background image files are only required when source monitoring is enabled in the pipeline run. Must be in the FITS file format. Example CASDA Filename meanMap.image.i.SB25600.cont.taylor.0.restored.conv.fits","title":"Background Image File"},{"location":"using/requireddata/#data-location","text":"Data should be placed in the directories denoted by RAW_IMAGE_DIR and HOME_DATA_DIR in the pipeline configuration. The HOME_DATA_DIR is designed to allow users to upload their own data to their home directory on the system where the pipeline is installed. By default, the HOME_DATA_DIR is set to scan the directory called vast-pipeline-extra-data in the users home area. Refer to the Pipeline Configuration section for more information.","title":"Data Location"},{"location":"using/restorerun/","text":"Restoring a Run \u00b6 This page details the process of restoring a pipeline run to the previous successful version. When images are added to a run, a backup is made of the run before proceeding which can be used to restore the run to the pre-addition version. For example, perhaps the wrong images were added or an error occurred mid-addition that could not be resolved. A pipeline run can only be restored by the creator or an administrator. Admin Tip This process can also be launched via the command line using the restorepiperun command. It is described in the admin section here . Warning: One time use This process can only be used to restore the run once. I.e. it is not possible to restore the run to an even earlier version. Step-by-step Guide \u00b6 In this example, the docs_example_run will be restored to the state before the images were added in the Adding Images to a Run example. 1. Navigate to the Run Detail Page \u00b6 Navigate to the detail page of the run you wish to restore. 2. Select the Restore Run Option \u00b6 Click the Restore Run option at the top-right of the page. This will open the restore confirmation modal. 3. Check the Restore Configuration \u00b6 Shown in the modal is the configuration file of the previous successful run. This can be used to check that the images listed here are those that are expected. Debug level logging can also be turned on using the toggle button. When ready, click the Restore Run button on the modal to submit the restore request. A notification will show to indicate whether the submission was successful. 4. Refresh and Check the Restore Log File \u00b6 While restoring the pipeline run will show a status of Restoring . It is possible to check the progress by looking at the Restore Log File which can be found on the run detail page. The log will not be refreshed automatically and instead the page needs to be manually refreshed. Upon a successful restoration the status will be changed back to Completed .","title":"Restoring a Run"},{"location":"using/restorerun/#restoring-a-run","text":"This page details the process of restoring a pipeline run to the previous successful version. When images are added to a run, a backup is made of the run before proceeding which can be used to restore the run to the pre-addition version. For example, perhaps the wrong images were added or an error occurred mid-addition that could not be resolved. A pipeline run can only be restored by the creator or an administrator. Admin Tip This process can also be launched via the command line using the restorepiperun command. It is described in the admin section here . Warning: One time use This process can only be used to restore the run once. I.e. it is not possible to restore the run to an even earlier version.","title":"Restoring a Run"},{"location":"using/restorerun/#step-by-step-guide","text":"In this example, the docs_example_run will be restored to the state before the images were added in the Adding Images to a Run example.","title":"Step-by-step Guide"},{"location":"using/restorerun/#1-navigate-to-the-run-detail-page","text":"Navigate to the detail page of the run you wish to restore.","title":"1. Navigate to the Run Detail Page"},{"location":"using/restorerun/#2-select-the-restore-run-option","text":"Click the Restore Run option at the top-right of the page. This will open the restore confirmation modal.","title":"2. Select the Restore Run Option"},{"location":"using/restorerun/#3-check-the-restore-configuration","text":"Shown in the modal is the configuration file of the previous successful run. This can be used to check that the images listed here are those that are expected. Debug level logging can also be turned on using the toggle button. When ready, click the Restore Run button on the modal to submit the restore request. A notification will show to indicate whether the submission was successful.","title":"3. Check the Restore Configuration"},{"location":"using/restorerun/#4-refresh-and-check-the-restore-log-file","text":"While restoring the pipeline run will show a status of Restoring . It is possible to check the progress by looking at the Restore Log File which can be found on the run detail page. The log will not be refreshed automatically and instead the page needs to be manually refreshed. Upon a successful restoration the status will be changed back to Completed .","title":"4. Refresh and Check the Restore Log File"},{"location":"using/runconfig/","text":"Run Configuration \u00b6 This page gives an overview of the configuration options available for a pipeline run. Default Configuration File \u00b6 Below is an example of a default config.yaml file. Note that no images or other input files have been provided. The file can be either edited directly or through the editor available on the run detail page. Warning Similarly to Python files, the indentation in the run configuration YAML file is important as it defines nested parameters. config.yaml # This file specifies the pipeline configuration for the current pipeline run. # You should review these settings before processing any images - some of the default # values will probably not be appropriate. run : # Path of the pipeline run path : ... # auto-filled by pipeline initpiperun command # Hide astropy warnings during the run execution. suppress_astropy_warnings : True inputs : # NOTE: all the inputs must match with each other, i.e. the catalogue for the first # input image (inputs.image[0]) must be the first input catalogue (inputs.selavy[0]) # and so on. image : # list input images here, e.g. (note the leading hyphens) # - /path/to/image1.fits # - /path/to/image2.fits selavy : # list input selavy catalogues here, as above with the images noise : # list input noise (rms) images here, as above with the images # Required only if source_monitoring.monitor is true, otherwise optional. If not providing # background images, remove the entire background section below. background : # list input background images here, as above with the images source_monitoring : # Source monitoring can be done both forward and backward in 'time'. # Monitoring backward means re-opening files that were previously processed and can be slow. monitor : True # Minimum SNR ratio a source has to be if it was placed in the area of minimum rms in # the image from which it is to be extracted from. If lower than this value it is skipped min_sigma : 3.0 # Multiplicative scaling factor to the buffer size of the forced photometry from the # image edge edge_buffer_scale : 1.2 # Passed to forced-phot as `cluster_threshold`. See docs for details. If unsure, leave # as default. cluster_threshold : 3.0 # Attempt forced-phot fit even if there are NaN's present in the rms or background maps. allow_nan : False source_association : # basic, advanced, or deruiter method : basic # Maximum source separation allowed during basic and advanced association in arcsec radius : 10.0 # Options that apply only to deruiter association deruiter_radius : 5.68 # unitless deruiter_beamwidth_limit : 1.5 # multiplicative factor # Split input images into sky region groups and run the association on these groups in # parallel. Best used when there are a large number of input images with multiple # non-overlapping patches of the sky. # Not recommended for smaller searches of <= 3 sky regions. parallel : False # If images have been submitted in epoch dictionaries then an attempt will be made by # the pipeline to remove duplicate sources. To do this a crossmatch is made between # catalgoues to match 'the same' measurements from different catalogues. This # parameter governs the distance for which a match is made in arcsec. Default is 2.5 # arcsec which is typically 1 pixel in ASKAP images. epoch_duplicate_radius : 2.5 # arcsec new_sources : # Controls when a source is labelled as a new source. The source in question must meet # the requirement of: min sigma > (source_peak_flux / lowest_previous_image_min_rms) min_sigma : 5.0 measurements : # Source finder used to produce input catalogues. Only selavy is currently supported. source_finder : selavy # Minimum error to apply to all flux measurements. The actual value used will either # be the catalogued value or this value, whichever is greater. This is a fraction, e.g. # 0.05 = 5% error, 0 = no minimum error. flux_fractional_error : 0.0 # Replace the selavy errors with Condon (1997) errors. condon_errors : True # Sometimes the local rms for a source is reported as 0 by selavy. # Choose a value to use for the local rms in these cases in mJy/beam. selavy_local_rms_fill_value : 0.2 # Create 'measurements.arrow' and 'measurement_pairs.arrow' files at the end of # a successful run. write_arrow_files : False # The positional uncertainty of a measurement is in reality the fitting errors and the # astrometric uncertainty of the image/survey/instrument combined in quadrature. # These two parameters are the astrometric uncertainty in RA/Dec and they may be different. ra_uncertainty : 1.0 # arcsec dec_uncertainty : 1.0 # arcsec variability : # Only measurement pairs where the Vs metric exceeds this value are selected for the # aggregate pair metrics that are stored in Source objects. source_aggregate_pair_metrics_min_abs_vs : 4.3 Note Throughout the documentation we use dot-notation to refer to nested parameters, for example inputs.image refers to the list of input images. This page on YAML syntax from the Ansible documentation is a good brief primer on the basics. Configuration Options \u00b6 General Run Options \u00b6 run.path Path to the directory for the pipeline run. This parameter will be automatically filled if the configuration file is generate with the initpiperun management command or if the run was created with the web interface. run.suppress_astropy_warnings Boolean. Astropy warnings are suppressed in the logging output if set to True . Defaults to True . Input Images and Selavy Files \u00b6 Warning: Entry Order The order of the the inputs must be consistent between the different input types. I.e. if image1.fits is the first listed image then image1_selavy.txt must be the first selavy input listed. inputs.image Line entries or epoch headed entries. The full paths to the image FITS files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. config.yaml Normal mode Epoch mode inputs : image : - /full/path/to/image1.fits - /full/path/to/image2.fits - /full/path/to/image3.fits inputs : image : epoch01 : - /full/path/to/image1.fits - /full/path/to/image2.fits epoch02 : - /full/path/to/image3.fits Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10: inputs.selavy Line entries or epoch headed entries. The full paths to the selavy text files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. config.yaml Normal mode Epoch mode inputs : selavy : - /full/path/to/image1_selavy.txt - /full/path/to/image2_selavy.txt - /full/path/to/image3_selavy.txt inputs : selavy : epoch01 : - /full/path/to/image1_selavy.txt - /full/path/to/image2_selavy.txt epoch02 : - /full/path/to/image3_selavy.txt Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10: inputs.noise Line entries or epoch headed entries. The full paths to the image noise (RMS) FITS files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. config.yaml Normal mode Epoch mode inputs : noise : - /full/path/to/image1_rms.fits - /full/path/to/image2_rms.fits - /full/path/to/image3_rms.fits inputs : noise : epoch01 : - /full/path/to/image1_rms.fits - /full/path/to/image2_rms.fits epoch02 : - /full/path/to/image3_rms.fits Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10: inputs.background Line entries or epoch headed entries. The full paths to the image background (mean) FITS files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. The background images are only required to be defined if source_monitoring.monitor is set to True . config.yaml Normal mode Epoch mode inputs : background : - /full/path/to/image1_bkg.fits - /full/path/to/image2_bkg.fits - /full/path/to/image3_bkg.fits inputs : background : epoch01 : - /full/path/to/image1_bkg.fits - /full/path/to/image2_bkg.fits epoch02 : - /full/path/to/image3_bkg.fits Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10: Using glob expressions \u00b6 Instead of providing each input file explicitly, the inputs can be given as glob expressions which are resolved and sorted. Glob expressions must be provided as a mapping with the key glob . Both normal and epoch mode are supported. For example, the image input examples given above can be equivalently specified with the following glob expressions. config.yaml Normal mode Epoch mode inputs : image : glob : /full/path/to/image*.fits inputs : image : epoch01 : glob : /full/path/to/image[12].fits epoch02 : - /full/path/to/image3.fits Multiple glob expressions can also be provided as a list, in which case they are resolved and sorted in the order they are given. For example: config.yaml inputs : image : glob : - /full/path/to/A/image*.fits - /full/path/to/B/image*.fits Note that it is not valid YAML to mix a sequence/list and a mapping/dictionary, meaning that for each input type (or epoch if using epoch mode), the files may be given either as glob expressions or explicit file paths. For example, the following is invalid : Invalid config.yaml inputs : image : # Invalid! Thou shalt not mix sequences and mappings in YAML - /full/path/to/A/image1.fits glob : /full/path/to/B/image*.fits However, an explicit file path is a valid glob expression, so adding explicit paths alongside glob expressions is still possible by simply including the path in a list of glob expressions. For example, the following is valid: config.yaml inputs : image : glob : - /full/path/to/A/image1.fits - /full/path/to/B/image*.fits In the above example, the final resolved image input list would contain the image /full/path/to/A/image1.fits , followed by all files matching image*.fits in /full/path/to/B . Source Monitoring \u00b6 source_monitoring.monitor Boolean. Turns on or off forced extractions for non detections. If set to True then inputs.background must also be defined. Defaults to False . source_monitoring.min_sigma Float. For forced extractions to be performed they must meet a minimum signal-to-noise threshold with respect to the minimum rms value of the respective image. If the proposed forced measurement does not meet the threshold then it is not performed. I.e. \\[ \\frac{f_{peak}}{\\text{rms}_{image, min}} \\geq \\small{\\text{source_monitoring.min_sigma}}\\text{,} \\] where \\(f_{peak}\\) is the initial detection peak flux measurement of the source in question and \\(\\text{rms}_{image, min}\\) is the minimum RMS value of the image where the forced extraction is to take place. Defaults to 3.0 . source_monitoring.edge_buffer_scale Float. Monitor forced extractions are not performed when the location is within 3 beamwidths of the image edge. This parameter scales this distance by the value set, which can help avoid errors when the 3 beamwidth limit is insufficient to avoid extraction failures. Defaults to 1.2. source_monitoring.cluster_threshold Float. A argument directly passed to the forced photometry package used by the pipeline. It defines the multiple of major_axes to use for identifying clusters. Defaults to 3.0. source_monitoring.allow_nan Boolean. A argument directly passed to the forced photometry package used by the pipeline. It defines whether NaN values are allowed to be present in the extraction area in the rms or background maps. True would mean that NaN values are allowed. Defaults to False. Association \u00b6 Tip Refer to the association documentation for full details on the association methods. source_association.method String. Select whether to use the basic , advanced or deruiter association method, entered as a string of the method name. Defaults to \"basic\" . source_association.radius Float. The distance limit to use during basic and advanced association. Unit is arcseconds. Defaults to 10.0 . source_association.deruiter_radius Float. The de Ruiter radius limit to use during deruiter association only. The parameter is unitless. Defaults to 5.68 . source_association.deruiter_beamwidth_limit Float. The beamwidth limit to use during deruiter association only. Multiplicative factor. Defaults to 1.5 . source_association.parallel Boolean. When True , association is performed in parallel on non-overlapping groups of sky regions. Defaults to False . source_association.epoch_duplicate_radius Float. Applies to epoch based association only. Defines the limit at which a duplicate source is identified. Unit is arcseconds. Defaults to 2.5 (commonly one pixel for ASKAP images). New Sources \u00b6 new_sources.min_sigma Float. Defines the limit at which a source is classed as a new source based upon the would-be significance of detections in previous images where no detection was made. i.e. \\[ \\frac{f_{peak}}{\\text{rms}_{image, min}} \\geq \\small{\\text{new_sources.min_sigma}}\\text{,} \\] where \\(f_{peak}\\) is the initial detection peak flux measurement of the source in question and \\(\\text{rms}_{image, min}\\) is the minimum RMS value of the previous image(s) where no detection was made. If the requirement is met in any previous image then the source is flagged as new. Defaults to 5.0 . Measurements \u00b6 measurements.source_finder String. Signifies the format of the source finder text file read by the pipeline. Currently only supports \"selavy\" . Warning Source finding is not performed by the pipeline and must be completed prior to processing. measurements.flux_fractional_error Define a fractional flux error that will be added in quadrature to the extracted sources. Note that this will be reflected in the final source statistics and will not be applied directly to the measurements. Entered as a float between 0 - 1.0 which represents 0 - 100%. Defaults to 0.0 . measurements.condon_errors Boolean. Calculate the Condon errors of the extractions when read in from the source extraction file. If False then the errors directly from the source finder output are used. Recommended to set to True for selavy extractions. Defaults to True . measurements.selavy_local_rms_fill_value Float. Value to substitute for the local_rms parameter in selavy extractions if a 0.0 value is found. Unit is mJy. Defaults to 0.2 . measurements.write_arrow_files Boolean. When True then two arrow format files are produced: measurements.arrow - an arrow file containing all the measurements associated with the run. measurement_pairs.arrow - an arrow file containing the measurement pairs information pre-merged with extra information from the measurements. Producing these files for large runs (200+ images) is recommended for post-processing. Defaults to False . Note The arrow files can optionally be produced after the run has completed. See the Generating Arrow Files page . measurements.ra_uncertainty Float. Defines an uncertainty error to the RA that will be added in quadrature to the existing source extraction error. Used to represent a systematic positional error. Unit is arcseconds. Defaults to 1.0. measurements.dec_uncertainty Float. Defines an uncertainty error to the Dec that will be added in quadrature to the existing source extraction error. Used to represent systematic positional error. Unit is arcseconds. Defaults to 1.0. Variability \u00b6 variability.source_aggregate_pair_metrics_min_abs_vs Float. Defines the minimum \\(V_s\\) two-epoch metric value threshold used to attach the most significant pair value to the source. Defaults to 4.3 .","title":"Run Configuration"},{"location":"using/runconfig/#run-configuration","text":"This page gives an overview of the configuration options available for a pipeline run.","title":"Run Configuration"},{"location":"using/runconfig/#default-configuration-file","text":"Below is an example of a default config.yaml file. Note that no images or other input files have been provided. The file can be either edited directly or through the editor available on the run detail page. Warning Similarly to Python files, the indentation in the run configuration YAML file is important as it defines nested parameters. config.yaml # This file specifies the pipeline configuration for the current pipeline run. # You should review these settings before processing any images - some of the default # values will probably not be appropriate. run : # Path of the pipeline run path : ... # auto-filled by pipeline initpiperun command # Hide astropy warnings during the run execution. suppress_astropy_warnings : True inputs : # NOTE: all the inputs must match with each other, i.e. the catalogue for the first # input image (inputs.image[0]) must be the first input catalogue (inputs.selavy[0]) # and so on. image : # list input images here, e.g. (note the leading hyphens) # - /path/to/image1.fits # - /path/to/image2.fits selavy : # list input selavy catalogues here, as above with the images noise : # list input noise (rms) images here, as above with the images # Required only if source_monitoring.monitor is true, otherwise optional. If not providing # background images, remove the entire background section below. background : # list input background images here, as above with the images source_monitoring : # Source monitoring can be done both forward and backward in 'time'. # Monitoring backward means re-opening files that were previously processed and can be slow. monitor : True # Minimum SNR ratio a source has to be if it was placed in the area of minimum rms in # the image from which it is to be extracted from. If lower than this value it is skipped min_sigma : 3.0 # Multiplicative scaling factor to the buffer size of the forced photometry from the # image edge edge_buffer_scale : 1.2 # Passed to forced-phot as `cluster_threshold`. See docs for details. If unsure, leave # as default. cluster_threshold : 3.0 # Attempt forced-phot fit even if there are NaN's present in the rms or background maps. allow_nan : False source_association : # basic, advanced, or deruiter method : basic # Maximum source separation allowed during basic and advanced association in arcsec radius : 10.0 # Options that apply only to deruiter association deruiter_radius : 5.68 # unitless deruiter_beamwidth_limit : 1.5 # multiplicative factor # Split input images into sky region groups and run the association on these groups in # parallel. Best used when there are a large number of input images with multiple # non-overlapping patches of the sky. # Not recommended for smaller searches of <= 3 sky regions. parallel : False # If images have been submitted in epoch dictionaries then an attempt will be made by # the pipeline to remove duplicate sources. To do this a crossmatch is made between # catalgoues to match 'the same' measurements from different catalogues. This # parameter governs the distance for which a match is made in arcsec. Default is 2.5 # arcsec which is typically 1 pixel in ASKAP images. epoch_duplicate_radius : 2.5 # arcsec new_sources : # Controls when a source is labelled as a new source. The source in question must meet # the requirement of: min sigma > (source_peak_flux / lowest_previous_image_min_rms) min_sigma : 5.0 measurements : # Source finder used to produce input catalogues. Only selavy is currently supported. source_finder : selavy # Minimum error to apply to all flux measurements. The actual value used will either # be the catalogued value or this value, whichever is greater. This is a fraction, e.g. # 0.05 = 5% error, 0 = no minimum error. flux_fractional_error : 0.0 # Replace the selavy errors with Condon (1997) errors. condon_errors : True # Sometimes the local rms for a source is reported as 0 by selavy. # Choose a value to use for the local rms in these cases in mJy/beam. selavy_local_rms_fill_value : 0.2 # Create 'measurements.arrow' and 'measurement_pairs.arrow' files at the end of # a successful run. write_arrow_files : False # The positional uncertainty of a measurement is in reality the fitting errors and the # astrometric uncertainty of the image/survey/instrument combined in quadrature. # These two parameters are the astrometric uncertainty in RA/Dec and they may be different. ra_uncertainty : 1.0 # arcsec dec_uncertainty : 1.0 # arcsec variability : # Only measurement pairs where the Vs metric exceeds this value are selected for the # aggregate pair metrics that are stored in Source objects. source_aggregate_pair_metrics_min_abs_vs : 4.3 Note Throughout the documentation we use dot-notation to refer to nested parameters, for example inputs.image refers to the list of input images. This page on YAML syntax from the Ansible documentation is a good brief primer on the basics.","title":"Default Configuration File"},{"location":"using/runconfig/#configuration-options","text":"","title":"Configuration Options"},{"location":"using/runconfig/#general-run-options","text":"run.path Path to the directory for the pipeline run. This parameter will be automatically filled if the configuration file is generate with the initpiperun management command or if the run was created with the web interface. run.suppress_astropy_warnings Boolean. Astropy warnings are suppressed in the logging output if set to True . Defaults to True .","title":"General Run Options"},{"location":"using/runconfig/#input-images-and-selavy-files","text":"Warning: Entry Order The order of the the inputs must be consistent between the different input types. I.e. if image1.fits is the first listed image then image1_selavy.txt must be the first selavy input listed. inputs.image Line entries or epoch headed entries. The full paths to the image FITS files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. config.yaml Normal mode Epoch mode inputs : image : - /full/path/to/image1.fits - /full/path/to/image2.fits - /full/path/to/image3.fits inputs : image : epoch01 : - /full/path/to/image1.fits - /full/path/to/image2.fits epoch02 : - /full/path/to/image3.fits Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10: inputs.selavy Line entries or epoch headed entries. The full paths to the selavy text files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. config.yaml Normal mode Epoch mode inputs : selavy : - /full/path/to/image1_selavy.txt - /full/path/to/image2_selavy.txt - /full/path/to/image3_selavy.txt inputs : selavy : epoch01 : - /full/path/to/image1_selavy.txt - /full/path/to/image2_selavy.txt epoch02 : - /full/path/to/image3_selavy.txt Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10: inputs.noise Line entries or epoch headed entries. The full paths to the image noise (RMS) FITS files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. config.yaml Normal mode Epoch mode inputs : noise : - /full/path/to/image1_rms.fits - /full/path/to/image2_rms.fits - /full/path/to/image3_rms.fits inputs : noise : epoch01 : - /full/path/to/image1_rms.fits - /full/path/to/image2_rms.fits epoch02 : - /full/path/to/image3_rms.fits Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10: inputs.background Line entries or epoch headed entries. The full paths to the image background (mean) FITS files to be processed. Epoch mode is activated by including an extra key value with the epoch name, see the example below for a demonstration. Refer to this section of the documentation for more information on epoch based association. The background images are only required to be defined if source_monitoring.monitor is set to True . config.yaml Normal mode Epoch mode inputs : background : - /full/path/to/image1_bkg.fits - /full/path/to/image2_bkg.fits - /full/path/to/image3_bkg.fits inputs : background : epoch01 : - /full/path/to/image1_bkg.fits - /full/path/to/image2_bkg.fits epoch02 : - /full/path/to/image3_bkg.fits Make sure that the epoch names are orderable in the correct order, for example, use: epoch01: ... epoch09: epoch10: and not: epoch1: ... epoch9: epoch10:","title":"Input Images and Selavy Files"},{"location":"using/runconfig/#using-glob-expressions","text":"Instead of providing each input file explicitly, the inputs can be given as glob expressions which are resolved and sorted. Glob expressions must be provided as a mapping with the key glob . Both normal and epoch mode are supported. For example, the image input examples given above can be equivalently specified with the following glob expressions. config.yaml Normal mode Epoch mode inputs : image : glob : /full/path/to/image*.fits inputs : image : epoch01 : glob : /full/path/to/image[12].fits epoch02 : - /full/path/to/image3.fits Multiple glob expressions can also be provided as a list, in which case they are resolved and sorted in the order they are given. For example: config.yaml inputs : image : glob : - /full/path/to/A/image*.fits - /full/path/to/B/image*.fits Note that it is not valid YAML to mix a sequence/list and a mapping/dictionary, meaning that for each input type (or epoch if using epoch mode), the files may be given either as glob expressions or explicit file paths. For example, the following is invalid : Invalid config.yaml inputs : image : # Invalid! Thou shalt not mix sequences and mappings in YAML - /full/path/to/A/image1.fits glob : /full/path/to/B/image*.fits However, an explicit file path is a valid glob expression, so adding explicit paths alongside glob expressions is still possible by simply including the path in a list of glob expressions. For example, the following is valid: config.yaml inputs : image : glob : - /full/path/to/A/image1.fits - /full/path/to/B/image*.fits In the above example, the final resolved image input list would contain the image /full/path/to/A/image1.fits , followed by all files matching image*.fits in /full/path/to/B .","title":"Using glob expressions"},{"location":"using/runconfig/#source-monitoring","text":"source_monitoring.monitor Boolean. Turns on or off forced extractions for non detections. If set to True then inputs.background must also be defined. Defaults to False . source_monitoring.min_sigma Float. For forced extractions to be performed they must meet a minimum signal-to-noise threshold with respect to the minimum rms value of the respective image. If the proposed forced measurement does not meet the threshold then it is not performed. I.e. \\[ \\frac{f_{peak}}{\\text{rms}_{image, min}} \\geq \\small{\\text{source_monitoring.min_sigma}}\\text{,} \\] where \\(f_{peak}\\) is the initial detection peak flux measurement of the source in question and \\(\\text{rms}_{image, min}\\) is the minimum RMS value of the image where the forced extraction is to take place. Defaults to 3.0 . source_monitoring.edge_buffer_scale Float. Monitor forced extractions are not performed when the location is within 3 beamwidths of the image edge. This parameter scales this distance by the value set, which can help avoid errors when the 3 beamwidth limit is insufficient to avoid extraction failures. Defaults to 1.2. source_monitoring.cluster_threshold Float. A argument directly passed to the forced photometry package used by the pipeline. It defines the multiple of major_axes to use for identifying clusters. Defaults to 3.0. source_monitoring.allow_nan Boolean. A argument directly passed to the forced photometry package used by the pipeline. It defines whether NaN values are allowed to be present in the extraction area in the rms or background maps. True would mean that NaN values are allowed. Defaults to False.","title":"Source Monitoring"},{"location":"using/runconfig/#association","text":"Tip Refer to the association documentation for full details on the association methods. source_association.method String. Select whether to use the basic , advanced or deruiter association method, entered as a string of the method name. Defaults to \"basic\" . source_association.radius Float. The distance limit to use during basic and advanced association. Unit is arcseconds. Defaults to 10.0 . source_association.deruiter_radius Float. The de Ruiter radius limit to use during deruiter association only. The parameter is unitless. Defaults to 5.68 . source_association.deruiter_beamwidth_limit Float. The beamwidth limit to use during deruiter association only. Multiplicative factor. Defaults to 1.5 . source_association.parallel Boolean. When True , association is performed in parallel on non-overlapping groups of sky regions. Defaults to False . source_association.epoch_duplicate_radius Float. Applies to epoch based association only. Defines the limit at which a duplicate source is identified. Unit is arcseconds. Defaults to 2.5 (commonly one pixel for ASKAP images).","title":"Association"},{"location":"using/runconfig/#new-sources","text":"new_sources.min_sigma Float. Defines the limit at which a source is classed as a new source based upon the would-be significance of detections in previous images where no detection was made. i.e. \\[ \\frac{f_{peak}}{\\text{rms}_{image, min}} \\geq \\small{\\text{new_sources.min_sigma}}\\text{,} \\] where \\(f_{peak}\\) is the initial detection peak flux measurement of the source in question and \\(\\text{rms}_{image, min}\\) is the minimum RMS value of the previous image(s) where no detection was made. If the requirement is met in any previous image then the source is flagged as new. Defaults to 5.0 .","title":"New Sources"},{"location":"using/runconfig/#measurements","text":"measurements.source_finder String. Signifies the format of the source finder text file read by the pipeline. Currently only supports \"selavy\" . Warning Source finding is not performed by the pipeline and must be completed prior to processing. measurements.flux_fractional_error Define a fractional flux error that will be added in quadrature to the extracted sources. Note that this will be reflected in the final source statistics and will not be applied directly to the measurements. Entered as a float between 0 - 1.0 which represents 0 - 100%. Defaults to 0.0 . measurements.condon_errors Boolean. Calculate the Condon errors of the extractions when read in from the source extraction file. If False then the errors directly from the source finder output are used. Recommended to set to True for selavy extractions. Defaults to True . measurements.selavy_local_rms_fill_value Float. Value to substitute for the local_rms parameter in selavy extractions if a 0.0 value is found. Unit is mJy. Defaults to 0.2 . measurements.write_arrow_files Boolean. When True then two arrow format files are produced: measurements.arrow - an arrow file containing all the measurements associated with the run. measurement_pairs.arrow - an arrow file containing the measurement pairs information pre-merged with extra information from the measurements. Producing these files for large runs (200+ images) is recommended for post-processing. Defaults to False . Note The arrow files can optionally be produced after the run has completed. See the Generating Arrow Files page . measurements.ra_uncertainty Float. Defines an uncertainty error to the RA that will be added in quadrature to the existing source extraction error. Used to represent a systematic positional error. Unit is arcseconds. Defaults to 1.0. measurements.dec_uncertainty Float. Defines an uncertainty error to the Dec that will be added in quadrature to the existing source extraction error. Used to represent systematic positional error. Unit is arcseconds. Defaults to 1.0.","title":"Measurements"},{"location":"using/runconfig/#variability","text":"variability.source_aggregate_pair_metrics_min_abs_vs Float. Defines the minimum \\(V_s\\) two-epoch metric value threshold used to attach the most significant pair value to the source. Defaults to 4.3 .","title":"Variability"}]}